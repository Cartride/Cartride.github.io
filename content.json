{"pages":[{"title":"","text":"6649bb90de78fc0b38a5269b4e0c8cc2","link":"/baidu_verify_code-X0OEA0Cq6O.html"},{"title":"","text":"关于 我的足迹 我的足迹 个人简介 Cal EECS Phd Candidate","link":"/about/index.html"}],"posts":[{"title":"标识符","text":"关键字（25个） 1234if for func case struct import go type chan defer default packagemap const else break select interfacevar goto range return switch continue fallthrough 预定义名字（30+个） 123456789101112内建常量： true false iota nil内建类型： int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64bool： byte rune string error内建函数： make delete complex panic append copy close len cap real imag new recover 变量 变量声明 Go变量声明的三种方式： 123var a int // 声明一个变量，默认为0var b = 10 // 声明并初始化，且自动推导类型c := 20 // 初始化，且自动推导 注意： - :=定义变量只能在函数内部使用，所以经常用var定义全局变量 - Go对已经声明但未使用的变量会在编译阶段报错：** not used - Go中的标识符以字母或者下划线开头，大小写敏感 - Go推荐使用驼峰命名 多变量声明 12345678var a,b string var a1,b1 string = &quot;哼&quot;,&quot;哈&quot; var a2,b2 int = 1,2 //类型可以直接省略c,d := 1,2var( e int f bool) 零值机制 Go变量初始化会自带默认值，不像其他语言为空，下面列出各种数据类型对应的0值： 1234567891011int 0int8 0int32 0int64 0uint 0x0rune 0 //rune的实际类型是 int32byte 0x0 // byte的实际类型是 uint8float32 0 //长度为 4 bytefloat64 0 //长度为 8 bytebool falsestring &quot;&quot; 变量值互换 12m,n = n,m //变量值互换temp,_ = m,n //匿名变量：变量值互换，且丢弃变量n _丢弃变量 _是个特殊的变量名，任何赋予它的值都会被丢弃。该变量不占用命名空间，也不会分配内存。 1_, b := 34, 35 //将值`35`赋予`b`，并同时丢弃`34`： := 声明的注意事项 下面是正确的代码示例： 12in, err := os.Open(file)out, err := os.Create(file) // 此处的 err其实是赋值 但是如果在第二行赋值的变量名全部和第一行一致，则编译不通过： 12in, err := os.Open(file)in, err := os.Create(file) // 即 := 必须确保至少有一个变量是用于声明 :=只有对已经在同级词法域声明过的变量才和赋值操作语句等价，如果变量是在外部词法域声明的，那么:=将会在当前词法域重新声明一个新的变量。 常量 常量：在编译阶段就确定下来的值，程序运行时无法改变。 定义方式： 1234const A = 3const PI float32 = 3.1415const mask = 1 &lt;&lt; 3 //常量与表达式//const Home = os.GetEnv(“HOME”); //错误写法：常量赋值是一个编译期行为，右边的值不能出现在运行时才能得到结果的值。 数据类型 值类型与引用类型 12345678910111213141516值类型： 整型（int8,uint等） # 基础类型之数字类型 浮点型（float32，float64） # 基础类型之数字类型 复数() # 基础类型之数字类型 布尔型（bool） # 基础类型 字符串（string） # 基础类型 数组 # 复合类型 结构体（struct） # 复合类型引用类型：即保存的是对程序中一个变量的或状态的间接引用，对其修改将影响所有该引用的拷贝 指针 切片（slice） 字典（map） 函数 管道（chan） 接口（interface） 注意： - 基本数据类型是Go语言实际的原子 - 复合数据类型是由不同的方式组合基本类型构造出来的数据类型，如：数组，slice，map，结构体 - 没有字符型，使用byte来保存单个字母 类型判断： 1fmt.Printf(&quot;%T&quot;,a); //输出a的类型 注意：int32和int是两种不同的类型，编译器不会自动转换，需要类型转换。 常见格式化输出 1234567891011%% %字面量%b 二进制整数值，基数为2，或者是一个科学记数法表示的指数为2的浮点数%c 字符型%d 十进制数值，基数为10%e 科学记数法e表示的浮点或者复数%E 科学记数法E表示的浮点或者附属%f 标准计数法表示的浮点或者附属%o 8进制度%p 十六进制表示的一个地址值%s 字符串%T 输出值的类型 类型别名 12type bigint int64 //支持使用括号，同时起多个别名var a bigint 多数据分组书写 123456789101112const( i = 100 pi = 3.1415 prefix = &quot;Go_&quot;)var( i int pi float32 prefix string) 关键字iota 关键字iota声明初始值为0，每行递增1： 12345678910111213141516171819const ( a = iota // 0 b = iota // 1 c = iota // 2)const ( d = iota // 0 e // 1 f // 2)//如果iota在同一行，则值都一样const ( g = iota //0 h,i,j = iota,iota,iota // 1,1,1 // 此处不能定义缺省常量，如 k = 3 会编译错误 ) 内存结构","link":"/posts/8632.html"},{"title":"流程控制之-条件语句","text":"判断语句 if if判断示例： 123//初始化与判断写在一起： if a := 10; a == 10if i == '3' { } if的特殊写法： 12if err := Connect(); err != nil { // 这里的 err!=nil 才是真正的if判断表达式} 分支语句 switch 示例： 12345678switch num { case 1: // case 中可以是表达式 fmt.Println(&quot;111&quot;) case 2: fmt.Println(&quot;222&quot;) default: fmt.Println(&quot;000&quot;)} 贴士： - Go保留了break，用来跳出switch语句，上述案例的分支中默认就书写了该关键字 - Go也提供fallthrough，代表不跳出switch，后面的语句无条件执行 流程控制之-循环语句 for循环 Go只支持for一种循环语句，但是可以对应很多场景： 1234567891011121314151617181920212223//传统的for循环for init,condition;post{}//for循环简化var i intfor ; ; i++ { if(i &gt; 10){ break; }}//类似while循环for condition {}//死循环for{}//for range:一般用于遍历数组、切片、字符串、map、管道for k, v := range []int{1,2,3} {} 跳出循环 常用的跳出循环关键字： - break用于函数内跳出当前for、switch、select语句的执行 - continue用于跳出for循环的本次迭代。 - goto可以退出多层循环 break跳出循环案例(continue同下)： 1234567891011121314OuterLoop: for i := 0; i &lt; 2; i++ { for j := 0; j &lt; 5; j++ { switch j { case 2: fmt.Println(i,j) break OuterLopp case 3: fmt.Println(i,j) break OuterLopp } } } goto跳出多重循环案例： 123456789101112for x:=0; x&lt;10; x++ { for y:=0; y&lt;10; x++ { if y==2 { goto breakHere } } }breakHere: fmt.Println(&quot;break&quot;) 贴士：goto也可以用来统一错误处理。 123456if err != nil { goto onExit}onExit: fmt.Pritln(err) exitProcess()","link":"/posts/36713.html"},{"title":"运算符","text":"运算符汇总 123456算术运算符： + - * / % ++ -- 关系运算符： == != &lt;= &gt;= &lt; &gt; 逻辑运算符： ! &amp;&amp; ||位运算： &amp;（按位与） |（按位或） ^（按位取反） &lt;&lt;（左移） &gt;&gt;（右移）赋值运算符： = += -= *= /= %= &lt;&lt;= &gt;&gt;= &amp;= ^= |=其他运算符： &amp;（取地址） *（取指针值） &lt;-（Go Channel相关运算符） Go自增自减少 Go中只有后--和后++，且自增自减不能用于表达式中，只能独立使用： 123a = i++ //错误用法if i++ &gt; 0 {} //错误用法i++ //正确用法 位运算 12345&amp; 按位与，参与运算的两个数二进制位相与：同时为1，结果为1，否则为0| 按位或，参与运算的两个数二进制位相或：有一个为1，结果为1，否则为0^ 按位异或：二进位不同，结果为1，否则为0&lt;&lt; 按位左移：二进位左移若干位，高位丢弃，低位补0，左移n位其实就是乘以2的n次方&gt;&gt; 按位右移：二进位右移若干位，右移n位其实就是除以2的n次方 优先级","link":"/posts/4074.html"},{"title":"高级特性","text":"在Python中，代码越少越简单约好。基于这一思想，后面的几个篇章介绍Python一些非常有用的高级特性。 比方说构造一个1~99的奇数列表，可以简单地用循环实现： 12345L = []n = 1while n &lt;= 99: L.append(n) n = n + 2 切片 切片即取一个list或tuple部分元素的操作。 当我们需要取列表前n个元素，即索引0~N-1的元素时，有两种方法： 1.方法1是用循环 12345678&gt;&gt;&gt; L = ['Michael', 'Sarah', 'Tracy', 'Bob', 'Jack']&gt;&gt;&gt; r = []&gt;&gt;&gt; n = 3&gt;&gt;&gt; for i in range(n):... r.append(L[i])...&gt;&gt;&gt; r['Michael', 'Sarah', 'Tracy'] 2.方法2是利用切片操作符 12&gt;&gt;&gt; L[0:3]['Michael', 'Sarah', 'Tracy'] 如果经常要取指定的索引范围，用循环就显得太过繁琐了，Python提供了切片操作来简化这个过程。注意，切片操作的索引左闭右开。 如果索引从0开始，还可以改写为 L[:3]。 如果索引到列表最后结束，同样可以简略写为 L[0:]。 此外，Python还支持倒数切片。列表最后一项的索引在倒数中为-1。 1234&gt;&gt;&gt; L[-2:]['Bob', 'Jack']&gt;&gt;&gt; L[-2:-1]['Bob'] 特别地，切片操作还支持每隔k个元素取1个这样的操作。先创建一个0~99的整数列表： 123&gt;&gt;&gt; L = list(range(100))&gt;&gt;&gt; L[0, 1, 2, 3, ..., 99] 取后10个只需起始索引为-10即可： 12&gt;&gt;&gt; L[-10:][90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 前十个数隔两个取一个： 12&gt;&gt;&gt; L[:10:2][0, 2, 4, 6, 8] 所有数，隔五个取一个： 12&gt;&gt;&gt; L[::5][0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] 注意！对list进行切片操作得到的还是list；对tuple进行切片操作得到的还是tuple。 特别地，字符串也可看为一种list，同样可以使用切片操作。 迭代 Python中可迭代的对象包括字符串，list，tuple，dict，set和文件等等。 对这些可迭代对象可以使用 for...in 循环来遍历。Python对for循环的抽象程度高于Java和C，所以即使没有下标也能迭代。 比如循环遍历一个dict： 1234567&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}&gt;&gt;&gt; for key in d:... print(key, d[key])...a 1c 3b 2 直接打印key会打印所有dict中的key，更改迭代的写法为 for value in d.values() 就变为迭代dict中所有的value。 如果同时要访问key和value，还可以使用 for k, v in d.items()。 字符串同样可以用for循环迭代： 123456&gt;&gt;&gt; for ch in 'ABC':... print(ch)...ABC 要判断一个对象是否可迭代对象可以通过collections模块的 Iterable 类型来判断： 1234567&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance('abc', Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 正如上面迭代dict一样，for循环可以同时引用两个甚至多个变量： 12345678910111213&gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']):... print(i, value)...0 A1 B2 C&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print(x, y)...1 12 43 9 例子里的 enumerate 方法通过enumerate官方文档了解，它返回一个枚举对象，并且传入参数可迭代时它就是一个可迭代的对象。 可以用 list(enumerate(可迭代对象)) 把一个可迭代对象变为元素为tuple类型的list，每个tuple有两个元素，形式如：(序号，原可迭代对象内容)。 并且使用enumerate时可以指定开始的序号，enumerate(iterable, start=0)，不写时默认参数为0，即序号从0开始。 可以自己指定为其他数。 列表生成式 列表生成式即List Comprehensions，是Python内置的用于创建list的方式。 比方说生成1到10，可以： 12&gt;&gt;&gt; list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 要生成 [1x1, 2x2, 3x3, ..., 10x10]平方序列，笨办法是用循环： 123456&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)...&gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 用列表生成式只用一个语句就可以了： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 写列表生成式时，把要生成的元素 x * x 放到前面，后面跟for循环，就可以把list创建出来。 在for循环后面还可以加上if判断，比方说这个例子用于筛选出偶数的平方数： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 使用两层循环还可以生成全排列： 12&gt;&gt;&gt; [m + n for m in 'ABC' for n in 'XYZ']['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'] 列出当前目录下所有文件和目录名也非常简单： 123&gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到&gt;&gt;&gt; [d for d in os.listdir('.')] # os.listdir可以列出文件和目录['.emacs.d', '.Trash', 'Applications', 'Desktop', 'Documents'] 前面一节提到for循环迭代可以同时用两个变量，这里列表生成式同样可以用两个变量来生成list。 123&gt;&gt;&gt; d = {'x': 'A', 'y': 'B', 'z': 'C' }&gt;&gt;&gt; [k + '=' + v for k, v in d.items()]['y=B', 'x=A', 'z=C'] 把list中所有字符串的大写字母换成小写： 123&gt;&gt;&gt; L = ['Hello', 'World', 'IBM', 'Apple']&gt;&gt;&gt; [s.lower() for s in L]['hello', 'world', 'ibm', 'apple'] 生成器 通过列表生成式可以简单地创建列表，但受到内存限制，列表容量是有限的。如果列表元素很多，而我们仅需访问前面一部分元素，则会造成很大的存储空间的浪费。 生成器(generator)就意在解决这个问题，允许在循环过程中不断推算出后续元素，而不用创建完整的list。在Python中，这种边循环边计算的机制称为生成器。 和列表生成式的区别很简单，仅仅是把外层的[]方括号换成()圆括号。 123456&gt;&gt;&gt; L = [x * x for x in range(5)]&gt;&gt;&gt; L[0, 1, 4, 9, 16]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 生成器无法通过索引访问，因为它保存的是算法，要遍历生成器需要通过 next() 函数。 1234567891011121314&gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)4&gt;&gt;&gt; next(g)9&gt;&gt;&gt; next(g)16&gt;&gt;&gt; next(g)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 当到达最后一个元素时，再使用 next() 就会出现 StopIteration 错误。 当然，实际遍历生成器时不会这样一个一个用 next() 方法遍历，用for循环进行迭代即可。 123456789&gt;&gt;&gt; g = (x * x for x in range(5))&gt;&gt;&gt; for n in g:... print(n)...014916 当算法比较复杂，用简单for循环无法写出来时，还可以通过函数来实现： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return 'done' 比方说这个计算斐波那契数列的函数，稍微改写一下即可变成generator： 1234567def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b #只修改这里 a, b = b, a + b n = n + 1 return 'done' 这是定义generator的另一种方法，如果一个函数定义中包含yield关键字，则该函数就变为一个generator。 123&gt;&gt;&gt; f = fib(6)&gt;&gt;&gt; f&lt;generator object fib at 0x104feaaa0&gt; 函数是顺序执行，遇到return语句或到达最后一行函数语句就返回。而变成generator的函数，在每次调用 next() 的时候执行，遇到yield就返回，下次执行会从yield的地方开始。 123456789&gt;&gt;&gt; for n in fib(6):... print(n)...112358 同样地，把函数改成generator后，我们不需要用next()方法获取写一个返回值，而是只借用for循环进行迭代。 但是这样就拿不到fib函数return语句的值(即字符串done)，要获取这个值必须捕获 StopIteration 这个错误，它的value就是我们返回的值： 12345678910111213141516&gt;&gt;&gt; g = fib(6)&gt;&gt;&gt; while True:... try:... x = next(g)... print('g:', x)... except StopIteration as e:... print('Generator return value:', e.value)... break...g: 1g: 1g: 2g: 3g: 5g: 8Generator return value: done 生成器的工作原理是：在for循环的过程中不断计算出下一个元素，并在适当的条件结束for循环。 对于函数改成的generator来说，遇到return语句或者执行到函数体最后一行语句就结束generator，for循环随之结束。 普通函数和生成器函数可以通过调用进行区分，调用普通函数会直接返回结果，调用生成器函数则会返回一个生成器对象。 杨辉三角 要求使用生成器生成1~10行的杨辉三角。 提示：把每一行当作一个list。 1234567def triangles(max): n = 0 b = [1] while n &lt; max: yield b b = [1] + [ b[i] + b[i + 1] for i in range(len(b) - 1)] + [1] n = n + 1 这段代码非常短，但是已经充分实现了题目要求，值得欣赏! 123456789&gt;&gt;&gt; for L in triangles(6):... L...[1][1, 1][1, 2, 1][1, 3, 3, 1][1, 4, 6, 4, 1][1, 5, 10, 10, 5, 1] 代码里面有两个窍门，一是列表相加，注意不是列表元素相加。 列表相加相当于把后一个列表的元素全部append到前一个列表。如： 1234&gt;&gt;&gt; L = [1,2]&gt;&gt;&gt; R = [3,4]&gt;&gt;&gt; L+R[1, 2, 3, 4] 上面代码中的b即把每一行当作一个list，因为每一行的开头结尾都是1，所以可以每一行的list看作三个list的相加，一头一尾两个list是只有1个元素1的list，中间的list用列表生成式生成。 另一个窍门就是这里的列表生成式。 注意这里计算时还没赋值，引用列表b的内容是上一行的信息，所以能巧妙地借助上一行计算相邻两数之和，最终得到含有n-2项的中间列表。 补充解析一下代码执行的过程： 123456b = [1], n = 0b = [1] + [1] = [1,1], n = 1 # 无中间列表b = [1] + [1+1] + [1], n = 2 # 中间列表包含1个元素b = [1] + [1+2, 2+1] + [1], n = 3 # 中间列表包含2个元素b = [1] + [1+3, 3+3, 3+1] + [1], n = 4 # 中间列表包含4个元素... ... 迭代器 迭代器即Iterator， 前面说到可以通过collections模块的Iterable类型来判断一个对象是否可迭代对象。 这里引入Iterator的概念，可以通过类似的方式判断。 list，dict，str虽然都Iterable，却不是Iterator。 生成器都是Iterator。Iterator的特性允许对象通过next()函数不断返回下一个值。 123456789&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance({}, Iterator)False&gt;&gt;&gt; isinstance('abc', Iterator)False 要把list，dict，str变为Iterator可以使用 iter() 函数： 1234&gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter('abc'), Iterator)True Python的Iterator对象表示的其实是一个数据流，Iterator对象可以被 next() 函数调用并不断返回下一个数据，直到没有数据时抛出 StopIteration 错误。 可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过 next() 函数实现按需计算下一个数据，所以 Iterator 的计算是惰性的，只有在需要返回下一个数据时它才会计算，也因此能够节省空间。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。 小结 凡是可作用于for循环的对象都是 Iterable 类型； 凡是可作用于 next() 函数的对象都是 Iterator 类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是 Iterable 但不是 Iterator，不过可以通过 iter() 函数获得一个 Iterator 对象。 Python的for循环本质上就是通过不断调用 next() 函数实现的，例如： 12for x in [1, 2, 3, 4, 5]: pass 实际上完全等价于： 12345678910# 首先获得Iterator对象:it = iter([1, 2, 3, 4, 5])# 循环:while True: try: # 获得下一个值: x = next(it) except StopIteration: # 遇到StopIteration就退出循环 break","link":"/posts/49058.html"},{"title":"进制与转换","text":"常见进制 二进制：只有0和1，Go中不能直接使用二进制表示整数 八进制：0-7，以数字0开头 十进制：0-9 十六进制：0-9以及A-F，以0X开头，A-F以及X不区分大小写 #### 任意进制转换为十进制 二进制转十进制：从最低位开始，每个位上数乘以2（位数-1）次方然后求和 &gt; 1011 = 1*20 + 1*21 + 0*22 + 1*23 = 11 八进制转十进制：从最低位开始，每个位上数乘以8（位数-1）次方然后求和 &gt; 0123 = 3*80 + 2*81 + 1*82 + 0*83 = 83 其他进制转十进制同理。 十进制转其他进制 十进制转二进制：不断除以2，直到0为止,余数倒过来即可，如图： 十进制转八进制：不断除以8，直到0为止，余数倒过来即可。 十进制转十六进制：同上 其他进制互转 二进制转换八进制：将二进制数从低位开始，每三位一组，转换成八进制数即可 二进制转十六进制：将二进制数从低位开始，每四位一组，转换成十六进制数即可 八进制转换二进制：将八进制数每1位转换成一个3位的二进制数（首位0除外） 十六进制转二进制：将十六进制每1位转换成对应的一个4位的二进制数即可 反码补码 对于有符号数而言，二进制的最高为是符号位：0表示正数，1表示负数。 比如 1在二进制中： 121 二进制位：0000 0001-1 二进制位：1000 0001 正数的原码、反码、补码都一样，负数的反码=原码符号位不变，其他位取反，补码是反码+1 1234 1 -1原码 0000 0001 1000 0001反码 0000 0001 1111 1110补码 0000 0001 1111 1111 常见理解： - 0的反码补码都是0 - 计算机中是以补码形式运算的","link":"/posts/4263.html"},{"title":"数值类型","text":"整数类型 整数类型有无符号(如int)和带符号(如uint)两种,这两种类型的长度相同，但具体长度取决于不同编译器的实现。 int8、int16、int32和int64四种有符号整数类型，分别对应8、16、32、64bit大小的有符号整数，同样uint8、uint16、uint32和uint64对应四种无符号整数类型。 123456789101112131415# 有符号类型：int 32位系统占据4个字节（范围和int32一样），64位系统占据8个字节（范围和int64一样） int8 占据1字节 范围 -128 ~ 127int16 占据2字节 范围 -2(15次方) ~ 2（15次方）-1int32 占据4字节 范围 -2(31次方) ~ 2（31次方）-1int64 占据8字节 范围 -2(63次方) ~ 2（63次方）-1rune int32的别称# 无符号类型：uint 32位系统占据4个字节（范围和uint32一样），64位系统占据8个字节（范围和uint64一样） uint8 占据1字节 范围 0 ~ 255uint16 占据2字节 范围 0 ~ 2（16次方）-1uint32 占据4字节 范围 0 ~ 2（32次方）-1uint64 占据8字节 范围 0 ~ 2（64次方）-1byte uint8的别称 注意： - 上述类型的变量之间不允许互相赋值或操作，且int的长度是32 bit, 但int 与 int32并不可以互用。 - Go默认的整型类型是int - 查看数据所占据的字节数方法：unsafe.Sizeof() 浮点类型 浮点类型的分类： 12float32 单精度 占据4字节 范围 -3.403E38 ~ 3.403E38 (math.MaxFloat32)float64 双精度 占据8字节 范围 -1.798E208 ~ 1.798E308 (math.MaxFloat64) 由上看出： - 浮点数是有符号的，浮点数在机器中存放形式是：浮点数=符号位+指数位+尾数位 - 浮点型的范围是固定的，不受操作系统限制 - .512 这样数可以识别为 0.512 - 科学计数法：5.1234e2 = 5.12 * 10 的 2 次方 5.12E-2 = 5.12/10 的 2 次方 精度损失：float32可以提供大约6个十进制数的精度，float64大约可以提供15个十进制的精度（一般选择float64） 1234var num1 float32 = -123.0000901var num2 float64 = -123.0000901 fmt.Println(&quot;num1=&quot;,num1) //-123.00009fmt.Println(&quot;num2=&quot;,num2) //-123.0000901 使用 == 号判断浮点数，是不可行的，替代方案如下： 123func isEqual(f1,f2,p float64) bool { return math.Abs(f1-f2) &lt; p //p为用户自定义精度，如：0.00001} go中的NaN非数： 12var z float64fmt.Println(z, -z, 1/z, -1/z, z/z) // &quot;0 -0 +Inf -Inf NaN&quot; 注意：函数math.IsNaN用于测试一个数是否是非数NaN，math.NaN则返回非数对应的值。虽然可以用math.NaN来表示一个非法的结果，但是测试一个结果是否是非数NaN则是充满风险的，因为NaN和任何数都是不相等的。 12nan := math.NaN()fmt.Println(nan == nan, nan &lt; nan, nan &gt; nan) // &quot;false false false&quot; 复数 Go还支持复数。它的默认类型是complex128（64位实数+64位虚数）。如果需要小一些的，也有complex64(32位实数+32位虚数)。复数的形式为RE + IMi，其中RE是实数部分，IM是虚数部分，而最后的i是虚数单位。 如下所示： 12345var t complex128t = 2.1 + 3.14it1 = complex(2.1,3.14) //结果同上 fmt.Println(real(t)) //实部：2.1fmt.Println(imag(t)) //虚部：3.14","link":"/posts/51604.html"},{"title":"字符串","text":"字符型 Golang 中没有专门的字符类型，如果要存储单个字符(字母)，一般使用 byte 来保存，且使用单引号包裹。 字符类型可以以d%打印为整型。 123456789var c1 byte = 'a'var c2 byte = '0'fmt.Println(&quot;c1=&quot;, c1) //输出 97fmt.Println(&quot;c2=&quot;, c2) //输出48 fmt.Printf(&quot;c1=%c,c2=%c\\n&quot;, c1, c2) //输出原值 a 0 //var c3 byte = '北'//fmt.Printf(&quot;c3=%c&quot;, c3) //溢出错误 说明： - 如果我们保存的字符在 ASCII 表的,比如[0-1, a-z,A-Z..]直接可以保存到 byte - 如果我们保存的字符对应码值大于 255,这时我们可以考虑使用 int 类型保存 - 如果我们需要安装字符的方式输出，这时我们需要格式化输出，即 fmt.Printf(“%c”, c1) - 字符可以和整型进行运算 基本使用 字符串就是一串固定长度的字符连接起来的字符序列。Go 的字符串是由单个字节连接起来的。也 就是说对于传统的字符串是由字符组成的，而 Go 的字符串不同，它是由字节组成的。 字符串在Go语言中是基本类型，内容在初始化后不能修改。Go中的字符串都是采用UTF-8字符集编码，使用一对双引号（\"\"）或反引号定义。 使用``可以额外解析换行，即其没有字符转义功能。 123456str := &quot;Hello &quot; str2 := &quot; World!&quot; // str[0] = 'c' //编译报错： cannot assign to 因为字符串不可变 fmt.Println(str[0]) //输出字符串第一个字符 72 fmt.Println(len(str)) //输出长度 6 fmt.Println(str + str2) //输出不带空格的 字符串修改 Go中的字符串不可改变，有两种修改办法： 办法一：通过转换为[]byte类型，构造一个临时字符串 1234str := &quot;hello&quot;strTemp := []byte(str)strTemp[0] = 'c';strResult := string(strTemp) 办法二：字符串可以进行切片操作 12str := &quot;hello&quot;str = &quot;c&quot;+ str[1:] // 1: 表示从第1位开始到最后 Go和Java等语言一样，字符串默认是不可变的，这样保证了线程安全，大家使用的都是只读对象，无须加锁，且能很方便的共享内存，不必使用写时复制。 字符串遍历 遍历方式一：使用字节数组，注意每个中文在UTF-8中占据3个字节 1234str := &quot;hello&quot;for i := 0; i &lt; len(str); i++ { fmt.Println(i,str[i])} 遍历方式二：range关键字只是第一种遍历方式的简写 1234str := &quot;你好&quot;for i,ch := range str { fmt.Println(i,ch)} 注意：Unicode字符遍历需要使用range，原因见len()函数部分讲解。 字符串转换 字符串转化的函数在strconv中，如下也只是列出一些常用的： Append 系列函数将整数等转换为字符串后，添加到现有的字节数组中。 12345678910111213141516package mainimport ( &quot;fmt&quot; &quot;strconv&quot;)func main() { str := make([]byte, 0, 100) str = strconv.AppendInt(str, 4567, 10) str = strconv.AppendBool(str, false) str = strconv.AppendQuote(str, &quot;abcdefg&quot;) str = strconv.AppendQuoteRune(str, '单') fmt.Println(string(str))} Format 系列函数把其他类型的转换为字符串 1234567891011121314151617package mainimport ( &quot;fmt&quot; &quot;strconv&quot;)func main() { a := strconv.FormatBool(false) b := strconv.FormatFloat(123.23, 'g', 12, 64) c := strconv.FormatInt(1234, 10) d := strconv.FormatUint(12345, 10) e := strconv.Itoa(1023) fmt.Println(a, b, c, d, e)} Parse 系列函数把字符串转换为其他类型 1234567891011121314151617181920212223242526package mainimport ( &quot;fmt&quot; &quot;strconv&quot;)func checkError(e error){ if e != nil{ fmt.Println(e) }}func main() { a, err := strconv.ParseBool(&quot;false&quot;) checkError(err) b, err := strconv.ParseFloat(&quot;123.23&quot;, 64) checkError(err) c, err := strconv.ParseInt(&quot;1234&quot;, 10, 64) checkError(err) d, err := strconv.ParseUint(&quot;12345&quot;, 10, 64) checkError(err) e, err := strconv.Atoi(&quot;1023&quot;) checkError(err) fmt.Println(a, b, c, d, e)} len()函数 len()函数是go语言的内建函数，可以用来获取切片、字符串、通道等的长度。 12345678910111213141516package mainimport ( &quot;fmt&quot; &quot;unicode/utf8&quot;)func main() { str1 := &quot;hello world&quot; str2 := &quot;你好，&quot; fmt.Println(len(str1)) // 11 fmt.Println(len(str2)) // 9 fmt.Println(utf8.RuneCountInString(str2)) // 3} 第一个函数输出11很容易理解，第二个函数却输出了9，理论上我们会认为应该是3才对。这时因为Go的字符串都是以UTF-8格式保存，每个中文占据3个字节。Go中计算UTF-8字符串格式的长度应该使用utf8.RuneCountInString。 字符串索引 123456789101112131415161718package mainimport ( &quot;fmt&quot; &quot;strings&quot;)func main() { str := &quot;hello,world!&quot; index := strings.Index(str, &quot;w&quot;) fmt.Println(index) // 6 fmt.Println(str[index]) // 119 ASCII中的值 fmt.Println(str[index:]) // world!} strings.Index: 正向搜索字符串 strings.LastIndex: 反向搜索字符串 字符串连接 使用+能够连接字符串。但是该操作并不高效。Go中也拥有类似Java的StringBuilder机制来进行高效字符串连接： 123456789101112str1 := &quot;hellow&quot;str2 := &quot; world&quot;//创建字节缓冲var stringBuilder bytes.Buffer//把字符串写入缓冲stringBuilder.WriteString(str1)stringBuilder.WriteString(str2)//将缓冲以字符串形式输出fmt.Println(stringBuilder.String()) 注意：bytes.Buffer可以写入各种字节数组，字符串也是一种字节数组。 其他操作 123456789101112131415161718//查找s在字符串str中的索引Index(str, s string) int 示例：strings.Index(str, &quot;,&quot;)//判断str是否包含sContains(str, s string) bool//通过字符串str连接切片 sJoin(s []string, str string) string//替换字符串str中old字符串为new字符串，n表示替换的次数，小于0全部替换Replace(str,old,new string,n int) string//字符串str按照s分割，返回切片Splite(str,s string)[]string//Append系列函数将整数等转换为字符串后，添加到现有的字节数组中//Format系列函数可以把其他类型转换为字符串//Trim函数可以去除头部、尾部指定的字符串//Fields函数可以去除空格，返回切片","link":"/posts/47564.html"},{"title":"数据类型转换","text":"显式转换 Go在不同类型的变量之间赋值时需要显式转换。也就是说Golang中数据类型不能自动转换。 数值类型转换 123var i int32 = 100var n1 float64 = float64(i) fmt.Printf(&quot;n1=%v&quot;, n1) //输出100 注意：在转换中，比如将int64转成int8【-128---127】，编译时不会报错，只是转换的结果是按溢出处理，和我们希望的结果不一样。 因此在转换时，需要考虑范围。 基本数据类型与字符串转换 基本数据类型转字符串：fmt.Sprintf();该函数会返回转换后的字符串 12345var b bool = truevar str stringstr = fmt.Sprintf(&quot;%t&quot;, b) fmt.Printf(str) //true 字符串转基本数据类型：使用包strconv 1234var str string = &quot;true&quot;var b boolb, _ = strconv.ParseBool(str) fmt.Printf(&quot;%v&quot;, b) 注意：在将String类型转成基本数据类型时，要确保String类型能够转成有效的数据，比如可以把\"123\",转成一个整数，但不能转换\"hello\"，如果这样做，Golang 直接将其转成0，其它类型也是一样的道理，如：float =&gt; 0 bool =&gt; false。 类型别名 类型别名的使用 Go在1.9版本加入了类型别名。主要用于代码升级、迁移中类型的兼容问题（C/C++中使用宏来解决重构升级带来的问题）。 Go1.9之前的版本内建类型： 12type byte uint8type rune int32 Go1.9之后的版本内建类型： 12type byte = uint8type rune = int32 类型定义是定义了一个全新的类型的类型。类型别名只是某个类型的小名，并非创造了新的类型。 12345678910// 自定义两个类型type MyInt inttype AliasInt = intvar a1 MyIntfmt.Printf(&quot;a1 type: %T\\n&quot;, a1) //main.MyIntvar a2 AliasIntfmt.Printf(&quot;a2 type: %T\\n&quot;, a2) //int 不同包下的类型别名 注意：不能为不在同一个包中的类型上定义方法。 123456789101112131415package mainimport ( &quot;time&quot;)type MyDuration = time.Durationfunc (m MyDuration) Test(str string) {}func main() {} 解决方案：使用别名定义（type MyDuration time.Duration），且将定义放在time包中","link":"/posts/42985.html"},{"title":"数组","text":"数组的声明 数组是一段固定长度的连续内存区域。数组的长度定义后不可更改，长度使用 len() 获取。 123456var arr1 [10]int //定义长度为10的整型数组，很少这样使用arr2 [5]int := [5]int{1,2,3,4,5} //定义并初始化arr3 := [5]int{1,2,3,4,5} //自动推导并初始化arr4 := [5]int{1,2} //指定总长度，前几位被初始化，没有的使用零值arr5 := [5]int{2:10, 4:11} //有选择的初始化，没被初始化的使用零值arr6 := [...]int{2,3,4} //自动计算长度 数组常用操作 1234arr[:] 代表所有元素arr[:5] 代表前五个元素，即区间的左闭右开arr[5:] 代表从第5个开始（不包含第5个）len(arr) 数组的长度 贴士：上述操作会引发类型的变化，数组将会转化为Go中新的数据类型slice，见09节 数组的遍历 方式一：for循环遍历 12345arr := [3]int{1,2,3}for i := 0; i &lt; len(arr); i++ { fmt.Println(arr[i])} 方式二：for-range遍历 123456arr := [3]int{1,2,3}for i, v := range arr { fmt.Println(i) //元素位置 fmt.Println(v) //元素值} 数组使用注意事项 数组创建完长度就固定，不可以再追加元素； 长度是数组类型的一部分，因此[3]int与[4]int是不同的类型； 数组之间的赋值是值的赋值，即当把一个数组作为参数传入函数的时候，传入的其实是该函数的副本，而不是他的指针。","link":"/posts/32766.html"},{"title":"结构体","text":"谈谈go设计思想 Go语言通过用自定义的方式形成新的类型。Go语言使用结构体和结构体成员来描述真实世界。 Go语言没有类的概念，也不支持类的继承等面向对象思想。Go语言的结构体内嵌配合接口比面向对象具有更高的扩展性和灵活性。 Go语言不仅认为结构体能拥有方法，且每种自定义类型也可以拥有自己的方法。 ### 结构体的作用 结构体可以用来声明新的类型，作为其他类型的属性/字段的容器，如下定义一个学生结构体： 1234567891011121314151617181920type Student struct { name string age int}//按顺序初始化：每个成员都必须初始化var s1 Student = Student{&quot;lisi&quot;, 20}//制定成员初始化：没有被初始化的，自动赋零值s2 := Student{age:30}//new 申请结构体s3 := new(Student) //被new生成的结构体实例其实是指针类型s3.name = &quot;zs&quot; //这里的.语法只是语法糖，将s3.name转换成了(*s3).names3.age = 27//直接声明var s4 Students4.name = &quot;ww&quot;s4.age = 30 struct的结构中的类型可以是任意类型，且存储空间是连续的，其字段按照声明时的顺序存放。 如果结构体的所有的成员都是可以比较的，那么结构体本身也是可以比较的，使用 == != ，不支持 &gt; 和 &lt;。 注意：如果结构体的成员要被包外调用，需要大写首字母。\b 取结构体地址与实例化 前面说过，对结构体的new其实是生成了一个指针类型。其实对结构体进行&amp;取地址操作时，也可以视为对该类型进行一次new的实例化操作。 1234ins := &amp;T{}# T是结构体类型# ins为结构体的实例，类型为*T，是指针类型 匿名字段 struct的字段名与类型一一对应，如果不提供名字，则为匿名字段。 如果匿名字段是一个struct时，这个struct拥有的全部字段都被隐式引入了当前的struct。 12345678910type Human struct { name string age int weight int}type Student struct { Human // 匿名字段，那么默认Student就包含了Human的所有字段 speciality string} 不仅仅是struct，其他所有内置类型和自定义类型都可以作为匿名字段： 12345678910111213141516171819202122232425262728293031type Human struct { name string age int weight int}type Student struct { Human // 匿名字段，struct Skills // 匿名字段，自定义的类型string slice int // 内置类型作为匿名字段 speciality string}func main() { // 初始化学生Jane jane := Student{Human:Human{&quot;Jane&quot;, 35, 100}, speciality:&quot;Biology&quot;} // 现在我们来访问相应的字段 fmt.Println(&quot;Her name is &quot;, jane.name) fmt.Println(&quot;Her age is &quot;, jane.age) fmt.Println(&quot;Her weight is &quot;, jane.weight) fmt.Println(&quot;Her speciality is &quot;, jane.speciality) // 我们来修改他的skill技能字段 jane.Skills = []string{&quot;anatomy&quot;} fmt.Println(&quot;Her skills are &quot;, jane.Skills) fmt.Println(&quot;She acquired two new ones &quot;) jane.Skills = append(jane.Skills, &quot;physics&quot;, &quot;golang&quot;) fmt.Println(&quot;Her skills now are &quot;, jane.Skills) // 修改匿名内置类型字段 jane.int = 3 fmt.Println(&quot;Her preferred number is&quot;, jane.int)} 这里有一个问题：如果human里面有一个字段叫做phone，而student也有一个字段叫做phone，那么该怎么办呢？ Go里面很简单的解决了这个问题，最外层的优先访问，也就是当你通过student.phone访问的时候，是访问student里面的字段，而不是human里面的字段。 内嵌结构体 当前结构体可以直接访问其内嵌结构体的内部字段： 1234567891011121314151617181920212223242526272829303132333435363738394041package mainimport &quot;fmt&quot;type Animal struct { Age int}type Person struct { Animal Name string}type Student struct { Person ClassName string}func main() { // 初始化方式1 s1 := Student{ Person{ Animal: Animal { Age: 15, }, Name:&quot;lisi&quot;, }, &quot;一班&quot;, } fmt.Println(s1.Age) // 正确输出15 fmt.Println(s1.Person.Name) // 正确输出lisi // 初始化方式2 var s2 Student s2.Name = &quot;zs&quot; s2.Age = 30 s2.ClassName = &quot;二班&quot; fmt.Println(s2.Age) // 正确输出30 fmt.Println(s2.Person.Name) // 正确输出zs}","link":"/posts/6880.html"},{"title":"无类型常量","text":"一个常量可以有任意一个确定的基础类型，例如int或float64，或者是类似time.Duration这样命名的基础类型，但是许多常量并没有一个明确的基础类型。编译器为这些没有明确基础类型的数字常量提供比基础类型更高精度的算术运算；你可以认为至少有256bit的运算精度。这里有六种未明确类型的常量类型，分别是无类型的布尔型、无类型的整数、无类型的字符、无类型的浮点数、无类型的复数、无类型的字符串。 通过延迟明确常量的具体类型，无类型的常量不仅可以提供更高的运算精度，而且可以直接用于更多的表达式而不需要显式的类型转换。例如，例子中的ZiB和YiB的值已经超出任何Go语言中整数类型能表达的范围，但是它们依然是合法的常量，而且像下面的常量表达式依然有效（译注：YiB/ZiB是在编译期计算出来的，并且结果常量是1024，是Go语言int变量能有效表示的）： 1fmt.Println(YiB/ZiB) // &quot;1024&quot; 另一个例子，math.Pi无类型的浮点数常量，可以直接用于任意需要浮点数或复数的地方： 123var x float32 = math.Pivar y float64 = math.Pivar z complex128 = math.Pi 如果math.Pi被确定为特定类型，比如float64，那么结果精度可能会不一样，同时对于需要float32或complex128类型值的地方则会强制需要一个明确的类型转换： 12345const Pi64 float64 = math.Pivar x float32 = float32(Pi64)var y float64 = Pi64var z complex128 = complex128(Pi64) 对于常量面值，不同的写法可能会对应不同的类型。例如0、0.0、0i和\\u0000虽然有着相同的常量值，但是它们分别对应无类型的整数、无类型的浮点数、无类型的复数和无类型的字符等不同的常量类型。同样，true和false也是无类型的布尔类型，字符串面值常量是无类型的字符串类型。 前面说过除法运算符/会根据操作数的类型生成对应类型的结果。因此，不同写法的常量除法表达式可能对应不同的结果： 1234var f float64 = 212fmt.Println((f - 32) * 5 / 9) // &quot;100&quot;; (f - 32) * 5 is a float64fmt.Println(5 / 9 * (f - 32)) // &quot;0&quot;; 5/9 is an untyped integer, 0fmt.Println(5.0 / 9.0 * (f - 32)) // &quot;100&quot;; 5.0/9.0 is an untyped float 只有常量可以是无类型的。当一个无类型的常量被赋值给一个变量的时候，就像下面的第一行语句，或者出现在有明确类型的变量声明的右边，如下面的其余三行语句，无类型的常量将会被隐式转换为对应的类型，如果转换合法的话。 1234var f float64 = 3 + 0i // untyped complex -&gt; float64f = 2 // untyped integer -&gt; float64f = 1e123 // untyped floating-point -&gt; float64f = 'a' // untyped rune -&gt; float64 上面的语句相当于: 1234var f float64 = float64(3 + 0i)f = float64(2)f = float64(1e123)f = float64('a') 无论是隐式或显式转换，将一种类型转换为另一种类型都要求目标可以表示原始值。对于浮点数和复数，可能会有舍入处理： 123456789const ( deadbeef = 0xdeadbeef // untyped int with value 3735928559 a = uint32(deadbeef) // uint32 with value 3735928559 b = float32(deadbeef) // float32 with value 3735928576 (rounded up) c = float64(deadbeef) // float64 with value 3735928559 (exact) d = int32(deadbeef) // compile error: constant overflows int32 e = float64(1e309) // compile error: constant overflows float64 f = uint(-1) // compile error: constant underflows uint) 对于一个没有显式类型的变量声明（包括简短变量声明），常量的形式将隐式决定变量的默认类型，就像下面的例子： 1234i := 0 // untyped integer; implicit int(0)r := '\\000' // untyped rune; implicit rune('\\000')f := 0.0 // untyped floating-point; implicit float64(0.0)c := 0i // untyped complex; implicit complex128(0i) 注意有一点不同：无类型整数常量转换为int，它的内存大小是不确定的，但是无类型浮点数和复数常量则转换为内存大小明确的float64和complex128。 如果不知道浮点数类型的内存大小是很难写出正确的数值算法的，因此Go语言不存在整型类似的不确定内存大小的浮点数和复数类型。 如果要给变量一个不同的类型，我们必须显式地将无类型的常量转化为所需的类型，或给声明的变量指定明确的类型，像下面例子这样： 12var i = int8(0)var i int8 = 0 当尝试将这些无类型的常量转为一个接口值时（见第7章），这些默认类型将显得尤为重要，因为要靠它们明确接口对应的动态类型。 1234fmt.Printf(&quot;%T\\n&quot;, 0) // &quot;int&quot;fmt.Printf(&quot;%T\\n&quot;, 0.0) // &quot;float64&quot;fmt.Printf(&quot;%T\\n&quot;, 0i) // &quot;complex128&quot;fmt.Printf(&quot;%T\\n&quot;, '\\000') // &quot;int32&quot; (rune)","link":"/posts/5895.html"},{"title":"切片初识","text":"切片介绍 在初始定义数组时，我们并不知道需要多大的数组，因此我们就需要“动态数组”。在Go里面这种数据结构叫slice。 切片解决了数组长度不能扩展，以及基本类型数组传递时产生副本的问题。 ### 切片\b创建 常用创建方式： 1234var s1 []int // 和声明数组一样，只是没有长度，但是这样做没有意义，因为底层的数组指针为nils2 := []byte {'a','b','c'}fmt.Println(s1) //输出 []fmt.Print(s2) //输出 [97 98 99] 从数组创建：slice可以从一个数组再次声明。slice通过array[i:j]来获取，其中i是数组的开始位置，j是结束位置，但不包含array[j]，它的长度是j-i: 12345678910// 声明一个含有10个元素元素类型为byte的数组var arr = [10]byte {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'}// 声明两个含有byte的slicevar a, b []byte// a指向数组的第3个元素开始，并到第五个元素结束，现在a含有的元素: ar[2]、ar[3]和ar[4]a = arr[2:5] // b是数组arr的另一个slicre,b的元素是：ar[3]和ar[4]b = arr[3:5] 注意：声明数组时，方括号内写明了数组的长度或使用...自动计算长度，而声明slice时，方括号内没有任何字符。 从切片创建： 12oldSlice := []int{1,2,3}newSlice := oldSlice[:6] //基于切片前6个元素创建，没有的默认0 注意：如果选择的旧切片长度超出了旧切片的cap()值（切片存储长度），则不合法。 使用make函数创建： 123slice1 := make([]int,5) // 创建初始值为0，个数为5的切片slice2 := make([]int,5,10) //创建初始值为10，个数为5的切片slice3 := []int{1,2,3,4,5} //创建并初始化 切片常见操作 切片常见内置函数 切片常用内置函数： - len() 返回切片长度 - cap() 返回切片底层数组容量 - append() 对切片追加元素 - func copy(dst, src []Type) int：将src中数据拷贝到dst中，返回拷贝的元素个数 切片空间与元素个数： 1234slice1 := make([]int, 5, 10)fmt.Println(len(slice1)) // 5fmt.Println(cap(slice1)) // 10fmt.Println(slice1) // [0 0 0 0 0] 切片操作 123456789101112131415161718192021222324252627282930//切片增加slice1 = append(slice1,1,2)fmt.Println(slice1) //输出[0 0 0 0 0 1 2]//切片增加一个新切片sliceTemp := make([]int,3)slice1 = append(slice1,sliceTemp...)fmt.Println(slice1) //输出[0 0 0 0 0 1 2 0 0 0]//切片拷贝s1 := []int{1,3,6,9}s2 := make([]int, 10) //必须给与充足的空间num := copy(s2, s1)fmt.Println(s1) //[1 3 6 9]fmt.Println(s2) //[1 3 6 9 0 0 0 0 0 0]fmt.Println(num) //4//切片中删除元素s1 := []int{1,3,6,9}index := 2 //删除该位置元素s1 = append(s1[:index], s1[index+1:]...)fmt.Println(s1) //[1 3 9]// 切片拷贝s1 := []int{1,2,3,4,5}s2 := []int{6,7,8}copy(s1,s2) //复制s2前三个元素到slice1前3位置copy(s2,s1) //复制s1前三个元素到slice2 注意：没有...会编译错误，默认第二个参数后是元素值，传入切片需要展开。如果追加的长度超过当前已分配的存储空间，切片会自动分配更大的内存。 切片的一些简便操作 slice的默认开始位置是0，ar[:n]等价于ar[0:n] slice的第二个序列默认是数组的长度，ar[n:]等价于ar[n:len(ar)] 如果从一个数组里面直接获取slice，可以这样ar[:]，因为默认第一个序列是0，第二个是数组的长度，即等价于ar[0:len(ar)] 切片的遍历可以使用for循环，也可以使用range函数 123456789101112131415161718192021222324252627282930313233// 声明一个数组var array = [10]byte{'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'}// 声明两个slicevar aSlice, bSlice []byte// 演示一些简便操作aSlice = array[:3] // 等价于aSlice = array[0:3] aSlice包含元素: a,b,caSlice = array[5:] // 等价于aSlice = array[5:10] aSlice包含元素: f,g,h,i,jaSlice = array[:] // 等价于aSlice = array[0:10] 这样aSlice包含了全部的元素// 从slice中获取sliceaSlice = array[3:7] // aSlice包含元素: d,e,f,g，len=4，cap=7bSlice = aSlice[1:3] // bSlice 包含aSlice[1], aSlice[2] 也就是含有: e,fbSlice = aSlice[:3] // bSlice 包含 aSlice[0], aSlice[1], aSlice[2] 也就是含有: d,e,fbSlice = aSlice[0:5] // 对slice的slice可以在cap范围内扩展，此时bSlice包含：d,e,f,g,hbSlice = aSlice[:] // bSlice包含所有aSlice的元素: d,e,f,g``` ### 切片的截取- `s[n]`：切片s中索引为位置为n的项- `s[:]`：从切片s的索引位置0到`len(s)-1`所获得的切片- `s[low:]`：从切片s的索引位置low到`len(s)-1`所获得的切片- `s[:high]`：从切片s的索引位置0到high所获得的切片- `s[low:high]`：从切片s的索引位置low到high所获得的切片- `s[low:high:max]`：从low到high的切片，且容量`cap=max-low`### 字符串转切片 ```go str := &quot;hello,世界&quot; a := []byte(str) //字符串转换为[]byte类型切片 b := []rune(str) //字符串转换为[]rune类型切片 切片存储结构 与数组相比，切片多了一个存储能力值的概念，即元素个数与分配空间可以是两个不同的值，其结构如下所示： 12345type slice struct { arrary = unsafe.Pointer //指向底层数组的指针 len int //切片元素数量 cap int //底层数组的容量} 所以切片通过内部的指针和相关属性引用数组片段，实现了变长方案，Slice并不是真正意义上的动态数组。 合理设置存储能力，可以大幅提升性能，比如知道最多元素个数为50，那么提前设置为50，而不是先设为30，可以明显减少重新分配内存的操作。","link":"/posts/29902.html"},{"title":"集合map","text":"算法贴士 需要使用任意类型的关联，就需要用到集合，比如学号和名字。Go语言提供了映射关系容器是map，内部使用散列表（hash）实现。 大多数语言中映射关系容器使用两种算法：散列表和平衡树。 散列表可以简单的描述为一个数组，数组的每个元素都是列表，根据散列函数获得每个元素的特征值，将特征值作为映射的键。如果特征值重复，表示元素发生了碰撞，需要尽量避免碰撞，这样就需要多容器扩容，每次扩容，元素都需要重新放入，较为耗时。 平衡树类似于有父子关系的一棵数据数，每个元素在放入树时，都要与一些节点进行比较。 map的创建 Go内置了map类型，map是一个无序键值对集合（也有一些书籍翻译为字典）。 普通创建： 123//声明一个map类型，[]内的类型指任意可以进行比较的类型 int指值类型m := map[string]int{&quot;a&quot;:1,&quot;b&quot;:2}fmt.Print(m[&quot;a&quot;]) make方式创建map： 123456789101112131415type Person struct{ ID string Name string}func main() { var m map[string] Person m = make(map[string] Person) m[&quot;123&quot;] = Person{&quot;123&quot;,&quot;Tom&quot;} p,isFind := m[&quot;123&quot;] fmt.Println(isFind) //true fmt.Println(p) //{123 Tom}} 注意：key 可以是什么类型？ golang 中的 map，的 key 可以是很多种类型，比如 bool, 数字，string, 指针, channel , 还可以是只 包含前面几个类型的 接口, 结构体, 数组。 通常 key 为 int 、string。 注意: slice， map 还有 function 不可以，因为他们不能使用 == 来判断 map的使用 map的读取和设置也类似slice一样，通过key来操作，只是slice的index只能是｀int｀类型，而map多了很多类型，可以是int，可以是string及所有完全定义了==与!=操作的类型。 12345678910// 声明一个key是字符串，值为int的字典,这种方式的声明需要在使用之前使用make初始化var numbers map[string]int// 另一种map的声明方式numbers = make(map[string]int)numbers[&quot;one&quot;] = 1 //赋值numbers[&quot;ten&quot;] = 10 //赋值numbers[&quot;three&quot;] = 3fmt.Println(&quot;第三个数字是: &quot;, numbers[&quot;three&quot;]) // 读取数据// 打印出来如:第三个数字是: 3 map的遍历：同数组一样，使用for-range 的结构遍历 注意： map是无序的，每次打印出来的map都会不一样，它不能通过index获取，而必须通过key获取； map的长度是不固定的，也就是和slice一样，也是一种引用类型 内置的len函数同样适用于map，返回map拥有的key的数量 map的值可以很方便的修改，通过numbers[\"one\"]=11可以很容易的把key为one的字典值改为11 map和其他基本型别不同，它不是thread-safe，在多个go-routine存取时，必须使用mutex lock机制 删除元素 1234567891011// 初始化一个字典rating := map[string]float32{&quot;C&quot;:5, &quot;Go&quot;:4.5, &quot;Python&quot;:4.5, &quot;C++&quot;:2 }// map有两个返回值，第二个返回值，如果不存在key，那么ok为false，如果存在ok为truecsharpRating, ok := rating[&quot;C#&quot;]if ok { fmt.Println(&quot;C# is in the map and its rating is &quot;, csharpRating)} else { fmt.Println(&quot;We have no rating associated with C# in the map&quot;)}delete(rating, &quot;C&quot;) // 删除key为C的元素 注意：go没有提供清空元素的方法，可以重新make一个新的map，不用担心垃圾回收的效率，因为go中并行垃圾回收效率比写一个清空函数高效很多。 sync.Mpa Go内置的map只读是线程安全的，读写是线程不安全的，并发安全的map可以使用标准包sync中的map。 演示并发读写map的问题： 123456789101112131415161718192021package mainfunc main() { m := make(map[int]int) go func() { for { //无限写入 m[1] = 1 } }() go func() { for { //无限读取 _ = m[1] } }() for {} //无限循环，让并发程序在后台执行} 错误提示：fatal error: concurrent map read and map write，即出现了并发读写，因为用两个并发程序不断的对map进行读和写，产生了竞态问题。map内部会对这种错误进行检查并提前发现。 需要并发读写时，一般都是加锁，但是这样做性能不高，在go1.9版本中提供了更高效并发安全的sync.Map。 sync.Map的特点： - 无须初始化，直接声明即可 - sync.Map不能使用map的方式进行取值和设值操作，而是使用sync.Map的方法进行调用。Store表示存储，Load表示获取，Delete表示删除。 - 使用Range配合一个回调函数进行遍历操作，通过回调函数返回内部遍历出来的值，需要继续迭代时，返回true，终止迭代返回false。 12345678910111213141516171819202122232425package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main() { var scene sync.Map //保存键值对 scene.Store(&quot;id&quot;,1) scene.Store(&quot;name&quot;,&quot;lisi&quot;) //根据键取值 fmt.Println(scene.Load(&quot;name&quot;)) //遍历 scene.Range(func(k, v interface{}) bool{ fmt.Println(k,v) return true })} 注意：map没有提供获取map数量的方法，可以在遍历时手动计算。sync.Map为了并发安全。损失了一定的性能。","link":"/posts/41544.html"},{"title":"指针","text":"指针的创建 Go保留了指针，代表某个内存地址，默认值为nil，使用&amp;取变量地址，通过*访问目标对象。 简单示例： 12345v := &quot;3&quot;ptr := &amp;vvalue := *ptrfmt.Printf(&quot;指针地址为：%p\\n&quot;, ptr) // 输出0x.....16进制数fmt.Printf(&quot;指针地址内存储的值为：%s\\n&quot;, value) // 输出3 贴士：变量、指针和地址二者的关系是: 每个变量都拥有地址，指针的值就是地址。 指针类型的声明方式一 1234var a int = 10var p *int = &amp;a //声明指针类型fmt.Println(p) //输出 0xc.....16进制数fmt.Println(*p) // 10 指针类型的声明方式二 1234var p *intp = new(int) //申请一个int类型的地址空间*p = 666 //存储地址内内容为666fmt.Println(p) 注意： - Go同样支持多级指针，如 **T - 空指针：声明但未初始化的指针 - 野指针：引用了无效的地址 - Go不支持-&gt;运算符指针运算，可以直接使用 . 访问目标成员 指针还可以使用new创建： 123str := new(string) //申请一个string类型的指针内存*str = &quot;hello&quot;fmt.Println(*str) 指针实现变量值交换 123func swap (p1,p2 *int) { *p1,*p2 = *p2,*p1} 结构体指针 结构体指针访问结构体字段仍然使用 . 语法，Go中没有 -&gt; 操作符，案例如下： 1234567891011121314type User struct{ name string age int}func main() { var u = User{ name:&quot;lisi&quot;, age: 18, } p := &amp;u fmt.Println(u.name) //输出李四 fmt.Println(p.name) //输出李四} Go不支持指针运算 由于垃圾回收机制的存在，指针运算造成许多困扰，所以Go直接禁止了指针运算 123a := 1p := &amp;ap++ //报错：non-numeric type *int 变量生命周期与栈逃逸机制 函数中允许返回局部变量的地址，Go编译器使用栈逃逸机制将这种局部变量分配在堆上: 12345var p = f()func f() *int { v := 1 return &amp;v // 返回函数中的局部变量地址是安全的，因为p仍然在引用他} 变量的生命周期指在程序运行期间变量有效存在的时间段： - 包级别声明的变量，其生命周期和整个程序的运行周期是一致的 - 局部变量的生命周期是动态的，每次从创建新变量的声明语句开始到不再引用为止，变量的存储空间可能被回收 函数的参数变量和返回值变量都是局部变量，它们在函数每次被调用的时候创建。 Go的GC判断变量是否回收的实现思路：从每个包级的变量、每个当前运行的函数的局部变量开始，通过指针和引用的访问路径遍历，是否可以找到该变量，如果不存在这样的访问路径，那么说明该变量是不可达的，也就是说它是否存在并不会影响后续计算结果。 示例： 12345678910var global *intfunc f() { var x int x = 1 global = &amp;x}func g() { y := new(int) *y = 1} 上述的函数调用结果说明： - 虽然x变量定义在f函数内部，但是其必定在堆上分配，因为函数退出后仍然能通过包一级变量global找到，这样的变量，我们称之为从函数f中逃逸了 - g函数返回时，变量*y不可达，因此没有从函数g中逃逸，其内存分配在栈上，会马上被被回收。（当然也可以选择在堆上分配，然后由Go语言的GC回收这个变量的内存空间）","link":"/posts/40851.html"},{"title":"函数","text":"函数声明 函数声明格式 1234func 函数名字 (参数列表) (返回值列表）{ // 函数体 return 返回值列表} 注意： - 函数名首字母小写为私有，大写为公有； - 参数列表可以有0-多个，多参数使用逗号分隔，不支持默认参数； - 返回值列表返回值类型可以不用写变量名 - 如果只有一个返回值且不声明类型，可以省略返回值列表与括号 - 如果有返回值，函数内必须有return Go中函数常见写法： 12345678910111213141516171819202122232425//无返回值，默认返回0，所以也可以写为 func fn() int {}func fn(){} //Go推荐给函数返回值起一个变量名func fn1()(result int){ return 1}//第二种返回值写法func fn2()(result int){ result = 1 return }//多返回值情func fn3 () (int, int, int) { return 1,2,3}//Go返回值推荐多返回值写法：func fn4 () (a int, b int, c int) { 多个参数类型如果相同，可以简写为： a,b int a , b, c = 1, 2, 3 return } 变量作用域 局部变量：函数内部声明，作用域仅限于函数内部 全局变量：函数外部声明，作用域为整个包，如果首字母大写，则整个程序都可使用。 注意：Go中，大写字母开头的变量是可导出的，也就是其它包可以读取的，是公有变量；小写字母开头的就是不可导出的，是私有变量。 不定参数 不定参数必须是最后一个参数，只是一个语法糖，在内部机制上，不定参数是一个切片，即：[]type，这是因为参数args可以用for循环来获取每个传入的参数。 12345//返回int类型的不定参数func fn1(args ...int){ fmt.Println(len(args))}//注意：所有不定参数的类型必须相同，且必须是最后一个参数，不定参数在函数内其实就相当于切片，对切片的操作同样适合\b不定参数。 匿名函数 1234567891011121314151617181920212223242526272829func main() { a := 3; f1 := func() { // f1 即为匿名函数 fmt.Println(a) //匿名函数访问外部变量 } f1() func() { //匿名函数自调 fmt.Println(a); }()}//匿名函数实战：取最大值,最小值x, y := func(i,j int) (max,min int) { if i &gt; j { max = i min = j } else { max = j min = i } return}(10,20)fmt.Println(x + ' ' + y) Go函数特性总结 支持有名称的返回值； 不支持默认值参数； 不支持重载； 不支持命名函数嵌套，匿名函数可以嵌套； Go函数从实参到形参的传递永远是值拷贝，有时函数调用后实参指向的值发生了变化，是因为参数传递的是指针的拷贝，实参是一个指针变量，传递给形参的是这个指针变量的副本，实质上仍然是值拷贝； Go函数支持不定参数； 函数类型 函数类型：函数去掉函数名、参数名和{}\b后的结果，使用%T打印该结果。 两个函数类型相同的前提是：拥有相同的形参列表和返回值列表，且列表元素的次序、类型都相同，形参名可以不同。 定义了函数类型，就可以使用该类型进行传参。 123456789101112131415func mathSum(a, b int) int { return a + b}func mathSub(a, b int) int { return a - b}//定义一个函数类型type MyMath func(int, int) int//定义的函数类型作为参数使用func Test(f MyMath, a , b int) int{ return f(a,b)} 通常我们可以把函数类型当做一种引用类型，实际函数类型变量和函数名都可以当做指针变量，只想函数代码开始的位置，没有初始化的函数默认值是nil。 匿名函数 匿名函数可以看做函数字面量，所有直接使用函数类型变量的地方都可以由匿名函数代替。匿名函数可以直接赋值给函数变量，可以当做实参，也可以作为返回值使用，还可以直接被调用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport &quot;fmt&quot;//匿名函数1var sumFunc = func(a, b int) int { return a + b}//匿名函数2var subFunc = func(a, b int) int { return a - b}func Test (f func(int, int) int, a , b int) int { return f(a, b)}func wrap (op string) func(int, int) int { switch op { case &quot;sum&quot;: return func(a, b int) int { return a + b } case &quot;sub&quot;: return func(a, b int) int { return a + b } default: return nil }}func main() { //方式一：匿名函数被直接调用 defer func(){ if err := recover(); err != nil{ fmt.Println(err) } }() //方式二：使用匿名函数变量名调用 sumFunc(1, 2) //方式三：匿名函数作为实参 Test(func(x, y int) int { return x + y },1 ,2) //方式四： myFunc := wrap(&quot;sum&quot;) result := myFunc(2,3) fmt.Println(result)} init函数 Go语言中，除了可以在全局声明中初始化实体，也可以在init函数中初始化。init函数是一个特殊的函数，它会在包完成初始化后自动执行，执行优先级高于main函数，并且不能手动调用init函数，每一个文件有且仅有一个init函数，初始化过程会根据包的以来关系顺序单线程执行。 1234567891011package mainimport ( &quot;fmt&quot;)func init() { //在这里可以书写一些初始化操作 fmt.Println(&quot;init...&quot;)}func main() { fmt.Println(&quot;main...&quot;)} new函数 new函数可以用来创建变量。表达式new(T)将创建一个T类型的匿名变量，初始化为T类型的零值，然后返回变量地址，返回的指针类型为*T： 1234p := new(int) // p 为 *int类型，只想匿名的int变量fmt.Println(*p) // &quot;0&quot;*p = 2 // 设置 int匿名变量值为2fmt.Println(*p) 贴士：new函数其实是语法糖，不是新概念，如下所示的两个函数其实拥有相同的行为。 12345678func newInt1() *int { return new(int)}func newInt2() *int { var dummy int return &amp;dummy} 注意：new只是一个预定义函数，并不是一个关键字，所以new也有可能会被项目定义为别的类型。 闭包 闭包概念 闭包是引用了自由变量的函数，被引用的自由变量和函数一同存在，即使己经离开了自由变量的环境也不会被释放或者删除，在闭包中可以继续使用这个自由变量。 简单的说 : 函数+引用环境=闭包 贴士：闭包( Closure)在某些编程语言中也被称为 Lambda表达式（如Java） 在闭包中可以修改引用的变量： 123456str := &quot;hello&quot;foo := func(){ // 声明一个匿名函数 str = &quot;world&quot;}foo() // 调用匿名函数，修改str值fmt.Print(str) // world 闭包案例一 简单示例 12345678910111213141516171819func fn1(a int) func(i int) int { return func(i int) int { print(&amp;a, a) return a }}func main() { f := fn1(1) //输出地址 g := fn1(2) //输出地址 fmt.Println(f(1)) //输出1 fmt.Println(f(1)) //输出1 fmt.Println(g(2)) //输出2 fmt.Println(g(2)) //输出2} 闭包案例二 实现累加器 12345678910111213func Accumulate(value int) func() int { return func() int { // 返回一个闭包 value++ return value }}func main() { accAdd := Accumulate(1) fmt.Println(accAdd()) // 2 fmt.Println(accAdd()) // 3} 函数参数传递 值传递和引用传递 不管是值传递还是引用传递，传递给函数的都是变量的副本，不同的是，值传递的是值的 拷贝，引用传递的是地址的拷贝，一般来说，地址拷贝效率高，因为数据量小，而值拷贝决定拷贝的 数据大小，数据越大，效率越低。 如果希望函数内的变量能修改函数外的变量，可以传入变量的地址&amp;，函数内以指针的方式操作变量。 可变参数 可变参数变量是一个包含所有参数的切片。如果要在多个可变参数中传递参数 ，可以在传递时在可变参数变量中默认添 加“ ...”，将切片中的元素进行传递，而不是传递可变参数变量本身。 示例：对可变参数列表进行遍历 1234567891011func joinStrings(slist ...string) string { var buf bytes.Buffer for _, s := range slist { buf.WriteString(s) } return buf.String()}func main() { fmt.Println(joinStrings(&quot;pig&quot;, &quot; and&quot;, &quot; bird&quot;))} 示例：参数传递 12345678910111213141516// 实际打印函数func rawPrint(rawList ...interface{}) { for _, a := range rawList { fmt.Println(a) }}// 封装打印函数func print(slist ...interface{}) { // 将slist可变参数切片完整传递给下一个函数 rawPrint(slist...)}func main() { print(1,2,3)} 常用函数 常用字符串函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//字符串的字节长度：汉字占用3个字节，字符占据1个字节len(&quot;hello&quot;)//字符串遍历：可以同时处理中文问题r := []rune(&quot;hello北京&quot;)fmt.Println(r[2]) // 查看第3个//字符串转整数n, err := strconv.Atoi(&quot;hello&quot;)//整数转字符串str = strconv.Itoa(12345)//字符串转[]bytevar bytes = []byte(&quot;hello&quot;)// []byte 转 字符串 str = string([]byte{97, 98, 99})//10进制转2 8 16进制str = strconv.FormatInt(123, 2) // 2-&gt; 8 , 16//查找子字符串是否在指定的字符串中 strings.Contains(&quot;seafood&quot;, &quot;foo&quot;) //true//统计一个字符串有几个指定的子串strings.Count(&quot;ceheese&quot;, &quot;e&quot;) //4//不区分大小写的字符串比较(==是区分字母大小写的)fmt.Println(strings.EqualFold(&quot;abc&quot;, &quot;Abc&quot;)) // true//返回子串在字符串第一次出现的 index 值，如果没有返回-1strings.Index(&quot;NLT_abc&quot;, &quot;abc&quot;) // 4// 返回子串在字符串最后一次出现的 index，如没有返回-1strings.LastIndex(&quot;go golang&quot;, &quot;go&quot;)// 将指定的子串替换成 另外一个子串strings.Replace(&quot;go go hello&quot;, &quot;go&quot;, &quot;go 语言&quot;, n) //n 可以指 定你希望替换几个，如果 n=-1 表示全部替换// 按照指定的某个字符，为分割标识，将一个字符串拆分成字符串数组strings.Split(&quot;hello,wrold,ok&quot;, &quot;,&quot;)// 将字符串的字母进行大小写的转换strings.ToLower(&quot;Go&quot;) // go strings.ToUpper(&quot;Go&quot;) // GO// 将字符串左右两边的空格去掉strings.TrimSpace(&quot; tn a lone gopher ntrn &quot;)// 将字符串左右两边指定的字符去掉 同样有 TrimLeft和 TrimRightstrings.Trim(&quot;! hello! &quot;, &quot; !&quot;)// 判断字符串是否以指定的字符串开头strings.HasPrefix(&quot;ftp://192.168.10.1&quot;, &quot;ftp&quot;) // true// 判断字符串是否以指定的字符串结束strings.HasSuffix(&quot;NLT_abc.jpg&quot;, &quot;abc&quot;) //false 时间函数 123456789now := time.Now()fmt.Printf(now.Format(&quot;2018-10-10 15:04:05&quot;))fmt.Printf(now.Format(&quot;2018-10-10&quot;))fmt.Printf(now.Format(&quot;15:04:05&quot;))//时间戳now.Unix() //10位 从1970年J 1 到现在的秒数now.Unixnano() //20位 同上，单位是纳秒","link":"/posts/21513.html"},{"title":"defer延迟执行","text":"defer延迟执行修饰符 在函数中，程序员经常需要创建资源(比如:数据库连接、文件句柄、锁等) ，为了在函数执行完 毕后，及时的释放资源，Go设计者提供了defer(延时机制): 123456func main() { //当执行到defer语句时，暂不执行，会将defer后的语句压入到独立的栈中,当函数执行完毕后，再从该栈按照先入后出的方式出栈执行 defer fmt.Println(&quot;defer1...&quot;) defer fmt.Println(&quot;defer2...&quot;) fmt.Println(&quot;main...&quot;)} 上述代码执行结果： 123main...defer2...defer1... defer将语句放入到栈时，也会将相关的值拷贝同时入栈: 123456func main() { num := 0 defer fmt.Println(&quot;defer中：num=&quot;, num) num = 3 fmt.Println(&quot;main中：num=&quot;,num)} 输出结果： 12main中：num= 3defer中：num= 0 defer最佳实践 defer最佳实践：用于关闭资源，比如：defer connect.close()。 案例一：defer处理资源 没有使用defer时打开文件处理代码： 123456789101112131415f,err := os.Open(file)if err != nil { return 0}info,err := f.Stat()if err != nil { f.Close() return 0}f.Close()return 0; 使用defer优化： 123456789101112131415161718f,err := os.Open(file)if err != nil { return 0}defer f.Close()info,err := f.Stat()if err != nil { // f.Close() //这句已经不需要了 return 0}//后续一系列文件操作后执行关闭// f.Close() //这句已经不需要了return 0; 案例二：并发使用map的函数。 无defer代码： 12345678910var ( mutex sync.Mutex testMap = make(map[string]int))func getMapValue(key string) int { mutex.Lock() //对共享资源加锁 value := testMap[key] mutex.Unlock() return value} 上述案例是很常见的对并发map执行加锁执行的安全操作，使用defer可以对上述语义进行简化： 123456789var ( mutex sync.Mutex testMap = make(map[string]int))func getMapValue(key string) int { mutex.Lock() //对共享资源加锁 defer mutex.Unlock() return testMap[key]} defer无法处理全局资源 使用defer语句, 可以方便地组合函数/闭包和资源对象，即使panic时，defer也能保证资源的正确释放。但是上述案例都是在局部使用和释放资源，如果资源的生命周期很长， 而且可能被多个模块共享和随意传递的话，defer语句就不好处理了，需要下面的方式。 Go的runtime包的func SetFinalize(x, f interface{})函数可以提供类似C++析构函数的机制，比如我们可以包装一个文件对象，在没有人使用的时候能够自动关闭： 123456789101112type MyFile struct { f *os.File}func NewFile(name string) (&amp;MyFile, error){ f, err := os.Open(name) if err != nil { return nil, err } runtime.SetFinalizer(f, f.f.Close) return &amp;MyFile{f: f}, nil} 在使用runtime.SetFinalizer时, 需要注意的地方是尽量要用指针访问内部资源，这样的话, 即使*MyFile`对象忘记释放, 或者是被别的对象无意中覆盖, 也可以保证内部的文件资源可以正确释放。 二 错误Error 2.1 Go自带的错误接口 error是go语言声明的接口类型： 123type error interface { Error() string} 所有符合Error()string格式的方法，都能实现错误接口，Error()方法返回错误的具体描述。 自定义错误 返回错误前，需要定义会产生哪些可能的错误，在Go中，使用errors包进行错误的定义，格式如下： 1var err = errors.New(&quot;发生了错误&quot;) 提示：错误字符串相对固定，一般在包作用于声明，应尽量减少在使用时直接使用errors.New返回。 下面这个例子演示了如何使用errors.New: 123456func Sqrt(f float64) (float64, error) { if f &lt; 0 { return 0, errors.New(&quot;math: square root of negative number&quot;) } // implementation} 在C语言里面是通过返回-1或者NULL之类的信息来表示错误，但是对于使用者来说，不查看相应的API说明文档，根本搞不清楚这个返回值究竟代表什么意思，比如:返回0是成功，还是失败,而Go定义了一个叫做error的类型，来显式表达错误。在使用时，通过把返回的error变量与nil的比较，来判定操作是否成功。例如os.Open函数在打开文件失败时将返回一个不为nil的error变量 1func Open(name string) (file *File, err error) 下面这个例子通过调用os.Open打开一个文件，如果出现错误，那么就会调用log.Fatal来输出错误信息： 12345f, err := os.Open(&quot;filename.ext&quot;)if err != nil { log.Fatal(err)} 类似于os.Open函数，标准包中所有可能出错的API都会返回一个error变量，以方便错误处理，这个小节将详细地介绍error类型的设计，和讨论开发Web应用中如何更好地处理error。 自定义错误案例 案例一：简单的错误字符串提示 1234567891011121314151617181920212223package mainimport ( &quot;errors&quot; &quot;fmt&quot;)//定义除数为0的错误var errByZero = errors.New(&quot;除数为0&quot;)func div(num1, num2 int) (int, error) { if num2 == 0 { return 0, errByZero } return num1 / num2, nil}func main() { fmt.Println(div(1, 0))} 案例二：实现错误接口 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( &quot;fmt&quot;)//声明一种解析错误type ParseError struct { Filename string Line int}//实现error接口，返回错误描述func (e *ParseError) Error() string { return fmt.Sprintf(&quot;%s:%d&quot;, e.Filename, e.Line)}//创建一些解析错误func newParseError(filename string, line int) error { return &amp;ParseError{filename, line}}func main() { var e error e = newParseError(&quot;main.go&quot;, 1) fmt.Println(e.Error()) switch detail := e.(type) { case *ParseError: fmt.Printf(&quot;Filename: %s Line:%d \\n&quot;, detail.Filename, detail.Line) default: fmt.Println(&quot;other error&quot;) }} errors包分析 Go中的erros包对New的定义非常简单: 123456789101112131415//创建错误对象func New(text string) error { return &amp;errorString{text}}//错误字符串type errorString struct { s string}//返回发生何种错误func (e *errorString) Error() string { return e.s} 错误对象都要事先error接口的Error()方法，这样，所有的错误都可以获得字符串的描述。 panic 宕机 手动触发宕机 Go语言可以在程序中手动触发宕机，让程序崩溃，这样开发者可以及时发现错误。 Go语言程序在宕机时，会将堆栈和goroutine信息输出到控制台，所以宕机有额可以方便知晓发生错误的位置。如果在编译时加入的调试信息甚至连崩溃现场的变量值、运行状态都可以获取，那么如何触发宕机？ 1234567package mainfunc main() { panic(&quot;crash&quot;)} 运行结果是： 123456panic: crashgoroutine 1 [running]:main.main() /Users/username/Desktop/TestGo/src/main.go:5 +0x39exit status 2 使用panic函数可以制造崩溃，panic声明如下； 1func panic(v interface{}) panic()参数可以是任意类型。 注意：手动触发宕机并不是一种偷懒的方式，反而能迅速报错，终止程序继续运行，防止更大的错误产生，但是如果任何错误都使用宕机处理，也不是一个良好的设计。 defer与panic 当panic()发生宕机，panic()后面的代码将不会被执行，但是在panic前面已经运行过的defer语句依然会在宕机时发生作用： 12345678910package mainimport &quot;fmt&quot;func main() { defer fmt.Println(&quot;before&quot;) panic(&quot;crash&quot;)} recover 宕机恢复 让程序在崩溃时继续执行 无论是代码运行错误由Runtime层抛出的panic崩溃，还是主动触发的panic崩溃，都可以配合defer和recover实现错误捕捉和处理，让代码在发生崩溃后允许继续执行。 在其他语言里，宕机往往以异常的形式存在，底层抛出异常，上层逻辑通过try/catch机制捕获异常，没有被捕获的严重异常会导致宕机，捕获的异常可以被忽略，让代码继续执行。Go没有异常系统，使用panic触发宕机类似于其他语言的抛出异常，recover的宕机恢复机制就对应try/catch机制。 使用defer与recover处理错误 123456789101112131415func test(num1 int, num2 int){ defer func(){ err := recover() //recover内置函数，可以捕获异常 if err != nil { fmt.Println(&quot;err=&quot;, err); } }() fmt.Println(num1/num2)}func main() { test(2,0)} panic recover综合示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package mainimport ( &quot;fmt&quot; &quot;runtime&quot;)//崩溃时需要传递的上下文信息type panicContext struct { function string}//保护方式允许一个函数func ProtectRun(entry func()) { defer func() { err := recover() //发生宕机时，获取panic传递的上下文并打印 switch err.(type) { case runtime.Error: fmt.Println(&quot;runtime error:&quot;, err) default: fmt.Println(&quot;error:&quot;, err) } }() entry()}func main() { fmt.Println(&quot;运行前&quot;) ProtectRun(func(){ fmt.Println(&quot;手动宕机前&quot;) panic(&amp;panicContext{&quot;手动触发panic&quot;,}) fmt.Println(&quot;手动宕机后&quot;) }) ProtectRun(func(){ fmt.Println(&quot;赋值宕机前&quot;) var a *int *a = 1 fmt.Println(&quot;赋值宕机后&quot;) }) fmt.Println(&quot;运行后&quot;)} 运行结果： 123456运行前手动宕机前error: &amp;{手动触发panic}赋值宕机前runtime error: runtime error: invalid memory address or nil pointer dereference运行后 panic和recover关系 panic和defer的组合： - 有panic没有recover，程序宕机 - 有panic也有recover，程序不会宕机，执行完对应的defer后，从宕机点退出当前函数后继续执行","link":"/posts/4154.html"},{"title":"正则表达式","text":"简介 字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取 @ 前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。 正则表达式是一种用来匹配字符串的强大工具。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。 所以我们判断一个字符串是否是合法的Email的方法是： 创建一个匹配Email的正则表达式； 用该正则表达式去匹配用户的输入来判断是否合法。 因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。 在正则表达式中，如果直接给出字符，就是精确匹配。用 \\d 可以匹配一个数字，\\w 可以匹配一个字母或数字，\\s 可以匹配一个空格（也包括Tab等空白符）。所以： '00\\d' 可以匹配 '007'，但无法匹配 '00A'； '\\d\\d\\d' 可以匹配 '010'； '\\w\\w\\d' 可以匹配 'py3'； . 可以匹配任意字符，所以： 'py.' 可以匹配 'pyc'、'pyo'、'py!' 等等。 要匹配变长的字符，在正则表达式中，用 * 表示任意个字符（包括0个），用 + 表示至少一个字符，用 ? 表示0个或1个字符，用 {n} 表示n个字符，用 {n,m} 表示n-m个字符： 来看一个复杂的例子：\\d{3}\\s+\\d{3,8} 我们来从左到右解读一下： \\d{3} 表示匹配3个数字，例如 '010'； \\s+ 表示至少有一个空格，例如匹配 ' '，' ' 等； \\d{3,8} 表示3-8个数字，例如 '1234567'。 综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。 如果要匹配 '010-12345' 这样的号码呢？由于 '-' 是特殊字符，在正则表达式中，要用 '\\' 转义，所以，用于匹配的正则表达式应为 \\d{3}\\-\\d{3,8}。 但是，上面的方法无法匹配 '010 - 12345'，我们需要更复杂的匹配方式。 进阶 要做更精确地匹配，可以用 [] 表示一个范围，比如： [0-9a-zA-Z\\_] 可以匹配一个数字、字母或者下划线； [0-9a-zA-Z\\_]+ 可以匹配至少由一个数字、字母或者下划线组成的字符串，比如'a100'，'0_Z'，'Py3000'等等； [a-zA-Z\\_][0-9a-zA-Z\\_]* 可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量； [a-zA-Z\\_][0-9a-zA-Z\\_]{0, 19} 更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。 A|B 可以匹配A或B，所以 (P|p)ython 既可以匹配 'Python' 又可以匹配 'python'。 ^ 表示行的开头，^\\d 表示必须以数字开头。 $ 表示行的结束，\\d$ 表示必须以数字结束。 有趣的是 py 也可以用来匹配 'python'，但是加上 ^py$ 就变成了整行匹配，只能匹配 'py' 了。 re模块 有了准备知识，我们就可以在Python中使用正则表达式了。Python提供了 re 模块，包含所有正则表达式的功能。注意，在正则表达式中，我们使用 \\ 来表示转义，但在Python的字符串中，\\ 同样是一个转义符。因此，使用时我们就要特别注意字符串中的 \\ 是否能正确地起作用。比方说： 1s = 'ABC\\\\-001' 这个字符串中，第一个 \\ 用于为第二个 \\ 转义，所以打印这个字符串实际上得到的是 'ABC-001'，这样用作正则表达式时，就能正确地转义 -。而： 1s = 'ABC\\-001' 这个字符串实际上是 'ABC-001'，由于 - 在正则表达式中是一个特殊字符，而这里我们希望的是它只作为用于匹配字符 - 的功能，所以这样得到的正则表达式就出错了。 因此在书写正则表达式时，推荐使用Python字符串的 r 前缀，这样就不用考虑字符转义的问题了： 1s = r'ABC\\-001' r 前缀表示后面的字符串中无需转义字符。 接下来看看如何判断正则表达式是否与字符串匹配： 12345&gt;&gt;&gt; import re&gt;&gt;&gt; re.match(r'^\\d{3}\\-\\d{3,8}$', '010-12345')&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; re.match(r'^\\d{3}\\-\\d{3,8}$', '010 12345')&gt;&gt;&gt; re 模块的 match() 方法判断是否匹配，如果匹配成功，返回一个 Match 对象，否则返回 None。常见的判断方法就是： 12345test = '用户输入的字符串'if re.match(r'正则表达式', test): print('ok')else: print('failed') 切分字符串 用正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码： 12&gt;&gt;&gt; 'a b c'.split(' ')['a', 'b', '', '', 'c'] 嗯，无法识别连续的空格，用正则表达式试试： 12&gt;&gt;&gt; re.split(r'\\s+', 'a b c') # '\\s+'表示至少匹配一个空白字符['a', 'b', 'c'] 无论多少个空格都可以正常分割。加入 , 试试： 12&gt;&gt;&gt; re.split(r'[\\s\\,]+', 'a,b, c d')['a', 'b', 'c', 'd'] 再加入 ; 试试： 12&gt;&gt;&gt; re.split(r'[\\s\\,\\;]+', 'a,b;; c d')['a', 'b', 'c', 'd'] 如果用户输入了一组标签，下次记得可以用正则表达式来把不规范的输入转化成正确的数组。 分组 除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用 () 表示的就是要提取的分组（Group）。比如： ^(\\d{3})-(\\d{3,8})$ 分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码： 123456789&gt;&gt;&gt; m = re.match(r'^(\\d{3})-(\\d{3,8})$', '010-12345')&gt;&gt;&gt; m&lt;_sre.SRE_Match object; span=(0, 9), match='010-12345'&gt;&gt;&gt;&gt; m.group(0)'010-12345'&gt;&gt;&gt; m.group(1)'010'&gt;&gt;&gt; m.group(2)'12345' 如果正则表达式中定义了组，就可以在 Match 对象上用 group() 方法提取出某个子串。注意到 group(0) 永远是原始字符串，group(1)、group(2) …… 表示第1、2、……个子串。 提取子串非常有用。来看一个更凶残的例子： 1234&gt;&gt;&gt; t = '19:05:30'&gt;&gt;&gt; m = re.match(r'^(0[0-9]|1[0-9]|2[0-3]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$', t)&gt;&gt;&gt; m.groups()('19', '05', '30') 这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期： 1'^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$' 对于'2-30'，'4-31'这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要编写其他代码来配合识别了。 贪婪匹配 最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： 12&gt;&gt;&gt; re.match(r'^(\\d+)(0*)$', '102300').groups()('102300', '') 由于 \\d+ 采用贪婪匹配，直接把后面的0全部匹配了，结果 0* 就只能匹配到空字符串了。 必须让 \\d+ 采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，这时我们可以使用 ? 来让 \\d+ 采用非贪婪匹配： 12&gt;&gt;&gt; re.match(r'^(\\d+?)(0*)$', '102300').groups()('1023', '00') 编译 当我们在Python中使用正则表达式时，re 模块内部会干两件事情： 编译正则表达式，如果正则表达式的字符串本身不合法，会报错； 用编译后的正则表达式去匹配字符串。 如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译正则表达式，接下来重复使用时就不需要编译这个步骤了，可以直接匹配： 12345678&gt;&gt;&gt; import re# 编译:&gt;&gt;&gt; re_telephone = re.compile(r'^(\\d{3})-(\\d{3,8})$')# 使用：&gt;&gt;&gt; re_telephone.match('010-12345').groups()('010', '12345')&gt;&gt;&gt; re_telephone.match('010-8086').groups()('010', '8086') 编译后生成 Regular Expression 对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。 小结 正则表达式非常强大，要在短短的一节里讲完是不可能的。要讲清楚正则的所有内容，可以写一本厚厚的书了。如果你经常遇到正则表达式的问题，你可能需要一本正则表达式的参考书。 练习 习题一 请尝试写一个验证Email地址的正则表达式，可以验证类似以下格式的Email： 12someone@gmail.combill.gates@microsoft.com 代码： 12345678910import rere_email = re.compile(r'^[a-z.]+?@[a-z]+?.com$')while True: test = input('\\nPlease input your email address: ') if re_email.match(test): print('ok') else: print('failed') 习题二 继续上一题，但这次的Email地址带名字，要既能验证地址又能提取出名字： 12&lt;Tom Paris&gt; tom@voyager.com&lt;Mary Liu&gt; mary@microsoft.com 代码： 1234567891011import rere_email = re.compile(r'^&lt;([A-Za-z\\s]+?)\\s([A-Za-z\\s]+?)&gt;\\s([a-z.]+?@[a-z]+?.com)$')while True: test = input('\\nPlease input your email address: ') match = re_email.match(test) if match: print(match.group(1)+' '+match.group(2)+&quot;'s email address is: &quot;+match.group(3)) else: print('failed')","link":"/posts/46563.html"},{"title":"包","text":"package与import 在实际的开发中，我们往往需要在不同的文件中，去调用其它文件的定义的函数，比如 main.go 中，需要使用\"fmt\"包中的Println()函数： 12package mainimport &quot;fmt&quot; 在Go中，Go的每一个文件都是属于一个包的，也就是说Go是以包的形式来管理文件和项目目录结构。所以如果要导入某些第三方包，直接输入包所在地址即可。文件的包名通常和文件所在的文件夹名一致，一般为小写字母。 123456引入方式 1:import &quot;包名&quot;引入方式 2:import ( &quot;包名&quot; &quot;包名&quot; ) package 指令在 文件第一行，然后是 import 指令 在 import 包时，路径从 $GOPATH 的 src 下开始，不用带 src , 编译器会自动从 src 下开始引入 为了让其它包的文件，可以访问到本包的函数，则该函数名的首字母需要大写，类似其它语言 的 public ,这样才能跨包访问 在访问其它包函数，变量时，其语法是 包名.函数名 GoPath GoPath目录用来存放代码文件、可运行文件、编译后的包文件。 从1.1版本到1.7必须设置这个变量，而且不能和Go的安装目录一样，1.8版本后会有默认值： 12Unix:$HOME/goWindows:%USERPROFILE%/go。 GOPATH允许多个目录，多个目录的时候Windows是分号，Linux系统是冒号隔开。当有多个GOPATH时，默认会将go get的内容放在第一个目录下，\\(GOPATH 目录约定有三个子目录： - src:存放源代码，一般一个项目分配一个子目录; - pkg:编译后生成的文件，如.a文件 - bin:编译后生成的可执行文件,可以加入\\)PATH中 &gt;注意：一般建议package的名称和目录名保持一致 1.3 包中的函数调用方式 函数调用的方式： - 同包下：直接调用即可 - 不同包下：包名.函数名 go mod go mod 的使用 go的项目依赖管理一直饱受诟病，在go1.11后正式引入了go mod功能，类似nodejs的npm。在go1.13版本中将会默认启用。 使用Gomod可以让项目完全摆脱GOPATH的困扰。 go mod 初步使用： 123456789101112# 开启go modexport GO111MODULE=on# 在新建的项目根目录下（src）下使用该命令go mod init 项目名 # 此时会生成一个go.mod文件# 使用在项目中可以随时import依赖，当 go run 时候，会自动安装依赖，比如：import ( &quot;github.com/gin-gonic/gin&quot;) go run 后的 go.mod: 12345678910111213module api_servergo 1.12require ( github.com/gin-contrib/sse v0.0.0-20190301062529-5545eab6dad3 // indirect github.com/gin-gonic/gin v1.3.0 // indirect github.com/golang/protobuf v1.3.1 // indirect github.com/mattn/go-isatty v0.0.7 // indirect github.com/ugorji/go/codec v0.0.0-20190320090025-2dc34c0b8780 // indirect gopkg.in/go-playground/validator.v8 v8.18.2 // indirect gopkg.in/yaml.v2 v2.2.2 // indirect) 使用go mod后，run产生的依赖源码不会安装在当前项目中，而是安装在：$GOPATH/pkg/mod 详细使用 详细使用地址：https://zhuanlan.zhihu.com/p/59687626 翻墙问题解决 推荐方式 GOPROXY 从 Go 1.11 版本开始，还新增了 GOPROXY 环境变量，如果设置了该变量，下载源代码时将会通过这个环境变量设置的代理地址，而不再是以前的直接从代码库下载。goproxy.io 这个开源项目帮我们实现好了我们想要的。该项目允许开发者一键构建自己的 GOPROXY 代理服务。同时，也提供了公用的代理服务 https://goproxy.io，我们只需设置该环境变量即可正常下载被墙的源码包了： 12345678910# 开发时设置Goland的Prefrence-Go-proxy即可# linux开启代理export GOPROXY=https://goproxy.io # 注意：必须开启go mod才能使用# win开启代理$env:GOPROXY = &quot;https://goproxy.io&quot;# 关闭代理export GOPROXY= replace方式 从 Go 1.11 版本开始，新增支持了 go modules 用于解决包依赖管理问题。该工具提供了 replace，就是为了解决包的别名问题，也能替我们解决 golang.org/x 无法下载的的问题。 go module 被集成到原生的 go mod 命令中，但是如果你的代码库在 $GOPATH 中，module 功能是默认不会开启的，想要开启也非常简单，通过一个环境变量即可开启 export GO111MODULE=on。 123456789module example.com/hellorequire ( golang.org/x/text v0.3.0)replace ( golang.org/x/text =&gt; github.com/golang/text v0.3.0) 手动下载 旧版go的解决 我们常见的 golang.org/x/... 包，一般在 GitHub 上都有官方的镜像仓库对应。比如 golang.org/x/text 对应 github.com/golang/text。所以，我们可以手动下载或 clone 对应的 GitHub 仓库到指定的目录下。 mkdir $GOPATH/src/golang.org/x cd $GOPATH/src/golang.org/x git clone git@github.com:golang/text.git rm -rf text/.git 当如果需要指定版本的时候，该方法就无解了，因为 GitHub 上的镜像仓库多数都没有 tag。并且，手动嘛，程序员怎么能干呢，尤其是依赖的依赖，太多了。 go mod引起的变化 引包方式变化： - 不使用go mod 引包：\"./test\" 引入test文件夹 - 使用go mod 引包：\"projectmodlue/test\" 使用go.mod中的modlue名/包名 因为在go1.11后如果开启了go mod，需要在src目录下存在go.mod文件，并书写主module名（一般为项目名），否则无法build。 开启go mod编译运行变化： - 使用vscode开发，必须在src目录下使用 go build命令执行，不要使用code runner插件 - 使用IDEA开发，项目本身配置go.mod文件扔不能支持，开发工具本身也要开启go mod支持（位于配置的go设置中）","link":"/posts/38068.html"},{"title":"面向对象初识","text":"模拟构造函数 Go和传统的面向对象语言如Java有着很大区别。结构体没有构造函数初始化功能，可以通过以下方式模拟： 1234567891011121314151617181920212223242526272829package mainimport ( &quot;fmt&quot;)type Person struct { Name string Age int}func NewPersonByName(name string) *Person { return &amp;Person{ Name: name, }}func NewPersonByAge(age int) *Person { return &amp;Person{ Age: age, }}func main() { p := NewPersonByName(&quot;zs&quot;) fmt.Println(p) // {zs 0}} 贴士：因为Go没有函数重载，为了避免函数名字冲突，使用了NewPersonByName和NewPersonByAge两个不同的函数表示不同的Person构造过程。 父子关系结构体初始化 Person可以看做父类，Student是子类，子类需要继承父类的成员，该如何处理？ 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( &quot;fmt&quot;)type Person struct { Name string Age int}type Student struct { Person ClassName string}//构造父类func NewPerson(name string, age int) *Person { return &amp;Person{ Name: name, Age: age, }}//构造子类func NewStudent(classname string) *Student { p := &amp;Student{} p.ClassName = classname return p}func main() { s := NewStudent(&quot;一班&quot;) fmt.Println(s) // &amp;{{ 0} 一班}} 我们发现Student中的Person也被一并实例化了。Go中没有提供构造函数相关的特殊机制，用户根据自己的需求，将参数使用函数传递到结构体构造参数中即可完成构造函数的任务。 Go中的面向对象初识 在Go中，可以给任意类型（除了指针）添加相应方法： 12345678910type Interger intfunc (i Interger) Less (j Interger) bool { return i &lt; j}func main() { var i Interger = 1 fmt.Print(i.Less(5))} 方法 方法的定义 面向过程的函数书写案例： 12345678910111213141516171819202122package mainimport ( &quot;fmt&quot;)type Person struct { Name string Age int}func run(p *Person, name string) { p.Name = name fmt.Printf(&quot;%s is runnig...\\n&quot;, p.Name)}func main() { p1 := &amp;Person{} run(p1, &quot;zs&quot;)} 在某些情况下，我们要需要声明(定义)方法。比如 Person 结构体:除了有一些字段外( 年龄，姓名..),Person 结构体还有一些行为比如:可以说话、跑步..,通过学习，还可以做算术题，这时就要用方法才能完成。 Golang 中的方法是作用在指定的数据类型上的(即:和指定的数据类型绑定)，因此自定义类型，都可以有方法，而不仅仅是 struct。 方法的声明和调用： 1234func (recevier type) methodName(参数列表) (返回值列表){ //方法体 return 返回值} 示例代码： 123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot;)type Person struct { Name string Age int}func (p *Person) run() { // p为接收器 fmt.Printf(&quot;%s is runnig...\\n&quot;, p.Name)}func main() { //方式一 p1 := Person{ Name: &quot;Tom&quot;, Age: 18, } p1.run() //方式二 p2 := new(Person) p2.Name = &quot;Jerry&quot; p2.run()} Go方法本质 Go的方法是一种作用于特定类型变量的函数，这种特定类型的变量叫做接收器（Receiver）。如果特定类型理解为结构体或者“类”时，接收器就类似于其他语言的this或者self。 在Go中，接收器可以是任何类型，不仅仅是结构体。 依此我们看出，Go中的方法和其他语言的方法类似，但是Go语言的接收器强调方法的作用对象是实例。 方法与函数的区别就是：函数没有作用对象。 理解接收器 上述Person案例中，接收器类型是*Person，属于指针类型，非常接近Java中的this，由于指针的特性，调用方法时，修改接收器指针的任意长远变量，在方法结束后，修改都是有效的。 当方法作用于非指针接收器时，Go语言会在代码运行时将接收器的值复制一份，在非指针接收器的方法中可以获取接收器的成员值，但修改后无效，如下所示： 1234567891011121314151617181920212223242526272829package mainimport &quot;fmt&quot;//定义一个表示点的结构体type Point struct { X int Y int}//非指针接收器func (p Point) Add(otherP Point) Point { return Point{ p.X + otherP.X, p.Y + otherP.Y, }}func main() { p1 := Point{1, 1} p2 := Point{2, 2} result := p1.Add(p2) fmt.Println(result) // {3 3}} 一般情况下，小对象由于复制时速度较快，适合使用非指针接收器，大对象因为复制性能较低，适合使用指针接收器，此时再接收器和参数之间传递时不进行复制，只传递指针。","link":"/posts/36489.html"},{"title":"方法与函数的混合使用","text":"Go语言可以将类型的方法与普通函数作为一个概念，从而简化方法和函数混合作为回调类型时的复杂性。该特性与C#中的delegate类似，调用者无须关心谁来支持调用，系统会自动处理是否调用普通函数还是类型方法。 以下案例将实现一个普通函数FuncDo，一个类型方法ClassDo，二者签名一致： 123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;fmt&quot;)type class struct {}//方法func (c *class) ClassDo(v int) { fmt.Println(&quot;class do: &quot;, v)}//函数func FuncDo(v int) { fmt.Println(&quot;func do: &quot;, v)}func main() { //声明一个函数回调 var delegate func(int) c := new(class) //将回调设为c的类型方法 delegate = c.ClassDo delegate(100) // class do: 100 //将回调设为c的普通函数 delegate = FuncDo delegate(50) // func do: 50} 事件系统原理 事件系统可以将事件派发者与事件处理者解耦，例如网络底层可以生成各种事件，在网络连接上后，网络底层只需要将事件派发出去，而不需要关心到底哪些代码来响应连接上的逻辑。 一个事件系统拥有如下特征： - 能够实现事件的一方，可以根据事件ID或名字注册对应的事件 - 事件发起者，会根据注册信息通知这些注册者 - 一个事件可以有多个实现方响应 事件系统开发 事件注册 事件系统需要为外部提供一个注册入口，这个注册入口传入注册的事件名称和对应事件名称的响应函数，事件注册的过程就是将事件名称和响应函数关联并保存起来。 1234567891011121314151617//实例化一个通过字符串映射函数切片的mapvar EventMap = make(map[string][]func(interface{}))//注册事件，提供事件名和回调函数func RegistEvent(eventName string, callback func(interface{})) { //通过名字查找事件列表 eventList := EventMap[eventName] //在列表切片中添加函数 eventList = append(eventList, callback) //保存修改的事件列表切片 EventMap[eventName] = eventList } 事件调用 事件调用方是事发现场，负责将事件和事件发生的参数通过事件系统派发出去，而不关心事件到底由谁处理。 12345678910111213//调用事件func CallEvent(eventName string, param interface{}) { //通过名称找到事件列表 eventList := EventMap[eventName] //遍历这个事件的所有回调 for _, callback := range eventList { callback(param) }} 事件调用 1234567891011121314151617181920212223242526//声明角色的结构体type Actor struct {}//为角色添加一个事件处理函数func (a *Actor) Test(param interface{}) { fmt.Println(&quot;test event: &quot;, param)}//全局事件func GlobalEvent(param interface{}) { fmt.Println(&quot;global event: &quot;, param)}func main() { a := new(Actor) RegistEvent(&quot;OnSkill&quot;, a.Test) //注册名为OnSkill的事件 RegistEvent(&quot;OnSkill&quot;, GlobalEvent) //再次注册全局事件 CallEvent(&quot;OnSkill&quot;, 100)}上述事件按照注册的顺序被触发。","link":"/posts/63864.html"},{"title":"面向对象三大特性","text":"封装 封装就是把抽象出的字段和对字段的操作封装在一起,数据被保护在内部,程序的其它包只有通过被授权的操作(方法),才能对字段进行操作。 通过封装，可以： - 隐藏实现细节 - 可以对数据进行验证，保证安全合理 封装的实现步骤： 1234567891011121 将结构体、字段(属性)的首字母小写(私有化)；2 给结构体所在包提供一个工厂模式的函数，首字母大写，类似一个构造函数；3 提供一个首字母大写的 Set 方法(类似其它语言的 public)，用于对属性判断并赋值： func (var 结构体类型名) SetXxx(参数列表) (返回值列表) { //加入数据验证的业务逻辑 var.字段 = 参数 }1) 提供一个首字母大写的 Get 方法(类似其它语言的 public)，用于获取属性的值： func (var 结构体类型名) GetXxx() { return var.age }特别说明:在 Golang 开发中并没有特别强调封装，这点并不像 Java. 所以提醒学过 java 的朋友， 不用总是用 java 的语法特性来看待 Golang, Golang 本身对面向对象的特性做了简化的. 123456789101112131415161718192021222324252627282930313233343536373839404142//person.gopackage modelimport &quot;fmt&quot;type person struct { Name string age int //年龄是隐私，不允许其他包访问}//工厂函数（类似构造函数）func Person(name string) *person { return &amp;person{ Name: name, }}func (p *person) SetAge(age int) { if age &gt; 0 &amp;&amp; age &lt; 150 { //校验 p.age = age } else { fmt.Println(&quot;年龄不合法&quot;) }}func (p *person) GetAge() int { return p.age}//main.gopackage mainimport ( &quot;./model&quot; &quot;fmt&quot;)func main() { p := model.Person(&quot;Tom&quot;) p.SetAge(18) fmt.Println(p)} 继承 在 Golang 中，如果一个 struct 嵌套了另一个匿名结构体，那么这个结构体可以直接访 问匿名结构体的字段和方法，从而实现了继承特性。 1234567891011121314151617181920212223242526272829303132package mainimport ( &quot;fmt&quot;)type Father struct { Name string age int}func (f *Father) run() { fmt.Println(f.Name + &quot; like running...&quot;)}type Son struct { Father //嵌套匿名结构体}func main() { var s Son //s.Father.Name = &quot;Tom&quot; //s.Father.age = 10 //可以访问未导出属性 //s.Father.run() //可以访问未导出方法 //上述可以简写为： s.Name = &quot;Tom&quot; s.age = 10 s.run()} 注意： - 当结构体和匿名结构体有相同的字段或者方法时，编译器采用就近访问原则访问，如希望访问 匿名结构体的字段和方法，可以通过匿名结构体名来区分。 - 结构体嵌入两个(或多个)匿名结构体，如两个匿名结构体有相同的字段和方法(同时结构体本身 没有同名的字段和方法)，在访问时，就必须明确指定匿名结构体名字，否则编译报错。 - 如果一个 struct 嵌套了一个有名结构体，这种模式就是组合，如果是组合关系，那么在访问组合的结构体的字段或方法时，必须带上结构体的名字。 关于多重继承： 如果一个 struct 嵌套了多个匿名结构体，那么该结构体可以直接访问嵌套的匿名结构体的字段和方法，从而实现了多重继承。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package mainimport ( &quot;fmt&quot;)type Father1 struct { Name string age int}func (f *Father1) run() { fmt.Println(f.Name + &quot; like running...&quot;)}type Father2 struct { Like string}type Son1 struct { Father1 Father2}type Son2 struct { *Father1 *Father2}func main() { s1 := Son1 { Father1{ Name: &quot;Tom&quot;, age: 10, }, Father2{ Like: &quot;伏特加&quot;, }, } fmt.Println(s1) s2 := Son2{ &amp;Father1{ Name: &quot;Tom&quot;, age: 10, }, &amp;Father2{ Like: &quot;伏特加&quot;, }, } fmt.Println(*s2.Father1)} 如嵌入的匿名结构体有相同的字段名或者方法名，则在访问时，需要通过匿名结构体类型名来 区分。为了保证代码的简洁性，建议大家尽量不使用多重继承。 多态 多态与接口（interface）有关联，参见接口章节","link":"/posts/15667.html"},{"title":"接口 interface","text":"接口定义 接口（interface）是调用方和实现方均需要遵守的一种约束，大家按照统一的方法命名、参数类型和数量来协调逻辑处理的过程。实际上，接口就是一组不需实现的方法声明，不能包含任何变量。到某个自定义类型要使用的时候,在根据具体情况把这些方法写出来(实现)。 Go的接口是非侵入式设计的，接口编写者无需知道接口被哪些类型实现，接口实现者只需要知道实现的是什么样子的接口，但无需指明实现了哪个接口。编译器知道最终编译时使用哪个类型实现哪个接口，或者接口应该由谁来实现。 接口语法： 12345type 接口类型名 interface { 方法名1(参数列表) 返回值列表 方法名2(参数列表) 返回值列表 ...} 注意： - Go语言的接口在命名时，一般会在单词后面添加er，如写操作的接口叫做Writer - 当方法名首字母大写，且实现的接口首字母也是大写，则该方法可以被接口所在包之外的代码访问 - 参数列表和返回值列表中的变量名可以被忽略，如：type writer interfae{ Write([]byte) error} - Go标准包，每个接口包含的方法很少，Go希望一个接口精准描述它自己的功能。 接口实现 接口实现的条件： - 方法与接口中的方法签名一致（方法名、参数列表、返回列表都必须一致） - 接口中所有的方法都必须被实现 示例： 123456789101112131415161718192021222324252627//定义一个数据写入接口type DataWriter interface { WriteData(data interface{}) error}//定义文件对象，实现WriteData()方法type file struct {}//实现接口func (f *file) WriteData(data interface{}) error { fmt.Print(&quot;WriteData: &quot;, data) //模拟写入 return nil}func main() { f := new(file) var writer DataWriter //声明一个DataWriter接口 writer = f //将接口赋值给f，即*file类型 writer.WriteData(&quot;data...&quot;) //模拟数据写入} 注意： - 如果在编译到writer = f发现实现接口的方法签名不一致，则会报错：does not implement。 - 如上所示，Go无须像Java那样显式声明实现了哪个接口，即为非侵入式。 - 使用writer接口调用了接头体file的方法，也可以理解为实现了面向对象中的多态 接口与类型的关系 类型和接口之间有一对多和多对一的关系，即： - 一个类型可以实现多个接口，接口间是彼此独立的，互相不知道对方的实现 - 多个类型也可以实现相同的接口。 123456789101112131415161718192021222324252627type Service interface { Start() Log(string)}// 日志器type Logger struct {}//日志输出方法func (g *Logger) Log(s string){ fmt.Println(&quot;日志：&quot;, s)}// 游戏服务type GameService struct { Logger}// 实现游戏服务的Start方法func (g *GameService) Start() { fmt.Println(&quot;游戏服务启动&quot;)}func main() { s := new(GameService) s.Start() s.Log(&quot;hello&quot;)} 在上述案例中，即使没有接口也能运行，但是当存在接口时，会隐式实现接口，让接口给类提供约束。 接口嵌套 Go中不仅结构体之间可以嵌套，接口之间也可以嵌套。接口与接口嵌套形成了新的接口，只要接口的所有方法被实现，则这个接口中所有嵌套接口的方法均可以被调用。 123456789101112type Writer interface { Write(p []byte) (n int, e error)}type Closer interface { Close() error}type WriteCloser interface { Writer Closer} 接口断言 类型断言定义 Go中使用接口断言（type assertions）将接口转换成另外一个接口，也可以将接口转换为别的类型，这是非常常见的使用。 类型断言格式: 12t := i.(T) //不安全写法：如果i没有完全实现T接口的方法，这个语句将会触发宕机t, ok := i.(T) // 安全写法：如果接口未实现接口，将会把ok掷为false，t掷为T类型的0值 - i代表接口变量 - T代表转换的目标类型 - t代表转换后的变量 接口转换为其他接口 实现某个接口的类型同时实现了另外一个接口，此时可以在两个接口间转换。 鸟和猪都具有不同的特性，鸟可以飞，猪不能飞，但是二者都可以走，如果使用结构体实现鸟和猪，让他具备各自的特性Fly()和Walk()方法就让鸟和猪各自实现了飞行动物接口Flyer和行走动物接口Walker。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364//飞行动物接口type Flyer interface { Fly()}//行走动物接口type Walker interface { Walk()}//鸟类type bird struct {}//鸟类实现飞行动物接口func (b *bird) Fly() { fmt.Println(&quot;bird: fly&quot;)}//鸟类实现行走动物接口func (b *bird) Walk() { fmt.Println(&quot;bird: walk&quot;)}//猪类type pig struct {}//猪类实现行走动物接口，没有实现飞行接口func (p *pig) Walk() { fmt.Println(&quot;pig: walk&quot;)}func main() { //创建动物名字到实例的映射 animals := map[string]interface{}{ &quot;bird&quot;: new(bird), &quot;pig&quot;: new(pig), } for name, obj := range animals { // 断言：判断对象是否是飞行动物，行走动物 f, isFlyer := obj.(Flyer) w, isWalker := obj.(Walker) fmt.Printf(&quot;name is %s \\n&quot;, name) if isFlyer { f.Fly() } if isWalker { w.Walk() } } } 将接口转换为其他类型 在上述代码中，可以实现将接口转换为普通的指针类型，例如将Walker接口转换为*pig类型： 1234567p1 := new(pig)var a Walker = p1p2 := a.(*pig)fmt.Printf(&quot;p1=%p p2=%p&quot;, p1, p2) //发现二者相同 类型查询 在Go中，可以直接询问接口指向的对象实例的类型： 123456var v1 interfaceP{} = ...switch v := v1.(type) { case int: case string: ...} 接口实现多态 多态是面向对象的三大特性之一。 示例：动物都具备Move的动作，如果是鸟类的Move，则是飞翔，如果是鱼类的Move则是游泳 12345678910111213141516171819202122232425262728293031323334353637type Animal interface { Move()}type Bird struct { Name string}func (b *Bird) Move() { fmt.Println(&quot;bird fly...&quot;)}type Fish struct { Name string}func (f *Fish) Move() { fmt.Println(&quot;fish swim...&quot;)}func Factory(name string) Animal { switch name { case &quot;bird&quot;: return &amp;Bird{} case &quot;fish&quot;: return &amp;Fish{} default: return nil }}func main() { a := Factory(&quot;bird&quot;) a.Move()} 空接口 空接口定义 空接口是接口的特殊形式，没有任何方法，因此任何类型都无须实现空接口，故而空接口可以保存任何值，（可以简单的将空接口理解为Java中的Object）。 1234567var any interface{}any = 1fmt.Println(any)any = &quot;hello&quot;fmt.Println(any) 从空接口获取值 保存到空接口的值，如果直接取出指定类型的值时，会发生编译错误： 123var a int = 1var i interface{} = avar b int = i //这里编译报错（类型不一致），可以这样做：b := i 空接口值比较 类型不同的空接口比较： 1234var a interface{} = 100var b interface{} = &quot;hi&quot;fmt.Println(a == b) //false 不能比较空接口中的动态值： 123var c interface{} = []int{10}var d interface{} = []int{20}fmt.Println(c == d) //运行报错 空接口的类型和可比较性： | 类型 | 说明 | | ---- | ---- | | map | 不可比较，会发生宕机错误 | | 切片 | 不可比较，会发生宕机错误 | | 通道 | 可比较，必须由同一个make生成，即同一个通道才是true | | 数组 | 可比较，编译期即可知道是否一致 | | 结构体 | 可比较，可诸葛比较结构体的值 | | 函数 | 可比较 |","link":"/posts/19438.html"},{"title":"反射简介","text":"反射是指在程序运行期对程序本身进行访问和修改的能力。即可以在运行时动态获取变量的各种信息，比如变量的类型（type），类别（kind），如果是结构体变量，还可以获取到结构体本身的信息（字段与方法），通过反射，还可以修改变量的值，可以调用关联的方法。 反射常用在框架的开发上，一些常见的案例，如JSON序列化时候tag标签的产生，适配器函数的制作等，都需要用到反射。 贴士： - C，C++没有支持反射功能，只能通过 typeid提供非常弱化的程序运行时类型信息。 - Java、 C#等语言都支持完整的反射功能。 - Lua、 JavaScript类动态语言，由于其本身的语法特性就可以让代码在运行期访问程序自身的值和类型信息，因此不需要反射系统 。 Go程序的反射系统无法获取到一个可执行文件空间中或者是一个包中的所有类型信息，需要配合使用标准库中对应的词法、语法解析器和抽象语法树( AST) 对源码进行扫描后获得这些信息 。 Go中反射相关的包是reflect。 反射操作数据 反射初识 12345var a intfmt.Println(reflect.ValueOf(a)) // 0 变量值fmt.Println(reflect.TypeOf(a)) // int 变量类型对象名，其类型为 reflect.Type()fmt.Println(reflect.TypeOf(a).Name()) // int 变量类型对象的类型名fmt.Println(reflect.TypeOf(a).Kind()) // int 变量类型对象的种类名 编程中，使用最多的是类型，但在反射中，当需要区分一个大品种的类型时，就会用到种类(Kind)。例 如，需要统一判断类型中的指针时，使用种类 (Kind)信息就较为方便，即： - Type是系统原生数据类型： int、 string、 boo!、 float32 ，以及 type 定义的类型，对应的反射获取方法是 reflect.Type 中 的 Name() Kind是对象归属的品种：Int、Bool、Float32、Chan、String、Struct、Ptr（指针）、Map、Interface、Fune、Array、Slice、Unsafe Pointer等 操作简单数据类型 12345678var num int64 = 100rValue := reflect.ValueOf(num)//第一种运算方式fmt.Println(num + rValue.Int()) //200//第二种运算方式fmt.Println(num + rValue.Interface().(int64)) //200 反射操作指针 123456789101112type cat struct {}c := &amp;cat{}typeOfCat := reflect.TypeOf(c)fmt.Println(&quot;name: &quot;, typeOfCat.Name()) // 空fmt.Println(&quot;kind: &quot;, typeOfCat.Kind()) // ptrtypeOfCat = typeOfCat.Elem()fmt.Println(&quot;element name: &quot;, typeOfCat.Name()) // catfmt.Println(&quot;element kind: &quot;, typeOfCat.Kind()) // struct 反射操作结构体 通过reflect.TypeOf(）获取到结构体的对象信息后，可以通过 反射值对象( reflect.Type)的 NumField()和 Field()方法获得结构体成员的详细信息。 reflect.Type 的 Field()方法返回 StructField 结构： 123456789type StructField struct { Name string //字段名 PkgPath string //字段路径 Type Type //字段反射类型对象 Tag StructTag //字段的结构体标签 Offset uintptr //字段在结构体中的相对偏移 Index []int //Type.FielddByindex 中的返回的索引值 Anonymous bool //是否为匿名字段} 示例： 12345678910111213141516171819202122type Student struct { Name string Age int `json:&quot;age&quot; id:&quot;100&quot;` // 结构体标签}s := Student{ Name: &quot;zs&quot;, Age: 1,}typeOfStudent := reflect.TypeOf(s)for i := 0; i &lt; typeOfStudent.NumField(); i++ { fieldType := typeOfStudent.Field(i) fmt.Println( fieldType.Name, fieldType.Tag) // Name Age json: &quot;age&quot; id:&quot;100&quot;}fmt.Println(&quot;-------------------&quot;)if studentAge, ok := typeOfStudent.FieldByName(&quot;Age&quot;); ok { fmt.Println(studentAge.Tag.Get(&quot;json&quot;), studentAge.Tag.Get(&quot;id&quot;)) // age 100} 通过类型创建类型的实例 当己知 reflect.Type 时，可以动态地创建这个类型的实例，实例的类型为指针。例如reflect.Type的类型为 int时，创建 int的指针，即*int： 1234var a inttypeOfA := reflect.TypeOf(a)// 创建实例aIns := reflect.New(typeOfA) 使用反射调用函数 如果反射值对象(reflect.Value)中值的类型为函数时，可以通过 reflect.Value调用该 函数。使用反射调用函数时，需要将参数使用反射值对象的切片 口reflect.Value 构造后传入 Call()方法中 ， 调用完成时，函数的返回值通过 []reflect.Value 返回 。 123456funcValue : = reflect .ValueOf(add)paramList := []reflect. Value {reflect. ValueOf (1 0) , reflect.Va l u e O f ( 2 0 ) }// 调用函数retList : = funcValue . Call (paramL工st )","link":"/posts/45341.html"},{"title":"Binary Tree Preorder Traversal","text":"题目地址 https://leetcode.com/problems/binary-tree-preorder-traversal/description/ 题目描述 1234567891011121314Given a binary tree, return the preorder traversal of its nodes' values.Example:Input: [1,null,2,3] 1 \\ 2 / 3Output: [1,2,3]Follow up: Recursive solution is trivial, could you do it iteratively? 思路 这道题目是前序遍历，这个和之前的leetcode 94 号问题 - 中序遍历完全不一回事。 前序遍历是根左右的顺序，注意是根开始，那么就很简单。直接先将根节点入栈，然后 看有没有右节点，有则入栈，再看有没有左节点，有则入栈。 然后出栈一个元素，重复即可。 其他树的非递归遍历课没这么简单 关键点解析 二叉树的基本操作（遍历） &gt; 不同的遍历算法差异还是蛮大的 如果非递归的话利用栈来简化操作 如果数据规模不大的话，建议使用递归 递归的问题需要注意两点，一个是终止条件，一个如何缩小规模 终止条件，自然是当前这个元素是null（链表也是一样） 由于二叉树本身就是一个递归结构， 每次处理一个子树其实就是缩小了规模， 难点在于如何合并结果，这里的合并结果其实就是mid.concat(left).concat(right), mid是一个具体的节点，left和right递归求出即可 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/* * @lc app=leetcode id=144 lang=javascript * * [144] Binary Tree Preorder Traversal * * https://leetcode.com/problems/binary-tree-preorder-traversal/description/ * * algorithms * Medium (50.36%) * Total Accepted: 314K * Total Submissions: 621.2K * Testcase Example: '[1,null,2,3]' * * Given a binary tree, return the preorder traversal of its nodes' values. * * Example: * * * Input: [1,null,2,3] * ⁠ 1 * ⁠ \\ * ⁠ 2 * ⁠ / * ⁠ 3 * * Output: [1,2,3] * * * Follow up: Recursive solution is trivial, could you do it iteratively? * *//** * Definition for a binary tree node. * function TreeNode(val) { * this.val = val; * this.left = this.right = null; * } *//** * @param {TreeNode} root * @return {number[]} */var preorderTraversal = function(root) { // 1. Recursive solution // if (!root) return []; // return [root.val].concat(preorderTraversal(root.left)).concat(preorderTraversal(root.right)); // 2. iterative solutuon if (!root) return []; const ret = []; const stack = [root]; let left = root.left; let t = stack.pop(); while(t) { ret.push(t.val); if (t.right) { stack.push(t.right); } if (t.left) { stack.push(t.left); } t =stack.pop(); } return ret;};","link":"/posts/3510445862.html"},{"title":"4种不同颜色的色块，它们的颜色真的不同吗","text":"背景介绍 在上面这幅背景为灰色的图片中（使用turtle绘制），我们看到了4种不同颜色的色块。它们的颜色真的不同吗？ 答案是否定的。 这里的灰色实际上是很小的蓝色和黄色像素的混合产物。由于这些像素太小，混合在一起不会引发拮抗过程，也就无法形成对比。 彩色电视机之所以能够利用颜色差异很小的像素呈现不同色彩就是这个原理。 (感兴趣的读者可以用放大镜亲自验证一下)绿松石色和淡黄绿色色块实际上分别由很小的绿色像素与蓝色背景像素混合和与黄色背景像素混合而来。红色像素与背景中的黄色像素混合形成橙色，与背景中的蓝色像素混合则成紫色。 导入模块 123import turtleimport randomimport time API说明 定义自己的画图类MyTurtle，常用方法介绍如下： get_color() 随机取rgb模式下的颜色的三个参数 set_pen_color() 设置画笔使用颜色 move(x, y) 控制海龟移动到指定x,y坐标(移动轨迹不着色) draw_shape(sides, length)画 sides 条边，length 长度的图形 draw_square(length) 调用 draw_shape() 函数画边长为 length 正方形 draw_triangle(length)调用 draw_shape() 函数画边长为 length 等边三角形draw_circle(length)调用 draw_shape() 函数画半径为 length 圆形 fill_color_shape(shape, length, fill_color)用 fill_color 颜色, 填充 length 边长, shape （draw_square, draw_triangle, draw_circle）设定的图形 此例子使用的 turtle 内置函数介绍 &gt;left(x)/right(x) 使海龟 逆时针/顺时针 旋转 &gt;circle(x)以x为半径，以当前方向开始画圆 &gt;forward(x)|fd(x) 前进x，单位为像素 &gt;backward(x)|bk(x)|back(x) 后退x，单位为像素 &gt;goto(x,y)|setpos(x,y)|setposition(x,y)使海龟沿直线移动到(x,y)坐标处 &gt;setx(x)/sety(y)设置 x/y 坐标，使海龟水平移动 &gt;dot(size,color)在当前位置以直径为size画点，颜色为color &gt;speed(n) 设置海龟的移动速度 &gt;pendown()|pd()|down() 使海龟“落地”，移动轨迹在屏幕上显示 &gt;penup()|up() 使海龟“起飞”,移动轨迹在屏幕上不显示 &gt;pensize(x) 设置画笔宽度为x，单位为像素 &gt;pencolor(color) 设置画笔颜色 &gt;fillcolor() 设置填充颜色，当轨迹形成闭合图形时填充的颜色 &gt;begin_fill()/end_fill() 控制颜色填充的时间段，只有在begin_fill()与end_fill()中间的闭合图形才会填充颜色 自定义绘图类 MyTurtle 123456789101112131415161718192021222324252627282930313233343536373839404142434445class MyTurtle(turtle.Turtle): def get_color(self): rgb = [] for i in range(3): rgb.append(random.randint(0, 255)) return rgb def set_pen_color(self): ''' ## 设置画笔的颜色 + ### colormode(cmode) &gt;- cmode 为1 或者 255， &gt;- 随后rgb三元组的值必须在0~comde之间 + ### pencolor(color=None) &gt; 设置画笔颜色 ''' self.screen.colormode(255) self.pencolor(self.get_color()) def move(self, x: int, y: int) -&gt; None: self.penup() self.goto(x, y) self.pendown() def draw_shape(self, sides: int, length: int) -&gt; None: _angle = 360.0/sides for side in range(sides): self.forward(length) self.left(_angle) def draw_square(self, length: int) -&gt; None: self.draw_shape(4, length) def draw_triangle(self, length: int) -&gt; None: self.draw_shape(3, length) def draw_circle(self, length: int) -&gt; None: self.draw_shape(360, length) def fill_color_shape(self, shape: str, length: int, fill_color: str) -&gt; None: _command = f'self.{shape}(length)' self.begin_fill() self.color(fill_color) eval(_command) self.end_fill() 定义函数 根据 star 列表（通常为[x, y]起始坐标）， end 坐标最大值，step 步长，来计算出所有坐标。 返回坐标列表（[[x,y], [x, y1], [x1, y], [x1, y1], …]） 12345678def diff_color_square_address(star: list, end: int, step: int) -&gt; list: x = list(range(star[0], end+1, step)) y = list(range(star[1], end+1, step)) res_array = [] for xi in x: for yi in y: res_array.append([xi, yi]) return res_array 画图主函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576def draw_main(b_val: int, s_len: int) -&gt; None: ''' + ### bgcolor(color) &gt; 设置屏幕颜色，方式同 color() + ### speed(s=None) &gt; s为0-10的整数或者速度字符串 - None：返回当前速度 - &quot;fastest&quot;：0 - &quot;fast&quot;：10 - &quot;normal&quot;：6 - &quot;slow&quot;：3 - &quot;slowest&quot;：1 ''' star_time = time.time() t = MyTurtle() t.screen.bgcolor(&quot;black&quot;) # white t.speed(100) tem_b = b_val + 1 tem_s = b_val - tem_b % 2 # 取单数数量 sx = list(x1 for x1 in range(1, tem_s)) # 小方格基本坐标 sy = list(y1 for y1 in range(1, tem_s)) bx = list(x1 for x1 in range(1, tem_b)) # 大方格基本坐标 by = list(y1 for y1 in range(1, tem_b)) b_len = len(sx) # 大方格边长 different_colors_square_list = [ diff_color_square_address([2, 2], b_val, 4), # 混合橙色坐标 diff_color_square_address([4, 4], b_val, 4), # 混合淡绿色坐标 diff_color_square_address([4, 2], b_val, 4), # 混合紫色坐标 diff_color_square_address([2, 4], b_val, 4) # 混合绿松石色坐标 ] color_list = [ ['red', 'yellow'], # 混合橙色 ['green', 'yellow'], # 混合淡绿色 ['red', 'blue'], # 混合紫色 ['green', 'blue'], # 混合绿松石色 ['yellow', 'blue'], # 混合灰色 ] offset = (s_len * b_len * (b_val + 2) + s_len) / 2 print( f's_len: {s_len}, b_len: {b_len}, offset: {offset}, turtle: {t._screen.screensize()}') turtle.tracer(False) # 如果想看看程序的画图过程设成True for x3 in bx: for y3 in by: for x4 in sx: for y4 in sy: x = x3 * s_len * b_len + x4 * s_len - offset y = y3 * s_len * b_len + y4 * s_len - offset t.move(int(x), int(y)) # 判断坐标是否在特定颜色组坐标中，是选用对应颜色组 if [x3, y3] in different_colors_square_list[0]: colors = color_list[0] elif [x3, y3] in different_colors_square_list[1]: colors = color_list[1] elif [x3, y3] in different_colors_square_list[2]: colors = color_list[2] elif [x3, y3] in different_colors_square_list[3]: colors = color_list[3] else: colors = color_list[4] if ((x4 + y4) % 2) == 0: color = colors[0] else: color = colors[1] t.fill_color_shape('draw_square', s_len, color) t.move(800, 800) turtle.update() end_time = time.time() - star_time # print_time = time.strftime(&quot;%b %d %Y %H:%M:%S&quot;, end_time) print(end_time) t.screen.mainloop() 调用函数绘图 123456789if __name__ == '__main__': big_input = 9 big_value = 9 if big_input == '' or int(big_input) &lt;= 0 else int(big_input) small_input = 5 small_len = 5 if small_input == '' or int( small_input) &lt;= 0 else int(small_input) print('big_value:', big_value, type(big_value), '\\n', 'small_len:', small_len, type(small_len)) draw_main(big_value, small_len)","link":"/posts/1235119209.html"},{"title":"Attention机制","text":"Encoder-Decoder 所谓encoder-decoder模型，又叫做编码-解码模型。这是一种应用于seq2seq问题的模型。 什么是seq2seq呢？简单的说，就是根据一个序列x，来生成另一个输出序列y。seq2seq有很多应用，例如翻译，文档摘要，问答系统等等。在翻译中，输入序列是待翻译的文本，输出序列是翻译后的文本；在问答系统中，输入序列是提出的问题，而输出序列是答案。 为了解决seq2seq问题，有人提出了encoder-decoder模型，也就是编码-解码模型。所谓编码，就是将输入序列转化成一个固定长度的向量；解码，就是将之前生成的固定向量再转化成输出序列。 当然了，这个只是大概的思想，具体实现的时候，编码器和解码器都不是固定的，可选的有CNN/RNN/BiRNN/GRU/LSTM等等，你可以自由组合。比如说，你在编码时使用BiRNN，解码时使用RNN，或者编码时使用RNN，解码时使用LSTM等等。 这边为了方便阐述，选取了编码和解码都是RNN的组合。在RNN中，当前时间的隐藏状态是由上一时间的状态和当前时间输入决定的，也就是 \\[ h_t=f(h_{t-1},x_t) \\] 获得了各个时间段的隐藏层以后，再将隐藏层的信息汇总，生成最后的语义向量 \\[ C=q(h_1,h_2,h_3,...,h_{T_{x}}) \\] 一种简单的方法是将最后的隐藏层作为语义向量\\(C\\)，即 \\[ C=q(h_1,h_2,h_3,...h_{T_{x}})=h_{T_{x}} \\] 解码阶段可以看做编码的逆过程。这个阶段，我们要根据给定的语义向量\\(C\\)和之前生成的输出序列\\(y_1,y_2,….y_{t-1}\\)来预测下一个输出的单词\\(y_t\\)，即 \\[ y_t=argmaxP(y_t)=\\prod^T _{t=1}p(y_t \\mid \\lbrace y_1,...y_{t-1} \\rbrace,C) \\] 也可以写作 \\[ y_t=g(\\lbrace y_1,...y_{t-1}\\rbrace,C) \\] 而在RNN中，上式又可以简化成 \\[ y_t=g(y_{t-1},s_t,C) \\] 其中\\(s\\)是输出RNN中的隐藏层，\\(C\\)代表之前提过的语义向量，\\(y_{t-1}\\)表示上个时间段的输出，反过来作为这个时间段的输入，而\\(g\\)则可以是一个非线性的多层的神经网络，产生词典中各个词语属于\\(y_t\\)的概率。 encoder-decoder模型虽然非常经典，但是局限性也非常大。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量\\(C\\)。也就是说，编码器要将整个序列信息压缩进一个固定长度的向量中去。但是这样做有两个弊端，一是语义向量无法完全表示整个序列的信息，还有就是先输入的内容携带的信息会被后输入的信息稀释掉，或者说，被覆盖了。输入序列越长，这个现象就越严重。这就使得在解码的时候一开始就没有获得输入序列足够的信息，那么解码的准确度自然也就要打个折扣了。 Attention模型 为了解决这个问题，提出了Attention模型，或者说注意力模型。简单的说，这种模型在产生输出的时候，还会产生一个‘注意力范围’表示接下来输出的时候要重点关注输入序列的哪些部分，然后根据关注的区域来产生下一个输出，如此往复。模型的大概示意图如下所示 相比之前的encoder-decoder模型，attention的模型最大的区别就在于它不在要求编码器将所有输入信息都编码进一个固定长度的向量之中。相反，此时编码器需要将输入编码成一个向量的序列，而在解码的时候，每一步都会选择性的从向量序列中挑选一个子集进行一步处理。这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。而且这种方法在翻译任务中取得了非常不错的效果。 编码 在单向的RNN中，数据是按顺序输入的，因此在第\\(j\\)个隐藏状态\\(h_j\\)只能携带第\\(j\\)个单词本身以及之前的一些信息；而如果逆序输入，则\\(h_j\\)包含第\\(j\\)个单词及以后的一些信息。如果把这个两个结合起来，\\(h_j=[h\\to j,h \\gets j]\\)就包含了第\\(j\\)个输入和前后的信息。 解码 解码部分使用了attention模型。类似的，我们可以将之前定义的条件概率写作 \\[ p(y_i\\mid y_1,...,y_{i-1},X)=g(y_{i-1},s_i,c_i) \\] 上式\\(s_i\\)表示解码器\\(i\\)时刻的隐藏状态。计算公式是 \\[ s_i=f(s_{i-1},y_{i-1},c_i) \\] 注意这里的条件概率与每个目标输出\\(y_i\\)相对应的内容向量\\(c_i\\)有关。而在传统的方式中，只有一个内容向量\\(C\\)。那么这里的内容向量\\(c_i\\)又该怎么算呢？其实\\(c_i\\)是由编码时的隐藏向量序列\\((h_1,…h_{T_{x}})\\)按权重相加得到的。 \\[ c_i=\\sum_{j=1}^{T_x}\\alpha_{ij}h_j \\] 由于编码使用了双向RNN，因此可以认为\\(h_i\\)中包含了输入序列中第\\(i\\)个词以及前后一些词的信息。将隐藏向量序列按权重相加，表示在生成第\\(j\\)个输出的时候注意力分配是不同的。\\(\\alpha_{ij}\\)的值越高，表示第\\(i\\)个输出在第\\(j\\)个输入上分配的注意力越多，在生成第\\(i\\)个输出的时候受第\\(j\\)个输入的影响也就越大。那么我们现在又有新问题了，\\(\\alpha_{ij}\\)又是怎么得到呢？这个其实是由第\\(i-1\\)个输出隐藏状态\\(s_{i-1}\\)和输入中各个隐藏状态共同决定的。也就是 \\[ \\alpha_{ij}=\\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}\\\\e_{ij}=a(s_{i-1},h_j) \\] 也就是说，\\(s_{i-1}\\)先跟每个\\(h\\)分别计算得到一个数值，然后使用softmax得到\\(i\\)时刻的输出在\\(T_x\\)个输入隐藏状态中的注意力分配向量。这个分配向量也就是计算\\(c_i\\)的权重。我们现在再把公式按照执行顺序汇总一下： 上面这些公式就是解码器在第\\(i\\)个时间段内要做的事情。作者还给了一个示意图: 序列中的Attention Attention机制的基本思想是，打破了传统编码器-解码器结构在编解码结构时都依赖于内部一个固定长度向量的限制。 Attention机制的实现是通过保留LSTM编码器对输入序列的中间输出结果，然后训练一个模型来对这些输入进行选择性的学习并且在模型输出时将输出序列与之进行关联。 换一个角度而言，输出序列的每一项的生成概率取决于在输入序列中选择了哪些项。 在文本翻译任务上，使用Attention机制的模型每生成一个词时都会在输入序列中找出一个与之最相关的词集合。之后模型根据当前的上下文向量(context vectors)和之前生成出的词来预测下一个目标词。 …它将输入序列转化为一堆向量并自适应地从中选择一个子集来解码出目标翻译文本。这感觉上像是用于文本翻译的神经网络模型需要‘压缩’输入文本中的所有信息为一个固定长度的向量，不论输入文本的长短。 via：Dzmitry Bahdanau, et al., Neural machine translation by jointly learning to align and translate, 2015 Attention is all you need Google的一般化Attention思路也是一个编码序列的方案，因此我们也可以认为它跟RNN、CNN一样，都是一个序列编码的层。 事实上Google给出的方案是很具体的。首先，它先把Attention的定义给了出来： \\[ Attention(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}) = softmax\\left(\\frac{\\boldsymbol{Q}\\boldsymbol{K}^{\\top}}{\\sqrt{d_k}}\\right)\\boldsymbol{V} \\] 这里用的是跟Google的论文一致的符号,其中\\(\\boldsymbol{Q}\\in\\mathbb{R}^{n\\times d_k}, \\boldsymbol{K}\\in\\mathbb{R}^{m\\times d_k}, \\boldsymbol{V}\\in\\mathbb{R}^{m\\times d_v}\\)。 如果忽略激活函数\\(softmax\\)的话，那么事实上它就是三个\\(n\\times d_k,d_k\\times m, m\\times d_v\\)的矩阵相乘，最后的结果就是一个\\(n*d_v\\)的序列。 那怎么理解这种结构呢？我们不妨逐个向量来看。 \\[ Attention(\\boldsymbol{q}_t,\\boldsymbol{K},\\boldsymbol{V}) = \\sum_{s=1}^m \\frac{1}{Z}\\exp\\left(\\frac{\\langle\\boldsymbol{q}_t, \\boldsymbol{k}_s\\rangle}{\\sqrt{d_k}}\\right)\\boldsymbol{v}_s \\] 其中\\(Z\\)是归一化因子。事实上\\(q.k,v\\)分别是\\(query,key,value\\)的简写，\\(K,V\\)是一一对应的，它们就像是key-value的关系，那么上式的意思就是通过\\(q_t\\)这个query,通过与各个\\(k_S\\)内积的softmax的方式，来得到\\(q_t\\)和各个\\(v_s\\)的相似度，然后加权求和，得到一个\\(d_v\\)维的向量。其中因子\\(\\sqrt d_k\\)起到调节作用，使得内积不至于太大（太大的话softmax后就非0即1了，不够“soft”了）。 事实上这种Attention的定义并不新鲜，但由于Google的影响力，我们可以认为现在是更加正式地提出了这个定义，并将其视为一个层地看待；此外这个定义只是注意力的一种形式，还有一些其他选择，比如query跟key的运算方式不一定是点乘（还可以是拼接后再内积一个参数向量），甚至权重都不一定要归一化，等等。 多头attention(Mutli-head attention) ,Query,Key,Value首先经过一个线性变换，然后输入到缩放点积attention,注意这里要做h次，其实也就是所谓的多头，每一次算一个头。而且每次Q,K,V进行线性变换的参数 W 是不一样的。然后将h次的放缩点积attention 结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。可以看到，google提出来的多头attention的不同之处在于进行了h次计算而不仅仅计算一次，论文中说到这样的好处是可以允许模型在不同的表示空间里学到相关的信息。 self-attention可以是一般的attention的一种特殊情况，在self-attention中，\\(Q=K=V\\)每个序列中的单元和该序列中所有单元进行attention计算。Google提出的多头attention通过计算多次来捕获不同子空间上的相关信息。self-attention的特点在于无视词之间的距离直接计算依赖关系，能够学习一个句子的内部结构，实现也较为简单并行可以并行计算。 总结 采用传统编码器-解码器结构的LSTM/RNN模型存在一个问题，不论输入长短都将其编码成一个固定长度的向量表示，这使模型对于输入长序列的学习效果很差(解码效果很差)。 而Attention机制则克服了上述问题，原理是在模型输出时会选择性地专注考虑输入中的对应相关的信息。 使用Attention机制的方法被广泛应用在各种序列预测任务上，包括文本翻译，语音识别等。","link":"/posts/2591765099.html"},{"title":"Backpack II","text":"Question lintcode: (125) Backpack II Problem Statement Given n items with size \\[Ai\\] and value Vi, and a backpack with size m. What's the maximum value can you put into the backpack? Example Given 4 items with size [2, 3, 5, 7] and value [1, 5, 2, 4], and a backpack with size 10. The maximum value is 9. Note You cannot divide item into small pieces and the total size of items you choose should smaller or equal to m. Challenge O(n x m) memory is acceptable, can you do it in O(m) memory? 题解 首先定义状态 \\[K(i,w)\\] 为前 \\[i\\] 个物品放入size为 \\[w\\] 的背包中所获得的最大价值，则相应的状态转移方程为： \\[K(i,w) = \\max \\{K(i-1, w), K(i-1, w - w_i) + v_i\\}\\] 详细分析过程见 Knapsack C++ - 2D vector for result 12345678910111213141516171819202122232425262728293031323334class Solution {public: /** * @param m: An integer m denotes the size of a backpack * @param A &amp; V: Given n items with size A[i] and value V[i] * @return: The maximum value */ int backPackII(int m, vector&lt;int&gt; A, vector&lt;int&gt; V) { if (A.empty() || V.empty() || m &lt; 1) { return 0; } const int N = A.size() + 1; const int M = m + 1; vector&lt;vector&lt;int&gt; &gt; result; result.resize(N); for (vector&lt;int&gt;::size_type i = 0; i != N; ++i) { result[i].resize(M); std::fill(result[i].begin(), result[i].end(), 0); } for (vector&lt;int&gt;::size_type i = 1; i != N; ++i) { for (int j = 0; j != M; ++j) { if (j &lt; A[i - 1]) { result[i][j] = result[i - 1][j]; } else { int temp = result[i - 1][j - A[i - 1]] + V[i - 1]; result[i][j] = max(temp, result[i - 1][j]); } } } return result[N - 1][M - 1]; }}; Java 12345678910111213141516171819202122232425public class Solution { /** * @param m: An integer m denotes the size of a backpack * @param A &amp; V: Given n items with size A[i] and value V[i] * @return: The maximum value */ public int backPackII(int m, int[] A, int V[]) { if (A == null || V == null || A.length == 0 || V.length == 0) return 0; final int N = A.length; final int M = m; int[][] bp = new int[N + 1][M + 1]; for (int i = 0; i &lt; N; i++) { for (int j = 0; j &lt;= M; j++) { if (A[i] &gt; j) { bp[i + 1][j] = bp[i][j]; } else { bp[i + 1][j] = Math.max(bp[i][j], bp[i][j - A[i]] + V[i]); } } } return bp[N][M]; }} 源码分析 使用二维矩阵保存结果result 返回result矩阵的右下角元素——背包size限制为m时的最大价值 按照第一题backpack的思路，这里可以使用一维数组进行空间复杂度优化。优化方法为逆序求result[j]，优化后的代码如下： C++ 1D vector for result 12345678910111213141516171819202122232425262728293031class Solution {public: /** * @param m: An integer m denotes the size of a backpack * @param A &amp; V: Given n items with size A[i] and value V[i] * @return: The maximum value */ int backPackII(int m, vector&lt;int&gt; A, vector&lt;int&gt; V) { if (A.empty() || V.empty() || m &lt; 1) { return 0; } const int M = m + 1; vector&lt;int&gt; result; result.resize(M); std::fill(result.begin(), result.end(), 0); for (vector&lt;int&gt;::size_type i = 0; i != A.size(); ++i) { for (int j = m; j &gt;= 0; --j) { if (j &lt; A[i]) { // result[j] = result[j]; } else { int temp = result[j - A[i]] + V[i]; result[j] = max(temp, result[j]); } } } return result[M - 1]; }}; Reference Lintcode: Backpack II - neverlandly - 博客园 九章算法 | 背包问题","link":"/posts/946488787.html"},{"title":"Backpack","text":"Question lintcode: (92) Backpack Problem Statement Given n items with size \\(A_i\\), an integer m denotes the size of a backpack. How full you can fill this backpack? Example If we have 4 items with size [2, 3, 5, 7], the backpack size is 11, we can select [2, 3, 5], so that the max size we can fill this backpack is 10. If the backpack size is 12. we can select [2, 3, 7] so that we can fulfill the backpack. You function should return the max size we can fill in the given backpack. Note You can not divide any item into small pieces. Challenge O(n x m) time and O(m) memory. O(n x m) memory is also acceptable if you do not know how to optimize memory. 题解1 本题是典型的01背包问题，每种类型的物品最多只能选择一件。参考前文 Knapsack 中总结的解法，这个题中可以将背包的 size 理解为传统背包中的重量；题目问的是能达到的最大 size, 故可将每个背包的 size 类比为传统背包中的价值。 考虑到数组索引从0开始，故定义状态bp[i + 1][j]为前 i 个物品中选出重量不超过j时总价值的最大值。状态转移方程则为分A[i] &gt; j 与否两种情况考虑。初始化均为0，相当于没有放任何物品。 Java 1234567891011121314151617181920212223242526public class Solution { /** * @param m: An integer m denotes the size of a backpack * @param A: Given n items with size A[i] * @return: The maximum size */ public int backPack(int m, int[] A) { if (A == null || A.length == 0) return 0; final int M = m; final int N = A.length; int[][] bp = new int[N + 1][M + 1]; for (int i = 0; i &lt; N; i++) { for (int j = 0; j &lt;= M; j++) { if (A[i] &gt; j) { bp[i + 1][j] = bp[i][j]; } else { bp[i + 1][j] = Math.max(bp[i][j], bp[i][j - A[i]] + A[i]); } } } return bp[N][M]; }} 源码分析 注意索引及初始化的值，尤其是 N 和 M 的区别，内循环处可等于 M。 复杂度分析 两重 for 循环，时间复杂度为 \\(O(m \\times n)\\), 二维矩阵的空间复杂度为 \\[O(m \\times n)\\], 一维矩阵的空间复杂度为 \\(O(m)\\). 题解2 接下来看看 九章算法 的题解，这种解法感觉不是很直观，推荐使用题解1的解法。 状态: result[i][S] 表示前i个物品，取出一些物品能否组成体积和为S的背包 状态转移方程: \\[f[i][S] = f[i-1][S-A[i]] ~or~ f[i-1][S]\\] (A[i]为第i个物品的大小) 欲从前i个物品中取出一些组成体积和为S的背包，可从两个状态转换得到。 \\[f[i-1][S-A[i]]\\]: 放入第i个物品，前 \\[i-1\\] 个物品能否取出一些体积和为 \\[S-A[i]\\] 的背包。 \\[f[i-1][S]\\]: 不放入第i个物品，前 \\[i-1\\] 个物品能否取出一些组成体积和为S的背包。 状态初始化: \\[f[1 \\cdots n][0]=true; ~f[0][1 \\cdots m]=false\\]. 前1~n个物品组成体积和为0的背包始终为真，其他情况为假。 返回结果: 寻找使 \\[f[n][S]\\] 值为true的最大S (\\[1 \\leq S \\leq m\\]) C++ - 2D vector 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution {public: /** * @param m: An integer m denotes the size of a backpack * @param A: Given n items with size A[i] * @return: The maximum size */ int backPack(int m, vector&lt;int&gt; A) { if (A.empty() || m &lt; 1) { return 0; } const int N = A.size() + 1; const int M = m + 1; vector&lt;vector&lt;bool&gt; &gt; result; result.resize(N); for (vector&lt;int&gt;::size_type i = 0; i != N; ++i) { result[i].resize(M); std::fill(result[i].begin(), result[i].end(), false); } result[0][0] = true; for (int i = 1; i != N; ++i) { for (int j = 0; j != M; ++j) { if (j &lt; A[i - 1]) { result[i][j] = result[i - 1][j]; } else { result[i][j] = result[i - 1][j] || result[i - 1][j - A[i - 1]]; } } } // return the largest i if true for (int i = M; i &gt; 0; --i) { if (result[N - 1][i - 1]) { return (i - 1); } } return 0; }}; 源码分析 异常处理 初始化结果矩阵，注意这里需要使用resize而不是reserve，否则可能会出现段错误 实现状态转移逻辑，一定要分j &lt; A[i - 1]与否来讨论 返回结果，只需要比较result[N - 1][i - 1]的结果，返回true的最大值 状态转移逻辑中代码可以进一步简化，即： 12345678for (int i = 1; i != N; ++i) { for (int j = 0; j != M; ++j) { result[i][j] = result[i - 1][j]; if (j &gt;= A[i - 1] &amp;&amp; result[i - 1][j - A[i - 1]]) { result[i][j] = true; } }} 考虑背包问题的核心——状态转移方程，如何优化此转移方程？原始方案中用到了二维矩阵来保存result，注意到result的第i行仅依赖于第i-1行的结果，那么能否用一维数组来代替这种隐含的关系呢？我们在内循环j处递减即可。如此即可避免result[i][S]的值由本轮result[i][S-A[i]]递推得到。 C++ - 1D vector 1234567891011121314151617181920212223242526272829303132333435class Solution {public: /** * @param m: An integer m denotes the size of a backpack * @param A: Given n items with size A[i] * @return: The maximum size */ int backPack(int m, vector&lt;int&gt; A) { if (A.empty() || m &lt; 1) { return 0; } const int N = A.size(); vector&lt;bool&gt; result; result.resize(m + 1); std::fill(result.begin(), result.end(), false); result[0] = true; for (int i = 0; i != N; ++i) { for (int j = m; j &gt;= 0; --j) { if (j &gt;= A[i] &amp;&amp; result[j - A[i]]) { result[j] = true; } } } // return the largest i if true for (int i = m; i &gt; 0; --i) { if (result[i]) { return i; } } return 0; }}; 复杂度分析 两重 for 循环，时间复杂度均为 \\[O(m \\times n)\\], 二维矩阵的空间复杂度为 \\[O(m \\times n)\\], 一维矩阵的空间复杂度为 \\[O(m)\\]. Reference 《挑战程序设计竞赛》第二章 Lintcode: Backpack - neverlandly - 博客园 九章算法 | 背包问题 崔添翼 § 翼若垂天之云 › 《背包问题九讲》2.0 alpha1","link":"/posts/4114868032.html"},{"title":"Best Time to Buy and Sell Stock III","text":"Question leetcode: Best Time to Buy and Sell Stock III | LeetCode OJ lintcode: (151) Best Time to Buy and Sell Stock III 123456789101112Say you have an array forwhich the ith element is the price of a given stock on day i.Design an algorithm to find the maximum profit.You may complete at most two transactions.ExampleGiven an example [4,4,6,1,1,4,2,5], return 6.NoteYou may not engage in multiple transactions at the same time(ie, you must sell the stock before you buy again). 题解 与前两道允许一次或者多次交易不同，这里只允许最多两次交易，且这两次交易不能交叉。咋一看似乎无从下手，我最开始想到的是找到排在前2个的波谷波峰，计算这两个差值之和。原理上来讲应该是可行的，但是需要记录 \\[O(n^2)\\] 个波谷波峰并对其排序，实现起来也比较繁琐。 除了以上这种直接分析问题的方法外，是否还可以借助分治的思想解决呢？最多允许两次不相交的交易，也就意味着这两次交易间存在某一分界线，考虑到可只交易一次，也可交易零次，故分界线的变化范围为第一天至最后一天，只需考虑分界线两边各自的最大利润，最后选出利润和最大的即可。 这种方法抽象之后则为首先将 [1,n] 拆分为 [1,i] 和 [i+1,n], 参考卖股票系列的第一题计算各自区间内的最大利润即可。[1,i] 区间的最大利润很好算，但是如何计算 [i+1,n] 区间的最大利润值呢？难道需要重复 n 次才能得到？注意到区间的右侧 n 是个不变值，我们从 [1, i] 计算最大利润是更新波谷的值，那么我们可否逆序计算最大利润呢？这时候就需要更新记录波峰的值了。逆向思维大法好！Talk is cheap, show me the code! Python 12345678910111213141516171819202122232425262728class Solution: &quot;&quot;&quot; @param prices: Given an integer array @return: Maximum profit &quot;&quot;&quot; def maxProfit(self, prices): if prices is None or len(prices) &lt;= 1: return 0 n = len(prices) # get profit in the front of prices profit_front = [0] * n valley = prices[0] for i in xrange(1, n): profit_front[i] = max(profit_front[i - 1], prices[i] - valley) valley = min(valley, prices[i]) # get profit in the back of prices, (i, n) profit_back = [0] * n peak = prices[-1] for i in xrange(n - 2, -1, -1): profit_back[i] = max(profit_back[i + 1], peak - prices[i]) peak = max(peak, prices[i]) # add the profit front and back profit = 0 for i in xrange(n): profit = max(profit, profit_front[i] + profit_back[i]) return profit C++ 12345678910111213141516171819202122232425262728293031class Solution {public: /** * @param prices: Given an integer array * @return: Maximum profit */ int maxProfit(vector&lt;int&gt; &amp;prices) { if (prices.size() &lt;= 1) return 0; int n = prices.size(); // get profit in the front of prices vector&lt;int&gt; profit_front = vector&lt;int&gt;(n, 0); for (int i = 1, valley = prices[0]; i &lt; n; ++i) { profit_front[i] = max(profit_front[i - 1], prices[i] - valley); valley = min(valley, prices[i]); } // get profit in the back of prices, (i, n) vector&lt;int&gt; profit_back = vector&lt;int&gt;(n, 0); for (int i = n - 2, peak = prices[n - 1]; i &gt;= 0; --i) { profit_back[i] = max(profit_back[i + 1], peak - prices[i]); peak = max(peak, prices[i]); } // add the profit front and back int profit = 0; for (int i = 0; i &lt; n; ++i) { profit = max(profit, profit_front[i] + profit_back[i]); } return profit; }}; Java 12345678910111213141516171819202122232425262728293031class Solution { /** * @param prices: Given an integer array * @return: Maximum profit */ public int maxProfit(int[] prices) { if (prices == null || prices.length &lt;= 1) return 0; // get profit in the front of prices int[] profitFront = new int[prices.length]; profitFront[0] = 0; for (int i = 1, valley = prices[0]; i &lt; prices.length; i++) { profitFront[i] = Math.max(profitFront[i - 1], prices[i] - valley); valley = Math.min(valley, prices[i]); } // get profit in the back of prices, (i, n) int[] profitBack = new int[prices.length]; profitBack[prices.length - 1] = 0; for (int i = prices.length - 2, peak = prices[prices.length - 1]; i &gt;= 0; i--) { profitBack[i] = Math.max(profitBack[i + 1], peak - prices[i]); peak = Math.max(peak, prices[i]); } // add the profit front and back int profit = 0; for (int i = 0; i &lt; prices.length; i++) { profit = Math.max(profit, profitFront[i] + profitBack[i]); } return profit; }}; 源码分析 整体分为三大部分，计算前半部分的最大利润值，然后计算后半部分的最大利润值，最后遍历得到最终的最大利润值。 复杂度分析 三次遍历原数组，时间复杂度为 \\[O(n)\\], 利用了若干和数组等长的数组，空间复杂度也为 \\[O(n)\\]. Reference soulmachine 的卖股票系列","link":"/posts/2275853462.html"},{"title":"Best Time to Buy and Sell Stock IV","text":"Question leetcode: Best Time to Buy and Sell Stock IV | LeetCode OJ lintcode: (393) Best Time to Buy and Sell Stock IV 123456789101112131415Say you have an array forwhich the ith element is the price of a given stock on day i.Design an algorithm to find the maximum profit.You may complete at most k transactions.ExampleGiven prices = [4,4,6,1,1,4,2,5], and k = 2, return 6.NoteYou may not engage in multiple transactions at the same time(i.e., you must sell the stock before you buy again).ChallengeO(nk) time. 题解1 卖股票系列中最难的一道，较易实现的方法为使用动态规划，动规的实现又分为大约3大类方法，这里先介绍一种最为朴素的方法，过不了大量数据，会 TLE. 最多允许 k 次交易，由于一次增加收益的交易至少需要两天，故当 k &gt;= n/2时，此题退化为卖股票的第二道题，即允许任意多次交易。当 k &lt; n/2 时，使用动规来求解，动规的几个要素如下： f[i][j] 代表第 i 天为止交易 k 次获得的最大收益，那么将问题分解为前 x 天交易 k-1 次，第 x+1 天至第 i 天交易一次两个子问题，于是动态方程如下： 1f[i][j] = max(f[x][j - 1] + profit(x + 1, i)) 简便起见，初始化二维矩阵为0，下标尽可能从1开始，便于理解。 Python 1234567891011121314151617181920212223242526272829303132333435363738class Solution: &quot;&quot;&quot; @param k: an integer @param prices: a list of integer @return: an integer which is maximum profit &quot;&quot;&quot; def maxProfit(self, k, prices): if prices is None or len(prices) &lt;= 1 or k &lt;= 0: return 0 n = len(prices) # k &gt;= prices.length / 2 ==&gt; multiple transactions Stock II if k &gt;= n / 2: profit_max = 0 for i in xrange(1, n): diff = prices[i] - prices[i - 1] if diff &gt; 0: profit_max += diff return profit_max f = [[0 for i in xrange(k + 1)] for j in xrange(n + 1)] for j in xrange(1, k + 1): for i in xrange(1, n + 1): for x in xrange(0, i + 1): f[i][j] = max(f[i][j], f[x][j - 1] + self.profit(prices, x + 1, i)) return f[n][k] # calculate the profit of prices(l, u) def profit(self, prices, l, u): if l &gt;= u: return 0 valley = 2**31 - 1 profit_max = 0 for price in prices[l - 1:u]: profit_max = max(profit_max, price - valley) valley = min(valley, price) return profit_max C++ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution {public: /** * @param k: An integer * @param prices: Given an integer array * @return: Maximum profit */ int maxProfit(int k, vector&lt;int&gt; &amp;prices) { if (prices.size() &lt;= 1 || k &lt;= 0) return 0; int n = prices.size(); // k &gt;= prices.length / 2 ==&gt; multiple transactions Stock II if (k &gt;= n / 2) { int profit_max = 0; for (int i = 1; i &lt; n; ++i) { int diff = prices[i] - prices[i - 1]; if (diff &gt; 0) { profit_max += diff; } } return profit_max; } vector&lt;vector&lt;int&gt; &gt; f = vector&lt;vector&lt;int&gt; &gt;(n + 1, vector&lt;int&gt;(k + 1, 0)); for (int j = 1; j &lt;= k; ++j) { for (int i = 1; i &lt;= n; ++i) { for (int x = 0; x &lt;= i; ++x) { f[i][j] = max(f[i][j], f[x][j - 1] + profit(prices, x + 1, i)); } } } return f[n][k]; }private: int profit(vector&lt;int&gt; &amp;prices, int l, int u) { if (l &gt;= u) return 0; int valley = INT_MAX; int profit_max = 0; for (int i = l - 1; i &lt; u; ++i) { profit_max = max(profit_max, prices[i] - valley); valley = min(valley, prices[i]); } return profit_max; }}; Java 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution { /** * @param k: An integer * @param prices: Given an integer array * @return: Maximum profit */ public int maxProfit(int k, int[] prices) { if (prices == null || prices.length &lt;= 1 || k &lt;= 0) return 0; int n = prices.length; if (k &gt;= n / 2) { int profit_max = 0; for (int i = 1; i &lt; n; i++) { if (prices[i] - prices[i - 1] &gt; 0) { profit_max += prices[i] - prices[i - 1]; } } return profit_max; } int[][] f = new int[n + 1][k + 1]; for (int j = 1; j &lt;= k; j++) { for (int i = 1; i &lt;= n; i++) { for (int x = 0; x &lt;= i; x++) { f[i][j] = Math.max(f[i][j], f[x][j - 1] + profit(prices, x + 1, i)); } } } return f[n][k]; } private int profit(int[] prices, int l, int u) { if (l &gt;= u) return 0; int valley = Integer.MAX_VALUE; int profit_max = 0; for (int i = l - 1; i &lt; u; i++) { profit_max = Math.max(profit_max, prices[i] - valley); valley = Math.min(valley, prices[i]); } return profit_max; }}; 源码分析 注意 Python 中的多维数组初始化方式，不可简单使用[[0] * k] * n], 具体原因是因为 Python 中的对象引用方式。可以优化的地方是 profit 方法及最内存循环。 复杂度分析 三重循环，时间复杂度近似为 \\[O(n^2 \\cdot k)\\], 使用了 f 二维数组，空间复杂度为 \\[O(n \\cdot k)\\]. Reference [LeetCode] Best Time to Buy and Sell Stock I II III IV | 梁佳宾的网络日志 Best Time to Buy and Sell Stock IV 参考程序 Java/C++/Python leetcode-Best Time to Buy and Sell Stock 系列 // 陈辉的技术博客 [LeetCode]Best Time to Buy and Sell Stock IV | 书影博客","link":"/posts/738539787.html"},{"title":"Best Time to Buy and Sell Stock","text":"Question leetcode: Best Time to Buy and Sell Stock | LeetCode OJ lintcode: (149) Best Time to Buy and Sell Stock 123456789Say you have an array forwhich the ith element is the price of a given stock on day i.If you were only permitted to complete at most one transaction(ie, buy one and sell one share of the stock),design an algorithm to find the maximum profit.ExampleGiven an example [3,2,3,1,2], return 1 题解 最多只允许进行一次交易，显然我们只需要把波谷和波峰分别找出来就好了。但是这样的话问题又来了，有多个波峰和波谷时怎么办？——找出差值最大的一对波谷和波峰。故需要引入一个索引用于记录当前的波谷，结果即为当前索引值减去波谷的值。 Python 12345678910111213141516class Solution: &quot;&quot;&quot; @param prices: Given an integer array @return: Maximum profit &quot;&quot;&quot; def maxProfit(self, prices): if prices is None or len(prices) &lt;= 1: return 0 profit = 0 cur_price_min = 2**31 - 1 for price in prices: profit = max(profit, price - cur_price_min) cur_price_min = min(cur_price_min, price) return profit C++ 12345678910111213141516171819class Solution {public: /** * @param prices: Given an integer array * @return: Maximum profit */ int maxProfit(vector&lt;int&gt; &amp;prices) { if (prices.size() &lt;= 1) return 0; int profit = 0; int cur_price_min = INT_MAX; for (int i = 0; i &lt; prices.size(); ++i) { profit = max(profit, prices[i] - cur_price_min); cur_price_min = min(cur_price_min, prices[i]); } return profit; }}; Java 123456789101112131415161718public class Solution { /** * @param prices: Given an integer array * @return: Maximum profit */ public int maxProfit(int[] prices) { if (prices == null || prices.length &lt;= 1) return 0; int profit = 0; int curPriceMin = Integer.MAX_VALUE; for (int price : prices) { profit = Math.max(profit, price - curPriceMin); curPriceMin = Math.min(curPriceMin, price); } return profit; }} 源码分析 善用max和min函数，减少if的使用。 复杂度分析 遍历一次 prices 数组，时间复杂度为 \\[O(n)\\], 使用了几个额外变量，空间复杂度为 \\[O(1)\\]. Reference soulmachine 的卖股票系列","link":"/posts/1618163482.html"},{"title":"Binary Tree Zigzag Level Order Traversal","text":"Question leetcode: Binary Tree Zigzag Level Order Traversal | LeetCode OJ lintcode: (71) Binary Tree Zigzag Level Order Traversal 1234567891011121314151617181920Given a binary tree, return the zigzag level order traversal of its nodes' values.(ie, from left to right, then right to left for the next level and alternate between).ExampleGiven binary tree {3,9,20,#,#,15,7}, 3 / \\ 9 20 / \\ 15 7return its zigzag level order traversal as:[ [3], [20,9], [15,7]] 题解1 - 队列 二叉树的广度优先遍历使用队列非常容易实现，这道题要求的是蛇形遍历，我们可以发现奇数行的遍历仍然可以按照广度优先遍历的方式实现，而对于偶数行，只要翻转一下就好了。 Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Definition of TreeNode: * public class TreeNode { * public int val; * public TreeNode left, right; * public TreeNode(int val) { * this.val = val; * this.left = this.right = null; * } * } */public class Solution { /** * @param root: The root of binary tree. * @return: A list of lists of integer include * the zigzag level order traversal of its nodes' values */ public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; zigzagLevelOrder(TreeNode root) { ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if (root == null) return result; boolean odd = true; Queue&lt;TreeNode&gt; q = new LinkedList&lt;TreeNode&gt;(); q.offer(root); while (!q.isEmpty()) { // level traversal int qLen = q.size(); ArrayList&lt;Integer&gt; level = new ArrayList&lt;Integer&gt;(); for (int i = 0; i &lt; qLen; i++) { TreeNode node = q.poll(); level.add(node.val); if (node.left != null) q.offer(node.left); if (node.right != null) q.offer(node.right); } // add level order reverse for even if (odd) { result.add(level); } else { Collections.reverse(level); result.add(level); } // flip odd and even odd = !odd; } return result; }} 源码分析 区分奇数偶数行使用额外变量。 复杂度分析 需要 reverse 的节点数目近似为 n/2, 故时间复杂度 \\[O(n)\\]. 最下层节点数目最多 n/2, 故reverse 操作的空间复杂度可近似为 \\[O(n/2)\\]. 总的时间复杂度为 \\[O(n)\\], 空间复杂度也为 \\[O(n)\\]. Reference Binary Tree Zigzag Level Order Traversal 参考程序 Java/C++/Python Printing a Binary Tree in Zig Zag Level-Order | LeetCode","link":"/posts/3930075265.html"},{"title":"CYK算法详解与代码实现","text":"概述 在计算机科学领域，CYK算法（也称为Cocke–Younger–Kasami算法）是一种用来对上下文无关文法（CFG，Context Free Grammar）进行语法分析（parsing）的算法。该算法最早由John Cocke, Daniel Younger and Tadao Kasami分别独立提出，其中John Cocke还是1987年度的图灵奖得主。CYK算法是基于动态规划思想设计的一种自底向上语法分析算法。 乔姆斯基范式 我们首先来谈谈CNF的话题。通常把一门语言定义成一些由单词组成的词串（也就是句子）构成的集合。所以如果问两种语法（或文法）是否等价，其实就是要考察它们能否生成完全一样的词串集合。事实上，两个完全不同的CFG是不可能生成相同语言的。 而谈到两种语法“等价”，我们又可以定义弱等价和强等价两种类型的等价： 如果两种语法能够生成相同的词串集合，且为每个句子都赋与相同的短语结构（phrase structure），也就是说仅允许对non-terminal symbols进行重命名，那么它们就是强等价的。 如果两种语法能够生成相同的词串集合，但不会为每个句子都赋与相同的短语结构，那么它们就是弱等价的。 CNF(Chomsky Normal Form)是一种这样的语法标准： 如果一个\\(CFG是 \\varepsilon-free\\)，而且它的规则只有如下两种形式: \\(A\\rightarrow BC\\) \\(A\\rightarrow a\\) 那么这个CFG就是CNF形式的，可见CNF语法都是二分叉的。任何语法都可以转化成一个弱等价的CNF形式，具体方法如下： Step 1: Convert \\(A\\rightarrow Bc\\) to \\(A\\rightarrow BC\\),\\(C\\rightarrow c\\) Step 2: Convert \\(A\\rightarrow BCD\\) to \\(A\\rightarrow BX,X\\rightarrow CD\\) CYK算法 CYK算法处理的语法必须是CNF形式的，所以如果输入的是任意文法，那么需要按照前面的步骤把CFG转换成CNF形式。 CYK算法是用来判断一个字符串是否属于某个CNF语法，故设输入的字符串w长度为n。 接下来我们需要用程序填一个动态规划的状态转移表，这里我们叫这个表parse table。 parse table的规模为\\((n + 1) \\times n\\) 算法原理 注意，我们前面说过CYK是一种自底向上的算法，这里的自底向上意思是从单词开始，朝向 S(句子)工作。所以在上图我们填写的大方向是从左到右填写的。S 位于表的右上角，表示成功。算法描述如下： 其中，i 和 j 指示的内容如下图所示： 我们定义\\(PT[n + 1][n]\\)表示parse table，且\\(PT[n, :]\\)依次存储字符串w中的每一个符号\\(a_1, a_2, \\dots, a_n\\)。 \\[ \\begin{bmatrix} &amp; &amp;\\dots &amp; \\\\ &amp; \\vdots &amp; \\ddots &amp; \\\\ a_1 &amp; a_2 &amp; \\dots &amp;a_n \\end{bmatrix} %]]&gt; \\] 我们设根据给定CNF，即G能推导出w中第i到第j个字符的串的集合为\\(x_{i,j}\\) 为了填写这个表，我们一行一行，自下而上地处理。每一行对应一种长度的子串。最下面一行对应长度为1的子串，倒数第二行对应长度为2的子串，以此类推。最上面一行就对应长度为n的子串，即w本身。计算该表的任何一个表项的方法如下： 对于最下面一行的元素，即\\(x_{i,i}\\)，是使得\\(A \\rightarrow a_i\\)是G的产生式的变元A的集合。 对于不在最下面一行的元素，我们需要找到符合以下条件的变元A的集合： 1、整数k满足\\(i \\leq k &lt; j\\) 2、\\(B \\in X_{i,k}\\) 3、\\(C \\in X_{k+1, j}\\) 4、\\(A \\rightarrow BC\\)是G的产生式 根据这样的方法，我们可以填出一个下三角矩阵。 例如： CNF文法G的产生式： \\[ S \\rightarrow AB|BC \\\\ A \\rightarrow BA|a \\\\ B \\rightarrow CC|b \\\\ C \\rightarrow AB|a \\] 对L(G)测试字符串\\(w = baaba\\)的成员性构造Parse Table如下： \\[ \\begin{bmatrix} x_{1,5}=\\{S, A,C\\} &amp; &amp; &amp; &amp; \\\\ &amp; x_{2,5}=\\{S, A, C\\} &amp; &amp; &amp; \\\\ &amp; x_{2,4}=\\{B\\} &amp; x_{3,5}=\\{B\\} &amp; &amp; \\\\ x_{1,2} = \\{S, A\\} &amp; x_{2,3}=\\{B\\} &amp; x_{3,4}=\\{S, C\\} &amp; x_{4,5}=\\{S, A\\} &amp;\\\\ x_{1,1} = \\{B\\} &amp; x_{2,2} = \\{A, C\\} &amp; x_{3,3} =\\{A, C\\} &amp; x_{4,4}=\\{B \\} &amp; x_{5,5}=\\{A,C\\} \\\\ a_1 = b &amp; a_2 = a &amp; a_3 = a &amp; a_4 = b &amp; a_5 = a \\end{bmatrix} %]]&gt; \\] 最终得到\\(x_{1,5}\\)集合之后，判断起始变元\\(S\\)是否属于\\(x_{1,5}\\)。如果是，则w可被G接受，反之不接受。 代码实现 cyk.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#!usr/bin/env/python 3.6.5#-*- coding: utf-8 -*-'''Python 3.6.5installed module: - tkinter'''import reimport itertoolsimport tkinterfrom tkinter import ttkclass CNF(object): def __init__(self): self.__rules = {} def read_file(self, filename): with open(filename, 'r') as inFile: for line in inFile.readlines(): line = re.sub('[\\n\\t ]', '', line) rec_begin = line[:line.find('-')] for element in line[line.find('&gt;') + 1:].split('|'): if element in self._CNF__rules: self.__rules[element].append(rec_begin) else: self._CNF__rules[element] = [rec_begin] def get_inf(self, tar): if isinstance(tar, list) == False: exit() inf_set = [] for tarEle in tar: inf_set.extend(self.__rules.get(tarEle, [])) return list(set(inf_set))class CYK(object): def __init__(self, filename): if isinstance(filename, str) == False: exit() self.__str = '' self.__srtlen = 0 self.__canvas = [] self.__myCNF = CNF() self.__myCNF.read_file(filename) def get_str(self): self._CYK__str = input('input string:\\n').strip() if len(self._CYK__str) == 0: exit() self._CYK__srtlen = len(self._CYK__str) # MaxRow == MaxCol + 1 self._CYK__canvas = list(list([] for tmp in range(self._CYK__srtlen)) for tmp in range(self._CYK__srtlen + 1)) for iter in range(self._CYK__srtlen): self._CYK__canvas[self._CYK__srtlen][iter].append(self._CYK__str[iter]) def CYK_process(self): # for lowest level for col in range(self._CYK__srtlen): self._CYK__canvas[self._CYK__srtlen - 1][col].extend(self._CYK__myCNF.get_inf(self._CYK__canvas[self._CYK__srtlen][col])) # for upper level for row in range(self._CYK__srtlen - 2, -1, -1): for col in range(row + 1): mid_set = set() idx_i, idx_j = col + 1, col - row + self._CYK__srtlen for mid_k in range(idx_i, idx_j): fir_row, fir_col = idx_i - mid_k - 1 + self._CYK__srtlen, idx_i - 1 sec_row, sec_col = mid_k - idx_j + self._CYK__srtlen, mid_k mid_set |= set(obj[0] + obj[1] for obj in itertools.product(self._CYK__canvas[fir_row][fir_col], self._CYK__canvas[sec_row][sec_col])) self._CYK__canvas[row][col].extend(self._CYK__myCNF.get_inf(list(mid_set))) # get answer if 'S' in self._CYK__canvas[0][0]: print ('%s can be accepted.' % self._CYK__str) else: print ('%s can not be accepted.' % self._CYK__str) def GUI_show(self): def exc(line, step, row): if isinstance(line, list) == False and isinstance(line[0], list) == False: exit() for col in range(len(line)): line[col] = str('{' + '%s, ' * (len(line[col]) - 1) + '%s' * (len(line[col]) &gt; 0) + '}') % (tuple(line[col])) if col &lt;= row: line[col] = 'X%d,%d = ' % (col + 1, step + col + 1) + line[col] return (line) # default window = tkinter.Tk() window.geometry('800x400') window.title('CYK algorithm') table = ttk.Treeview(window, height = 10, show = 'headings') table['columns'] = (list(elem for elem in range(self._CYK__srtlen))) for col in range(self._CYK__srtlen): table.column(str(col), width = 100) # y&amp;x scrollbar yscrollbar = tkinter.Scrollbar(window, orient = tkinter.VERTICAL, command = table.yview) table.configure(yscrollcommand = yscrollbar.set) yscrollbar.pack(side = tkinter.RIGHT, fill = tkinter.Y) xscrollbar = tkinter.Scrollbar(window, orient = tkinter.HORIZONTAL, command = table.xview) table.configure(xscrollcommand = xscrollbar.set) xscrollbar.pack(side = tkinter.TOP, fill = tkinter.X) # insert information for row in range(self._CYK__srtlen): table.insert('', row, values = exc(self._CYK__canvas[row], self._CYK__srtlen - row - 1, row)) table.insert('', self._CYK__srtlen, values = (self._CYK__canvas[self._CYK__srtlen])) # end table.pack(side = tkinter.TOP, expand = 1, fill = tkinter.BOTH) window.mainloop()def main(): myCYK = CYK('./CNF.cfg') myCYK.get_str() myCYK.CYK_process() myCYK.GUI_show()if __name__ == '__main__': main() 实验效果 参考文献 概率上下文无关文法PCFG Tagging Problems, and Hidden Markov Models NLP底层技术之语言模型","link":"/posts/2242938984.html"},{"title":"深拷贝与浅拷贝的区别","text":"关于Python中的深拷贝和浅拷贝，有一篇很好的文章：图解Python深拷贝和浅拷贝，这篇文章用示例代码和图解很好地阐释了两者的区别。这里自己简单地归纳一下。 ## 直接赋值 首先，如果我们不进行拷贝，而是直接赋值，很有可能会出现意料之外的结果。比如a是一个列表，b=a，那么修改a的同时，b也会同样被修改，因为Python对象的赋值都是进行引用（内存地址）传递的，实际上a和b指向的都是同一个对象。 123456789&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = a&gt;&gt;&gt; a[2] = 4&gt;&gt;&gt; a[1, 2, 4]&gt;&gt;&gt; b[1, 2, 4]&gt;&gt;&gt; a is bTrue 浅拷贝 为了避免这种情况发生，我们可以使用浅拷贝： 1234567&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; import copy&gt;&gt;&gt; b = copy.copy(a)&gt;&gt;&gt; b is aFalse&gt;&gt;&gt; a[0] is b[0]True 浅拷贝会创建一个新的对象，然后把生成的新对象赋值给新变量。注意这句话的意思，上面这个例子中1，2，3这三个int型对象并没有创建新的，新的对象是指copy创建了一个新的列表对象，这样a和b这两个变量指向的列表对象就不是同一个，但和两个列表对象里面的元素依然是按引用传递的，所以a列表中的对象1和b列表中的对象1是同一个。 但是这时修改a列表的不可变对象，b列表不会受到影响： 12345&gt;&gt;&gt; a[0] = 4&gt;&gt;&gt; a[4, 2, 3]&gt;&gt;&gt; b[1, 2, 3] 由于浅拷贝时，对于对象中的元素，浅拷贝只会使用原始元素的引用（内存地址），所以如果对象中的元素是可变对象，浅拷贝就没辙了。比方说列表中包含一个列表，这时改动a，浅拷贝的b依然可能受影响： 123456789&gt;&gt;&gt; a = [1,2,[3,]]&gt;&gt;&gt; b = copy.copy(a)&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a[2].append(4)&gt;&gt;&gt; a[1, 2, [3, 4]]&gt;&gt;&gt; b[1, 2, [3, 4]] 可以产生浅拷贝的操作有以下几种： 使用切片[:]操作 使用工厂函数（如list/dir/set） &gt; 工厂函数看上去像函数，实质上是类，调用时实际上是生成了该类型的一个实例，就像工厂生产货物一样. 使用copy模块中的copy()函数 深拷贝 对于这个问题，又引入了深拷贝机制，这时不仅创建了新的对象，连对象中的元素都是新的，深拷贝都会重新生成一份，而不是简单的使用原始元素的引用（内存地址）。注意了，对象中的元素，不可变对象还是使用引用，因为没有重新生成的必要，变量改动时会自动生成另一个不可变对象，然后改变引用的地址。但可变对象的内容是可变的，改动后不会产生新的对象，也不会改变引用地址，所以需要重新生成。 12345678&gt;&gt;&gt; a = [1,2,[3,]]&gt;&gt;&gt; b = copy.deepcopy(a)&gt;&gt;&gt; a is bFalse&gt;&gt;&gt; a[0] is b[0]True&gt;&gt;&gt; a[2] is b[2]False 这时再改变a中的元素对b就完全没有影响了： 12345678[1, 2, [3]]&gt;&gt;&gt; a = [1,2,[3,]]&gt;&gt;&gt; b = copy.deepcopy(a)&gt;&gt;&gt; a[2].append(4)&gt;&gt;&gt; a[1, 2, [3, 4]]&gt;&gt;&gt; b[1, 2, [3]] 特殊情况 对于非容器类型（如数字、字符串、和其他'原子'类型的对象）是没有拷贝这个说法的。 1234567&gt;&gt;&gt; a = 'hello'&gt;&gt;&gt; b = copy.copy(a)&gt;&gt;&gt; c = copy.deepcopy(a)&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a is cTrue 对于这种类型的对象，无论是浅拷贝还是深拷贝都不会创建新的对象。 如果元祖变量只包含原子类型对象，则不能深拷贝： 原子类型指所有的数值类型以及字符串 12345678&gt;&gt;&gt; a=(1,2,3)&gt;&gt;&gt; a = (1,2,3)&gt;&gt;&gt; b = copy.copy(a)&gt;&gt;&gt; c = copy.deepcopy(a)&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a is cTrue 元组本身是不可变对象，如果元组里的元素也是不可变对象，就没有进行拷贝的必要了。实测如果元组里面的元素是只包含原子类型对象的元组，则也属于这个范畴。 1234567&gt;&gt;&gt; a = (1,2,(3,))&gt;&gt;&gt; b = copy.copy(a)&gt;&gt;&gt; c = copy.deepcopy(a)&gt;&gt;&gt; a is cTrue&gt;&gt;&gt; a is bTrue 总结 Python中对象的赋值都是进行对象引用（内存地址）传递 使用copy.copy()，可以进行对象的浅拷贝，它复制了对象，但对于对象中的元素，依然使用原始的引用. 如果需要复制一个容器对象，以及它里面的所有元素（包含元素的子元素），可以使用copy.deepcopy()进行深拷贝 非容器类型（如数字、字符串、和其他'原子'类型的对象）不存在拷贝。 只包含原子类型对象的元组变量不存在拷贝。","link":"/posts/16175.html"},{"title":"Distinct Subsequences","text":"Question leetcode: Distinct Subsequences lintcode: Distinct Subsequences ### Problem Statement Given a string S and a string T, count the number of distinct subsequences of T in S. A subsequence of a string is a new string which is formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (ie, \"ACE\" is a subsequence of \"ABCDE\" while \"AEC\" is not). Here is an example: S = \"rabbbit\", T = \"rabbit\" Return 3. 题解1 首先分清 subsequence 和 substring 两者的区别，subsequence 可以是不连续的子串。题意要求 S 中子序列 T 的个数。如果不考虑程序实现，我们能想到的办法是逐个比较 S 和 T 的首字符，相等的字符删掉，不等时则删除 S 中的首字符，继续比较后续字符直至 T 中字符串被删完。这种简单的思路有这么几个问题，题目问的是子序列的个数，而不是是否存在，故在字符不等时不能轻易删除掉 S 中的字符。那么如何才能得知子序列的个数呢？ 要想得知不同子序列的个数，那么我们就不能在 S 和 T 中首字符不等时简单移除 S 中的首字符了，取而代之的方法应该是先将 S 复制一份，再用移除 S 中首字符后的新字符串和 T 进行比较，这点和深搜中的剪枝函数的处理有点类似。 Python 1234567891011121314151617class Solution: # @param S, T: Two string. # @return: Count the number of distinct subsequences def numDistinct(self, S, T): if S is None or T is None: return 0 if len(S) &lt; len(T): return 0 if len(T) == 0: return 1 num = 0 for i, Si in enumerate(S): if Si == T[0]: num += self.numDistinct(S[i + 1:], T[1:]) return num C++ 12345678910111213141516171819202122class Solution {public: /** * @param S, T: Two string. * @return: Count the number of distinct subsequences */ int numDistinct(string &amp;S, string &amp;T) { if (S.size() &lt; T.size()) return 0; if (T.empty()) return 1; int num = 0; for (int i = 0; i &lt; S.size(); ++i) { if (S[i] == T[0]) { string Si = S.substr(i + 1); string t = T.substr(1); num += numDistinct(Si, t); } } return num; }}; Java 123456789101112131415161718192021public class Solution { /** * @param S, T: Two string. * @return: Count the number of distinct subsequences */ public int numDistinct(String S, String T) { if (S == null || T == null) return 0; if (S.length() &lt; T.length()) return 0; if (T.length() == 0) return 1; int num = 0; for (int i = 0; i &lt; S.length(); i++) { if (S.charAt(i) == T.charAt(0)) { // T.length() &gt;= 1, T.substring(1) will not throw index error num += numDistinct(S.substring(i + 1), T.substring(1)); } } return num; }} 源码分析 对 null 异常处理(C++ 中对 string 赋NULL 是错的，函数内部无法 handle 这种情况) S 字符串长度若小于 T 字符串长度，T 必然不是 S 的子序列，返回0 T 字符串长度为0，证明 T 是 S 的子序列，返回1 由于进入 for 循环的前提是 T.length() &gt;= 1, 故当 T 的长度为1时，Java 中对 T 取子串T.substring(1)时产生的是空串\"\"而并不抛出索引越界的异常。 复杂度分析 最好情况下，S 中没有和 T 相同的字符，时间复杂度为 \\[O(n)\\]; 最坏情况下，S 中的字符和 T 中字符完全相同，此时可以画出递归调用栈，发现和深搜非常类似，数学关系为 \\[f(n) = \\sum _{i = 1} ^{n - 1} f(i)\\], 这比 Fibonacci 的复杂度还要高很多。 题解2 - Dynamic Programming 从题解1 的复杂度分析中我们能发现由于存在较多的重叠子状态(相同子串被比较多次), 因此可以想到使用动态规划优化。但是动规的三大要素如何建立？由于本题为两个字符串之间的关系，故可以尝试使用双序列(DP_Two_Sequence)动规的思路求解。 定义f[i][j]为 S[0:i] 中子序列为 T[0:j] 的个数，接下来寻找状态转移关系，状态转移应从 f[i-1][j], f[i-1][j-1], f[i][j-1] 中寻找，接着寻找突破口——S[i] 和 T[j] 的关系。 S[i] == T[j]: 两个字符串的最后一个字符相等，我们可以选择 S[i] 和 T[j] 配对，那么此时有 f[i][j] = f[i-1][j-1]; 若不使 S[i] 和 T[j] 配对，而是选择 S[0:i-1] 中的某个字符和 T[j] 配对，那么 f[i][j] = f[i-1][j]. 综合以上两种选择，可得知在S[i] == T[j]时有 f[i][j] = f[i-1][j-1] + f[i-1][j] S[i] != T[j]: 最后一个字符不等时，S[i] 不可能和 T[j] 配对，故 f[i][j] = f[i-1][j] 为便于处理第一个字符相等的状态(便于累加)，初始化f[i][0]为1, 其余为0. 这里对于 S 或 T 为空串时返回0，返回1 也能说得过去。 Python 123456789101112131415161718192021class Solution: # @param S, T: Two string. # @return: Count the number of distinct subsequences def numDistinct(self, S, T): if S is None or T is None: return 0 if len(S) &lt; len(T): return 0 if len(T) == 0: return 1 f = [[0 for i in xrange(len(T) + 1)] for j in xrange(len(S) + 1)] for i, Si in enumerate(S): f[i][0] = 1 for j, Tj in enumerate(T): if Si == Tj: f[i + 1][j + 1] = f[i][j + 1] + f[i][j] else: f[i + 1][j + 1] = f[i][j + 1] return f[len(S)][len(T)] C++ 12345678910111213141516171819202122232425class Solution {public: /** * @param S, T: Two string. * @return: Count the number of distinct subsequences */ int numDistinct(string &amp;S, string &amp;T) { if (S.size() &lt; T.size()) return 0; if (T.empty()) return 1; vector&lt;vector&lt;int&gt; &gt; f(S.size() + 1, vector&lt;int&gt;(T.size() + 1, 0)); for (int i = 0; i &lt; S.size(); ++i) { f[i][0] = 1; for (int j = 0; j &lt; T.size(); ++j) { if (S[i] == T[j]) { f[i + 1][j + 1] = f[i][j + 1] + f[i][j]; } else { f[i + 1][j + 1] = f[i][j + 1]; } } } return f[S.size()][T.size()]; }}; Java 12345678910111213141516171819202122232425public class Solution { /** * @param S, T: Two string. * @return: Count the number of distinct subsequences */ public int numDistinct(String S, String T) { if (S == null || T == null) return 0; if (S.length() &lt; T.length()) return 0; if (T.length() == 0) return 1; int[][] f = new int[S.length() + 1][T.length() + 1]; for (int i = 0; i &lt; S.length(); i++) { f[i][0] = 1; for (int j = 0; j &lt; T.length(); j++) { if (S.charAt(i) == T.charAt(j)) { f[i + 1][j + 1] = f[i][j + 1] + f[i][j]; } else { f[i + 1][j + 1] = f[i][j + 1]; } } } return f[S.length()][T.length()]; }} 源码分析 异常处理部分和题解1 相同，初始化时维度均多一个元素便于处理。 复杂度分析 由于免去了重叠子状态的计算，双重 for 循环，时间复杂度为 \\[O(n^2)\\], 使用了二维矩阵保存状态，空间复杂度为 \\[O(n^2)\\]. 空间复杂度可以通过滚动数组的方式优化，详见 Dynamic Programming - 动态规划. 空间复杂度优化之后的代码如下： Java 1234567891011121314151617181920212223public class Solution { /** * @param S, T: Two string. * @return: Count the number of distinct subsequences */ public int numDistinct(String S, String T) { if (S == null || T == null) return 0; if (S.length() &lt; T.length()) return 0; if (T.length() == 0) return 1; int[] f = new int[T.length() + 1]; f[0] = 1; for (int i = 0; i &lt; S.length(); i++) { for (int j = T.length() - 1; j &gt;= 0; j--) { if (S.charAt(i) == T.charAt(j)) { f[j + 1] += f[j]; } } } return f[T.length()]; }} Reference LeetCode: Distinct Subsequences（不同子序列的个数） - 亦忘却_亦纪念 soulmachine leetcode-cpp 中 Distinct Subsequences 部分 Distinct Subsequences | Training dragons the hard way","link":"/posts/838402821.html"},{"title":"Dynamic Grid","text":"Source Dashboard - Round D APAC Test 2016 - Problem A. Dynamic Grid Problem We have a grid with R rows and C columns in which every entry is either 0 or 1. We are going to perform N operations on the grid, each of which is one of the following: Operation M: Change a number in one cell of the grid to 0 or 1 Operation Q: Determine the number of different connected regions of 1s. A connected region of 1s is a subset of cells that are all 1, in which any cell in the region can be reached from any other cell in the region by traveling between cells along edges (not corners). Input The first line of the input gives the number of test cases, T. T test cases follow. Each test case starts with one line with two integers, R and C, which represent the number of rows and columns in the grid. Then, there are R lines of C characters each, in which every character is either 0 or 1. These lines represent the initial state of the grid. The next line has one integer, N, the number of operations to perform on the grid. N more lines follow; each has one operation. All operation Ms will be of the form M x y z, meaning that the cell at row x and column y should be changed to the value z. All operation Qs will be of the form Q. Output For each test case, output one line containing \"Case #x:\", where x is the test case number (starting from 1). Then, for every operation Q in the test case, in order, output one line containing the number of connected regions of 1s. Limits 123451 ≤ T ≤ 10.1 ≤ R, C ≤ 100.0 ≤ x &lt; R.0 ≤ y &lt; C.0 ≤ z ≤ 1. Small dataset 11 ≤ N ≤ 10. Large dataset 11 ≤ N ≤ 1000. Smaple 12345678910111213141516Input Output1 Case #1:4 4 40101 20010 20100 211117QM 0 2 1QM 2 2 0QM 2 1 0Q 题解 简单题，DFS 即可解决。从矩阵中为1且未访问过的坐标开始 DFS，直至所有点均已遍历过，从矩阵中为1的坐标调用 DFS 的次数即为题中所求。具体实现可以使用布尔型矩阵标记访问过的坐标或者原地修改数组值。这里使用原地修改的方式，代码量略小一点。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import java.util.*;public class Solution { public static void main(String[] args) { Scanner in = new Scanner(System.in); int T = in.nextInt(); // System.out.println(&quot;T = &quot; + T); for (int t = 1; t &lt;= T; t++) { int R = in.nextInt(), C = in.nextInt(); int[][] grid = new int[R][C]; // System.out.println(&quot;R = &quot; + R + &quot;, C = &quot; + C); in.nextLine(); for (int r = 0; r &lt; R; r++) { String row = in.nextLine(); // System.out.println(row); for (int c = 0; c &lt; C; c++) { int z = Character.getNumericValue(row.charAt(c)); grid[r][c] = z; } } int N = in.nextInt(); in.nextLine(); System.out.printf(&quot;Case #%d:\\n&quot;, t); int[][] gridNew = new int[R][C]; for (int i = 0; i &lt; N; i++) { String[] tokens = in.nextLine().split(&quot; &quot;); if (tokens[0].equals(&quot;Q&quot;)) { for (int r = 0; r &lt; R; r++) { for (int c = 0; c &lt; C; c++) { gridNew[r][c] = grid[r][c]; } } int ans = solve(gridNew); // System.out.printf(&quot;M = %d, N = %d\\n&quot;, M, N); System.out.println(ans); } else { int tx = Integer.parseInt(tokens[1]); int ty = Integer.parseInt(tokens[2]); int tz = Integer.parseInt(tokens[3]); grid[tx][ty] = tz; } } } } public static int solve(int[][] grid) { if (grid == null || grid.length == 0) { return -1; } int R = grid.length, C = grid[0].length; int res = 0; for (int r = 0; r &lt; R; r++) { for (int c = 0; c &lt; C; c++) { if (grid[r][c] == 1) { dfs(grid, r, c); res++; } } } return res; } private static void dfs(int[][] grid, int x, int y) { int R = grid.length, C = grid[0].length; grid[x][y] = 0; for (int dx = -1; dx &lt;= 1; dx++) { for (int dy = -1; dy &lt;= 1; dy++) { int nx = x + dx, ny = y + dy; // up, down, left, right if (Math.abs(nx + ny - x - y) != 1) continue; if (0 &lt;= nx &amp;&amp; nx &lt; R &amp;&amp; 0 &lt;= ny &amp;&amp; ny &lt; C) { if (grid[nx][ny] == 1) dfs(grid, nx, ny); } } } }} 源码分析 Google Code Jam 上都是自己下载输入文件，上传结果，这里我们使用输入输出重定向的方法解决这个问题。举个例子，将这段代码保存为Solution.java, 将标准输入重定向至输入文件，标准输出重定向至输出文件。编译好之后以如下方式运行： 1java Solution &lt; A-large-practice.in &gt; A-large-practice.out 这种方式处理各种不同 OJ 平台的输入输出较为方便。 复杂度分析 对应每一个 Q，每个坐标点有四个方向，故单次求解时间复杂度 \\[O(4RC)\\], 空间复杂度 \\[O(RC)\\]. 有 T 次测试数据及 N/2 次查询，故最终总的时间复杂度为 \\[O(TNRC)\\], 代入 Large 数据集可知约为 \\[O(10^8)\\], 几分钟内是可以解出来的。 Reference 《挑战程序设计竞赛》穷竭搜索一章，POJ 八连通变形题","link":"/posts/4242456587.html"},{"title":"Dynamic Programming","text":"DP方法简介 由于其大量的计算损耗，已经不实用，但理论上非常重要。 本书后续的所有方法可以看做想要取得和DP类似的效果；只不过是减少了计算或者假设没有完美的环境模型。 假设解决的问题是有限的MDP，即给定动作a，状态s，和奖励r，可以通过\\(p(s',r|s,a)\\)描述动态变化。 Policy Evaluation 评估一个策略的好坏。 策略评估：计算某个policy对应的价值函数，也被称为prediction problem。 更新方法：使用上一章讲的Bellman Expectation Euqation for \\(v_{\\pi}\\)：\\(v_{\\pi}(s) = \\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\;\\;\\forall s \\in S\\) 核心代码： 1234567src = new_state_values if in_place else state_values...value = 0for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) value += ACTION_PROB * (reward + src[next_i, next_j])new_state_values[i, j] = value 其中，step函数即MDP的模型，会根据当前状态和动作产生下一个状态和奖励。 Policy Improvement 在当前的策略和相应的价值函数的基础上，使用价值函数贪婪地更新当前策略的过程。 policy improvement theorem：对所有的\\(s \\in S\\)，有\\(q_{\\pi}(s, \\pi'(s)) \\geq v_{\\pi}(s)\\)，则\\(v_{\\pi'}\\geq v_{\\pi}(s)\\)，即策略\\(\\pi'\\)优于策略\\(\\pi\\)。 greedy policy：\\(\\pi'(s)=\\underset{a}{argmax}q_{\\pi}(s,a)=\\underset{a}{argmax}\\sum_{s', r}p(s', r|s,a)[r+\\gamma v_{\\pi}(s')]\\) 核心代码： 123456789policy = np.zeros(value.shape, dtype=np.int)...# policy improvementnew_policy = np.copy(policy)for i in range(MAX_CARS + 1): for j in range(MAX_CARS + 1): action_returns = [] ... new_policy[i, j] = actions[np.argmax(action_returns)] Policy Iteration 其中，\\(E\\)代表策略的evaluation，\\(I\\)代表策略的improvement。简单地说，Policy Iteration就是不断地评估策略然后改进策略。 核心代码： 1234policy_change = (new_policy != policy).sum()if policy_change == 0: ... break Value Iteration Policy Iteration的缺点：每一次迭代都要评估策略，而每一次策略评估本身都是一个迭代过程。 \\(v_{k+1}(s)=\\underset{a}{max} E[R_{t+1}+\\gamma v_k(S_{t+1})|S_t=s, A_t=a]\\) \\(=\\underset{a}{max}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]\\) 实际上就是Bellman Optimality Euqation for \\(v_*(s)\\)。 Value Iteration简单地说就是每次评估价值的时候直接用可能的用最优价值函数更新价值函数（这样的每一步不涉及策略本身）；在确定已经获得比较准确的价值评估之后，再一次性确定策略。 核心代码： 12345678910111213141516 # value iterationwhile True: delta = 0.0 for state in STATES[1:GOAL]: # get possilbe actions for current state actions = np.arange(min(state, GOAL - state) + 1) action_returns = [] for action in actions: action_returns.append( HEAD_PROB * state_value[state + action] + (1 - HEAD_PROB) * state_value[state - action]) new_value = np.max(action_returns) delta += np.abs(state_value[state] - new_value) # update state value state_value[state] = new_value if delta &lt; 1e-9: break","link":"/posts/48659.html"},{"title":"Edit Distance","text":"Question leetcode: Edit Distance | LeetCode OJ lintcode: (119) Edit Distance 12345678910Given two words word1 and word2, find the minimum number of steps required to convert word1 to word2. (each operation is counted as 1 step.)You have the following 3 operations permitted on a word:Insert a characterDelete a characterReplace a characterExampleGiven word1 = &quot;mart&quot; and word2 = &quot;karma&quot;, return 3. 题解1 - 双序列动态规划 两个字符串比较，求最值，直接看似乎并不能直接找出解决方案，这时往往需要使用动态规划的思想寻找递推关系。使用双序列动态规划的通用做法，不妨定义f[i][j]为字符串1的前i个字符和字符串2的前j个字符的编辑距离，那么接下来寻找其递推关系。增删操作互为逆操作，即增或者删产生的步数都是一样的。故初始化时容易知道f[0][j] = j, f[i][0] = i, 接下来探讨f[i][j] 和f[i - 1][j - 1]的关系，和 LCS 问题类似，我们分两种情况讨论，即word1[i] == word2[j] 与否，第一种相等的情况有： i == j, 且有word1[i] == word2[j], 则由f[i - 1][j - 1] -&gt; f[i][j] 不增加任何操作，有f[i][j] = f[i - 1][j - 1]. i != j, 由于字符数不等，肯定需要增/删一个字符，但是增删 word1 还是 word2 是不知道的，故可取其中编辑距离的较小值，即f[i][j] = 1 + min{f[i - 1][j], f[i][j - 1]}. 第二种不等的情况有： i == j, 有f[i][j] = 1 + f[i - 1][j - 1]. i != j, 由于字符数不等，肯定需要增/删一个字符，但是增删 word1 还是 word2 是不知道的，故可取其中编辑距离的较小值，即f[i][j] = 1 + min{f[i - 1][j], f[i][j - 1]}. 最后返回f[len(word1)][len(word2)] Python 123456789101112131415161718192021class Solution: # @param word1 &amp; word2: Two string. # @return: The minimum number of steps. def minDistance(self, word1, word2): len1, len2 = 0, 0 if word1: len1 = len(word1) if word2: len2 = len(word2) if not word1 or not word2: return max(len1, len2) f = [[i + j for i in xrange(1 + len2)] for j in xrange(1 + len1)] for i in xrange(1, 1 + len1): for j in xrange(1, 1 + len2): if word1[i - 1] == word2[j - 1]: f[i][j] = min(f[i - 1][j - 1], 1 + f[i - 1][j], 1 + f[i][j - 1]) else: f[i][j] = 1 + min(f[i - 1][j - 1], f[i - 1][j], f[i][j - 1]) return f[len1][len2] C++ 12345678910111213141516171819202122232425262728293031323334353637class Solution {public: /** * @param word1 &amp; word2: Two string. * @return: The minimum number of steps. */ int fistance(string word1, string word2) { if (word1.empty() || word2.empty()) { return max(word1.size(), word2.size()); } int len1 = word1.size(); int len2 = word2.size(); vector&lt;vector&lt;int&gt; &gt; f = \\ vector&lt;vector&lt;int&gt; &gt;(1 + len1, vector&lt;int&gt;(1 + len2, 0)); for (int i = 0; i &lt;= len1; ++i) { f[i][0] = i; } for (int i = 0; i &lt;= len2; ++i) { f[0][i] = i; } for (int i = 1; i &lt;= len1; ++i) { for (int j = 1; j &lt;= len2; ++j) { if (word1[i - 1] == word2[j - 1]) { f[i][j] = min(f[i - 1][j - 1], 1 + f[i - 1][j]); f[i][j] = min(f[i][j], 1 + f[i][j - 1]); } else { f[i][j] = min(f[i - 1][j - 1], f[i - 1][j]); f[i][j] = 1 + min(f[i][j], f[i][j - 1]); } } } return f[len1][len2]; }}; Java 12345678910111213141516171819202122232425262728293031323334public class Solution { public int minDistance(String word1, String word2) { int len1 = 0, len2 = 0; if (word1 != null &amp;&amp; word2 != null) { len1 = word1.length(); len2 = word2.length(); } if (word1 == null || word2 == null) { return Math.max(len1, len2); } int[][] f = new int[1 + len1][1 + len2]; for (int i = 0; i &lt;= len1; i++) { f[i][0] = i; } for (int i = 0; i &lt;= len2; i++) { f[0][i] = i; } for (int i = 1; i &lt;= len1; i++) { for (int j = 1; j &lt;= len2; j++) { if (word1.charAt(i - 1) == word2.charAt(j - 1)) { f[i][j] = Math.min(f[i - 1][j - 1], 1 + f[i - 1][j]); f[i][j] = Math.min(f[i][j], 1 + f[i][j - 1]); } else { f[i][j] = Math.min(f[i - 1][j - 1], f[i - 1][j]); f[i][j] = 1 + Math.min(f[i][j], f[i][j - 1]); } } } return f[len1][len2]; }} 源码解析 边界处理 初始化二维矩阵(Python 中初始化时 list 中 len2 在前，len1 在后) i, j 从1开始计数，比较 word1 和 word2 时注意下标 返回f[len1][len2] 复杂度分析 两重 for 循环，时间复杂度为 \\[O(len1 \\cdot len2)\\]. 使用二维矩阵，空间复杂度为 \\[O(len1 \\cdot len2)\\].","link":"/posts/1385044751.html"},{"title":"Find Peak Element","text":"Question leetcode: Find Peak Element | LeetCode OJ lintcode: (75) Find Peak Element ### Problem Statement A peak element is an element that is greater than its neighbors. Given an input array where num[i] ≠ num[i+1], find a peak element and return its index. The array may contain multiple peaks, in that case return the index to any one of the peaks is fine. You may imagine that num[-1] = num[n] = -∞. For example, in array [1, 2, 3, 1], 3 is a peak element and your function should return the index number 2. Note: Your solution should be in logarithmic complexity. Credits: Special thanks to [@ts](https://oj.leetcode.com/discuss/user/ts) for adding this problem and creating all test cases. 题解1 由时间复杂度的暗示可知应使用二分搜索。首先分析若使用传统的二分搜索，若A[mid] &gt; A[mid - 1] &amp;&amp; A[mid] &lt; A[mid + 1]，则找到一个peak为A[mid]；若A[mid - 1] &gt; A[mid]，则A[mid]左侧必定存在一个peak，可用反证法证明：若左侧不存在peak，则A[mid]左侧元素必满足A[0] &gt; A[1] &gt; ... &gt; A[mid -1] &gt; A[mid]，与已知A[0] &lt; A[1]矛盾，证毕。同理可得若A[mid + 1] &gt; A[mid]，则A[mid]右侧必定存在一个peak。如此迭代即可得解。 由于题中假设端点外侧的值均为负无穷大，即num[-1] &lt; num[0] &amp;&amp; num[n-1] &gt; num[n], 那么问题来了，这样一来就不能确定峰值一定存在了，因为给定数组为单调序列的话就咩有峰值了，但是实际情况是——题中有负无穷的假设，也就是说在单调序列的情况下，峰值为数组首部或者尾部元素，谁大就是谁了。 备注：如果本题是找 first/last peak，就不能用二分法了。 Python 123456789101112131415161718class Solution: #@param A: An integers list. #@return: return any of peek positions. def findPeak(self, A): if not A: return -1 l, r = 0, len(A) - 1 while l + 1 &lt; r: mid = l + (r - l) / 2 if A[mid] &lt; A[mid - 1]: r = mid elif A[mid] &lt; A[mid + 1]: l = mid else: return mid mid = l if A[l] &gt; A[r] else r return mid C++ 12345678910111213141516171819202122232425class Solution {public: /** * @param A: An integers array. * @return: return any of peek positions. */ int findPeak(vector&lt;int&gt; A) { if (A.size() == 0) return -1; int l = 0, r = A.size() - 1; while (l + 1 &lt; r) { int mid = l + (r - l) / 2; if (A[mid] &lt; A[mid - 1]) { r = mid; } else if (A[mid] &lt; A[mid + 1]) { l = mid; } else { return mid; } } int mid = A[l] &gt; A[r] ? l : r; return mid; }}; Java 12345678910111213141516171819202122232425class Solution { /** * @param A: An integers array. * @return: return any of peek positions. */ public int findPeak(int[] A) { if (A == null || A.length == 0) return -1; int lb = 0, ub = A.length - 1; while (lb + 1 &lt; ub) { int mid = lb + (ub - lb) / 2; if (A[mid] &lt; A[mid + 1]) { lb = mid; } else if (A[mid] &lt; A[mid - 1]){ ub = mid; } else { // find a peak return mid; } } // return a larger number return A[lb] &gt; A[ub] ? lb : ub; }} 源码分析 典型的二分法模板应用，需要注意的是需要考虑单调序列的特殊情况。当然也可使用紧凑一点的实现如改写循环条件为l &lt; r，这样就不用考虑单调序列了，见实现2. 复杂度分析 二分法，时间复杂度 \\[O(\\log n)\\]. Java - compact implementation1 123456789101112131415161718192021public class Solution { public int findPeakElement(int[] nums) { if (nums == null || nums.length == 0) { return -1; } int start = 0, end = nums.length - 1, mid = end / 2; while (start &lt; end) { if (nums[mid] &lt; nums[mid + 1]) { // 1 peak at least in the right side start = mid + 1; } else { // 1 peak at least in the left side end = mid; } mid = start + (end - start) / 2; } return start; }} C++ 的代码可参考 Java 或者 @xuewei4d 的实现。 Warning leetcode 和 lintcode 上给的方法名不一样，leetcode 上的为findPeakElement而 lintcode 上为findPeak，弄混的话会编译错误。 Reference Java - Binary-Search Solution - Leetcode Discuss↩︎","link":"/posts/1240038533.html"},{"title":"Finite Markov DecisionProcesses","text":"Agent和Environment的交互 学习者和决策者称为agent。 agent交互的对象，外部环境，称为Environment。 在时刻t，agent的所处的环境用状态：\\(S_t \\in S\\)表示，\\(S\\)是可能的状态集。假设agent采用了动作\\(A_t\\in A(S_t)\\)，\\(A(S_t)\\)代表在状态\\(S_t\\)下可能的动作集。 到了下一个时刻t+1，agent收到了一个奖励：\\(R_{t+1} \\in R\\)，并且发现自己处在一个新的state中：\\(S_{t+1}\\)。 什么是有限 有限MDP中的有限意味着：状态空间S、动作空间A和奖励空间R都是离散且有限的。 目标和奖励的区别 每个时间节点上，agent都会收到一个奖励的数值：\\(R_t\\)。 但是，agent的目标应该是：所有时间结点上的奖励的和的最大化。 即：\\(G_t = R_{t+1} + R_{t+2} + ... + R_T\\) 什么是Episode 一系列的agent和environment交互序列。每个episode之间相互不影响，且都是有一个相同的终止状态（terminate state）。 Continuing Tasks 不同于Episode类的任务，这类任务没有终止状态，也就是说\\(T = \\infty\\)。因此，如果考虑这类任务，那么\\(G_t\\)将是无穷大，于是引入discounting。 Episode和Continuing Tasks的统一规范 在Episode的尾部加入吸收状态（absorbing state），在此状态下，奖励永远是0，且下一个状态永远是当前状态。 这样收益可以统一使用下面的Expected Discounted Return表示。 Expected Discounted Return 回报：\\(G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\sum_{k=0}^{\\infty} \\gamma ^k R_{t+k+1}\\) \\(\\gamma\\)是参数，且\\(0\\leq \\gamma \\leq 1\\)，被称为discount rate。 含义：一个奖励，如果是在k个时间节点以后收到，那么对于当前来说，它的价值是即时奖励的\\(\\gamma^{k-1}\\)倍。 从\\(G_t\\)的定义，很容易获得递推式：\\(G_t = R_{t+1} + \\gamma G_{t+1}\\) 马尔科夫性质（Markov property） 核心思想：当前state继承了所有的环境历史信息。 \\(Pr\\{S_{t+1}=s', R_{t+1}=r|S_0,A_0,R_1,...,S_{t-1},A_{t_1},R_t,S_t,A_t\\} = Pr\\{S_{t+1}=s', R_{t+1}=r|S_t,A_t\\}\\) 即便state是非马尔科夫的，我们也希望近似到马尔科夫。 Markov Dicision Processes（MDP） 满足马尔科夫性质的强化学习任务称为MDP。 如果state和action空间有限，则称为有限MDP（涵盖了现代强化学习90%的问题）。 用\\(p(s',r|s,a)\\)表示\\(Pr\\{S_{t+1}={s}', R_{t+1}=r|S_t,A_t\\}\\)，这个条件概率是描绘了整个MDP的动态（Dynamics）。 state-action期望奖励：\\(r(s,a) = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\\sum_{r\\in R}r\\sum_{s'\\in S}p(s', r|s,a)\\) 状态转移概率：\\(p(s'|s,a) = Pr\\{S_{t+1}=s'|S_t=s, A_t=a\\}=\\sum_{r\\in R}p(s', r| s, a)\\) state-action-nextstate期望奖励：\\(r(s, a, s') = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a, S_{t+1}=s'] = \\sum_{r\\in R}r\\frac{p(s',r|s,a)}{p(s'|s,a)}\\) 价值函数 关于策略\\(\\pi\\)的state-value函数：\\(v_{\\pi}(s) = {\\mathbb{E}}_{\\pi}[G_t|S_t=s] = \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s]\\) 即，在使用策略\\(\\pi\\)的前提下，衡量处于某个state有多好。 关于策略\\(\\pi\\)的action-value函数：\\(q_{\\pi}(a,s) = \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a] = \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s,A_t=a]\\) 即，在使用策略\\(\\pi\\)的前提下，衡量处于某个state下，执行某个action有多好。 Bellman Euqation Bellman Expectation Euqation for \\(v_{\\pi}\\)：\\(v_{\\pi}(s) = \\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\;\\;\\forall s \\in S\\) 理解： 1.方括号中是根据后继状态的价值重新估计的价值函数，再在动作空间、后继状态空间和动作空间用相应的概率做加权求和。 2.表达的是某个状态的价值和其后继状态的价值之间的关系。 backup：是强化学习方法的核心，以时序意义上的回退，用下一个时刻的值去评估当前时刻的值。 Bellman Expectation Euqation for \\(q_{\\pi}\\)：\\(q_{\\pi}(s,a) = \\sum_{s'}p(s',r|s,a)[r+\\gamma \\sum_{a'}q(s',a')]\\) 推导： \\(v_\\pi(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]\\) \\(= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]\\) \\(= \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) [r + \\gamma v_\\pi(s')].\\) \\(q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a]\\) \\(= \\mathbb{E}_\\pi [R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a]\\) \\(= \\sum_{s',r}p(s',r|s,a)[r+\\gamma \\sum_{a'}q(s',a')]\\) 参考资料：https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/ 最优化价值函数 \\(v_*(s) = \\underset{\\pi}{max}v_{\\pi}(s)\\) \\(q_*(s,a) = \\underset{\\pi}{max}q_{\\pi}(s,a)\\) Bellman Optimality Euqation for \\(v_*(s)\\)：\\(v_*(s)=\\underset{a\\in A(s)}{max}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]\\) Bellman Optimality Euqation for \\(q_*(s,a)\\)：\\(q_*(s,a)=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\underset{a'}{max}q_*(s', a')]\\) 代码分析 完整源码 问题描述： 动作空间：上下左右。 奖励：到A点会有10的奖励，且会被带到A'，到B有10的奖励，且会被带到B'。如果动作要把agent带离格子世界，则agent不懂，且奖励是-1。其他动作奖励是0。 假设初始策略是等概率地选择上下左右这四个动作： 123456# left, up, right, downACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]ACTION_PROB = 0.25 下面代码描述了给定当前状态和动作，下一个状态和奖励是什么。 123456789101112131415def step(state, action): if state == A_POS: return A_PRIME_POS, 10 if state == B_POS: return B_PRIME_POS, 5 state = np.array(state) next_state = (state + action).tolist() x, y = next_state if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE: reward = -1.0 next_state = state else: reward = 0 return next_state, reward 下面代码使用bellman equation做backup，去迭代价值函数： 123456for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): for action in ACTIONS: (next_i, next_j), reward = step([i, j], action) # bellman equation for value function new_value[i, j] += ACTION_PROB * (reward + DISCOUNT * value[next_i, next_j])","link":"/posts/32633.html"},{"title":"First Bad Version","text":"Question lintcode: (74) First Bad Version ### Problem Statement The code base version is an integer start from 1 to n. One day, someone committed a bad version in the code case, so it caused this version and the following versions are all failed in the unit tests. Find the first bad version. You can call isBadVersion to help you determine which version is the first bad one. The details interface can be found in the code's annotation part. Example Given n = 5: isBadVersion(3) -&gt; false isBadVersion(5) -&gt; true isBadVersion(4) -&gt; true Here we are 100% sure that the 4th version is the first bad version. Note Please read the annotation in code area to get the correct way to call isBadVersion in different language. For example, Java is VersionControl.isBadVersion(v) Challenge You should call isBadVersion as few as possible. 题解 基础算法中 Binary Search 的 lower bound. 找出满足条件的下界即可。 Python 12345678910111213141516171819202122#class VersionControl:# @classmethod# def isBadVersion(cls, id)# # Run unit tests to check whether verison `id` is a bad version# # return true if unit tests passed else false.# You can use VersionControl.isBadVersion(10) to check whether version 10 is a# bad version.class Solution: &quot;&quot;&quot; @param n: An integers. @return: An integer which is the first bad version. &quot;&quot;&quot; def findFirstBadVersion(self, n): lb, ub = 0, n + 1 while lb + 1 &lt; ub: mid = lb + (ub - lb) / 2 if VersionControl.isBadVersion(mid): ub = mid else: lb = mid return lb + 1 C++ 12345678910111213141516171819202122232425262728/** * class VersionControl { * public: * static bool isBadVersion(int k); * } * you can use VersionControl::isBadVersion(k) to judge whether * the kth code version is bad or not.*/class Solution {public: /** * @param n: An integers. * @return: An integer which is the first bad version. */ int findFirstBadVersion(int n) { int lb = 0, ub = n + 1; while (lb + 1 &lt; ub) { int mid = lb + (ub - lb) / 2; if (VersionControl::isBadVersion(mid)) { ub = mid; } else { lb = mid; } } return lb + 1; }}; Java 1234567891011121314151617181920212223242526/** * public class VersionControl { * public static boolean isBadVersion(int k); * } * you can use VersionControl.isBadVersion(k) to judge whether * the kth code version is bad or not.*/class Solution { /** * @param n: An integers. * @return: An integer which is the first bad version. */ public int findFirstBadVersion(int n) { int lb = 0, ub = n + 1; while (lb + 1 &lt; ub) { int mid = lb + (ub - lb) / 2; if (VersionControl.isBadVersion(mid)) { ub = mid; } else { lb = mid; } } return lb + 1; }} 源码分析 lower bound 的实现，这里稍微注意下lb 初始化为 0，因为 n 从1开始。ub 和 lb 分别都在什么条件下更新就好了。另外这里并未考虑 n &lt;= 0 的情况。 复杂度分析 二分搜索，\\[O(\\log n)\\].","link":"/posts/674945262.html"},{"title":"First Position of Target","text":"Question lintcode: First Position of Target ### Problem Statement For a given sorted array (ascending order) and a target number, find the first index of this number in O(log n) time complexity. If the target number does not exist in the array, return -1. Example If the array is [1, 2, 3, 3, 4, 5, 10], for given target 3, return 2. Challenge If the count of numbers is bigger than \\[2^{32}\\], can your code work properly? 题解 对于已排序升序(升序)数组，使用二分查找可满足复杂度要求，注意数组中可能有重复值，所以需要使用类似lower_bound中提到的方法。 Java 12345678910111213141516171819202122232425262728293031class Solution { /** * @param nums: The integer array. * @param target: Target to find. * @return: The first position of target. Position starts from 0. */ public int binarySearch(int[] nums, int target) { if (nums == null || nums.length == 0) { return -1; } int start = -1, end = nums.length; int mid; while (start + 1 &lt; end) { // avoid overflow when (end + start) mid = start + (end - start) / 2; if (nums[mid] &lt; target) { start = mid; } else { end = mid; } } if (end == nums.length || nums[end] != target) { return -1; } else { return end; } }} 源码分析 首先对输入做异常处理，数组为空或者长度为0。 初始化 start, end, mid三个变量，这里start初始化为-1主要是考虑到end为1。注意mid的求值方法，可以防止两个整型值相加时溢出。 使用迭代而不是递归进行二分查找，因为工程中递归写法存在潜在溢出的可能。 while终止条件应为start + 1 &lt; end而不是start &lt;= end，start == end时可能出现死循环。即循环终止条件是相邻或相交元素时退出。由于这里初始化时start &lt; end，所以一定是start + 1 == end时退出循环。 迭代终止时有两种情况，一种是在原数组中找到了，这种情况下一定是end, 因为start的更新只在nums[mid] &lt; target. 最后判断end和target的关系，先排除end为数组长度这种会引起越界的情况，然后再判断和目标值是否相等。 复杂度分析 时间复杂度 \\[O(\\log n)\\], 空间复杂度 \\[(1)\\]. 对于题中的 follow up, Java 中数组不允许使用 long 型，如果使用 long 型，那么数组大小可大 17GB 之巨！！几乎没法用。 Reference 《挑战程序设计竞赛》3.1节","link":"/posts/3156097806.html"},{"title":"GloVe数学原理详解","text":"什么是GloVe？ 正如论文的标题而言，GloVe的全称叫Global Vectors for Word Representation，它是一个基于全局词频统计（count-based &amp; overall statistics）的词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等。我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。 GloVe是如何实现的？ 根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）\\(X\\)，矩阵中的每一个元素\\(X_{ij}\\)代表单词\\(i\\)和上下文单词\\(j\\)在特定大小的上下文窗口（context window）内共同出现的次数。一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离\\(d\\)，提出了一个衰减函数（decreasing weighting）：\\(decay = 1/d\\)用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。 &gt;In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count. 构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系： \\[ w_{i}^{T}\\tilde{w_{j}} + b_i + \\tilde{b_j} = \\log(X_{ij}) \\tag{1} \\] 其中，\\(w_{i}^{T}\\)和\\(\\tilde{w_{j}}\\)是我们最终要求解的词向量；\\(b_i\\)和\\(\\tilde{b_j}\\)分别是两个词向量的bias term。当然你对这个公式一定有非常多的疑问，比如它到底是怎么来的，为什么要使用这个公式，为什么要构造两个词向量\\(w_{i}^{T}\\)和\\(\\tilde{w_{j}}\\)？下文我们会详细介绍。 有了公式1之后我们就可以构造它的loss function了： \\[ J = \\sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\\tilde{w_{j}} + b_i + \\tilde{b_j} – \\log(X_{ij}) )^2 \\tag{2} \\] 这个loss function的基本形式就是最简单的mean square loss，只不过在此基础上加了一个权重函数\\(f(X_{ij})\\)，那么这个函数起了什么作用，为什么要添加这个函数呢？我们知道在一个语料库中，肯定存在很多单词他们在一起出现的次数是很多的（frequent co-occurrences），那么我们希望： 这些单词的权重要大于那些很少在一起出现的单词（rare co-occurrences），所以这个函数要是非递减函数（non-decreasing） 但我们也不希望这个权重过大（overweighted），当到达一定程度之后应该不再增加； 如果两个单词没有在一起出现，也就是\\(X_{ij}=0\\)，那么他们应该不参与到loss function的计算当中去，也就是\\(f(x)\\)要满足\\(f(0)=0\\) 满足以上两个条件的函数有很多，作者采用了如下形式的分段函数： \\[ \\begin{equation} f(x) = \\Biggl\\{ \\begin{array}{lr} (x/x_{max})^\\alpha \\quad if \\; x &lt; x_{max}, &amp; \\\\ 1 \\quad otherwise\\\\ \\end{array} \\end{equation} \\tag{3} \\] 这个函数图像如下所示： 这篇论文中的所有实验，\\(\\alpha\\)的取值都是0.75，而\\(x_{max}\\)取值都是100。以上就是GloVe的实现细节，那么GloVe是如何训练的呢？ GloVe是如何训练的？ 虽然很多人声称GloVe是一种无监督（unsupervised learing）的学习方式（因为它确实不需要人工标注label），但其实它还是有label的，这个label就是公式2中的\\(\\log(X_{ij})\\)，而公式2中的向量\\(w\\)和\\(\\tilde{w}\\)就是要不断更新/学习的参数，所以本质上它的训练方式跟监督学习的训练方法没什么不一样，都是基于梯度下降的。具体地，这篇论文里的实验是这么做的：采用了AdaGrad的梯度下降算法，对矩阵\\(X\\)中的所有非零元素进行随机采样，学习曲率（learning rate）设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。最终学习得到的是两个vector是\\(w\\)和\\(\\tilde{w}\\)，因为\\(X\\)是对称的（symmetric），所以从原理上讲\\(w\\)和\\(\\tilde{w}\\)是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样。所以这两者其实是等价的，都可以当成最终的结果来使用。但是为了提高鲁棒性，我们最终会选择两者之和\\(w + \\tilde{w}\\)作为最终的vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）。在训练了400亿个token组成的语料后，得到的实验结果如下图所示： 这个图一共采用了三个指标：语义准确度，语法准确度以及总体准确度。那么我们不难发现Vector Dimension在300时能达到最佳，而context Windows size大致在6到10之间。 Glove与LSA、word2vec的比较 LSA（Latent Semantic Analysis）是一种比较早的count-based的词向量表征工具，它也是基于co-occurance matrix的，只不过采用了基于奇异值分解（SVD）的矩阵分解技术对大矩阵进行降维，而我们知道SVD的复杂度是很高的，所以它的计算代价比较大。还有一点是它对所有单词的统计权重都是一致的。而这些缺点在GloVe中被一一克服了。而word2vec最大的缺点则是没有充分利用所有的语料，所以GloVe其实是把两者的优点结合了起来。从这篇论文给出的实验结果来看，GloVe的性能是远超LSA和word2vec的，但网上也有人说GloVe和word2vec实际表现其实差不多。 公式推导 写到这里GloVe的内容基本就讲完了，唯一的一个疑惑就是公式1到底是怎么来的？如果你有兴趣可以继续看下去，如果没有，可以关掉浏览器窗口了。为了把这个问题说清楚，我们先定义一些变量： \\(X_{ij}\\)表示单词\\(j\\)出现在单词\\(i\\)的上下文中的次数； \\(X_i\\)表示单词\\(i\\)的上下文中所有单词出现的总次数，即\\(X_{i} = \\sum^{k} X_{ik}\\) \\(P_{ij} = P(j|i) = X_{ij}/X_{i}\\)，即表示单词\\(j\\)出现在单词\\(i\\)的上下文中的概率； 有了这些定义之后，我们来看一个表格： 理解这个表格的重点在最后一行，它表示的是两个概率的比值（ratio），我们可以使用它观察出两个单词\\(i\\)和\\(j\\)相对于单词\\(k\\)哪个更相关（relevant）。比如，ice和solid更相关，而stream和solid明显不相关，于是我们会发现\\(P(solid|ice)/P(solid|steam)\\)比1大更多。同样的gas和steam更相关，而和ice不相关，那么\\(P(gas|ice)/P(gas|steam)\\)就远小于1；当都有关（比如water）或者都没有关(fashion)的时候，两者的比例接近于1；这个是很直观的。因此，以上推断可以说明通过概率的比例而不是概率本身去学习词向量可能是一个更恰当的方法，因此下文所有内容都围绕这一点展开。 于是为了捕捉上面提到的概率比例，我们可以构造如下函数： \\[ F(w_i,w_j,\\tilde{w_k}) = \\frac{P_{ik}}{P_{jk}},\\tag{4} \\] 其中，函数\\(F\\)的参数和具体形式未定，它有三个参数\\(w_i\\),\\(w_j\\)和\\(\\tilde{w_k}\\)，\\(w\\)和\\(\\tilde{w}\\)是不同的向量； 因为向量空间是线性结构的，所以要表达出两个概率的比例差，最简单的办法是作差，于是我们得到： \\[ F(w_i-w_j,\\tilde{w_k}) = \\frac{P_{ik}}{P_{jk}},\\tag{5} \\] 这时我们发现公式5的右侧是一个数量，而左侧则是一个向量，于是我们把左侧转换成两个向量的内积形式： \\[ F((w_i-w_j)^T \\tilde{w_k}) = \\frac{P_{ik}}{P_{jk}},\\tag{6} \\] 我们知道\\(X\\)是个对称矩阵，单词和上下文单词其实是相对的，也就是如果我们做如下交换： \\(w\\leftrightarrow \\tilde{w_k}\\)，\\(X \\leftrightarrow X^T\\)公式6应该保持不变，那么很显然，现在的公式是不满足的。为了满足这个条件，首先，我们要求函数\\(F\\)要满足同态特性（homomorphism）： \\[ F((w_i-w_j)^T \\tilde{w_k}) = \\frac{F(w_i^T \\tilde{w_k}) }{F(w_j^T \\tilde{w_k})},\\tag{7} \\] 结合公式6，我们可以得到： \\[ F(w_i^T \\tilde{w_k}) = P_{ik} = \\frac{X_{ik}}{X_i},\\tag{8} \\] 然后，我们令\\(F = exp\\)，于是我们有： \\[ w_i^T \\tilde{w_k} = \\log(P_{ik}) = \\log(X_{ik}) – \\log({X_i}),\\tag{9} \\] 此时，我们发现因为等号右侧的\\(\\log(X_i)\\)的存在，公式9是不满足对称性（symmetry）的，而且这个\\(\\log(X_i)\\)其实是跟\\(k\\)独立的，它只跟\\(i\\)有关，于是我们可以针对\\(w_i\\)增加一个bias term \\(b_i\\)把它替换掉，于是我们有： \\[ w_i^T \\tilde{w_k} + b_i= \\log(X_{ik}), \\tag{10} \\] 但是公式10还是不满足对称性，于是我们针对\\(w_k\\)增加一个bias term \\(b_k\\)，从而得到公式1的形式： \\[ w_i^T \\tilde{w_k} + b_i + b_k= \\log(X_{ik}), \\tag{11} \\] 以上内容其实不能完全称之为推导，因为有很多不严谨的地方，只能说是解释作者如何一步一步构造出这个公式的，仅此而已。","link":"/posts/2109189733.html"},{"title":"迭代再加权最小二乘","text":"原理 迭代再加权最小二乘(IRLS)用于解决特定的最优化问题，这个最优化问题的目标函数如下所示： \\[arg min_{\\beta} \\sum_{i=1}^{n}|y_{i} - f_{i}(\\beta)|^{p}\\] 这个目标函数可以通过迭代的方法求解。在每次迭代中，解决一个带权最小二乘问题，形式如下： \\[\\beta ^{t+1} = argmin_{\\beta} \\sum_{i=1}^{n} w_{i}(\\beta^{(t)}))|y_{i} - f_{i}(\\beta)|^{2} = (X^{T}W^{(t)}X)^{-1}X^{T}W^{(t)}y\\] 在这个公式中，\\(W^{(t)}\\)是权重对角矩阵，它的所有元素都初始化为1。每次迭代中，通过下面的公式更新。 \\[W_{i}^{(t)} = |y_{i} - X_{i}\\beta^{(t)}|^{p-2}\\] 源码分析 在spark ml中，迭代再加权最小二乘主要解决广义线性回归问题。下面看看实现代码。 更新权重 12345// Update offsets and weights using reweightFuncval newInstances = instances.map { instance =&gt; val (newOffset, newWeight) = reweightFunc(instance, oldModel) Instance(newOffset, newWeight, instance.features)} 这里使用reweightFunc方法更新权重。具体的实现在广义线性回归的实现中。 123456789101112131415/** * The reweight function used to update offsets and weights * at each iteration of [[IterativelyReweightedLeastSquares]]. */val reweightFunc: (Instance, WeightedLeastSquaresModel) =&gt; (Double, Double) = { (instance: Instance, model: WeightedLeastSquaresModel) =&gt; { val eta = model.predict(instance.features) val mu = fitted(eta) val offset = eta + (instance.label - mu) * link.deriv(mu) val weight = instance.weight / (math.pow(this.link.deriv(mu), 2.0) * family.variance(mu)) (offset, weight) }}def fitted(eta: Double): Double = family.project(link.unlink(eta)) 这里的model.predict利用带权最小二乘模型预测样本的取值，然后调用fitted方法计算均值函数\\(\\mu\\)。offset表示 更新后的标签值，weight表示更新后的权重。关于链接函数的相关计算可以参考广义线性回归的分析。 有一点需要说明的是，这段代码中标签和权重的更新并没有参照上面的原理或者说我理解有误。 训练新的模型 123456789101112131415// 使用更新过的样本训练新的模型 model = new WeightedLeastSquares(fitIntercept, regParam, elasticNetParam = 0.0, standardizeFeatures = false, standardizeLabel = false).fit(newInstances)// 检查是否收敛val oldCoefficients = oldModel.coefficientsval coefficients = model.coefficientsBLAS.axpy(-1.0, coefficients, oldCoefficients)val maxTolOfCoefficients = oldCoefficients.toArray.reduce { (x, y) =&gt; math.max(math.abs(x), math.abs(y))}val maxTol = math.max(maxTolOfCoefficients, math.abs(oldModel.intercept - model.intercept))if (maxTol &lt; tol) { converged = true} 训练完新的模型后，重复2.1步，直到参数收敛或者到达迭代的最大次数。 参考文献 【1】Iteratively reweighted least squares","link":"/posts/2146986695.html"},{"title":"HMM模型","text":"HMM模型基础 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。 什么样的问题需要HMM模型 首先我们来看看什么样的问题解决可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。 有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。比如：我现在在打字写博客，我在键盘上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。再举一个，我在和你说话，我发出的一串连续的声音就是观测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。 从这些例子中，我们可以发现，HMM模型可以无处不在。但是上面的描述还不精确，下面我们用精确的数学符号来表述我们的HMM模型。 HMM模型的定义 对于HMM模型，首先我们假设\\(Q\\)是所有可能的隐藏状态的集合，\\(V\\)是所有可能的观测状态的集合，即： \\[ Q=\\left \\{ q_1,q_2,...,q_N \\right \\},V=\\left \\{ v_1,v_2,...,v_M \\right \\} \\] 其中，\\(N\\)是可能的隐藏状态数，\\(M\\)是所有可能的观察状态数。 对于一个长度为\\(T\\)的序列，\\(I\\)对应的状态序列, \\(O\\)是对应的观察序列，即： \\[ I=\\left \\{ i_1,i_2,...,i_T \\right \\},O=\\left \\{ o_1,o_2,...,o_T \\right \\} \\] 其中，任意一个隐藏状态\\(i_t\\in Q\\),任意一个观察状态\\(o_t\\in V\\) HMM模型做了两个很重要的假设如下： 1）齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态，这个我们在MCMC(二)马尔科夫链中有详细讲述。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻𝑡的隐藏状态是\\(i_t=q_i\\),在时刻𝑡+1的隐藏状态是\\(i_t+1=q_j\\), 则从时刻𝑡到时刻\\(t+1\\)的HMM状态转移概率\\(a_{ij}\\)可以表示为： \\[ a_{ij}=P(i_{t+1}=q_j|i_t=q_i) \\] 这样\\(a_{ij}\\)可以组成马尔科夫链的状态转移矩阵\\(A\\): \\[ A=\\left [ a_{ij} \\right ]_{N\\times N} \\] 2） 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻𝑡的隐藏状态是\\(i_t=q_i\\), 而对应的观察状态为\\(o_t=v_k\\), 则该时刻观察状态𝑣𝑘在隐藏状态\\(qj\\)下生成的概率为\\(b_j(k)\\),满足： \\[ b_j(k)=P(o_t=v_k|i_t=q_j) \\] 这样\\(b_j(k)\\)可以组成观测状态生成的概率矩阵\\(B\\): \\[ B=\\left [ b_j(k) \\right ]_{N\\times M} \\] 除此之外，我们需要一组在时刻\\(t=1\\)的隐藏状态概率分布$$: \\[ \\prod =\\left [ \\pi (i) \\right ]_N,其中\\pi (i)=P(i_1=q_i) \\] 一个HMM模型，可以由隐藏状态初始概率分布$\\(, 状态转移概率矩阵\\)A\\(和观测状态概率矩阵\\)B\\(决定。\\)\\(,\\)A\\(决定状态序列，\\)B\\(决定观测序列。因此，HMM模型可以由一个三元组\\)$表示如下： \\[\\lambda = (A, B, \\Pi)\\] 一个HMM模型实例 下面我们用一个简单的实例来描述上面抽象出的HMM模型。这是一个盒子与球的模型，例子来源于李航的《统计学习方法》。 假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里 盒子 1 2 3 红球数 5 4 7 白球数 5 6 3 按照下面的方法从盒子里抽球，开始的时候，从第一个盒子抽球的概率是0.2，从第二个盒子抽球的概率是0.4，从第三个盒子抽球的概率是0.4。以这个概率抽一次球后，将球放回。然后从当前盒子转移到下一个盒子进行抽球。规则是：如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。如果当前抽球的盒子是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球。如此下去，直到重复三次，得到一个球的颜色的观测序列: \\[ O=\\left \\{红,白,红 \\right \\} \\] 注意在这个过程中，观察者只能看到球的颜色序列，却不能看到球是从哪个盒子里取出的。 那么按照我们上一节HMM模型的定义，我们的观察集合是: \\[ V=\\left \\{红,白 \\right \\}, M=2 \\] 我们的集合状态是： \\[ Q=\\left \\{盒子1,盒子2,盒子3 \\right \\}, N=3 \\] 而观察序列和状态序列的长度为3. 初始状态分布为： \\[ \\prod =\\left ( 0.2,0.4,0.4 \\right )^T \\] 状态转移分布矩阵为： \\[ A=\\begin{pmatrix} 0.5 &amp; 0.2 &amp;0.3 \\\\ 0.3&amp;0.5 &amp;0.2 \\\\ 0.2 &amp;0.3 &amp;0.5 \\end{pmatrix} \\] 观测状态概率矩阵为 \\[ B=\\begin{pmatrix} 0.5 &amp;0.5 \\\\ 0.4&amp;0.6 \\\\ 0.7&amp;0.3 \\end{pmatrix} \\] HMM观测序列的生成 从上一节的例子，我们也可以抽象出HMM观测序列生成的过程。 输入的是HMM的模型\\(\\lambda = (A, B, \\Pi)\\),观测序列的长度\\(T\\) 输出是观测序列\\(O=\\left \\{ o_1,o_2,...,o_T \\right \\}\\) 生成的过程如下： 1）根据初始状态概率分布\\(\\prod\\)生成隐藏状态\\(i_i\\) for t from 1 to T 按照隐藏状态\\(i_t\\)的观测状态分布\\(b_{i_t}\\)生成观察状态\\(o_t\\) 按照隐藏状态𝑖𝑡的状态转移概率分布\\(a_{i_t} i_{t+1}\\)产生隐藏状态\\(i_t+1\\) 所有的\\(o_t\\)一起形成观测序列\\(O=\\left \\{ o_1,o_2,...,o_T \\right \\}\\) HMM模型的三个基本问题 HMM模型一共有三个经典的问题需要解决： 1） 评估观察序列概率。即给定模型\\(\\lambda = (A, B, \\Pi)\\)和观测序列\\(O=\\left \\{ o_1,o_2,...,o_T \\right \\}\\)，计算在模型𝜆下观测序列𝑂出现的概率\\(P(O|\\lambda)\\)。这个问题的求解需要用到前向后向算法，我们在这个系列的第二篇会详细讲解。这个问题是HMM模型三个问题中最简单的。 2）模型参数学习问题。即给定观测序列\\(O=\\left \\{ o_1,o_2,...,o_T \\right \\}\\)，估计模型\\(\\lambda = (A, B, \\Pi)\\)的参数，使该模型下观测序列的条件概率𝑃(𝑂|𝜆)最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法， 我们在这个系列的第三篇会详细讲解。这个问题是HMM模型三个问题中最复杂的。 3）预测问题，也称为解码问题。即给定模型\\(\\lambda =(A,B,\\Pi)\\)和观测序列\\(O=\\left \\{ o_1,o_2,...,o_T \\right \\}\\)，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。 维特比算法解码隐藏状态序列 HMM模型的解码问题最常用的算法是维特比算法，当然也有其他的算法可以求解这个问题。同时维特比算法是一个通用的求序列最短路径的动态规划算法，也可以用于很多其他问题，比如之前讲到的文本挖掘的分词原理中我们讲到了单独用维特比算法来做分词。 HMM最可能隐藏状态序列求解概述 在HMM模型的解码问题中，给定模型\\(\\lambda = (A, B, \\Pi)\\)和观测序列\\(O=\\left \\{ o_1,o_2,...,o_T \\right \\}\\)，求给定观测序列O条件下，最可能出现的对应的状态序列\\(T^*=\\left \\{i_1^*,i_2^*,...,i_T^* \\right \\}\\),即\\(P(I^*|O)\\)要最大化。 一个可能的近似解法是求出观测序列𝑂在每个时刻𝑡最可能的隐藏状态\\(i_t^*\\)然后得到一个近似的隐藏状态序列\\(T^*=\\left \\{i_1^*,i_2^*,...,i_T^* \\right \\}\\)。要这样近似求解不难，利用隐马尔科夫模型HMM前向后向算法评估观察序列概率中的定义：在给定模型\\(\\lambda\\)和观测序列\\(O\\)时，在时刻𝑡处于状态\\(q_i\\)的概率是\\(\\gamma_t(i)\\)，这个概率可以通过HMM的前向算法与后向算法计算。这样我们有： \\[ i_t^* = arg \\max_{1 \\leq i \\leq N}[\\gamma_t(i)], \\; t =1,2,...T \\] 近似算法很简单，但是却不能保证预测的状态序列是整体是最可能的状态序列，因为预测的状态序列中某些相邻的隐藏状态可能存在转移概率为0的情况。 而维特比算法可以将HMM的状态序列作为一个整体来考虑，避免近似算法的问题，下面我们来看看维特比算法进行HMM解码的方法。 维特比算法概述 维特比算法是一个通用的解码算法，是基于动态规划的求序列最短路径的方法。在文本挖掘的分词原理中我们已经讲到了维特比算法的一些细节。 既然是动态规划算法，那么就需要找到合适的局部状态，以及局部状态的递推公式。在HMM中，维特比算法定义了两个局部状态用于递推。 第一个局部状态是在时刻𝑡隐藏状态为\\(i\\)所有可能的状态转移路径\\(i_1,i_2,...i_t\\)中的概率最大值。记为\\(\\delta_t(i)\\): \\[ \\delta_t(i) = \\max_{i_1,i_2,...i_{t-1}}\\;P(i_t=i, i_1,i_2,...i_{t-1},o_t,o_{t-1},...o_1|\\lambda),\\; i =1,2,...N \\] 由\\(\\delta_t(i)\\)的定义可以得到\\(\\delta\\)的递推表达式： \\[ \\begin{align} \\delta_{t+1}(i) &amp; = \\max_{i_1,i_2,...i_{t}}\\;P(i_{t+1}=i, i_1,i_2,...i_{t},o_{t+1},o_{t},...o_1|\\lambda) \\\\ &amp; = \\max_{1 \\leq j \\leq N}\\;[\\delta_t(j)a_{ji}]b_i(o_{t+1})\\end{align} \\] 第二个局部状态由第一个局部状态递推得到。我们定义在时刻𝑡隐藏状态为𝑖的所有单个状态转移路径\\((i_1,i_2,...,i_{t-1},i)\\)中概率最大的转移路径中第\\(t−1\\)个节点的隐藏状态为\\(\\Psi_t(i)\\),其递推表达式可以表示为： \\[ \\Psi_t(i) = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}] \\] 有了这两个局部状态，我们就可以从时刻0一直递推到时刻\\(T\\)，然后利用\\(\\Psi_t(i)\\)记录的前一个最可能的状态节点回溯，直到找到最优的隐藏状态序列。 维特比算法流程总结 现在我们来总结下维特比算法的流程： 输入：HMM模型\\(\\lambda = (A, B, \\Pi)\\)，观测序列\\(O=(o_1,o_2,...o_T)\\) 输出：最有可能的隐藏状态序列\\(I^*= \\{i_1^*,i_2^*,...i_T^*\\}\\) 1）初始化局部状态： \\[ \\delta_1(i) = \\pi_ib_i(o_1),\\;i=1,2...N \\\\ \\Psi_1(i)=0,\\;i=1,2...N \\] 2) 进行动态规划递推时刻\\(t=2,3,...T\\)时刻的局部状态： \\[ \\delta_{t}(i) = \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}]b_i(0_{t}),\\;i=1,2...N \\\\ \\Psi_t(i) = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}],\\;i=1,2...N \\] 3) 计算时刻\\(T\\)最大的\\(\\delta_t(i)\\)，即为最可能隐藏状态序列出现的概率。计算时刻\\(T\\)最大的\\(\\Psi_t(i)\\)，即为时刻\\(T\\)最可能的隐藏状态。 \\[ P* = \\max_{1 \\leq j \\leq N}\\delta_{T}(i) \\\\ i_T^* = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{T}(i)] \\] 4) 利用局部状态\\(\\Psi_t(i)\\)开始回溯。对于\\(t=T-1,T-2,...,1\\): \\[ i_t^* = \\Psi_{t+1}(i_{t+1}^*) \\] 最终得到最有可能的隐藏状态序列\\(I^*= \\{i_1^*,i_2^*,...i_T^*\\}\\) HMM维特比算法求解实例 下面我们仍然用HMM模型中盒子与球的例子来看看HMM维特比算法求解。 我们的观察集合是: \\[ V=\\{红，白\\}，M=2 \\] 我们的状态集合是: \\[ Q =\\{盒子1，盒子2，盒子3\\}， N=3 \\] 而观察序列和状态序列的长度为3. 初始状态分布为： \\[ \\Pi = (0.2,0.4,0.4)^T \\] 状态转移概率分布矩阵为： \\[ A = \\left( \\begin{array} {ccc} 0.5 &amp; 0.2 &amp; 0.3 \\\\ 0.3 &amp; 0.5 &amp; 0.2 \\\\ 0.2 &amp; 0.3 &amp;0.5 \\end{array} \\right) \\] 观测状态概率矩阵为： \\[ B = \\left( \\begin{array} {ccc} 0.5 &amp; 0.5 \\\\ 0.4 &amp; 0.6 \\\\ 0.7 &amp; 0.3 \\end{array} \\right) \\] 球的颜色的观测序列: \\[ O=\\{红，白，红\\} \\] 按照我们上一节的维特比算法，首先需要得到三个隐藏状态在时刻1时对应的各自两个局部状态，此时观测状态为1： \\[ \\delta_1(1) = \\pi_1b_1(o_1) = 0.2 \\times 0.5 = 0.1 \\\\ \\delta_1(2) = \\pi_2b_2(o_1) = 0.4 \\times 0.4 = 0.16 \\\\ \\delta_1(3) = \\pi_3b_3(o_1) = 0.4 \\times 0.7 = 0.28 \\\\ \\Psi_1(1)=\\Psi_1(2) =\\Psi_1(3) =0 \\] 现在开始递推三个隐藏状态在时刻2时对应的各自两个局部状态，此时观测状态为2： \\[ \\delta_2(1) = \\max_{1\\leq j \\leq 3}[\\delta_1(j)a_{j1}]b_1(o_2) = \\max_{1\\leq j \\leq 3}[0.1 \\times 0.5, 0.16 \\times 0.3, 0.28\\times 0.2] \\times 0.5 = 0.028 \\\\ \\Psi_2(1)=3 \\\\ \\delta_2(2) = \\max_{1\\leq j \\leq 3}[\\delta_1(j)a_{j2}]b_2(o_2) = \\max_{1\\leq j \\leq 3}[0.1 \\times 0.2, 0.16 \\times 0.5, 0.28\\times 0.3] \\times 0.6 = 0.0504 \\\\ \\Psi_2(2)=3 \\\\ \\delta_2(3) = \\max_{1\\leq j \\leq 3}[\\delta_1(j)a_{j3}]b_3(o_2) = \\max_{1\\leq j \\leq 3}[0.1 \\times 0.3, 0.16 \\times 0.2, 0.28\\times 0.5] \\times 0.3 = 0.042 \\\\ \\Psi_2(3)=3 \\] 继续递推三个隐藏状态在时刻3时对应的各自两个局部状态，此时观测状态为1： \\[ \\delta_3(1) = \\max_{1\\leq j \\leq 3}[\\delta_2(j)a_{j1}]b_1(o_3) = \\max_{1\\leq j \\leq 3}[0.028 \\times 0.5, 0.0504 \\times 0.3, 0.042\\times 0.2] \\times 0.5 = 0.00756 \\\\ \\Psi_3(1)=2 \\\\ \\delta_3(2) = \\max_{1\\leq j \\leq 3}[\\delta_2(j)a_{j2}]b_2(o_3) = \\max_{1\\leq j \\leq 3}[0.028 \\times 0.2, 0.0504\\times 0.5, 0.042\\times 0.3] \\times 0.4 = 0.01008 \\\\ \\Psi_3(2)=2 \\\\ \\delta_3(3) = \\max_{1\\leq j \\leq 3}[\\delta_2(j)a_{j3}]b_3(o_3) = \\max_{1\\leq j \\leq 3}[0.028 \\times 0.3, 0.0504 \\times 0.2, 0.042\\times 0.5] \\times 0.7 = 0.0147 \\\\ \\Psi_3(3)=3 \\] 此时已经到最后的时刻，我们开始准备回溯。此时最大概率为\\(\\delta_3(3)\\),从而得到\\(i_3^* =3\\) 由于\\(\\Psi_3(3)=3\\),所以\\(i_2^* =3\\), 而又由于\\(\\Psi_2(3)=3\\),所以𝑖∗1=3。从而得到最终的最可能的隐藏状态序列为：\\((3,3,3)\\) HMM模型维特比算法总结 如果大家看过之前写的文本挖掘的分词原理中的维特比算法，就会发现这两篇之中的维特比算法稍有不同。主要原因是在中文分词时，我们没有观察状态和隐藏状态的区别，只有一种状态。但是维特比算法的核心是定义动态规划的局部状态与局部递推公式，这一点在中文分词维特比算法和HMM的维特比算法是相同的，也是维特比算法的精华所在。 维特比算法也是寻找序列最短路径的一个通用方法，和dijkstra算法有些类似，但是dijkstra算法并没有使用动态规划，而是贪心算法。同时维特比算法仅仅局限于求序列最短路径，而dijkstra算法是通用的求最短路径的方法。","link":"/posts/1528548331.html"},{"title":"Insertion Sort List","text":"Question leetcode: Insertion Sort List lintcode: Insertion Sort List ### Problem Statement Sort a linked list using insertion sort. A graphical example of insertion sort. The partial sorted list (black) initially contains only the first element in the list. With each iteration one element (red) is removed from the input data and inserted in-place into the sorted list Algorithm of Insertion Sort: Insertion sort iterates, consuming one input element each repetition, and growing a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain. Example 1: **Input:** 4-&gt;2-&gt;1-&gt;3 **Output:** 1-&gt;2-&gt;3-&gt;4 Example 2: **Input:** -1-&gt;5-&gt;3-&gt;4-&gt;0 **Output:** -1-&gt;0-&gt;3-&gt;4-&gt;5 题解1 - 从首到尾遍历 插入排序常见的实现是针对数组的，如前几章总的的 Insertion Sort，但这道题中的排序的数据结构为单向链表，故无法再从后往前遍历比较值的大小了。好在天无绝人之路，我们还可以从前往后依次遍历比较和交换。 由于排序后头节点不一定，故需要引入 dummy 大法，并以此节点的next作为最后返回结果的头节点，返回的链表从dummy-&gt;next这里开始构建。首先我们每次都从dummy-&gt;next开始遍历，依次和上一轮处理到的节点的值进行比较，直至找到不小于上一轮节点值的节点为止，随后将上一轮节点插入到当前遍历的节点之前，依此类推。文字描述起来可能比较模糊，大家可以结合以下的代码在纸上分析下。 Python 12345678910111213141516171819202122232425&quot;&quot;&quot;Definition of ListNodeclass ListNode(object): def __init__(self, val, next=None): self.val = val self.next = next&quot;&quot;&quot;class Solution: &quot;&quot;&quot; @param head: The first node of linked list. @return: The head of linked list. &quot;&quot;&quot; def insertionSortList(self, head): dummy = ListNode(0) cur = head while cur is not None: pre = dummy while pre.next is not None and pre.next.val &lt; cur.val: pre = pre.next temp = cur.next cur.next = pre.next pre.next = cur cur = temp return dummy.next C++ 1234567891011121314151617181920212223242526272829303132333435/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: The head of linked list. */ ListNode *insertionSortList(ListNode *head) { ListNode *dummy = new ListNode(0); ListNode *cur = head; while (cur != NULL) { ListNode *pre = dummy; while (pre-&gt;next != NULL &amp;&amp; pre-&gt;next-&gt;val &lt; cur-&gt;val) { pre = pre-&gt;next; } ListNode *temp = cur-&gt;next; cur-&gt;next = pre-&gt;next; pre-&gt;next = cur; cur = temp; } return dummy-&gt;next; }}; Java 1234567891011121314151617181920212223242526/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { public ListNode insertionSortList(ListNode head) { ListNode dummy = new ListNode(0); ListNode cur = head; while (cur != null) { ListNode pre = dummy; while (pre.next != null &amp;&amp; pre.next.val &lt; cur.val) { pre = pre.next; } ListNode temp = cur.next; cur.next = pre.next; pre.next = cur; cur = temp; } return dummy.next; }} 源码分析 新建 dummy 节点，用以处理最终返回结果中头节点不定的情况。 以cur表示当前正在处理的节点，在从 dummy 开始遍历前保存cur的下一个节点作为下一轮的cur. 以pre作为遍历节点，直到找到不小于cur值的节点为止。 将pre的下一个节点pre-&gt;next链接到cur-&gt;next上，cur链接到pre-&gt;next, 最后将cur指向下一个节点。 返回dummy-&gt;next最为最终头节点。 Python 的实现在 lintcode 上会提示 TLE, leetcode 上勉强通过，这里需要注意的是采用if A is not None:的效率要比if A:高，不然 leetcode 上也过不了。具体原因可参考 Stack Overflow 上的讨论。 复杂度分析 最好情况：原链表已经逆序，每得到一个新节点仅需要一次比较, 时间复杂度为 \\[O(n)\\], 使用了 dummy 和 pre, 空间复杂度近似为 \\[O(1)\\]. 最坏情况：原链表正好升序，由于是单向链表只能从前往后依次遍历，交换和比较次数均为 \\[1/2 O(n^2)\\], 总的时间复杂度近似为 \\[O(n^2)\\], 空间复杂度同上，近似为 \\[O(1)\\]. 题解2 - 优化有序链表 从题解1的复杂度分析可以看出其在最好情况下时间复杂度都为 \\[O(n^2)\\] ，这显然是需要优化的。 仔细观察可发现最好情况下的比较次数 是可以优化到 \\[O(n)\\] 的。思路自然就是先判断链表是否有序，仅对降序的部分进行处理。优化之后的代码就没题解1那么容易写对了，建议画个图自行纸上分析下。 Python 12345678910111213141516171819202122232425262728293031&quot;&quot;&quot;Definition of ListNodeclass ListNode(object): def __init__(self, val, next=None): self.val = val self.next = next&quot;&quot;&quot;class Solution: &quot;&quot;&quot; @param head: The first node of linked list. @return: The head of linked list. &quot;&quot;&quot; def insertionSortList(self, head): dummy = ListNode(0) dummy.next = head cur = head while cur is not None: if cur.next is not None and cur.next.val &lt; cur.val: # find insert position for smaller(cur-&gt;next) pre = dummy while pre.next is not None and pre.next.val &lt; cur.next.val: pre = pre.next # insert cur-&gt;next after pre temp = pre.next pre.next = cur.next cur.next = cur.next.next pre.next.next = temp else: cur = cur.next return dummy.next C++ 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: The head of linked list. */ ListNode *insertionSortList(ListNode *head) { ListNode *dummy = new ListNode(0); dummy-&gt;next = head; ListNode *cur = head; while (cur != NULL) { if (cur-&gt;next != NULL &amp;&amp; cur-&gt;next-&gt;val &lt; cur-&gt;val) { ListNode *pre = dummy; // find insert position for smaller(cur-&gt;next) while (pre-&gt;next != NULL &amp;&amp; pre-&gt;next-&gt;val &lt;= cur-&gt;next-&gt;val) { pre = pre-&gt;next; } // insert cur-&gt;next after pre ListNode *temp = pre-&gt;next; pre-&gt;next = cur-&gt;next; cur-&gt;next = cur-&gt;next-&gt;next; pre-&gt;next-&gt;next = temp; } else { cur = cur-&gt;next; } } return dummy-&gt;next; }}; Java 123456789101112131415161718192021222324252627282930313233/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { public ListNode insertionSortList(ListNode head) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode cur = head; while (cur != null) { if (cur.next != null &amp;&amp; cur.next.val &lt; cur.val) { // find insert position for smaller(cur-&gt;next) ListNode pre = dummy; while (pre.next != null &amp;&amp; pre.next.val &lt; cur.next.val) { pre = pre.next; } // insert cur-&gt;next after pre ListNode temp = pre.next; pre.next = cur.next; cur.next = cur.next.next; pre.next.next = temp; } else { cur = cur.next; } } return dummy.next; }} 源码分析 新建 dummy 节点并将其next 指向head 分情况讨论，仅需要处理逆序部分。 由于已经确认链表逆序，故仅需将较小值(cur-&gt;next而不是cur)的节点插入到链表的合适位置。 将cur-&gt;next插入到pre之后，这里需要四个步骤，需要特别小心！ 如上图所示，将cur-&gt;next插入到pre节点后大致分为3个步骤。 复杂度分析 最好情况下时间复杂度降至 \\[O(n)\\], 其他同题解1. Reference Explained C++ solution (24ms) - Leetcode Discuss Insertion Sort List - 九章算法","link":"/posts/2918877992.html"},{"title":"Interleaving String","text":"Question leetcode: Interleaving String | LeetCode OJ lintcode: (29) Interleaving String 12345678910Given three strings: s1, s2, s3,determine whether s3 is formed by the interleaving of s1 and s2.ExampleFor s1 = &quot;aabcc&quot;, s2 = &quot;dbbca&quot;When s3 = &quot;aadbbcbcac&quot;, return true.When s3 = &quot;aadbbbaccc&quot;, return false.ChallengeO(n2) time or better 题解1 - bug 题目意思是 s3 是否由 s1 和 s2 交叉构成，不允许跳着从 s1 和 s2 挑选字符。那么直觉上可以对三个字符串设置三个索引，首先从 s3 中依次取字符，然后进入内循环，依次从 s1 和 s2 中取首字符，若能匹配上则进入下一次循环，否则立即返回 false. 我们先看代码，再分析 bug 之处。 Java 12345678910111213141516171819202122232425262728293031323334public class Solution { /** * Determine whether s3 is formed by interleaving of s1 and s2. * @param s1, s2, s3: As description. * @return: true or false. */ public boolean isInterleave(String s1, String s2, String s3) { int len1 = (s1 == null) ? 0 : s1.length(); int len2 = (s2 == null) ? 0 : s2.length(); int len3 = (s3 == null) ? 0 : s3.length(); if (len3 != len1 + len2) return false; int i1 = 0, i2 = 0; for (int i3 = 0; i3 &lt; len3; i3++) { boolean result = false; if (i1 &lt; len1 &amp;&amp; s1.charAt(i1) == s3.charAt(i3)) { i1++; result = true; continue; } if (i2 &lt; len2 &amp;&amp; s2.charAt(i2) == s3.charAt(i3)) { i2++; result = true; continue; } // return instantly if both s1 and s2 can not pair with s3 if (!result) return false; } return true; }} 源码分析 异常处理部分：首先求得 s1, s2, s3 的字符串长度，随后用索引 i1, i2, i3 巧妙地避开了繁琐的 null 检测。这段代码能过前面的一部分数据，但在 lintcode 的第15个 test 跪了。不想马上看以下分析的可以自己先 debug 下。 我们可以注意到以上代码还有一种情况并未考虑到，那就是当 s1[i1] 和 s2[i2] 均和 s3[i3] 相等时，我们可以拿 s1 或者 s2 去匹配，那么问题来了，由于不允许跳着取，那么可能出现在取了 s1 中的字符后，接下来的 s1 和 s2 首字符都无法和 s3 匹配到，因此原本应该返回 true 而现在返回 false. 建议将以上代码贴到 OJ 上看看测试用例。 以上 bug 可以通过加入对 (s1[i1] == s3[i3]) &amp;&amp; (s2[i2] == s3[i3]) 这一特殊情形考虑，即分两种情况递归调用 isInterleave, 只不过 s1, s2, s3 为新生成的字符串。 复杂度分析 遍历一次 s3, 时间复杂度为 \\[O(n)\\], 空间复杂度 \\[O(1)\\]. 题解2 在 (s1[i1] == s3[i3]) &amp;&amp; (s2[i2] == s3[i3]) 时分两种情况考虑，即让 s1[i1] 和 s3[i3] 配对或者 s2[i2] 和 s3[i3] 配对，那么嵌套调用时新生成的字符串则分别为 s1[1+i1:], s2[i2], s3[1+i3:] 和 s1[i1:], s2[1+i2], s3[1+i3:]. 嵌套调用结束后立即返回最终结果，因为递归调用时整个结果已经知晓，不立即返回则有可能会产生错误结果，递归调用并未影响到调用处的 i1 和 i2. Python 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution: &quot;&quot;&quot; @params s1, s2, s3: Three strings as description. @return: return True if s3 is formed by the interleaving of s1 and s2 or False if not. @hint: you can use [[True] * m for i in range (n)] to allocate a n*m matrix. &quot;&quot;&quot; def isInterleave(self, s1, s2, s3): len1 = 0 if s1 is None else len(s1) len2 = 0 if s2 is None else len(s2) len3 = 0 if s3 is None else len(s3) if len3 != len1 + len2: return False i1, i2 = 0, 0 for i3 in xrange(len(s3)): result = False if (i1 &lt; len1 and s1[i1] == s3[i3]) and \\ (i1 &lt; len1 and s1[i1] == s3[i3]): # s1[1+i1:], s2[i2:], s3[1+i3:] case1 = self.isInterleave(s1[1 + i1:], s2[i2:], s3[1 + i3:]) # s1[i1:], s2[1+i2:], s3[1+i3:] case2 = self.isInterleave(s1[i1:], s2[1 + i2:], s3[1 + i3:]) return case1 or case2 if i1 &lt; len1 and s1[i1] == s3[i3]: i1 += 1 result = True continue if i2 &lt; len2 and s2[i2] == s3[i3]: i2 += 1 result = True continue # return instantly if both s1 and s2 can not pair with s3 if not result: return False return True C++ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution {public: /** * Determine whether s3 is formed by interleaving of s1 and s2. * @param s1, s2, s3: As description. * @return: true of false. */ bool isInterleave(string s1, string s2, string s3) { int len1 = s1.size(); int len2 = s2.size(); int len3 = s3.size(); if (len3 != len1 + len2) return false; int i1 = 0, i2 = 0; for (int i3 = 0; i3 &lt; len3; ++i3) { bool result = false; if (i1 &lt; len1 &amp;&amp; s1[i1] == s3[i3] &amp;&amp; i2 &lt; len2 &amp;&amp; s2[i2] == s3[i3]) { // s1[1+i1:], s2[i2:], s3[1+i3:] bool case1 = isInterleave(s1.substr(1 + i1), s2.substr(i2), s3.substr(1 + i3)); // s1[i1:], s2[1+i2:], s3[1+i3:] bool case2 = isInterleave(s1.substr(i1), s2.substr(1 + i2), s3.substr(1 + i3)); // return instantly return case1 || case2; } if (i1 &lt; len1 &amp;&amp; s1[i1] == s3[i3]) { i1++; result = true; continue; } if (i2 &lt; len2 &amp;&amp; s2[i2] == s3[i3]) { i2++; result = true; continue; } // return instantly if both s1 and s2 can not pair with s3 if (!result) return false; } return true; }}; Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Solution { /** * Determine whether s3 is formed by interleaving of s1 and s2. * @param s1, s2, s3: As description. * @return: true or false. */ public boolean isInterleave(String s1, String s2, String s3) { int len1 = (s1 == null) ? 0 : s1.length(); int len2 = (s2 == null) ? 0 : s2.length(); int len3 = (s3 == null) ? 0 : s3.length(); if (len3 != len1 + len2) return false; int i1 = 0, i2 = 0; for (int i3 = 0; i3 &lt; len3; i3++) { boolean result = false; if (i1 &lt; len1 &amp;&amp; s1.charAt(i1) == s3.charAt(i3) &amp;&amp; i2 &lt; len2 &amp;&amp; s2.charAt(i2) == s3.charAt(i3)) { // s1[1+i1:], s2[i2:], s3[1+i3:] boolean case1 = isInterleave(s1.substring(1 + i1), s2.substring(i2), s3.substring(1 + i3)); // s1[i1:], s2[1+i2:], s3[1+i3:] boolean case2 = isInterleave(s1.substring(i1), s2.substring(1 + i2), s3.substring(1 + i3)); // return instantly return case1 || case2; } if (i1 &lt; len1 &amp;&amp; s1.charAt(i1) == s3.charAt(i3)) { i1++; result = true; continue; } if (i2 &lt; len2 &amp;&amp; s2.charAt(i2) == s3.charAt(i3)) { i2++; result = true; continue; } // return instantly if both s1 and s2 can not pair with s3 if (!result) return false; } return true; }} 题解3 - 动态规划 看过题解1 和 题解2 的思路后动规的状态和状态方程应该就不难推出了。按照经典的序列规划，不妨假设状态 f[i1][i2][i3] 为 s1的前i1个字符和 s2的前 i2个字符是否能交叉构成 s3的前 i3个字符，那么根据 s1[i1], s2[i2], s3[i3]的匹配情况可以分为8种情况讨论。咋一看这似乎十分麻烦，但实际上我们注意到其实还有一个隐含条件：len3 == len1 + len2, 故状态转移方程得到大幅简化。 新的状态可定义为 f[i1][i2], 含义为s1的前i1个字符和 s2的前 i2个字符是否能交叉构成 s3的前 i1 + i2 个字符。根据 s1[i1] == s3[i3] 和 s2[i2] == s3[i3] 的匹配情况可建立状态转移方程为： 12f[i1][i2] = (s1[i1 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1 - 1][i2]) || (s2[i2 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1][i2 - 1]) 这道题的初始化有点 trick, 考虑到空串的可能，需要单独初始化 f[*][0] 和 f[0][*]. Python 123456789101112131415161718192021222324252627282930class Solution: &quot;&quot;&quot; @params s1, s2, s3: Three strings as description. @return: return True if s3 is formed by the interleaving of s1 and s2 or False if not. @hint: you can use [[True] * m for i in range (n)] to allocate a n*m matrix. &quot;&quot;&quot; def isInterleave(self, s1, s2, s3): len1 = 0 if s1 is None else len(s1) len2 = 0 if s2 is None else len(s2) len3 = 0 if s3 is None else len(s3) if len3 != len1 + len2: return False f = [[True] * (1 + len2) for i in xrange (1 + len1)] # s1[i1 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1 - 1][i2] for i in xrange(1, 1 + len1): f[i][0] = s1[i - 1] == s3[i - 1] and f[i - 1][0] # s2[i2 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1][i2 - 1] for i in xrange(1, 1 + len2): f[0][i] = s2[i - 1] == s3[i - 1] and f[0][i - 1] # i1 &gt;= 1, i2 &gt;= 1 for i1 in xrange(1, 1 + len1): for i2 in xrange(1, 1 + len2): case1 = s1[i1 - 1] == s3[i1 + i2 - 1] and f[i1 - 1][i2] case2 = s2[i2 - 1] == s3[i1 + i2 - 1] and f[i1][i2 - 1] f[i1][i2] = case1 or case2 return f[len1][len2] C++ 1234567891011121314151617181920212223242526272829303132333435class Solution {public: /** * Determine whether s3 is formed by interleaving of s1 and s2. * @param s1, s2, s3: As description. * @return: true of false. */ bool isInterleave(string s1, string s2, string s3) { int len1 = s1.size(); int len2 = s2.size(); int len3 = s3.size(); if (len3 != len1 + len2) return false; vector&lt;vector&lt;bool&gt; &gt; f(1 + len1, vector&lt;bool&gt;(1 + len2, true)); // s1[i1 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1 - 1][i2] for (int i = 1; i &lt;= len1; ++i) { f[i][0] = s1[i - 1] == s3[i - 1] &amp;&amp; f[i - 1][0]; } // s2[i2 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1][i2 - 1] for (int i = 1; i &lt;= len2; ++i) { f[0][i] = s2[i - 1] == s3[i - 1] &amp;&amp; f[0][i - 1]; } // i1 &gt;= 1, i2 &gt;= 1 for (int i1 = 1; i1 &lt;= len1; ++i1) { for (int i2 = 1; i2 &lt;= len2; ++i2) { bool case1 = s1[i1 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1 - 1][i2]; bool case2 = s2[i2 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1][i2 - 1]; f[i1][i2] = case1 || case2; } } return f[len1][len2]; }}; Java 1234567891011121314151617181920212223242526272829303132333435public class Solution { /** * Determine whether s3 is formed by interleaving of s1 and s2. * @param s1, s2, s3: As description. * @return: true or false. */ public boolean isInterleave(String s1, String s2, String s3) { int len1 = (s1 == null) ? 0 : s1.length(); int len2 = (s2 == null) ? 0 : s2.length(); int len3 = (s3 == null) ? 0 : s3.length(); if (len3 != len1 + len2) return false; boolean [][] f = new boolean[1 + len1][1 + len2]; f[0][0] = true; // s1[i1 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1 - 1][i2] for (int i = 1; i &lt;= len1; i++) { f[i][0] = s1.charAt(i - 1) == s3.charAt(i - 1) &amp;&amp; f[i - 1][0]; } // s2[i2 - 1] == s3[i1 + i2 - 1] &amp;&amp; f[i1][i2 - 1] for (int i = 1; i &lt;= len2; i++) { f[0][i] = s2.charAt(i - 1) == s3.charAt(i - 1) &amp;&amp; f[0][i - 1]; } // i1 &gt;= 1, i2 &gt;= 1 for (int i1 = 1; i1 &lt;= len1; i1++) { for (int i2 = 1; i2 &lt;= len2; i2++) { boolean case1 = s1.charAt(i1 - 1) == s3.charAt(i1 + i2 - 1) &amp;&amp; f[i1 - 1][i2]; boolean case2 = s2.charAt(i2 - 1) == s3.charAt(i1 + i2 - 1) &amp;&amp; f[i1][i2 - 1]; f[i1][i2] = case1 || case2; } } return f[len1][len2]; }} 源码分析 为后面递推方便，初始化时数组长度多加1，for 循环时需要注意边界(取到等号)。 复杂度分析 双重 for 循环，时间复杂度为 \\[O(n^2)\\], 使用了二维矩阵，空间复杂度 \\[O(n^2)\\]. 其中空间复杂度可以优化。 Reference soulmachine 的 Interleaving String 部分 Interleaving String 参考程序 Java/C++/Python","link":"/posts/3646917699.html"},{"title":"Jump Game II","text":"Question lintcode: (117) Jump Game II 123456789101112Given an array of non-negative integers,you are initially positioned at the first index of the array.Each element in the array represents your maximum jump length at that position.Your goal is to reach the last index in the minimum number of jumps.ExampleGiven array A = [2,3,1,1,4]The minimum number of jumps to reach the last index is 2.(Jump 1 step from index 0 to 1, then 3 steps to the last index.) 题解(自顶向下-动态规划) 首先来看看使用动态规划的解法，由于复杂度较高在A元素较多时会出现TLE，因为时间复杂度接近 \\[O(n^3)\\]. 工作面试中给出动规的实现就挺好了。 State: f[i] 从起点跳到这个位置最少需要多少步 Function: f[i] = MIN(f[j]+1, j &lt; i &amp;&amp; j + A[j] &gt;= i) 取出所有能从j到i中的最小值 Initialization: f[0] = 0，即一个元素时不需移位即可到达 Answer: f[n-1] C++ Dynamic Programming 123456789101112131415161718192021222324252627class Solution {public: /** * @param A: A list of lists of integers * @return: An integer */ int jump(vector&lt;int&gt; A) { if (A.empty()) { return -1; } const int N = A.size() - 1; vector&lt;int&gt; steps(N, INT_MAX); steps[0] = 0; for (int i = 1; i != N + 1; ++i) { for (int j = 0; j != i; ++j) { if ((steps[j] != INT_MAX) &amp;&amp; (j + A[j] &gt;= i)) { steps[i] = steps[j] + 1; break; } } } return steps[N]; }}; 源码分析 状态转移方程为 1234if ((steps[j] != INT_MAX) &amp;&amp; (j + A[j] &gt;= i)) { steps[i] = steps[j] + 1; break;} 其中break即体现了MIN操作，最开始满足条件的j即为最小步数。 题解(贪心法-自底向上) 使用动态规划解Jump Game的题复杂度均较高，这里可以使用贪心法达到线性级别的复杂度。 贪心法可以使用自底向上或者自顶向下，首先看看我最初使用自底向上做的。对A数组遍历，找到最小的下标min_index，并在下一轮中用此min_index替代上一次的end, 直至min_index为0，返回最小跳数jumps。以下的实现有个 bug，细心的你能发现吗？ C++ greedy from bottom to top, bug version 1234567891011121314151617181920212223242526272829303132333435class Solution {public: /** * @param A: A list of lists of integers * @return: An integer */ int jump(vector&lt;int&gt; A) { if (A.empty()) { return -1; } const int N = A.size() - 1; int jumps = 0; int last_index = N; int min_index = N; for (int i = N - 1; i &gt;= 0; --i) { if (i + A[i] &gt;= last_index) { min_index = i; } if (0 == min_index) { return ++jumps; } if ((0 == i) &amp;&amp; (min_index &lt; last_index)) { ++jumps; last_index = min_index; i = last_index - 1; } } return jumps; }}; 源码分析 使用jumps记录最小跳数，last_index记录离终点最远的坐标，min_index记录此次遍历过程中找到的最小下标。 以上的bug在于当min_index为1时，i = 0, for循环中仍有--i，因此退出循环，无法进入if (0 == min_index)语句，因此返回的结果会小1个。 C++ greedy, from bottom to top 123456789101112131415161718192021222324252627282930313233class Solution {public: /** * @param A: A list of lists of integers * @return: An integer */ int jump(vector&lt;int&gt; A) { if (A.empty()) { return 0; } const int N = A.size() - 1; int jumps = 0, end = N, min_index = N; while (end &gt; 0) { for (int i = end - 1; i &gt;= 0; --i) { if (i + A[i] &gt;= end) { min_index = i; } } if (min_index &lt; end) { ++jumps; end = min_index; } else { // cannot jump to the end return -1; } } return jumps; }}; 源码分析 之前的 bug version 代码实在是太丑陋了，改写了个相对优雅的实现，加入了是否能到达终点的判断。在更新min_index的内循环中也可改为如下效率更高的方式： 123456for (int i = 0; i != end; ++i) { if (i + A[i] &gt;= end) { min_index = i; break; }} 题解(贪心法-自顶向下) 看过了自底向上的贪心法，我们再来瞅瞅自顶向下的实现。自顶向下使用farthest记录当前坐标出发能到达的最远坐标，遍历当前start与end之间的坐标，若i+A[i] &gt; farthest时更新farthest(寻找最小跳数)，当前循环遍历结束时递推end = farthest。end &gt;= A.size() - 1时退出循环，返回最小跳数。 C++ 1234567891011121314151617181920212223242526272829303132333435363738/** * http://www.jiuzhang.com/solutions/jump-game-ii/ */class Solution {public: /** * @param A: A list of lists of integers * @return: An integer */ int jump(vector&lt;int&gt; A) { if (A.empty()) { return 0; } const int N = A.size() - 1; int start = 0, end = 0, jumps = 0; while (end &lt; N) { int farthest = end; for (int i = start; i &lt;= end; ++i) { if (i + A[i] &gt;= farthest) { farthest = i + A[i]; } } if (end &lt; farthest) { ++jumps; start = end + 1; end = farthest; } else { // cannot jump to the end return -1; } } return jumps; }};","link":"/posts/982524241.html"},{"title":"Knapsack - 背包问题","text":"在一次抢珠宝店的过程中，抢劫犯只能抢走以下三种珠宝，其重量和价值如下表所述。 Item(jewellery) Weight Value 1 6 23 2 3 13 3 4 11 抢劫犯这次过来光顾珠宝店只带了一个最多只能承重 17 kg 的粉红色小包，于是问题来了，怎样搭配这些不同重量不同价值的珠宝才能不虚此行呢？唉，这年头抢劫也不容易啊... 用数学语言来描述这个问题就是： 背包最多只能承重 \\[W\\] kg, 有 \\[n\\] 种珠宝可供选择，这 \\[n\\] 种珠宝的重量分别为 \\[w_1,\\cdots,w_n\\], 相应的价值为 \\[v_1,\\cdots,v_n\\]. 问如何选择这些珠宝使得放进包里的珠宝价值最大化？ 现实场景中，我们遇到的问题往往是重量或者珠宝数有限，对于这 n 种珠宝，只有两种情况——要么取，要么不取，这也被称为著名的 01 背包问题；而同一种珠宝如果可以取无限多次，则对应完全背包问题。 Knapsack without repetition - 01 背包问题 我们先来处理一种珠宝最多只能带走一件这种情形下，抢劫犯该如何做才能使得背包中的珠宝价值总价最大？最为简单粗暴的方法自然是把这 n 种珠宝挨个试一遍，总共可能的组合数之和为 \\[C_n^0 + \\cdots + C_n^n = 2^n\\], 这种指数级别的时间复杂度显然是不能忍的，在搜索的过程中我们可以发现中间的一些子状态是可以避免重复计算的，下面我们使用动态规划的思路去进一步分析 01 背包问题。 在动态规划中，主要的问题之一就是——状态(子问题)是什么？在本题中我们可以从两个方面对原始问题进行化大为小：要么尝试更小的背包容量 \\[w \\leq W\\], 要么尝试更少的珠宝数目 \\[j \\leq n\\]. 由于涉及到两个自变量，考虑到按顺序挑选珠宝这一过程更容易理解，我们不难定义状态 \\[K(i, w)\\] 为挑选出前 i 件珠宝时，重量不超过 w 时的珠宝总价值的最大值。相应的状态转移方程为第 i 件珠宝要么不选，要么选，即 \\[K(i,w) = \\max \\{K(i-1, w), K(i-1, w- w_i) + v_i\\}\\]. 令 dp[i + 1][j] 表示从前 i 种物品中选出总重量不超过 j 时珠宝总价值的最大值。那么有转移方程： 1dp[i + 1][j] = max{dp[i][j], dp[i][j - w[i]] + v[i]} 时间复杂度为 \\[(O(nW))\\]. 这里的分析是以容量递推的，但是在容量特别大时，可以看出时间复杂度略高，这时我们可能需要以价值作为转移方程。定义状态 dp[i + 1][j] 为前 i 个物品中挑选出价值总和为 j 时总重量的最小值（所以对于不满足条件的索引应该用充分大的值而不是最大值替代，防止溢出）。相应的状态转移方程相思：即第 i 件珠宝选中与否，即 dp[i + 1][j] = min{dp[i][j], dp[i][j - v[i]] + w[i]}. 最终返回结果为 dp[n][j] ≤ W 中最大的 j. Knapsack with repetition - 物品重复可用的背包问题 相比于 01 背包问题，这类背包问题中同一物品可以被多次选择，因此称为 Knapsack with repetition, 又称 Unbounded knapsack problem(无界背包问题). 由 01 背包问题扩展后状态定义仍然可以不变，但状态变化则由原来的第 i 件珠宝选或者不选改为选不小于 0 任何整数件。 令 dp[i + 1][j] 表示从前 i 种物品中选出总重量不超过 j 时珠宝总价值的最大值。那么有转移方程： 1dp[i + 1][j] = max{dp[i][j - k × w[i]] + k × v[i] | 0 ≤ k} 最坏情况下时间复杂度为 \\[O(kW^2)\\]. 我们对上式进一步变形可得： 1234dp[i + 1][j] = max{dp[i][j - k × w[i]] + k × v[i] | 0 ≤ k} = max{dp[i][j], max{dp[i][j - k × w[i]] + k × v[i] | 1 ≤ k}} = max{dp[i][j], max{dp[i][(j - w[i]) - k × w[i]] + k × v[i] | 0 ≤ k} + v[i]} = max{dp[i][j], dp[i + 1][j - w[i]] + v[i]} 注意等式最后一行，咋看和01背包一样，实际上区别在于dp[i + 1][], 01背包中为dp[i][]. 此时时间复杂度简化为 \\[O(nW)\\]. 和 01 背包的递推关系区别在于第 i 件珠宝是否可以重复取。 扩展 以上我们只是求得了最终的最大获利，假如还需要输出选择了哪些项如何破？ 以普通的01背包为例，如果某元素被选中，那么其必然满足w[i] &gt; j且大于之前的dp[i][j], 这还只是充分条件，因为有可能被后面的元素代替。保险起见，我们需要跟踪所有可能满足条件的项，然后反向计算有可能满足条件的元素，有可能最终输出不止一项。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113import java.util.*;public class Backpack { // 01 backpack with small datasets(O(nW), W is small) public static int backpack(int W, int[] w, int[] v, boolean[] itemTake) { int N = w.length; int[][] dp = new int[N + 1][W + 1]; boolean[][] matrix = new boolean[N + 1][W + 1]; for (int i = 0; i &lt; N; i++) { for (int j = 0; j &lt;= W; j++) { if (w[i] &gt; j) { // backpack cannot hold w[i] dp[i + 1][j] = dp[i][j]; } else { dp[i + 1][j] = Math.max(dp[i][j], dp[i][j - w[i]] + v[i]); // pick item i and get value j matrix[i][j] = (dp[i][j - w[i]] + v[i] &gt; dp[i][j]); } } } // determine which items to take for (int i = N - 1, j = W; i &gt;= 0; i--) { if (matrix[i][j]) { itemTake[i] = true; j -= w[i]; } else { itemTake[i] = false; } } return dp[N][W]; } // 01 backpack with big datasets(O(n\\sigma{v}), W is very big) public static int backpack2(int W, int[] w, int[] v) { int N = w.length; // sum of value array int V = 0; for (int i : v) { V += i; } // initialize int[][] dp = new int[N + 1][V + 1]; for (int[] i : dp) { // should avoid overflow for dp[i][j - v[i]] + w[i] Arrays.fill(i, Integer.MAX_VALUE &gt;&gt; 1); } dp[0][0] = 0; for (int i = 0; i &lt; N; i++) { for (int j = 0; j &lt;= V; j++) { if (v[i] &gt; j) { // value[i] &gt; j dp[i + 1][j] = dp[i][j]; } else { // should avoid overflow for dp[i][j - v[i]] + w[i] dp[i + 1][j] = Math.min(dp[i][j], dp[i][j - v[i]] + w[i]); } } } // search for the largest i dp[N][i] &lt;= W for (int i = V; i &gt;= 0; i--) { // if (dp[N][i] &lt;= W) return i; if (dp[N][i] &lt;= W) return i; } return 0; } // repeated backpack public static int backpack3(int W, int[] w, int[] v) { int N = w.length; int[][] dp = new int[N + 1][W + 1]; for (int i = 0; i &lt; N; i++) { for (int j = 0; j &lt;= W; j++) { if (w[i] &gt; j) { // backpack cannot hold w[i] dp[i + 1][j] = dp[i][j]; } else { dp[i + 1][j] = Math.max(dp[i][j], dp[i + 1][j - w[i]] + v[i]); } } } return dp[N][W]; } public static void main(String[] args) { int[] w1 = new int[]{2, 1, 3, 2}; int[] v1 = new int[]{3, 2, 4, 2}; int W1 = 5; boolean[] itemTake = new boolean[w1.length + 1]; System.out.println(&quot;Testcase for 01 backpack.&quot;); int bp1 = backpack(W1, w1, v1, itemTake); // bp1 should be 7 System.out.println(&quot;Maximum value: &quot; + bp1); for (int i = 0; i &lt; itemTake.length; i++) { if (itemTake[i]) { System.out.println(&quot;item &quot; + i + &quot;, weight &quot; + w1[i] + &quot;, value &quot; + v1[i]); } } System.out.println(&quot;Testcase for 01 backpack with large W.&quot;); int bp2 = backpack2(W1, w1, v1); // bp2 should be 7 System.out.println(&quot;Maximum value: &quot; + bp2); int[] w3 = new int[]{3, 4, 2}; int[] v3 = new int[]{4, 5, 3}; int W3 = 7; System.out.println(&quot;Testcase for repeated backpack.&quot;); int bp3 = backpack3(W3, w3, v3); // bp3 should be 10 System.out.println(&quot;Maximum value: &quot; + bp3); }} Reference 《挑战程序设计竞赛》第二章 Chapter 6.4 Knapsack Algorithm - S. Dasgupta 0019算法笔记——【动态规划】0-1背包问题 - liufeng_king的专栏 崔添翼 § 翼若垂天之云 › 《背包问题九讲》2.0 alpha1 Knapsack.java","link":"/posts/2735872831.html"},{"title":"英语学习指南（一）","text":"扩充语料库的训练方法 经过上面分析，可以得出英语学习最关键的内容就是语料库，其次就是语法（识别引擎）。 语料库并不只是词汇，训练学习方法也并非简单的背单词。 如果通过听力识别器输入语料，首先我们要能区分口音，比如当前语料的英式发音、美式发音、澳洲发音、印度发音是什么样。之后就是一些连读、弱读和重音变化规则。通过口音、连读和重音等规则正确解析得到词句短语之后，开始进入解析器流程。我们需要知道其中每个单词的多重含义，将其含义一个个的匹配放入上下文语境结合重音等得到最准确的含义。这时还可能遇到一些词典里没有的新词或者老词新意，比如 “给力”、“呵呵” 或者各种歪曲解释的 “不可描述” 的成语等，这些都是人们新造或者赋予了新意思的词，这些词意词典并不一定会收录但广泛在一个圈子里使用和交流，这就涉及到文化、俚语、流行语等。此外还需要用语法进行下一步解析，比如时态、标点、语气、句型句式等。 经过上面流程才可以解析出比较准确的意思，做出回应还要逆向再来一遍。 首先你要按照一定逻辑和结构梳理好你想表达的意图。这一步其实跟语言无关，中文阅读理解能力好的人，英文只要看懂了阅读理解能力也会很强，有条理的人不只是说话有条理，做事情也会很有条理和逻辑性。想好表达之后你就要开始从语料库抽取最符合你意图的词句，并按照合适的语法进行组装，这时候就可以看出一个人语料库大小和水平。比如：表达我喜欢一个梳妆台，只有最基础语料库的我只能说出：我喜欢这个桌子上有镜子的东西。这时，看到这句话的人可能知道你想表达喜爱一张桌子和镜子的东西，但不知道具体是什么。如果语料库里有更庞大的名词库和程度形容词，就可以说出：这就是我梦寐以求的带有地中海风格雕刻、椭圆梳妆镜的梳妆台。看到这句话你脑子里应该可以出现这个梳妆台大体的样子吧。之后，你还需要发音说出来，先从语料库里抽取对应的词句声音，再结合略读、弱读、重读等规则控制舌头、喉咙肌肉发出来对应的声音，至此算是完成一次对话。 上面只是听力识别器，视力识别器大同小异，无非是识别各种字体、大小写、标点符号等，不再赘述。 通过上面分析可以得知，语料库里的每个语料通常需要以下信息： 多种口音的声音，比如：英美澳印等。 不同词性和时态下，不同的发音和重音位置变化，比如：resume 在名词下表简历，发音 /ˈrez.ə.meɪ/，而动词表继续之前暂停的事情，发音 /rɪˈzuːm/，如果你发错了声音，则会导致对方解读错误。 单词的多重含义，比如：current 常见意思是形容词当下的，但实际上还有名词的 水流、潮流 等意思。曾经做阅读理解看到带有水流意思的句子一脸懵逼，没法用 当下 的意思来解释和理解这个句子。同样 spring 除了 春天 还有 弹力、泉水 等完全联想不到的意思。 同义词及其对比、反义词等，因为你的回复可能需要否决或者加强观点，这些词可以帮助你更好更精准的表达。比如： raise 和 rise 有什么区别，什么场景下适用？还有 under、beneath、underneath 和 below 等，要怎么用。 语料相关的俚语、文化、衍生词等。比如：clump 这个单词本意是草丛之类的意思，但是你可以搜下 Google 图片，形容词 clumped 还算正常，但名词搜出来完全变成了一种奇怪的生物。因为这个单词正好是两个热门美国人物名字合起来的发音，所以老美虚构了一个人出来调侃。因此如果看到 twitter 上有这个单词，要多考虑下是不是在调侃政治人物。 相关的常见固定搭配和用法。比如：focus on 和 pay attention to 区别以及介词的不同。 名词的单复数形态和动词的各种时态变形及其发音规则。比如：hair 在不同词性下既可数又不可数，反过来说，也可以通过这个判断出此处 hair 要表达的意思。s 结尾通常有 z、s 和 əz 三个发音，ed 结尾也有 d、t 以及 id 等情况。read 更奇葩，过去式和过去分词都是 read，但是过去式发音变了，读作 /red/，换言之，需要通过这个发音来识别这个行为是过去发生的还是现在。 除了语料之外的语法，就靠单点专项突破，比如时态种类和规则、词性和句子组成结构以及时间和数字的组合规则和发音方法等等，这里不再赘述。 扩充语料库的训练方法实战 例如看到一个 clump 单词想要学习，可以这样做： 第一遍开始认识单词： 通过 Cambridge Dictionary 查询单词意思，可以看到是否可数、各种释义和对应的例句、关联词和对比等等。Merriam Webster 是美式在线字典，如有精力也可以作为知识补充，但是例句和界面不如 Cambridge Dictionary 好用。 查看音标，尝试发音（需要先把音标练熟，参照下面教程），收听词典中给出的英式和美式的真人发音，看下自己的发音是否准确？哪里不对？是音标还是重音？练对为止。 打开 Forvo 收听这个单词不同人的发音，可以尝试跟读以便录入自己的音频语料库。也强烈推荐使用 YouGlish ，它会搜索 Youtube 上面包含当前单词的视频，这样发音更贴合日常对话。 打开 Google Translate 语音输入，尽量带上耳机或者用耳麦尝试发音，查看是否能稳定识别出当前单词。如果不能，请回到上面步骤，对 Google Translate 播放母语真人发音音频查看能否识别。如果母语真人发音可以识别，说明你的发音有问题，请重复上面步骤调整发音到可以识别为止。 打开 Google 搜索，输入当前单词 + vs 即可看到近义词和易混淆词汇，搜索查看相关对比。 不是特别抽象的单词，可以打开 Google 图片搜索进行搜索，通过图像加深记忆。我用图片搜索 clump 时，才发现了两位候选人的合体的意思。再举一个例子，bay 和 gulf 在词典里都有海湾的意思，那它们之间有什么区别吗？用 Google 图片搜索一下你就可以发现，gulf 要比 bay 大的多。 将单词以及有趣的发现记录在 Anki 里面，只需要点击 Add 并添加单词即可，也可以在下方多加一些注释，可繁可简不需要花太多时间。将在后面的 QA 部分解释为什么要用 Anki。 第二遍在 Anki 里面复习，看到单词尝试发音识别，尝试回忆相关的意思、用法以及图像，尽可能的回忆。如果感觉没问题那么就点击 Good 或者 Easy，否则就选择 Again 或者 Hard。如果掌握不好，请重复第一遍步骤加深记忆。 第三遍重复第二步。 第 N 遍，已经基本掌握。 每隔一段时间应该导出一份单词列表，放在 Danci88 上面进行听写。这个工具虽然比较简陋，但是非常实用。通过听写可以看出对这个单词的掌握程度，如果你脑子瞬间出现这个单词则表示掌握了。 值得注意的是，上面步骤是相对完整的学习步骤，在实际学习过程中针对不同类别的单词应该可繁可简。比如我在 Anki 建立了 7 个语料库，分别是： 发音错题本：第一眼看到单词发出的声音无法识别或者与实际发音不符。这类单词要注重音标、发音识别、多语音辨音等，要用 Google 翻译来识别测试。 听力单词本：在听写或者听力时，没有听出并写出的词句。这类单词要注重发音和听写训练。 常见名词本：例如国家、地区等。这类单词只需注重发音和听力识别即可，常见单词注重拼写以及了解相关文化历史，不常见甚至不需要去学怎么拼写，毕竟写的机会不多，而且可以轻易搜到。 拼写错题本：这类单词认识意思、会读，但是拼写错了。其实很多常见的单词，都觉得习以为常，但真正让你拼写的时候却拿捏不准。这类单词就要注重拼写和听写，以及多在键盘上敲打建立肌肉记忆。 新单词本：这个就是遇到的比较重要的新单词，需要按照上面流程进行学习。这类单词就是重复刷记忆。 熟词生僻意思单词本：这个主要记录一些很熟悉的词的生僻意思。比如 champion 比较常见的是名词冠军，但其实也有动词捍卫的生僻意思。 连读训练本：这个是在练习口语阅读或者复述句子时，觉得很绕口的词句。比如 the very idea of a police force was seen as foreign as that is 这句话。这类语料要注重断句、重读、声调变化和气息控制，重复读到流畅即可。 上面步骤好像很多，而且需要记忆的内容量好大，有什么技巧来快速稳定的记忆吗？ 记忆单词、语料库唯一的银弹：重复训练 时间回到初中，我英语不是很好，路上遇到同行英语老师便问道学习英语有什么技巧吗？老师微笑说：Practice、Practice and Practice，我却不以为然，这应该是我走过的最大的弯路。 高中之后，为了提升记忆力，我做了很多尝试和训练。比如七田真的《超右脑照相记忆法》以及《魔术记忆》等，经常对着曼陀罗图片看、上学放学路上眨眼记车牌、瞬间记住一串手机号以及按照《魔术记忆》的联想技巧记忆一些东西，希望能练到过目不忘。看起来是挺神奇挺有效的，尤其是《魔术记忆》里面的“联想记忆法”、“定桩记忆法”、“记忆宫殿记忆法”等，随便一个人都可以快速记忆一串不相关的关键词。 正是如此，这类英语学习速记技巧也成了各种网上学习平台割韭菜的视频教程，还卖价不菲。通过几个单词联想让试看的观众觉得神奇，从而脑热买下教程，大部分人买了不看，即使看了练了也没啥用。 作为过来人回头来看，这完全是弯路，这些速记方法大多是表演性质的，根本没法固化成自己的能力。请你回忆一下，你为什么认识 commit 这个单词？是因为通过什么其他单词联想出来的吗？是因为通过某个图片关联想起来的吗？并不是，当你用 git 提交时，你几乎天天都遇到这个单词，同事天天交流用这个单词，正是一遍遍的在你面前出现、听到才让你非常熟练的掌握这个单词。如果你不认识 commit 这个单词，那么说说你是怎么记住你的那么长的身份证号码的？是通过对数字图像化、故事化编排的？还是大量重复见到和用到？ 你越早明白语言学习没有技巧，就会走越少的弯路、花越少的冤枉钱。——这是我走过 13 年的弯路近期得出的结论。 通过重复训练得到的能力还会更持久，就像你的 QQ 号，即便过了这么多年没用应该还可以熟练的背出来吧。这也是为什么要用 Anki 的原因，具体介绍详见 QA 章节。","link":"/posts/1306835048.html"},{"title":"LRU Cache","text":"Question leetcode: LRU Cache | LeetCode OJ lintcode: (134) LRU Cache ### Problem Statement Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and set. get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1. set(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item. 题解 Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class Solution {private int capacity; private HashMap&lt;Integer, Node&gt; map = new HashMap&lt;&gt;(); private Node head = new Node(-1, -1), tail = new Node(-1, -1); private class Node { Node prev, next; int val, key; public Node(int key, int val) { this.val = val; this.key = key; prev = null; next = null; }// @Override// public String toString() {// return &quot;(&quot; + key + &quot;, &quot; + val + &quot;) &quot; + &quot;last:&quot;// + (prev == null ? &quot;null&quot; : &quot;node&quot;);// } } public Solution(int capacity) { this.capacity = capacity; tail.prev = head; head.next = tail; } public int get(int key) { if (!map.containsKey(key)) { return -1; } // remove current Node currentNode = map.get(key); currentNode.prev.next = currentNode.next; currentNode.next.prev = currentNode.prev; // move current to tail; moveToTail(currentNode); return map.get(key).val; } public void set(int key, int value) { if (get(key) != -1) { map.get(key).val = value; return; } if (map.size() == capacity) { map.remove(head.next.key); head.next = head.next.next; head.next.prev = head; } Node insert = new Node(key, value); map.put(key, insert); moveToTail(insert); } private void moveToTail(Node current) { current.prev = tail.prev; tail.prev = current; current.prev.next = current; current.next = tail; }}","link":"/posts/4182335710.html"},{"title":"Language Modeling","text":"N-Gram 这里首先有个语料库，记录了很多条句子，然后预测给定句子出现的概率。给定一个句子，这里句子的长度为n，也就是\\(|V|=n\\)，第i个单词用字母\\(x_i\\)表示，那么概率公式表示为：\\(P(X_1=x_1,X_2=x_2,...X_n=x_n)\\) 使用马尔科夫假设第i个单词出现只与前面几个单词有关，这里如果和前面的全部单词有关，则有： \\[ P(X_1=x_1,X_2=x_2,...X_n=x_n)=P(X_1=x_1)\\prod_{i=2}^{n}{P(X_i=x_i|X_1=x_1,...,X_{i-1}=x_{i-1})} \\] 如果是TRIGRAM LANGUAGE MODELS ，有 \\[ P(X_1=x_1,X_2=x_2,...X_n=x_n)=\\Pi_{i=1}^{n}{P(X_i=x_i|X_{i-2 }=x_{i-2},X_{i-1}=x_{i-1})} \\] 即\\(p(x_1,x_2,...,x_n)=\\Pi_{i=1}^{n}q(x_i|x_{i-2},x_{i-1})\\)，而\\(q(w|u,v)=\\frac{c(u,v,w)}{c(u,v)}\\) 这里如果我们使用bigram模型的话，且\\(x_0=x_{-1}=*\\)，该式子就变为如下： \\[ P(X_1=x_1,X_2=x_2,...X_n=x_n)=\\prod_{i=1}^{n}{P(X_i=x_i|X_{i-1}=x_{i-1})} \\] 摘抄这个博客举的例子：假设语料库总词数为13,748 词和词频表如下所示： 那么对于句子I want to eat Chinese food，其概率 \\[ P(I\\, want\\, to \\,eat\\, Chinese\\, food)\\\\ =P(I)*P(want|I)*P(to|want)*P(eat|to)*P(Chinese|eat)*P(food|Chinese)\\\\ =0.25*1087/3437*786/1215*860/3256*19/938*120/213\\\\ =0.000154171 \\] 这里讲一下参数个数是\\(|V|^2\\)，可以从上面的第二个图中看到，是词序列频度的大小。 Evaluating Language Models：Perplexity 这里给出了评估语言模型的指标——困惑度，选取一些句子，这些句子不在语料库中，也就是新的句子，用\\(x^{(i)}\\)表示第i个句子，其中第i个句子又由单词\\(x_{1}^{(i)},x_{2}^{(i)},x_{3}^{(i)},...\\)组成，那么对于这些测试的句子，计算所有句子的概率积： \\[ \\Pi_{i=1}^{m}p(x^{(i)}) \\] 直观的感受是该结果越大，说明语言模型越好。[至于此处为什么，我也讲不清楚，可以理解成模型对未知句子的预测能力吧] 这里设第i个句子的长度是\\(n_i\\)，那么记\\(M=\\sum_{i=1}^{m}n_{i}\\)，定义平均log概率如下： \\[ \\frac{1}{M}log_2\\prod_{i=1}^mp(x^{(i)})\\\\ =\\frac{1}{M}\\sum_{i=1}^{m}log_2p(x^{(i)}) \\] 定义困惑度为\\(2^{-l}\\)，其中\\(l=\\frac{1}{M}\\sum_{i=1}^{m}log_2p(x^{(i)})\\)。于是有困惑度越低，语言模型在未知数据上表现越好，最后给出了对于词汇表为50000时，trigram模型的困惑度是74，bigram模型的困惑度是137，unigram模型的困惑度是955 Smoothed Estimation of Trigram Models 比较常见的两种平滑技术 linear interpolation和 discounting methods。 Linear interpolation \\[ q_{ML}(w|u,v)=\\frac{c(u,v,w)}{c(u,v)}\\\\ q_{ML}(w|v)=\\frac{c(v,w)}{c(v)}\\\\ q_{ML}(w)=\\frac{c(w)}{c()} \\] 这个主要的思想是将上面三个评估相加，如下： \\[ q(w|u,v)=\\lambda_1q_{ML}(w|u,v)+\\lambda_2q_{ML}(w|v)+\\lambda_3q_{ML}(w) \\] 其中\\(\\lambda_1\\geq 0,\\lambda_2\\geq 0,\\lambda_3\\geq 0\\)且\\(\\lambda_1+\\lambda_2+\\lambda_3=1\\) 那么如何得到参数\\(\\lambda_1,\\lambda_2,\\lambda_3\\)呢，这里用到log似然估计，其中选取held-out data（记 development data），这部分数据独立于training和test数据，其中记\\(c'(u,v,w)\\)为development data中trigram\\((u,v,w)\\)出现的次数，L函数如下： \\[ L(\\lambda_1,\\lambda_2,\\lambda_3)=\\sum_{u,v,w}c'(u,v,w)log\\,q(w|u,v)\\\\ =\\sum_{u,v,w}c'(u,v,w)log(\\lambda_1q_{ML}(w|u,v)+\\lambda_2q_{ML}(w|v)+\\lambda_3q_{ML}(w)) \\] 然后求解如下式子： \\[ arg\\,\\,\\,\\,max_{\\lambda_1,\\lambda_2,\\lambda_3}\\,\\,L(\\lambda_1,\\lambda_2,\\lambda_3)\\\\ \\lambda_1\\geq 0,\\lambda_2\\geq 0,\\lambda_3\\geq 0\\\\ \\lambda_1+\\lambda_2+\\lambda_3=1 \\] 当然还有bucketing方法来求解\\(\\lambda\\)，这里就不记录了，在Michael Collins上有 Discounting Methods notes上讲的也很清楚，就是设置一个参数\\(\\beta\\)，且\\(\\beta\\)在0到1之间。 然后对于bigram模型来说，将原来的\\(c(v,w)\\)减去\\(\\beta\\)作为\\(c^*(v,w)\\)，这里针对的是语料库，那么本来有\\(\\sum_{c(v,w)&gt;0}\\frac{c(v,w)}{c(v)}=1\\)，但是变为\\(c^*(v,w)\\)后就小于1了，这里记 \\[ \\alpha (v)=1-\\sum_{c(v,w)&gt;0}\\frac{c^*(v,w)}{c(v)} \\] 定义如下两个集合： \\[ A(v)=\\{w:c(v,w)&gt;0\\} \\\\ B(v)=\\{w:c(v,w)=0\\} \\] A(v)表示在语料库中有的，B(v)表示在语料库中没有的，主要是B(v)，虽然语料库中没有，但不能为0，所以将其变为N-1 gram，这里变为unigram，具体公式如下： \\[ q_D(w|v)=\\left\\{\\begin{matrix} \\frac{c^*(v,w)}{c(v)}&amp; If \\,\\,w\\in A(v) \\\\ \\alpha(v) \\frac{q_{ML}(w)}{\\sum_{w \\in B(v)}q_{ML}(w)} &amp; If\\,\\,w\\in B(v) \\end{matrix}\\right. \\] trigram模型同理，见notes。 那么怎么估计\\(\\beta\\)呢，和前面的一样，使用log似然函数，即： \\[ \\sum_{u,v,w}c'(u,v,w)log\\,q(w|u,v) \\] 然后给定一组\\(\\beta\\)集合{0.1,0.2,0.3,…0.9}，然后用上面的公式计算最大值对应的值就为\\(\\beta\\)的值。","link":"/posts/464973098.html"},{"title":"LeetCode 上最难的链表算法题，没有之一！","text":"题目来源于 LeetCode 第 23 号问题：合并 K 个排序链表。 该题在 LeetCode 官网上有关于链表的问题中标注为最难的一道题目：难度为 Hard ，通过率在链表 Hard 级别目前最低。 题目描述 合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。 示例: 1234567输入:[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 输入 图一 输出 图二 题目分析一 这里需要将这 k 个排序链表整合成一个排序链表，也就是说有多个输入，一个输出，类似于漏斗一样的概念。 因此，可以利用最小堆的概念。如果你对堆的概念不熟悉，可以戳这先了解一下~ 取每个 Linked List 的最小节点放入一个 heap 中，排序成最小堆。然后取出堆顶最小的元素，放入输出的合并 List 中，然后将该节点在其对应的 List 中的下一个节点插入到 heap 中，循环上面步骤，以此类推直到全部节点都经过 heap。 由于 heap 的大小为始终为 k ，而每次插入的复杂度是 logk ，一共插入了 nk 个节点。时间复杂度为 O(nklogk)，空间复杂度为O(k)。 动画演示 动画演示 代码实现 1234567891011121314151617181920212223242526272829class Solution { public ListNode mergeKLists(ListNode[] lists) { //用heap(堆)这种数据结构，也就是 java 里面的 PriorityQueue PriorityQueue&lt;ListNode&gt; pq = new PriorityQueue&lt;&gt;(new Comparator&lt;ListNode&gt;() { public int compare(ListNode a, ListNode b) { return a.val-b.val; } }); ListNode ret = null, cur = null; for(ListNode node: lists) { if(null != node) { pq.add(node); } } while(!pq.isEmpty()) { ListNode node = pq.poll(); if(null == ret) { ret = cur = node; } else { cur = cur.next = node; } if(null != node.next) { pq.add(node.next); } } return ret; }} 题目分析二 这道题需要合并 k 个有序链表，并且最终合并出来的结果也必须是有序的。如果一开始没有头绪的话，可以先从简单的开始：合并 两 个有序链表。 合并两个有序链表：将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例： 12输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 这道题目按照题目描述做下去就行：新建一个链表，比较原始两个链表中的元素值，把较小的那个链到新链表中即可。需要注意的一点时由于两个输入链表的长度可能不同，所以最终会有一个链表先完成插入所有元素，则直接另一个未完成的链表直接链入新链表的末尾。 所以代码实现很容易写： 123456789101112131415161718192021222324class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { //新建链表 ListNode dummyHead = new ListNode(0); ListNode cur = dummyHead; while (l1 != null &amp;&amp; l2 != null) { if (l1.val &lt; l2.val) { cur.next = l1; cur = cur.next; l1 = l1.next; } else { cur.next = l2; cur = cur.next; l2 = l2.next; } } // 注意点：当有链表为空时，直接连接另一条链表 if (l1 == null) { cur.next = l2; } else { cur.next = l1; } return dummyHead.next; } 现在回到一开始的题目：合并 K 个排序链表。 合并 K 个排序链表 与 合并两个有序链表 的区别点在于操作有序链表的数量上，因此完全可以按照上面的代码思路来实现合并 K 个排序链表。 这里可以参考 归并排序 的分治思想，将这 K 个链表先划分为两个 K/2 个链表，处理它们的合并，然后不停的往下划分，直到划分成只有一个或两个链表的任务，开始合并。 归并-分治 代码实现 根据上面的动画，实现代码非常简单也容易理解，先划分，直到不能划分下去，然后开始合并。 123456789101112131415161718192021222324252627282930313233343536373839class Solution { public ListNode mergeKLists(ListNode[] lists){ if(lists.length == 0) return null; if(lists.length == 1) return lists[0]; if(lists.length == 2){ return mergeTwoLists(lists[0],lists[1]); } int mid = lists.length/2; ListNode[] l1 = new ListNode[mid]; for(int i = 0; i &lt; mid; i++){ l1[i] = lists[i]; } ListNode[] l2 = new ListNode[lists.length-mid]; for(int i = mid,j=0; i &lt; lists.length; i++,j++){ l2[j] = lists[i]; } return mergeTwoLists(mergeKLists(l1),mergeKLists(l2)); } public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if (l1 == null) return l2; if (l2 == null) return l1; ListNode head = null; if (l1.val &lt;= l2.val){ head = l1; head.next = mergeTwoLists(l1.next, l2); } else { head = l2; head.next = mergeTwoLists(l1, l2.next); } return head; }}","link":"/posts/1728584467.html"},{"title":"Lexicalized Probabilistic Context-Free Grammars","text":"Weaknesses of PCFGs as Parsing Models 这一章是对上一章的优化，PCFGs主要有两个关键的弱点，第一个是缺乏对词汇信息的敏感度，第二个是缺乏对结构的偏好，第一个问题是我们使用lexicalized PCFGs的根本原因。 比如下面两棵 parse tree： 使用PCFGs来计算两棵树的概率： \\[ p(a)=q(S\\rightarrow NP\\,\\,VP)q(NP \\rightarrow NNS)q(VP\\rightarrow VP\\,\\,PP)q(VP\\rightarrow VBD\\,\\,NP)\\\\q(pp\\rightarrow IN \\,\\,NP)q(NP \\rightarrow NNS)q(NP\\rightarrow DT\\,\\,NN)...\\\\ p(b)=q(S\\rightarrow NP\\,\\,VP)q(NP\\rightarrow NNS)q(VP \\rightarrow VBD\\,\\,NP)q(NP\\rightarrow NP\\,\\,PP)\\\\q(NP\\rightarrow NNS)q(PP\\rightarrow IN\\,\\,NP)q(NP\\rightarrow DT\\,\\,NN) \\] 可以发现选择那棵树完全取决于\\(q(VP \\rightarrow VP\\,\\,PP)\\)和\\(q(NP \\rightarrow NP\\,\\,PP)\\)的大小，也就是说统计语料库中\\(VP\\rightarrow VP\\,\\,PP\\)和\\(NP \\rightarrow NP\\,\\,PP\\)的个数，那个更多，就选择那棵树，和词汇没有一点关系；但是如果PP是和介词into一起，统计的结果是VP PP出现的次数是NP PP的9倍；而如果介词是of，统计的结果是NP PP出现的次数是VP PP的100倍，这就说明了PP中的词汇是有一定作用的。 第二个问题是缺乏结构偏好，比如下面这两棵parse tree： 使用PCFGs计算的结果是一样的，无法区分这两棵树，但是我们可以通过在parse trees中统计上述两种树结构出现的次数来选取那一棵树，我们使用lexicalized PCFGs也可以做到这一点。 Lexicalized of a Treebank 见下图，a表示没有经过词汇标记的树，b表示使用Lexicalized标记的树： Lexicalized PCFGs 用Chmosky normal 形式定义G=(N,∑,R,S,q,γ)，参数表示如下： N表示的是非终止符 其中$$表示词汇集合，也就是终止符 R 是一个规则集合，这些规则属于下面三种中的一种： \\[ X(h) \\rightarrow_1 Y_1(h)Y_2(m) \\,\\,where\\,\\,X,Y_1,Y_2 \\in N,h,m \\in \\sum\\\\ X(h) \\rightarrow_2 Y_1(m)Y_2(h)\\,\\, where \\,\\,X,Y_1,Y_2 \\in N,h,m \\in \\sum\\\\ X(h) \\rightarrow h \\,\\,where\\,\\, X \\in N,h \\in \\sum \\] 这里的h就是head，m就是modify，也就是要改变的单词 对每一个规则\\(r\\)，用\\(q(r)\\)表示概率,有： \\[ \\sum_{r \\in R: LHS(r)=X(h)} q(r)=1 \\] 其中\\(LHS(r)\\)表示任何规则的左边 定义\\(\\gamma(X,h),X \\in N,h \\in \\sum\\)表示X的词汇是h的概率，有\\(\\sum_{X \\in N,h \\in \\sum}\\gamma(X,h)=1\\) 这样parse tree的概率就可以如下表示，其中\\(r_i\\)表示R中的规则： \\[ \\gamma(LHS(r_1)) \\prod_{i=1}^{N}q(r_i) \\] 如下图，就可以计算下面解析树的概率了： Parameter Estimation in Lexicalized PCFGs 举个例子，对于规则\\(S(examined) \\rightarrow_2 NP(laywer)\\,\\, VP(examined)\\)，写成概率形式如下： \\[ q(S(examined) \\rightarrow_2 NP(lawyer)\\,\\,VP(examined))\\\\ =P(R=S\\rightarrow_2 NP\\,\\,VP,M=lawyer|X=S,H=examined)\\\\ =P(R=S \\rightarrow_2 NP\\,\\,VP|X=S,H=examined)\\\\P(M=lawyer|R=S\\rightarrow_2 NP\\,\\,VP,X=S,H=examined) \\] 最后一个式子是条件概率展开，分为两部分，这里使用平滑技术求解，可以想象成前面的3-gram模型，避免0的出现，第一部分用下面两个式子结合得到： \\[ q_{ML}(S\\rightarrow_2 NP\\,\\,VP|S,examined)=\\frac{count(R=S\\rightarrow_2 NP\\,\\,VP,X=S,H=examined)}{count(X=S,H=examined)}\\\\ q_{ML}(S\\rightarrow_2 NP\\,\\,VP|S)=\\frac{count(R=S\\rightarrow_2 NP\\,\\,VP,X=S)}{count(X=S)} \\] 使用\\(\\lambda_1,0&lt;=\\lambda_1&lt;=1\\)表示如下： \\[ P(R=S\\rightarrow_2 NP\\,\\,VP|X=S,H=examined)\\\\ =\\lambda_1q_{ML}(S\\rightarrow_2 NP\\,\\,VP|S,examined)+(1-\\lambda_1)q_{ML}(S\\rightarrow_2 NP\\,\\,VP|S) \\] 第二部分式子可以作如下等价： \\[ P(M=lawyer|R=S\\rightarrow_2 NP\\,\\,VP,X=S,H=examined)\\\\ =P(M=lawyer|R=S\\rightarrow_2 NP\\,\\,VP,H=examined) \\] 继续使用平滑估计： \\[ q_{ML}(lawyer|S\\rightarrow_2 NP\\,\\,VP,examined)\\\\=\\frac{count(M=lawyer,R=\\rightarrow_2 NP\\,\\,VP,H=examined)}{count(R=S\\rightarrow_2 NP\\,\\,VP,H=examinde)}\\\\ q_{ML}(lawyer|S\\rightarrow_2 NP\\,\\,VP)\\\\=\\frac{count(M=lawyer,R=\\rightarrow_2 NP\\,\\,VP)}{count(R\\rightarrow_2 NP\\,\\,VP)} \\] 这样第二个式子使用\\(\\lambda_2,0&lt;=\\lambda_2&lt;=1\\)就可以表示如下： \\[ \\lambda_2 q_{ML}(lawyer|S\\rightarrow_2 NP\\,\\,VP,examined)+(1-\\lambda_2) q_{ML}(lawyer|S\\rightarrow_2 NP\\,\\,VP) \\] 如上，就是对于规则\\(S(examined) \\rightarrow_2 NP(laywer)\\,\\, VP(examined)\\)的求解，那么一棵树的概率就可以通过这样的方式求解出来。最后当然是要找出一个概率最高的树了，下面讲到。 Parsing with Lexicalized PCFGs 和PCFGs的算法类似，定义\\(\\pi(i,j,h,X)\\)表示对任意一棵以X为根节点，词汇索引为h，从\\(i\\)到\\(j\\)的单词串的树的最大概率，即\\(i\\)和\\(j\\)表示从第\\(i\\)个单词到第\\(j\\)个单词，\\(h \\in \\{i...j\\}\\)表示在i到j这些单词中选择一个单词作为head，X表示树根，且其词汇信息为head。 给定初始条件：\\(\\pi(i,i,i,X)=q(X(x_i) \\rightarrow x_i)\\) 通项公式如下图所示： 说明一下：m是下一个head对应的索引，有两种情况，一种是分解后，根的head落到第一部分，一种是分解后根的head落到第二部分，不过这里倒是默认分解为两部分。 算法如下图所示：","link":"/posts/1262734963.html"},{"title":"Linked List Cycle II","text":"Question leetcode: Linked List Cycle II | LeetCode OJ lintcode: (103) Linked List Cycle II 12345678Given a linked list, return the node where the cycle begins. If there is no cycle, return null.ExampleGiven -21-&gt;10-&gt;4-&gt;5, tail connects to node index 1，return node 10ChallengeFollow up:Can you solve it without using extra space? 题解 - 快慢指针 题 Linked List Cycle | Data Structure and Algorithm 的升级版，题目要求不适用额外空间，则必然还是使用快慢指针解决问题。首先设组成环的节点个数为 \\[r\\], 链表中节点个数为 \\[n\\]. 首先我们来分析下在链表有环时都能推出哪些特性： 快慢指针第一次相遇时快指针比慢指针多走整数个环, 这个容易理解，相遇问题。 每次相遇都在同一个节点。第一次相遇至第二次相遇，快指针需要比慢指针多走一个环的节点个数，而快指针比慢指针多走的步数正好是慢指针自身移动的步数，故慢指针恰好走了一圈回到原点。 从以上两个容易得到的特性可知，在仅仅知道第一次相遇时的节点还不够，相遇后如果不改变既有策略则必然找不到环的入口。接下来我们分析下如何从第一次相遇的节点走到环的入口节点。还是让我们先从实际例子出发，以下图为例。 slow和fast节点分别初始化为节点1和2，假设快慢指针第一次相遇的节点为0, 对应于环中的第i个节点 \\[C_i\\], 那么此时慢指针正好走了 \\[n - r - 1 + i\\] 步，快指针则走了 \\[2 \\cdot (n - r - 1 + i)\\] 步，且存在[^1]: \\[n - r - 1 + i + 1= l \\cdot r\\]. (之所以在i后面加1是因为快指针初始化时多走了一步) 快慢指针第一次相遇时慢指针肯定没有走完整个环，且慢指针走的步数即为整数个环节点个数，由性质1和性质2可联合推出。 现在分析下相遇的节点和环的入口节点之间的关联，要从环中第i个节点走到环的入口节点，则按照顺时针方向移动[^2]: \\[(l \\cdot r - i + 1)\\] 个节点 (\\[l\\] 为某个非负整数) 即可到达。现在来看看式[^1]和式[^2]间的关系。由式[^1]可以推知 \\[n - r = l \\cdot r - i\\]. 从头节点走到环的入口节点所走的步数可用 \\[n - r\\] 表示，故在快慢指针第一次相遇时让另一节点从头节点出发，慢指针仍从当前位置迭代，第二次相遇时的位置即为环的入口节点！ Note 由于此题快指针初始化为头节点的下一个节点，故分析起来稍微麻烦些，且在第一次相遇后需要让慢指针先走一步，否则会出现死循环。 对于该题来说，快慢指针都初始化为头节点会方便很多，故以下代码使用头节点对快慢指针进行初始化。 C++ 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: The node where the cycle begins. * if there is no cycle, return null */ ListNode *detectCycle(ListNode *head) { if (NULL == head || NULL == head-&gt;next) { return NULL; } ListNode *slow = head, *fast = head; while (NULL != fast &amp;&amp; NULL != fast-&gt;next) { fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; if (slow == fast) { fast = head; while (slow != fast) { fast = fast-&gt;next; slow = slow-&gt;next; } return slow; } } return NULL; }}; ###Java 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */public class Solution { public ListNode detectCycle(ListNode head) { if (head == null || head.next == null) { return null; } ListNode slow = head; ListNode fast = head; while (fast.next != null &amp;&amp; fast.next.next != null) { slow = slow.next; fast = fast.next.next; if (slow == fast) { fast = head; while (fast != slow) { fast = fast.next; slow = slow.next; } return fast; } } return null; }} 源码分析 异常处理。 找第一次相遇的节点。 将fast置为头节点，并只走一步，直至快慢指针第二次相遇，返回慢指针所指的节点。 复杂度分析 第一次相遇的最坏时间复杂度为 \\[O(n)\\], 第二次相遇的最坏时间复杂度为 \\[O(n)\\]. 故总的时间复杂度近似为 \\[O(n)\\], 空间复杂度 \\[O(1)\\]. Reference Linked List Cycle II | 九章算法","link":"/posts/3161622025.html"},{"title":"Linked List Cycle","text":"Question leetcode: Linked List Cycle | LeetCode OJ lintcode: (102) Linked List Cycle 12345678Given a linked list, determine if it has a cycle in it.ExampleGiven -21-&gt;10-&gt;4-&gt;5, tail connects to node index 1, return trueChallengeFollow up:Can you solve it without using extra space? 题解 - 快慢指针 对于带环链表的检测，效率较高且易于实现的一种方式为使用快慢指针。快指针每次走两步，慢指针每次走一步，如果快慢指针相遇(快慢指针所指内存为同一区域)则有环，否则快指针会一直走到NULL为止退出循环，返回false. 快指针走到NULL退出循环即可确定此链表一定无环这个很好理解。那么带环的链表快慢指针一定会相遇吗？先来看看下图。 在有环的情况下，最终快慢指针一定都走在环内，加入第i次遍历时快指针还需要k步才能追上慢指针，由于快指针比慢指针每次多走一步。那么每遍历一次快慢指针间的间距都会减少1，直至最终相遇。故快慢指针相遇一定能确定该链表有环。 C++ 123456789101112131415161718192021222324252627282930313233/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: True if it has a cycle, or false */ bool hasCycle(ListNode *head) { if (NULL == head || NULL == head-&gt;next) { return false; } ListNode *slow = head, *fast = head-&gt;next; while (NULL != fast &amp;&amp; NULL != fast-&gt;next) { fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; if (slow == fast) return true; } return false; }}; Java 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */public class Solution { public boolean hasCycle(ListNode head) { if (head == null || head.next == null) { return false; } ListNode slow = head; ListNode fast = head; while (fast.next != null &amp;&amp; fast.next.next != null) { slow = slow.next; fast = fast.next.next; if (slow == fast) { return true; } } return false; }} 源码分析 异常处理，将head-&gt;next也考虑在内有助于简化后面的代码。 慢指针初始化为head, 快指针初始化为head的下一个节点，这是快慢指针初始化的一种方法，有时会简化边界处理，但有时会增加麻烦，比如该题的进阶版。 复杂度分析 在无环时，快指针每次走两步走到尾部节点，遍历的时间复杂度为 \\[O(n/2)\\]. 有环时，最坏的时间复杂度近似为 \\[O(n)\\]. 最坏情况下链表的头尾相接，此时快指针恰好在慢指针前一个节点，还需 n 次快慢指针相遇。最好情况和无环相同，尾节点出现环。 故总的时间复杂度可近似为 \\[O(n)\\]. Reference Linked List Cycle | 九章算法","link":"/posts/3376454104.html"},{"title":"Log-Linear Models","text":"Introduction log-linear 模型在自然语言处理上使用非常广泛，其最主要的原因在于其灵活性，允许很多特征被使用在该模型上，相比于之前的简单估计技术(比如HMMs的标记问题，PCFGs的解析问题)。 Motivation 再次思考语言模型问题，其任务是得出下面条件概率的估计： \\[ P(W_i=w_i|W_1=w_1...W_{i-1}=w_{i-1})=p(w_i|w_1...w_{i-1}) \\] 对于任一序列\\(w_1...w_i\\)，得出在序列\\(w_1...w_{i-1}\\)条件下，出现单词\\(w_i\\)的概率。在trigram语言模型中，我们有如下假设： \\[ p(w_i|w_1...w_{i-1})=q(w_i|w_{i-2},w_{i-1}) \\] 对于上面的概率求解，我们前面提到过使用一种平滑技术： \\[ q(w|u,v)=\\lambda_1q_{ML}(w|u,v)+\\lambda_2q_{ML}(w|v)+\\lambda_3q_{ML}(w) \\] Trigram语言模型十分有效，但是在使用上下文 对于任一序列\\(w_1...w_i\\)，得出在序列上相对狭窄了，比如我们想去估计单词model出现的概率在 对于任一序列\\(w_1...w_i\\)，得出在序列条件下： \\[ P(W_i=model|W_1=w_1...W_{i-1}=w_{i-1}) \\] 我们可以思考在单词\\(w_{i-2}\\)下的概率，而忽略\\(w_{i-1}\\)：\\(P(W_i=model|W_{i-2}=any)\\)；我们也可以考虑前一个单词的词性来对当前单词的概率：\\(P(W_i=model|pos(W_{i-1})=adj)\\)；实考前一个单词后缀对当前单词的影响：\\(P(W_i=model|suff4(W_{i-1})=ical)\\)等等很多，这样我们给每一个赋予权重参数，就可以写成如下形式： \\[ p(model|w_1...w_{i-1})=\\\\ \\lambda_1q_{ML}(model|w_{i-2}=any,w_{i-1}=statistical)+\\\\ \\lambda_2q_{ML}(model|w_{i-1}=statistical)+\\\\ \\lambda_3q_{ML}(model)+\\\\ \\lambda_4q_{ML}(model|w_{i-2}=any)+\\\\ \\lambda_5q_{ML}(model|w_{i-1}\\,\\,is\\,\\,an\\,\\,adj)+\\\\ \\lambda_6q_{ML}(model|w_{i-1}\\,\\,ends\\,\\,in\\,\\,&quot;ical&quot;)+\\\\ ... \\] 显然如果这样做的话，会非常的麻烦，那么如何做呢，见下面部分。 Log-Linear Models 基本定义：\\(X\\)作为输入集，\\(Y\\)作为输出集(标签集，假设是一个有限集)。d表示模型特征和参数的个数，函数f表示任意一个\\((x,y)\\)对匹配的特征向量\\(f(x,y)\\)，参数向量\\(v \\in R^d\\). 将模型\\(p(y|x)\\)加上参数后定义的条件概率模型如下： \\[ p(y|x;v)=\\frac{e^{v\\cdot f(x,y)}}{\\sum_{y' \\in Y}e^{v \\cdot f(x,y')}} \\] 其中\\(v\\cdot f(x,y)=\\sum_{k=1}^{d}v_kf_k(x,y)\\)，\\(p(y|x;v)\\)读作在参数\\(v\\)下，基于x的y的条件概率。 Features 关于上面提到的特征向量\\(f\\)的计算如下图所示： 这里提到了trigram模型的例子，如上面所示，如果词汇表的大小为\\(V\\)，那么对trigram模型来说将会有\\(V^3\\)个特征，这里用\\(N(u,v,w)\\)表示将每一个trigram对应到一个独一无二的整数。同理bigram模型，Unigram模型也是如此，而且这三个模型对应的整数不重叠。 从上面来说，特征会非常的稀疏，这里我们只取值为1的特征： \\[ Z(x,y)=\\{k:f_k(x,y)=1\\} \\] 这样就将特征的复杂度从\\(O(d)\\)降到\\(O(|Z(x,y)|)\\)，且\\(|Z(x,y)| \\ll d\\)，这样特征向量的求解就变成如下： \\[ v\\cdot f(x,y)=\\sum_{k=1}^{d}v_kf_k(x,y)=\\sum_{k \\in Z(x,y)}v_k \\] Parameter Estimation in Log-Linear Models 这里对上面的条件概率取对数就是我们的对数线性模型，假设我们有训练集\\((x^{(i)},y^{(i)}), \\,\\,i \\in \\{1...n\\},\\,\\,x^{(i)} \\in X,\\,\\,y^{(i)} \\in Y\\)，给定一个参数\\(v\\)，对于任意的例子i，其对数条件概率如下： \\[ log \\,p(y^{(i)}|x^{(i)};v) \\] 那么全部训练集中最大似然的对数条件概率和如下： \\[ L(v)=\\sum_{i=1}^{n}log\\,p(y^{(i)}|x^{(i)};v) \\] 我们要找到使\\(L(v)\\)最大的\\(v\\)，即： \\[ v_{ML}=arg\\,\\,\\max_{v \\in R^d} L(v) \\] 这里举个例子，假设在训练集中的第100个例子中的trigram有: \\[ f_{N(any,statistical,model)}(x^{(100)},y^{(100)})=1 \\] 在其它例子中都没有，我们的目的是希望\\(L(v)\\)最大，这样在\\(v_100\\)就会取无穷大，那么对应例子的对数条件概率就会最大： \\[ p(y^{(100)}|x^{(100)};v)=1 \\] 由于其它例子没有该特征，\\(v_100\\)不会对其它例子产生影响，但是显然这样是不合理的，模型只会对当前的训练集起到很好的作用，即过拟合，泛化能力变差，我们需要模型的泛化能力变强，需要对参数v进行约束，即加上惩罚项： \\[ L'(v)=\\sum_{i=1}^{n}log\\,p(y^{(i)}|x^{(i)};v)-\\frac{\\lambda}{2}\\sum_{k}v_k^2 \\] 上面这个式子就是机器学习中的损失函数，使用梯度下降就可以找到最优解。 对\\(L'(v)\\)求导： \\[ \\frac{dL'(v)}{dv_k}=\\sum_{i=1}^{n}f_k(x^{(i)},y^{(i)})-\\sum_{i=1}^{n}\\sum_{y \\in Y}p(y|x^{(i)};v)f_k(x^{(i)},y)-\\lambda v_k \\] 关于上面的证明如下： 取其中的一部分分析 \\[ log\\,p(y^{(i)}|x^{(i)};v)=v\\cdot f(x^{(i)},y^{(i)})-log\\sum_{y' \\in Y}e^{v\\cdot f(x^{(i)},y')} \\] 对上面的第一部分求导： \\[ \\frac{d}{dv_k}\\bigg(v\\cdot f(x^{(i)},y^{(i)})\\bigg)=\\frac{d}{dv_k}\\bigg(\\sum_kv_k\\cdot f_k(x^{(i)},y^{(i)})\\bigg)=f_k(x^{(i)},y^{(i)}) \\] 对上面第二部分(应该是看成以e为底，不过常数不影响)： \\[ 记 g(v)=\\sum_{y' \\in Y}e^{v\\cdot f(x^{(i)},y')}\\\\ \\frac{d}{dv_k}log\\,g(v)=\\frac{\\frac{d}{dv_k}(g(v))}{g(v)}\\\\ =\\frac{\\sum_{y' \\in Y}f_k(x^{(i)},y')e^{v\\cdot f(x^{(i)},y')}}{\\sum_{y' \\in Y}e^{v\\cdot f(x^{(i)},y')}}\\\\ =\\sum_{y' \\in Y}\\bigg(f_k(x^{(i)},y')\\frac{e^{v\\cdot f(x^{(i)},y')}}{\\sum_{y' \\in Y e^{(v\\cdot f(x^{(i)},y'))}}}\\bigg)\\\\ =\\sum_{y' \\in Y}f_k(x^{(i)},y')p(y'|x;v) \\] 这样就完了，后面两部分比较简单，都是在讲前面的模型用现在的对数线性模型替换后的结果及其对比，就不再做笔记了，此部分完结。。。","link":"/posts/2322855446.html"},{"title":"Longest Common Subsequence","text":"Question lintcode: (77) Longest Common Subsequence 123456789101112131415Given two strings, find the longest common subsequence (LCS).Your code should return the length of LCS.Have you met this question in a real interview? YesExampleFor &quot;ABCD&quot; and &quot;EDCA&quot;, the LCS is &quot;A&quot; (or &quot;D&quot;, &quot;C&quot;), return 1.For &quot;ABCD&quot; and &quot;EACB&quot;, the LCS is &quot;AC&quot;, return 2.ClarificationWhat's the definition of Longest Common Subsequence?https://en.wikipedia.org/wiki/Longest_common_subsequence_problemhttp://baike.baidu.com/view/2020307.htm 题解 求最长公共子序列的数目，注意这里的子序列可以不是连续序列，务必问清楚题意。求『最长』类的题目往往与动态规划有点关系，这里是两个字符串，故应为双序列动态规划。 这道题的状态很容易找，不妨先试试以f[i][j]表示字符串 A 的前 i 位和字符串 B 的前 j 位的最长公共子序列数目，那么接下来试试寻找其状态转移方程。从实际例子ABCD和EDCA出发，首先初始化f的长度为字符串长度加1，那么有f[0][0] = 0, f[0][*] = 0, f[*][0] = 0, 最后应该返回f[lenA][lenB]. 即 f 中索引与字符串索引对应(字符串索引从1开始算起)，那么在A 的第一个字符与 B 的第一个字符相等时，f[1][1] = 1 + f[0][0], 否则f[1][1] = max(f[0][1], f[1][0])。 推而广之，也就意味着若A[i] == B[j], 则分别去掉这两个字符后，原 LCS 数目减一，那为什么一定是1而不是0或者2呢？因为不管公共子序列是以哪个字符结尾，在A[i] == B[j]时 LCS 最多只能增加1. 而在A[i] != B[j]时，由于A[i] 或者 B[j] 不可能同时出现在最终的 LCS 中，故这个问题可进一步缩小，f[i][j] = max(f[i - 1][j], f[i][j - 1]). 需要注意的是这种状态转移方程只依赖最终的 LCS 数目，而不依赖于公共子序列到底是以第几个索引结束。 Python 12345678910111213141516171819class Solution: &quot;&quot;&quot; @param A, B: Two strings. @return: The length of longest common subsequence of A and B. &quot;&quot;&quot; def longestCommonSubsequence(self, A, B): if not A or not B: return 0 lenA, lenB = len(A), len(B) lcs = [[0 for i in xrange(1 + lenA)] for j in xrange(1 + lenB)] for i in xrange(1, 1 + lenA): for j in xrange(1, 1 + lenB): if A[i - 1] == B[j - 1]: lcs[i][j] = 1 + lcs[i - 1][j - 1] else: lcs[i][j] = max(lcs[i - 1][j], lcs[i][j - 1]) return lcs[lenA][lenB] C++ 12345678910111213141516171819202122232425262728class Solution {public: /** * @param A, B: Two strings. * @return: The length of longest common subsequence of A and B. */ int longestCommonSubsequence(string A, string B) { if (A.empty()) return 0; if (B.empty()) return 0; int lenA = A.size(); int lenB = B.size(); vector&lt;vector&lt;int&gt; &gt; lcs = \\ vector&lt;vector&lt;int&gt; &gt;(1 + lenA, vector&lt;int&gt;(1 + lenB)); for (int i = 1; i &lt; 1 + lenA; i++) { for (int j = 1; j &lt; 1 + lenB; j++) { if (A[i - 1] == B[j - 1]) { lcs[i][j] = 1 + lcs[i - 1][j - 1]; } else { lcs[i][j] = max(lcs[i - 1][j], lcs[i][j - 1]); } } } return lcs[lenA][lenB]; }}; Java 1234567891011121314151617181920212223242526 public class Solution { /** * @param A, B: Two strings. * @return: The length of longest common subsequence of A and B. */ public int longestCommonSubsequence(String A, String B) { if (A == null || A.length() == 0) return 0; if (B == null || B.length() == 0) return 0; int lenA = A.length(); int lenB = B.length(); int[][] lcs = new int[1 + lenA][1 + lenB]; for (int i = 1; i &lt; 1 + lenA; i++) { for (int j = 1; j &lt; 1 + lenB; j++) { if (A.charAt(i - 1) == B.charAt(j - 1)) { lcs[i][j] = 1 + lcs[i - 1][j - 1]; } else { lcs[i][j] = Math.max(lcs[i - 1][j], lcs[i][j - 1]); } } } return lcs[lenA][lenB]; }} 源码分析 注意 Python 中的多维数组初始化方式，不可简单使用[[0] * len(A)] * len(B)], 具体原因是因为 Python 中的对象引用方式 1。 复杂度分析 两重for 循环，时间复杂度为 \\[O(lenA \\times lenB)\\], 使用了二维数组，空间复杂度也为 \\[O(lenA \\times lenB)\\]. Reference Python multi-dimensional array initialization without a loop - Stack Overflow↩︎","link":"/posts/577246273.html"},{"title":"Merge Two Sorted Lists","text":"Question leetcode: Merge Two Sorted Lists | LeetCode OJ lintcode: (165) Merge Two Sorted Lists ### Problem Statement Merge two sorted (ascending) linked lists and return it as a new sorted list. The new sorted list should be made by splicing together the nodes of the two lists and sorted in ascending order. Example Given 1-&gt;3-&gt;8-&gt;11-&gt;15-&gt;null, 2-&gt;null , return 1-&gt;2-&gt;3-&gt;8-&gt;11-&gt;15-&gt;null. 题解 此题为两个链表的合并，合并后的表头节点不一定，故应联想到使用dummy节点。链表节点的插入主要涉及节点next指针值的改变，两个链表的合并操作则涉及到两个节点的next值变化，若每次合并一个节点都要改变两个节点next的值且要对NULL指针做异常处理，势必会异常麻烦。嗯，第一次做这个题时我就是这么想的... 下面看看相对较好的思路。 首先dummy节点还是必须要用到，除了dummy节点外还引入一个curr节点充当下一次合并时的头节点。在l1或者l2的某一个节点为空指针NULL时，退出while循环，并将非空链表的头部链接到curr-&gt;next中。 C++ 12345678910111213141516171819202122232425262728293031/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) { ListNode *dummy = new ListNode(0); ListNode *lastNode = dummy; while ((NULL != l1) &amp;&amp; (NULL != l2)) { if (l1-&gt;val &lt; l2-&gt;val) { lastNode-&gt;next = l1; l1 = l1-&gt;next; } else { lastNode-&gt;next = l2; l2 = l2-&gt;next; } lastNode = lastNode-&gt;next; } // do not forget this line! lastNode-&gt;next = (NULL != l1) ? l1 : l2; return dummy-&gt;next; }}; Java 1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for ListNode. * public class ListNode { * int val; * ListNode next; * ListNode(int val) { * this.val = val; * this.next = null; * } * } */ public class Solution { /** * @param ListNode l1 is the head of the linked list * @param ListNode l2 is the head of the linked list * @return: ListNode head of linked list */ public ListNode mergeTwoLists(ListNode l1, ListNode l2) { ListNode dummy = new ListNode(0); ListNode curr = dummy; while ((l1 != null) &amp;&amp; (l2 != null)) { if (l1.val &gt; l2.val) { curr.next = l2; l2 = l2.next; } else { curr.next = l1; l1 = l1.next; } curr = curr.next; } // link to non-null list curr.next = (l1 != null) ? l1 : l2; return dummy.next; }} 源码分析 异常处理，包含在dummy-&gt;next中。 引入dummy和curr节点，此时curr指向的节点为dummy 对非空l1,l2循环处理，将l1/l2的较小者链接到curr-&gt;next，往后递推curr 最后处理l1/l2中某一链表为空退出while循环，将非空链表头链接到curr-&gt;next 返回dummy-&gt;next，即最终的首指针 注意curr的递推并不影响dummy-&gt;next的值，因为lastNode和dummy是两个不同的指针变量。 Note 链表的合并为常用操作，务必非常熟练，以上的模板非常精炼，有两个地方需要记牢。1. 循环结束条件中为条件与操作；2. 最后处理curr-&gt;next指针的值。 复杂度分析 最好情况下，一个链表为空，时间复杂度为 \\[O(1)\\]. 最坏情况下，curr遍历两个链表中的每一个节点，时间复杂度为 \\[O(l1+l2)\\]. 空间复杂度近似为 \\[O(1)\\]. Reference Merge Two Sorted Lists | 九章算法","link":"/posts/2804450247.html"},{"title":"Merge k Sorted Lists","text":"Question leetcode: Merge k Sorted Lists | LeetCode OJ lintcode: (104) Merge k Sorted Lists ## 题解1 - 选择归并(TLE) 参考 Merge Two Sorted Lists | Data Structure and Algorithm 中对两个有序链表的合并方法，这里我们也可以采用从 k 个链表中选择其中最小值的节点链接到lastNode-&gt;next(和选择排序思路有点类似)，同时该节点所在的链表表头节点往后递推一个。直至lastNode遍历完 k 个链表的所有节点，此时表头节点均为NULL, 返回dummy-&gt;next. 这种方法非常简单直接，但是时间复杂度较高，容易出现 TLE. C++ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param lists: a list of ListNode * @return: The head of one sorted list. */ ListNode *mergeKLists(vector&lt;ListNode *&gt; &amp;lists) { if (lists.empty()) { return NULL; } ListNode *dummy = new ListNode(INT_MAX); ListNode *last = dummy; while (true) { int count = 0; int index = -1, tempVal = INT_MAX; for (int i = 0; i != lists.size(); ++i) { if (NULL == lists[i]) { ++count; if (count == lists.size()) { last-&gt;next = NULL; return dummy-&gt;next; } continue; } // choose the min value in non-NULL ListNode if (NULL != lists[i] &amp;&amp; lists[i]-&gt;val &lt;= tempVal) { tempVal = lists[i]-&gt;val; index = i; } } last-&gt;next = lists[index]; last = last-&gt;next; lists[index] = lists[index]-&gt;next; } }}; 源码分析 由于头节点不定，我们使用dummy节点。 使用last表示每次归并后的新链表末尾节点。 count用于累计链表表头节点为NULL的个数，若与 vector 大小相同则代表所有节点均已遍历完。 tempVal用于保存每次比较 vector 中各链表表头节点中的最小值，index保存本轮选择归并过程中最小值对应的链表索引，用于循环结束前递推该链表表头节点。 复杂度分析 由于每次for循环只能选择出一个最小值，总的时间复杂度最坏情况下为 \\[O(k \\cdot \\sum ^{k}_{i=1}l_i)\\]. 空间复杂度近似为 \\[O(1)\\]. 题解2 - 迭代调用Merge Two Sorted Lists(TLE) 鉴于题解1时间复杂度较高，题解2中我们可以反复利用时间复杂度相对较低的 Merge Two Sorted Lists | Data Structure and Algorithm. 即先合并链表1和2，接着将合并后的新链表再与链表3合并，如此反复直至 vector 内所有链表均已完全合并1。 C++ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param lists: a list of ListNode * @return: The head of one sorted list. */ ListNode *mergeKLists(vector&lt;ListNode *&gt; &amp;lists) { if (lists.empty()) { return NULL; } ListNode *head = lists[0]; for (int i = 1; i != lists.size(); ++i) { head = merge2Lists(head, lists[i]); } return head; }private: ListNode *merge2Lists(ListNode *left, ListNode *right) { ListNode *dummy = new ListNode(0); ListNode *last = dummy; while (NULL != left &amp;&amp; NULL != right) { if (left-&gt;val &lt; right-&gt;val) { last-&gt;next = left; left = left-&gt;next; } else { last-&gt;next = right; right = right-&gt;next; } last = last-&gt;next; } last-&gt;next = (NULL != left) ? left : right; return dummy-&gt;next; }}; 源码分析 实现合并两个链表的子方法后就没啥难度了，mergeKLists中左半部分链表初始化为lists[0], for循环后迭代归并head和lists[i]. 复杂度分析 合并两个链表时最差时间复杂度为 \\[O(l_1+l_2)\\], 那么在以上的实现中总的时间复杂度可近似认为是 \\[l_1 + l_1+l_2 +...+l_1+l_2+...+l_k = O(\\sum _{i=1} ^{k} (k-i) \\cdot l_i)\\]. 比起题解1复杂度是要小一点，但量级上仍然差不太多。实际运行时间也证明了这一点，题解2的运行时间差不多时题解1的一半。那么还有没有进一步降低时间复杂度的可能呢？当然是有的，且看下题分解... 题解3 - 二分调用Merge Two Sorted Lists 题解2中merge2Lists优化空间不大，那咱们就来看看mergeKLists中的for循环，仔细观察可得知第i个链表 \\[l_i\\] 被遍历了 \\[k-i\\] 次，如果我们使用二分法对其进行归并呢？从中间索引处进行二分归并后，每个链表参与合并的次数变为 \\[\\log k\\], 故总的时间复杂度可降至 \\[\\log k \\cdot \\sum _{i=1} ^{k} l_i\\]. 优化幅度较大。 C++ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param lists: a list of ListNode * @return: The head of one sorted list. */ ListNode *mergeKLists(vector&lt;ListNode *&gt; &amp;lists) { if (lists.empty()) { return NULL; } return helper(lists, 0, lists.size() - 1); }private: ListNode *helper(vector&lt;ListNode *&gt; &amp;lists, int start, int end) { if (start == end) { return lists[start]; } else if (start + 1 == end) { return merge2Lists(lists[start], lists[end]); } ListNode *left = helper(lists, start, start + (end - start) / 2); ListNode *right = helper(lists, start + (end - start) / 2 + 1, end); return merge2Lists(left, right); } ListNode *merge2Lists(ListNode *left, ListNode *right) { ListNode *dummy = new ListNode(0); ListNode *last = dummy; while (NULL != left &amp;&amp; NULL != right) { if (left-&gt;val &lt; right-&gt;val) { last-&gt;next = left; left = left-&gt;next; } else { last-&gt;next = right; right = right-&gt;next; } last = last-&gt;next; } last-&gt;next = (NULL != left) ? left : right; return dummy-&gt;next; }}; 源码分析 由于需要建立二分递归模型，另建一私有方法helper引入起止位置较为方便。下面着重分析helper。 分两种边界条件处理，分别是start == end和start + 1 == end. 虽然第二种边界条件可以略去，但是加上会节省递归调用的栈空间。 使用分治思想理解helper, left和right的边界处理建议先分析几个简单例子，做到不重不漏。 注意merge2Lists中传入的参数，为lists[start]而不是start... 在mergeKLists中调用helper时传入的end参数为lists.size() - 1，而不是lists.size(). 复杂度分析 题解中已分析过，最坏的时间复杂度为 \\[\\log k \\cdot \\sum _{i=1} ^{k} l_i\\], 空间复杂度近似为 \\[O(1)\\]. 优化后的运行时间显著减少！由题解2中的500+ms 减至40ms 以内。 Reference soulmachine的LeetCode 题解↩︎","link":"/posts/3310798557.html"},{"title":"Monte Carlo Methods","text":"蒙特卡洛方法简介 使用蒙特卡洛方法不需要像DP一样，对环境要有完整的知识，而是通过经验去学习。所谓经验就是对状态、动作、奖励的采样（sample sequence）。 用sample的均值去近似期望。 使用蒙特卡洛通常需要完整的episode，因此蒙特卡洛的更新方式更像是episode-by-episode，而不是像DP的step-by-step。 优点： 1.可以从实际经验中学习； 2.可以从模拟的经验中学习； 3.可以直接从感兴趣的state开始采样episode。 蒙特卡洛预测（评估） 在一个episode中状态s可能出现多次，每一次出现称为一次对状态s的访问（visit）。 first-visit MC method:只是用每个episode中第一次对状态s的访问评估状态s的价值函数。 every-visit MC method:用每个episode中每次对状态s的访问评估状态s的价值函数。 \\(V(s)\\leftarrow average(Return(s))\\) 核心代码： 123456789# Monte Carlo Sample with On-Policydef monte_carlo_on_policy(episodes): ... for i in range(0, episodes): # play接受一个策略，然后模拟生成一个完整的轨迹和奖励 _, reward, player_trajectory = play(target_policy_player) ... # 返回价值函数的平均值 return states_usable_ace / states_usable_ace_count, states_no_usable_ace / states_no_usable_ace_count 蒙特卡洛评估动作价值函数（Action Values） 注意：如果我们的问题中，没有对环境建模，那么单纯评估状态价值函数是不够的。我们必须要评估动作价值函数。 主体思想：从评估state到评估state-action对。 可能存在的问题：某些state-action对可能不会被访问（稀疏性问题）。 蒙特卡洛控制 控制（control）的目的是找到最优策略。 其中，\\(E\\)代表策略的evaluation，\\(I\\)代表策略的improvement。 Monte Carlo Exploring Starts Exploring Starts：所有的state-action对都有可能被选为episode的开始（start）。 核心代码： 1234567891011121314151617181920212223# Monte Carlo with Exploring Startsdef monte_carlo_es(episodes): ... # behavior policy is greedy def behavior_policy(usable_ace, player_sum, dealer_card): ... # get argmax of the average returns(s, a) values_ = state_action_values[player_sum, dealer_card, usable_ace, :] / \\ state_action_pair_count[player_sum, dealer_card, usable_ace, :] return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # play for several episodes for episode in range(episodes): # for each episode, use a randomly initialized state and action initial_state = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))] initial_action = np.random.choice(ACTIONS) current_policy = behavior_policy if episode else target_policy_player _, reward, trajectory = play(current_policy, initial_state, initial_action) ... return state_action_values / state_action_pair_count 不使用Exploring Starts 如何才能不使用Exploring Starts？保证所有被选择的动作被持续地选择。 使用on-policy和off-policy。 on-policy vs off-policy on-policy只有一套policy，更简单，是首选。 off-policy使用两套policy，更复杂、更难收敛；但也更通用、更强大。 on-policy和off-policy本质依然是Exploit vs Explore的权衡。 on-policy 去评估和提高生成episode时采用的policy。全过程只有一种策略，MC ES属于on-policy。 off-policy 所有的MC控制方法都面临一个困境：它们都想找到一个最优的策略，但却必须采用非最优的策略去尽可能多地探索（explore）数据。 直接使用两套策略：采样用的policy称为behavior policy，即行为策略；最终的目标policy：target policy，即目标策略。这就是off-policy。 假设目标策略是\\(\\pi\\)，行为策略是\\(b\\)，那么对于所有的\\(\\pi(a|s)&gt;0\\)必然有\\(b(a|s)&gt;0\\)，这称为“覆盖”（coverage）。一个常见的例子是：行为策略使用价值函数的greedy policy，而目标策略使用ε-greedy policy。 重要性采样（importance sampling） 几乎所有的off-policy都使用重要性采样（importance sampling）。 为什么要使用重要性采样？我们希望在使用目标策略\\(\\pi\\)的情况下用均值估计价值的期望，但我们获得的是在使用行为策略\\(b\\)的情况下的均值，也就是：\\(\\mathbb{E}[G_t \\mid S_t =s] = v_b(s)\\)。这二者是有差距的。因此我们希望使用重要性采样去纠正。 给定初始状态\\(S_t\\)，后续的状态-动作轨迹在使用目标策略\\(\\pi\\)的情况下的概率为： \\(Pr\\{At,S_{t+1}, A_{t+1}, ... S_T \\mid S_t, A_{t:T −1} \\sim \\pi\\}\\) \\(=\\prod_{k=t}^{T-1}\\pi(A_k\\mid S_k)p(S_{k+1}\\mid S_k, A_k)\\) 引入重要性采样比例（the importance sampling ratio）： \\(\\rho_{t:T −1}=\\frac{\\prod_{k=t}^{T-1}\\pi(A_k\\mid S_k)p(S_{k+1}\\mid S_k, A_k)}{\\prod_{k=t}^{T-1}b(A_k\\mid S_k)p(S_{k+1}\\mid S_k, A_k)}\\) \\(=\\prod_{k=t}^{T-1}\\frac{\\pi(A_k\\mid S_k)}{b(A_k\\mid S_k)}\\) 上面这个式子正好巧妙地把MDP中未知的状态转移概率约掉。 于是return的期望又可以得到校正：\\(\\mathbb{E}[\\rho_{t:T−1}G_t \\mid S_t =s] = v_{\\pi}(s)\\) odinary importance sampling： \\[V(s) = \\frac{\\sum_{t\\in J(s)} \\rho_{t:T (t)-1}Gt}{\\mid J(s)\\mid} \\] weighted importance sampling： \\[V(s) = \\frac{\\sum_{t\\in J(s)} \\rho_{t:T (t)-1}Gt}{\\sum_{t\\in J(s)} \\rho_{t:T (t)-1}} \\] odinary importance sampling vs. weighted importance sampling: odinary importance sampling：无偏差，但方差没有保证。 weighted importance sampling：有偏差，方差有上限。 评估： 上面的评估使用了采样权重增量式的方法。 控制： 核心代码： 123456789101112131415161718192021222324252627282930313233343536# Monte Carlo Sample with Off-Policydef monte_carlo_off_policy(episodes): initial_state = [True, 13, 2] rhos = [] returns = [] for i in range(0, episodes): _, reward, player_trajectory = play(behavior_policy_player, initial_state=initial_state) # get the importance ratio numerator = 1.0 denominator = 1.0 for (usable_ace, player_sum, dealer_card), action in player_trajectory: if action == target_policy_player(usable_ace, player_sum, dealer_card): denominator *= 0.5 else: numerator = 0.0 break rho = numerator / denominator rhos.append(rho) returns.append(reward) rhos = np.asarray(rhos) returns = np.asarray(returns) weighted_returns = rhos * returns weighted_returns = np.add.accumulate(weighted_returns) rhos = np.add.accumulate(rhos) ordinary_sampling = weighted_returns / np.arange(1, episodes + 1) with np.errstate(divide='ignore',invalid='ignore'): weighted_sampling = np.where(rhos != 0, weighted_returns / rhos, 0) return ordinary_sampling, weighted_sampling","link":"/posts/2709.html"},{"title":"Multi-armed Bandits","text":"k-armed Bandit Problem，K臂老虎机 问题描述：重复面临有k种选择的情况，每一次选择之后，都会收到一个reward。这个reward服从一个跟你的选择相关的分布。你的目标是，在选择t次后，找到最大的期望reward。 为什么叫K臂老虎机：有K个单臂老虎机，每个时间节点，你可以选择K个中任意一个老虎机选择按下它的臂，然后获得一个奖励。目标自然是多次选择之后的累计奖励最大。 形式化定义：假设时刻\\(t\\)的动作是\\(A_t\\)，reward是\\(R_t\\)，定义价值函数：\\(q_*(a) = E[R_t|A_t=a]\\)。 特点：每个时刻下，\\(q_*(a)\\)是固定的，stationary。 如果我们知道每个动作a对应的价值q，那么这个问题对我们来说就已经解决了，即，我们选择q最大的那个a。 然而，我们并不知道实际上的q是什么，只能估计\\(Q_t(a)\\approx q_*(a)\\)。 在每个时间节点上，如果总是选择对应的估计Q最大的a，这被称为greedy。 \\(\\epsilon\\)-greedy：：\\(1-\\epsilon\\)的概率选择Q最大的a，\\(\\epsilon\\)的概率选择其他a。 估计\\(Q_t(a)\\)在时刻t之前，所有采用动作a获得的奖励之和在时刻t之前，采用动作a的次数= \\(\\frac{\\sum^{t-1}_{i=1}R_i\\cdot\\mathbf{1}_{A_i=a}}{\\sum^{t-1}_{i=1}\\mathbf{1}_{A_i=a}}\\)，被称为sample-average。 10-armed Testbed 设置一个10臂老虎机作模拟。 真实的价值函数\\(q_*(a)\\)服从标准高斯分布（均值是0，方差是1）。 t时刻的奖励\\(R_t\\)服从高斯分布，均值是\\(q_*(A_t)\\)，方差是1。 可以规定每次实验做1000次选择，称为一个run。 一共跑了2000个独立的run。 实验结果（\\(\\epsilon\\)-greedy的优越性）： 增量式的估计 上面的sample-average即：\\(Q_n = \\frac{R_1+R_2+...+R_{n-1}}{n-1}\\) 改写成增量式的形式：\\(Q_{n+1} = Q_n + \\frac{1}{n}[R_n-Q_n]\\) 即：新估计 = 老估计 + 步长 × [奖励 - 老估计] Optimistic Initial Values 设置初始的\\(Q_1(a)\\)为一些较大的值。 这样会鼓励explore。 对于nonstationary的问题不适用。 Upper-Confidence-Bound Action Selection（UCB） \\(A_t = \\underset{a}{argmax}[Q_t(a)+c\\sqrt{\\frac{log\\;t}{N_t(a)}}]\\) 其中，\\(N_t(a)\\)代表动作a在t之前出现的次数，根号项衡量动作a的不确定性，如果某个动作已经使用了很多次，则倾向使用使用次数少的，这样达到explore的效果。 Gradient Bandit Algorithms 使用一个数值表示对某个动作的偏好：\\(H_t(a)\\) \\(Pr\\{A_t=a\\}=\\frac{e^{H_t(a)}}{\\sum_{b=1}^k{e^{H_t(b)}}}=\\pi_t(a)\\) 更新规则： \\(H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t-\\overline R_t)(1-\\pi _t(A_t))\\) 各种方法对比 代码分析 完整源码 核心代码： 123456789101112131415161718192021222324252627282930313233343536373839404142# get an action for this banditdef act(self): if np.random.rand() &lt; self.epsilon: return np.random.choice(self.indices) if self.UCB_param is not None: UCB_estimation = self.q_estimation + \\ self.UCB_param * np.sqrt(np.log(self.time + 1) / (self.action_count + 1e-5)) q_best = np.max(UCB_estimation) return np.random.choice([action for action, q in enumerate(UCB_estimation) if q == q_best]) if self.gradient: exp_est = np.exp(self.q_estimation) self.action_prob = exp_est / np.sum(exp_est) return np.random.choice(self.indices, p=self.action_prob) q_best = np.max(self.q_estimation) return np.random.choice([action for action, q in enumerate(self.q_estimation) if q == q_best])# take an action, update estimation for this actiondef step(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.q_true[action] self.time += 1 self.average_reward = (self.time - 1.0) / self.time * self.average_reward + reward / self.time self.action_count[action] += 1 if self.sample_averages: # update estimation using sample averages self.q_estimation[action] += 1.0 / self.action_count[action] * (reward - self.q_estimation[action]) elif self.gradient: one_hot = np.zeros(self.k) one_hot[action] = 1 if self.gradient_baseline: baseline = self.average_reward else: baseline = 0 self.q_estimation = self.q_estimation + self.step_size * (reward - baseline) * (one_hot - self.action_prob) else: # update estimation with constant step size self.q_estimation[action] += self.step_size * (reward - self.q_estimation[action]) return reward","link":"/posts/30752.html"},{"title":"Numpy笔记","text":"numpy的属性 shape返回一个元组分别记录着行数与列数 size返回矩阵元素的个数 ndim返回行数 A.T返回矩阵的反向矩阵 123456789import numpy as nparray = np.array([[1, 2, 3], [2, 3, 4]])print arraypront 'number of dim:', array.ndimprint 'shape:', shapeprint 'size:', size'''反向矩阵'''print A.T #T应该为A的一个属性，不影响A的值 建创数组或矩阵 np.array()括号里输入矩阵，后有参数dtype选择类型可填int64等 np.linspace得到n(n为最后一个参数)个(起点终点也算在内)从起点到终点的等距离点 np.zeros与np.ones的参数是一个元祖，也可填参数dtpye A.reshape(n,m)可将数组或矩阵变形，参数为新形状 无论什么方法生成的在交互环境下去用print 得到的结果没有‘,’而直接输入名字结果有‘,’，list就不一样，用不用print都有‘,’ 1234567891011121314151617import numpy as np'''建创'''a = np.array([[2, 3, 4], [3, 4, 5]])a = np.arange(12) #类似于rangea = np.arange(1, 13, 1) #类似于rangea = np.linspace #建创线段a = np.zeros((3, 4)) #生成3行4列的的矩阵a = np.ones(3, 4)'''变形'''a = a.reshape(4, 3)a = np.arange(12).reshape(3, 4)'''定义类型'''a = np.zeros((3, 4), dtype = np.int64) python的基本运算 矩阵与数字之间的加减乘除为矩阵的每个元素与次数相运算 矩阵之间的+、-、*、/分别为对应位置的元素相互运算 矩阵之间的乘法表示为np.dot(a,b)，或a.doot(b) **表示的是乘方A ** n表示A的n次方不表示矩阵的点乘 numpy中也有sin等各种函数 矩阵之间的比较，返回都是True与False的矩阵 1234567891011121314151617181920212223import numpy as np'''numpy的运算及公式运用'''a = np.arange(16)b = np.linspace(5, 10, 16)c = a + bd = a - be = a * b #将数组或矩阵中的元素挨个相称后形成的新数组f = a / b #将数组或矩阵中的元素挨个相称后形成的新数组g = a + 1 #各元素依次加1, 其他的运算符类似e_dot = np.dot(a, b) #矩阵的乘法e_dot_ = a.dot(b) #另一种表示方法h = b ** 3 #表示三次方k = b ** n #表示n次方i = np.sin(a) #numpy中的函数print a, b, c, d, e, f, g, h, k, i'''numpy数列中的判断'''a = np.arange(1, 17).reshape(2, 8)b = np.arange(-6, 26, 2).reshape(2, 8)print a &gt; bprint a &gt; 5print b == 5#起返回的都是一个为True, Fales的矩阵. numpy的方法 np.argmin(A)、np.argmax(A)返回最大值与最小值的索引 sum()返回矩阵元素的和，min()，max()返回最小最大值，可加上参数axis = 0 or 1表示对列行进行运算 A.mean()、np.mean(A)、np.average(A)进行求平均值np.median(A)求中位数 A.cumsum()、np.cumsum()将矩阵元素累加，返回维度减少了一个的数组（只有一个中括号）A.diff(A)将矩阵元素累减返回少一列的矩阵 np.sort(A)排序不改变原来矩阵返回排好的矩阵，A.sort改变原来矩阵返回None np.clip(A, 5, 9)返回一个元素不大于9不小于5的矩阵，大于9小于5的分别被赋值成了9，5 np.transpose(A)反回A的反向矩阵 1234567891011121314151617181920212223242526import numpy as np'''索引'''print np.argmin(A)print np.argmax(A)'''随机矩阵生成'''a = np.random.random((2, 4))'''numpy最大最小值等方法'''print np.sum(a)print np.min(a, axis = 1) #axis为1表示列中求和最小值, 为0表示行中求和的最小值，无axis对每个元素求最小值.print np.max(a)'''平均数'''print np.mean(A) #A.mean()print np.average(A) #无A.average'''中位数'''np.median(A) #无A.meadian()'''累加累减形成的数列'''print A.cumsum() #累加， np.cumsuum(A)，返回新数列print np.diff(A) #累减， 无A.diff()，返回少一列的矩阵'''排序'''print np.sort(A) #从小到大按每行排序不改变数列，返回排好后的数列print A.sort() #按行从小到大排，改变数列，只返回None'''截取'''print np.clip(A,5, 9) #大于9的数被改成9小于5的被改成了5，不改变数列'''反向矩阵'''print np.transpose(A) #原数列不变，返回新的矩阵 numpy的索引 A[2]对行进行索引，A[2][2]进行二次索引，即索引到了行一次后又对这一行进行了索引 A[2, 1]对行列进行索引，也可以切片，行列都可以， for循环默认迭代每一行，可用A.T使其迭代其每一列，用A.flat迭代其每个元素，A.flatten()返回矩阵中的每个元素 123456789101112131415import numpy as npA = np.arange(2, 14).reshape(3, 4)print Aprint A[2] #对行索引print A[2][2] #对行和列索引print A[2, 1] #也可这样对行和列索引print A[1, 0:2] #也可切片索引for row in A: print row #这样迭代的时每一行for column in A.T: print column #迭代列print A.flatten() #此函数返回一个将矩阵拆分的数列所以可以用类似的方法迭代矩阵中的每个元素for item in A.flat: print item: numpy矩阵合并 合并的矩阵维度都是相同的（中括号的数量是相同的） np.vstack((A, B))用于快速合并上下合并，np.hstack((A, B))用于快速左右合并 只有一个大括号的矩阵可以用A[:, np.newaxis]增加维度，这其实是个索引符号，不改变原来的矩阵 np.concatenate((), axis = o or 1)可自由合并axis = 0表示对列进行操作所以是上下合并 1234567891011121314151617import numpy as npA = np.ones((1, 3))B = np.zeros((1, 3))print np.vstack((A, B)) #vertical stack上下合并C = np.vstack((A, B))print A.shape, C.shapeD = np.hstack((A, B)) #horizontal stack左右合并print D, '\\n', D.shapeE = np.array([1, 1, 1]) #这样生成的是个数组不是数列print出的只有一个中括号，有的方法得到的是有两层嵌套的矩阵而不是数列F = np.array([2, 2, 2])'''也可写成E = np.array([1, 1, 1]）[np.newaxis, :]，可生成一个带一个中括号的array后增加维度后合并，合并的两个矩阵都是两个括号的，在外层大括号下，突破内层大括号。'''print E[np.newaxis, :] #向下增加维度print F[:, np.newaxis] #向右增加维度G = np.vstack((A, B))print GH = np.concatenate((E, F, F, A), axis=0) #axis = 0 表示上下合并 矩阵分割 用np.split(A, 2, axis = 0 or 1)等量分割参数分别为要分割的矩阵和分成的快数，分割的方向 np.array_split(...)参数都一样不过分不了的会分成不等的几份 快速分割vsplit(A, 2)参数为矩阵和快数 1234567891011121314import numpy as npA = np.arange(12).reshape((3, 4))print Aprint np.split(A, 2, axis = 1)'''split函数参数：矩阵，快数，方向'''#此函数不能实现不等量分割#print np.split(A, 3, axis = 1)会报错'''实现不等分割'''print np.array_split(A, 3, axis = 1)'''快速分割'''print np.vsplit(A, 3) #横向分割print np.hsplit(A, 4) #纵向分割 numpy的cope与deep copy 普通的赋值只是得到了原矩阵的一个引用A.copy()返回了一个新的与元矩阵不再同一地址的矩阵 12345678910import numpy as npa = np.arange(4)b = ac = ad = bprint a, b, c, da[0] = 11print a, b, c, d #b, c, d都是指向a指向的空间当a变时b, c, d也变了。和list是一样的b = a.copy() #deep copy，将a的值附过去但a，b指向的不是一块空间不会相互影响","link":"/posts/1475683312.html"},{"title":"Palindrome Linked List","text":"Question leetcode: Palindrome Linked List | LeetCode OJ lintcode: Palindrome Linked List Function to check if a singly linked list is palindrome - GeeksforGeeks ### Problem Statement Implement a function to check if a linked list is a palindrome. Example Given 1-&gt;2-&gt;1, return true Challenge Could you do it in O(n) time and O(1) space? 题解1 - 使用辅助栈 根据栈的特性(FILO)，可以首先遍历链表并入栈(最后访问栈时则反过来了)，随后再次遍历链表并比较当前节点和栈顶元素，若比较结果完全相同则为回文。 又根据回文的特性，实际上还可以只遍历链表前半部分节点，再用栈中的元素和后半部分元素进行比较，分链表节点个数为奇数或者偶数考虑即可。由于链表长度未知，因此可以考虑使用快慢指针求得。 Python 123456789101112131415161718192021222324252627282930## Definition for singly-linked list# class ListNode:# def __init__(self, val):# self.val = val# self.next = Noneclass Solution: # @param head, a ListNode # @return a boolean def is_palindrome(self, head): if not head or not head.next: return True stack = [] slow, fast = head, head.next while fast and fast.next: stack.append(slow.val) slow = slow.next fast = fast.next.next # for even numbers add mid if fast: stack.append(slow.val) curt = slow.next while curt: if curt.val != stack.pop(): return False curt = curt.next return True 源码分析 注意， 在python code中， slow 和 fast pointer 分别指向head 和head.next。 这样指向的好处是：当linked－list 有奇数个数字的时候， 最终位置，slow会停在mid的位置， 而fast指向空。 当linked－list有偶数个node时， 最终位置，slow和slow.next为中间的两个元素， fast指向最后一个node。所以slow的最终位置总是mid 或者mid 偏左一点的位置。这样的位置非常方便分割linked－list，以及其他计算。推荐采用这种方法来寻找linked－list的mid位置。模版优势，请见solution2。 Java 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { /** * @param head a ListNode * @return a boolean */ public boolean isPalindrome(ListNode head) { if (head == null || head.next == null) return true; Deque&lt;Integer&gt; stack = new ArrayDeque&lt;Integer&gt;(); // find middle ListNode slow = head, fast = head; while (fast != null &amp;&amp; fast.next != null) { stack.push(slow.val); slow = slow.next; fast = fast.next.next; } // skip mid node if the number of ListNode is odd if (fast != null) slow = slow.next; ListNode rCurr = slow; while (rCurr != null) { if (rCurr.val != stack.pop()) return false; rCurr = rCurr.next; } return true; }} 源码分析 注意区分好链表中个数为奇数还是偶数就好了，举几个简单例子辅助分析。 复杂度分析 使用了栈作为辅助空间，空间复杂度为 \\[O(\\frac{1}{2}n)\\], 分别遍历链表的前半部分和后半部分，时间复杂度为 \\[O(n)\\]. 题解2 - 原地翻转 题解 1 的解法使用了辅助空间，在可以改变原来的链表的基础上，可使用原地翻转，思路为翻转前半部分，然后迭代比较。具体可分为以下四个步骤。 找中点。 翻转链表的后半部分。 逐个比较前后部分节点值。 链表复原，翻转后半部分链表。 Python 123456789101112131415161718192021222324252627282930313233# class ListNode:# def __init__(self, val):# self.val = val# self.next = Noneclass Solution: def is_palindrome(self, head): if not head or not head.next: return True slow, fast = head, head.next while fast and fast.next: fast = fast.next.next slow = slow.next mid = slow.next # break slow.next = None rhead = self.reverse(mid) while rhead: if rhead.val != head.val: return False rhead = rhead.next head = head.next return True def reverse(self, head): dummy = ListNode(-1) while head: temp = head.next head.next = dummy.next dummy.next = head head = temp return dummy.next 源码分析 对比Java code， 会发现，把slow 和fast pointer 放在head和head.next减少了对odd 或者even number的判断。因为slow总是在mid的位置或者mid偏左的位置上， 所以把mid assign 为slow.next总是对的。 Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { /** * @param head a ListNode * @return a boolean */ public boolean isPalindrome(ListNode head) { if (head == null || head.next == null) return true; // find middle ListNode slow = head, fast = head; while (fast != null &amp;&amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } // skip mid node if the number of ListNode is odd if (fast != null) slow = slow.next; // reverse right part of List ListNode rHead = reverse(slow); ListNode lCurr = head, rCurr = rHead; while (rCurr != null) { if (rCurr.val != lCurr.val) { reverse(rHead); return false; } lCurr = lCurr.next; rCurr = rCurr.next; } // recover right part of List reverse(rHead); return true; } private ListNode reverse(ListNode head) { ListNode prev = null; while (head != null) { ListNode after = head.next; head.next = prev; prev = head; head = after; } return prev; }} C++ 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution {public: bool isPalindrome(ListNode* head) { if (!head || !head-&gt;next) return true; // find middle ListNode* slow = head, *fast = head; while (fast &amp;&amp; fast-&gt;next) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; } // skip mid node if the number of ListNode is odd if (fast) slow = slow-&gt;next; // reverse right part of List ListNode* rHead = reverse(slow); ListNode* lCurr = head, *rCurr = rHead; while (rCurr) { if (rCurr-&gt;val != lCurr-&gt;val) { reverse(rHead); return false; } lCurr = lCurr-&gt;next; rCurr = rCurr-&gt;next; } // recover right part of List reverse(rHead); return true; } ListNode* reverse(ListNode* head) { ListNode* prev = NULL; while (head) { ListNode* after = head-&gt;next; head-&gt;next = prev; prev = head; head = after; } return prev; }} 源码分析 连续翻转两次右半部分链表即可复原原链表，将一些功能模块如翻转等尽量模块化。 复杂度分析 遍历链表若干次，时间复杂度近似为 \\[O(n)\\], 使用了几个临时遍历，空间复杂度为 \\[O(1)\\]. 题解3 - 递归(TLE) 递归需要两个重要条件，递归步的建立和递归终止条件。对于回文比较，理所当然应该递归比较第 i 个节点和第 n-i 个节点，那么问题来了，如何构建这个递归步？大致可以猜想出来递归的传入参数应该包含两个节点，用以指代第 i 个节点和第 n-i 个节点。返回参数应该包含布尔值(用以提前返回不是回文的情况)和左半部分节点的下一个节点(用以和右半部分的节点进行比较)。由于需要返回两个值，在 Java 中需要使用自定义类进行封装，C/C++ 中则可以使用指针改变在递归调用后进行比较时节点的值。 Python 1234567891011class Solution: def is_palindrome(self, head): result = [head, True] self.helper(head, result) return result[1] def helper(self, right, result): if right: self.helper(right.next, result) is_pal = result[0].val == right.val and result[1] result = [result[0].next, is_pal] Java 123456789101112131415161718192021222324252627282930313233343536373839/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */class Result { ListNode lNode; boolean isP; Result(ListNode node, boolean isP) { this.lNode = node; this.isP = isP; }}public class Solution { /** * @param head a ListNode * @return a boolean */ public boolean isPalindrome(ListNode head) { Result result = new Result(head, true); helper(head, result); return result.isP; } private void helper(ListNode right, Result result) { if (right != null) { helper(right.next, result); boolean equal = (result.lNode.val == right.val); result.isP = equal &amp;&amp; result.isP; result.lNode = result.lNode.next; } }} 源码分析 核心代码为如何在递归中推进左半部分节点而对右半部分使用栈的方式逆向获取节点。左半部分的推进需要借助辅助数据结构Result. 复杂度分析 递归调用 n 层，时间复杂度近似为 \\[O(n)\\], 使用了几个临时变量，空间复杂度为 \\[O(1)\\]. Bonus - Fancy Python Solution 1234567class Solution: def is_palindrome(self, head): nodes = [] while head: nodes.append(head.val) head = head.next return nodes == nodes[::-1] 源码分析 将linked－list问题，转化成判断一个array是否为palindrome的问题。 复杂度分析 时间复杂度 \\[O(n)\\], 空间复杂度也是 \\[O(n)\\] Reference Function to check if a singly linked list is palindrome - GeeksforGeeks 回文判断 | The-Art-Of-Programming-By-July/01.04.md ctci/QuestionB.java at master · gaylemcd/ctci","link":"/posts/1480312164.html"},{"title":"Password Attacker","text":"Source Dashboard - Round B APAC Test - Problem A. Password Attacker Problem Passwords are widely used in our lives: for ATMs, online forum logins, mobile device unlock and door access. Everyone cares about password security. However, attackers always find ways to steal our passwords. Here is one possible situation: Assume that Eve, the attacker, wants to steal a password from the victim Alice. Eve cleans up the keyboard beforehand. After Alice types the password and leaves, Eve collects the fingerprints on the keyboard. Now she knows which keys are used in the password. However, Eve won't know how many times each key has been pressed or the order of the keystroke sequence. To simplify the problem, let's assume that Eve finds Alice's fingerprints only occurs on M keys. And she knows, by another method, that Alice's password contains N characters. Furthermore, every keystroke on the keyboard only generates a single, unique character. Also, Alice won't press other irrelevant keys like 'left', 'home', 'backspace' and etc. Here's an example. Assume that Eve finds Alice's fingerprints on M=3 key '3', '7' and '5', and she knows that Alice's password is N=4-digit in length. So all the following passwords are possible: 3577, 3557, 7353 and 5735. (And, in fact, there are 32 more possible passwords.) However, these passwords are not possible: 12341357 // There is no fingerprint on key '1'3355 // There is fingerprint on key '7', so '7' must occur at least once.357 // Eve knows the password must be a 4-digit number. With the information, please count that how many possible passwords satisfy the statements above. Since the result could be large, please output the answer modulo 1000000007(109+7). Input The first line of the input gives the number of test cases, T. For the next T lines, each contains two space-separated numbers M and N, indicating a test case. Output For each test case, output one line containing \"Case #x: y\", where x is the test case number (starting from 1) and y is the total number of possible passwords modulo 1000000007(109+7). Limits Small dataset T = 15. 1 ≤ M ≤ N ≤ 7. Large dataset T = 100. 1 ≤ M ≤ N ≤ 100. Smaple 1234567Input Output41 1 Case #1: 13 4 Case #2: 365 5 Case #3: 12015 15 Case #4: 674358851 题解 题目看似很长，其实简单来讲就是用 M 个 不同的字符组成长度为 N 的字符串，问有多少种不同的排列。这里 M 小于 N，要是大于的话就是纯排列了。这道题我最开始想用纯数学方法推导公式一步到位，实践下来发现这种想法真是太天真了，这不是数学竞赛... 即使用推导也应该是推导类似动态规划的状态转移方程。 这里的动态规划不太明显，我们以状态dp[m][n]表示用 m 个不同的字符能组成长度为 n 的不同字符串的个数。这里需要注意的是最后长度为 n 的字符串中必须包含 m 个不同的字符，不多也不少。接下来就是寻找状态转移方程了，之前可能的状态为dp[m - 1][n -1], dp[m - 1][n], dp[m][n - 1]. 现在问题来了，怎么解释这些状态以寻找状态转移方程？常规方法为正向分析，即分析m ==&gt; n, 但很快我们可以发现dp[m - 1][n]这个状态很难处理。既然正向分析比较麻烦，我们不妨试试反向从n ==&gt; m分析，可以发现字符串个数由 n 变为 n-1，这减少的字符可以分为两种情况，一种是这个减少的字符就在前 n - 1个字符中，另一种则不在，如此一来便做到了不重不漏。相应的状态转移方程为： 1dp[i][j] = dp[m][n-1] * m + dp[m - 1][n - 1] * m 第一种和第二种情况下字符串的第 n 位均可由 m 个字符中的一个填充。初始化分两种情况，第一种为索引为0时，其值显然为0；第二种则是 m 为1时，容易知道相应的排列为1。最后返回 dp[M][N]. Java 12345678910111213141516171819202122232425262728293031import java.util.*;public class Solution { public static void main(String[] args) { Scanner in = new Scanner(System.in); int T = in.nextInt(); // System.out.println(&quot;T = &quot; + T); for (int t = 1; t &lt;= T; t++) { int M = in.nextInt(), N = in.nextInt(); long ans = solve(M, N); // System.out.printf(&quot;M = %d, N = %d\\n&quot;, M, N); System.out.printf(&quot;Case #%d: %d\\n&quot;, t, ans); } } public static long solve(int M, int N) { long[][] dp = new long[1 + M][1 + N]; long mod = 1000000007; for (int j = 1; j &lt;= N; j++) { dp[1][j] = 1; } for (int i = 2; i &lt;= M; i++) { for (int j = i; j &lt;= N; j++) { dp[i][j] = i * (dp[i][j - 1] + dp[i - 1][j - 1]); dp[i][j] %= mod; } } return dp[M][N]; }} 源码分析 Google Code Jam 上都是自己下载输入文件，上传结果，这里我们使用输入输出重定向的方法解决这个问题。举个例子，将这段代码保存为Solution.java, 将标准输入重定向至输入文件，标准输出重定向至输出文件。编译好之后以如下方式运行： 1java Solution &lt; A-large-practice.in &gt; A-large-practice.out 这种方式处理各种不同 OJ 平台的输入输出较为方便。 复杂度分析 时间复杂度 \\[O(mn)\\], 空间复杂度 \\[O(mn)\\]. Reference Google-APAC2015-\"Password Attacker\" - dmsehuang的专栏","link":"/posts/4134381544.html"},{"title":"Probabilistic Context-Free Grammars","text":"Context-Free Grammars Noam Chomsky 曾经把语言定义为按照一定规律构成的句子和符号串的有限或无限集合，形式语言是用来精确描述语言及其结构的手段，形式语言学也称为代数语言学。 形式语法是一个四元组\\(G=(N,\\sum,R,S)\\)，其中： N是一个非终结符号(non-terminal symbols)的有限集合 \\(\\sum\\)是一个终结符号(terminal symbols)的有限集合 R是一个规则集合，其形式如\\(X \\rightarrow Y_1Y_2...Y_n，where\\,\\,X \\in N\\,\\,,Y_i \\in (N\\cup \\sum )\\) \\(S \\in N\\)作为一个句子的开始符或初始符 这里举一个最左边推导(left-most derivation)来理解上面的形式语法： 对于一个最左边推导的一串字符串\\(s_1...s_n\\)，有如下： \\(s_1=S\\)是开始符 \\(s_n\\in \\sum^*\\),\\(s_n\\)是有终结符号组成的字符串(\\(\\sum^*\\)表示的是由\\(\\sum\\)中的单词组成的所有可能单词序列集合)【其实这里有个疑惑，最后的\\(s_n\\)难道不就是句子本身吗，那么就只可能有一个啊，给个求和符是什么意思？？？】 \\(s_i(i=2...n)\\)被从\\(s_{i−1}\\)中派生出来，通过将\\(s_{i−1}\\)中最左端的非终结符号\\(X\\)通过规则R中\\(X \\rightarrow \\beta\\)替换 如下例子： 上面关于四元组的定义不懂的话，结合这个图应该能够看懂。非终止符就是S，NP，VP等这样的符号，比如NP代表名词性短语，等等；S=S只是一个句子的开始符标记；终止符就是具体的单词，比如sleeps，等等；而规则R表示的就是递进组合关系，比如VP可以由Vi组成，也可以由Vt和NP组成，等等。下面就是上面讲的序列\\(s_1...s_n\\)，以句子the man sleeps为例: 其中\\(s_1\\)为开始符，即\\(s_1=S\\) 而\\(s_2\\)就是在\\(s_1\\)中找最左边的第一个非终止符，是S，在规则R中找到S可能对应的\\(\\beta\\)，即\\(S \\rightarrow NP\\,\\,VP\\)，这样\\(s_2=NP\\,\\,VP\\) \\(s_3=DT \\,\\,NN \\,\\,NP\\)，由前面的\\(s_2\\)中的NP在规则R中找，最为符合的就是DT NN \\(s_4=the\\,\\, NN \\,\\,NP\\)，由前面的\\(s_3\\)中的DT在规则中找到对应的\\(\\beta\\)为the \\(s_5= the\\,\\,man\\,\\,VP\\),... \\(s_6= the\\,\\,man\\,\\,Vi\\),... \\(s_7= the\\,\\,man\\,\\,sleeps.\\),... Ambiguity 这里和前面章节一样，讲到了句子通过形式语言来表示的话，会有歧义，比如句子the man saw the dog with the telescope用解析树(前面分析的串s可以被表示为parse tree)表示如下两种parse tree： Probabilistic Context-Free Grammars(PCFGs) 下面就来计算一个句子用形式语法表示成的不同种parse tree的概率，选择概率最大的作为该句子的形式语法的表示，这里需要做一些说明。 记\\(T_G\\)为在grammar G下的所有可能的parse tree，\\(T_G(s)\\)表示对于句子s的所有可能的parse tree集合，其中\\(s \\in \\sum^*，\\{t:t \\in T_G,yield(t)=s\\}\\)。这里用\\(p(t)\\)表示对于任意的\\(t \\in T_G\\)，生成该树的概率，这样有下面两个公式： \\[ p(t)\\geq0\\\\ \\sum_{t \\in T_G}p(t)=1 \\] 这样就将我们的问题转为如下式子： \\[ arg\\,\\, \\max_{t\\in T_G(s)}p(t) \\] 此处讲义给出了三个问题： 如何定义函数\\(p(t)\\)？ 如何学习\\(p(t)\\)模型的参数？ 对于给定的句子s，如何求解最大似然树？ 下面就围绕上面的三个问题来回答，这里就是我们要讲到的PCFGs，给出如下定义： A context-free \\(grammar G=(N,\\sum,S,R)\\) 参数\\(q(\\alpha \\rightarrow \\beta)\\),对于规则\\(\\alpha \\rightarrow \\beta \\in R\\),有如下约束： \\[ \\sum_{\\alpha \\rightarrow \\beta \\in R: \\alpha=X}q(\\alpha \\rightarrow \\beta)=1 \\] 那么p(t)的定义也就迎刃而解了： \\[ p(t)=\\prod_{i=1}^{n}q(\\alpha_i \\rightarrow \\beta_i ) \\] 在规则R中加入出现的概率，如下图 以句子the dog sleeps为例，下面解析树出现的概率如下： 其中\\(q(\\alpha \\rightarrow \\beta)\\)用最大似然估计： \\[ q_{ML}(\\alpha \\rightarrow \\beta)=\\frac{Count(\\alpha \\rightarrow \\beta)}{Count(\\alpha)} \\] 参数\\(Count(\\alpha \\rightarrow \\beta)\\)表示在所有的解析树\\(t_1...t_m\\)中，规则\\(\\alpha \\rightarrow \\beta\\)出现的次数，参数\\(Count(\\alpha)\\)表示在所有的解析树\\(t_1...t_m\\)中，\\(\\alpha\\)出现的次数。 最后一个问题是如何求解\\(arg\\,\\, \\max_{t\\in T_G(s)}p(t)\\)? 用下面的算法 CKY algorithm CKY算法是一个动态规划的算法，如下： 对于一个句子\\(x_1...x_n\\)，对任意的\\(X \\in N,1&lt;=i&lt;=j&lt;=n\\)，定义\\(T(i,j,X)\\)表示以X为根，句子\\(x_i...x_j\\)的所有解析树的集合。 定义 \\[ \\pi(i,j,X)=\\max_{t \\in T(i,j,X)}p(t) \\] 我们最终要求解的式子就可以表示为\\(\\pi(1,n,S)=arg \\,\\, \\max_{t \\in T_G(s)} p(t)\\)那么动态规划的初始条件如下： \\[ \\pi(i,i,X)=\\left\\{\\begin{matrix} q(X \\rightarrow x_i)&amp; if\\,\\,X \\rightarrow x_i \\in R\\\\ 0&amp;otherwise \\end{matrix}\\right. \\] 通项公式： \\[ \\pi(i,j,X)=\\max_{X\\rightarrow Y\\,Z \\in R \\,\\,s \\in \\{i...(j-1)\\}}\\big(q(X\\rightarrow Y\\,Z)\\pi(i,s,Y)\\pi(s+1,j,Z)\\big) \\] 最后用bp来记录规则\\(X \\rightarrow Y\\,Z\\)和分隔符s，算法如下： Inside Algorithm 这个算法我有点懵，不知道为什么要将这些可能的parse tree求和，而且求和的结果难道不是1吗？？？先放到这吧，后期明白了再补上。。。","link":"/posts/589295595.html"},{"title":"Python惯例","text":"“惯例”这个词指的是“习惯的做法，常规的办法，一贯的做法”，与这个词对应的英文单词叫“idiom”。由于Python跟其他很多编程语言在语法和使用上还是有比较显著的差别，因此作为一个Python开发者如果不能掌握这些惯例，就无法写出“Pythonic”的代码。下面我们总结了一些在Python开发中的惯用的代码。 1. 让代码既可以被导入又可以被执行。 12if __name__ == '__main__': 用下面的方式判断逻辑“真”或“假”。 123if x:if not x: 好的代码： 123456name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = {'1001': '骆昊', '1002': '王大锤'}if name and fruits and owners: print('I love fruits!') 不好的代码： 123456name = 'jackfrued'fruits = ['apple', 'orange', 'grape']owners = {'1001': '骆昊', '1002': '王大锤'}if name != '' and len(fruits) &gt; 0 and owners != {}: print('I love fruits!') 善于使用in运算符。 123if x in items: # 包含for x in items: # 迭代 好的代码： 1234name = 'Hao LUO'if 'L' in name: print('The name has an L in it.') 不好的代码： 1234name = 'Hao LUO'if name.find('L') != -1: print('This name has an L in it!') 不使用临时变量交换两个值。 12a, b = b, a 用序列构建字符串。 好的代码： 1234chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''.join(chars)print(name) # jackfrued 不好的代码： 123456chars = ['j', 'a', 'c', 'k', 'f', 'r', 'u', 'e', 'd']name = ''for char in chars: name += charprint(name) # jackfrued EAFP优于LBYL。 EAFP - Easier to Ask Forgiveness than Permission. LBYL - Look Before You Leap. 好的代码： 1234567d = {'x': '5'}try: value = int(d['x']) print(value)except (KeyError, TypeError, ValueError): value = None 不好的代码： 12345678d = {'x': '5'}if 'x' in d and isinstance(d['x'], str) \\ and d['x'].isdigit(): value = int(d['x']) print(value)else: value = None 使用enumerate进行迭代。 好的代码： 1234fruits = ['orange', 'grape', 'pitaya', 'blueberry']for index, fruit in enumerate(fruits): print(index, ':', fruit) 不好的代码： 123456fruits = ['orange', 'grape', 'pitaya', 'blueberry']index = 0for fruit in fruits: print(index, ':', fruit) index += 1 用生成式生成列表。 好的代码： 1234data = [7, 20, 3, 15, 11]result = [num * 3 for num in data if num &gt; 10]print(result) # [60, 45, 33] 不好的代码： 1234567data = [7, 20, 3, 15, 11]result = []for i in data: if i &gt; 10: result.append(i * 3)print(result) # [60, 45, 33] 用zip组合键和值来创建字典。 好的代码： 12345keys = ['1001', '1002', '1003']values = ['骆昊', '王大锤', '白元芳']d = dict(zip(keys, values))print(d) 不好的代码： 1234567keys = ['1001', '1002', '1003']values = ['骆昊', '王大锤', '白元芳']d = {}for i, key in enumerate(keys): d[key] = values[i]print(d) 说明：这篇文章的内容来自于网络，有兴趣的读者可以阅读原文。","link":"/posts/2232347017.html"},{"title":"Remove Duplicates from Sorted List II","text":"Question leetcode: Remove Duplicates from Sorted List II | LeetCode OJ lintcode: (113) Remove Duplicates from Sorted List II ### Problem Statement Given a sorted linked list, delete all nodes that have duplicate numbers, leaving only distinct numbers from the original list. Example Given 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5, return 1-&gt;2-&gt;5. Given 1-&gt;1-&gt;1-&gt;2-&gt;3, return 2-&gt;3. 题解 上题为保留重复值节点的一个，这题删除全部重复节点，看似区别不大，但是考虑到链表头不确定(可能被删除，也可能保留)，因此若用传统方式需要较多的if条件语句。这里介绍一个处理链表头节点不确定的方法——引入dummy node. 123ListNode *dummy = new ListNode(0);dummy-&gt;next = head;ListNode *node = dummy; 引入新的指针变量dummy，并将其next变量赋值为head，考虑到原来的链表头节点可能被删除，故应该从dummy处开始处理，这里复用了head变量。考虑链表A-&gt;B-&gt;C，删除B时，需要处理和考虑的是A和C，将A的next指向C。如果从空间使用效率考虑，可以使用head代替以上的node，含义一样，node比较好理解点。 与上题不同的是，由于此题引入了新的节点dummy，不可再使用node-&gt;val == node-&gt;next-&gt;val，原因有二： 此题需要将值相等的节点全部删掉，而删除链表的操作与节点前后两个节点都有关系，故需要涉及三个链表节点。且删除单向链表节点时不能删除当前节点，只能改变当前节点的next指向的节点。 在判断val是否相等时需先确定node-&gt;next和node-&gt;next-&gt;next均不为空，否则不可对其进行取值。 说多了都是泪，先看看我的错误实现： C++ - Wrong 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution{public: /** * @param head: The first node of linked list. * @return: head node */ ListNode * deleteDuplicates(ListNode *head) { if (head == NULL || head-&gt;next == NULL) { return NULL; } ListNode *dummy; dummy-&gt;next = head; ListNode *node = dummy; while (node-&gt;next != NULL &amp;&amp; node-&gt;next-&gt;next != NULL) { if (node-&gt;next-&gt;val == node-&gt;next-&gt;next-&gt;val) { int val = node-&gt;next-&gt;val; while (node-&gt;next != NULL &amp;&amp; val == node-&gt;next-&gt;val) { ListNode *temp = node-&gt;next; node-&gt;next = node-&gt;next-&gt;next; delete temp; } } else { node-&gt;next = node-&gt;next-&gt;next; } } return dummy-&gt;next; }}; 错因分析 错在什么地方？ 节点dummy的初始化有问题，对类的初始化应该使用new 在else语句中node-&gt;next = node-&gt;next-&gt;next;改写了dummy-next中的内容，返回的dummy-next不再是队首元素，而是队尾元素。原因很微妙，应该使用node = node-&gt;next;，node代表节点指针变量，而node-&gt;next代表当前节点所指向的下一节点地址。具体分析可自行在纸上画图分析，可对指针和链表的理解又加深不少。 remove_duplicates_from_sorted_listd内存分析 图中上半部分为ListNode的内存示意图，每个框底下为其内存地址。dummy指针变量本身的地址为ox7fff5d0d2500，其保存着指针变量值为0x7fbe7bc04c50. head指针变量本身的地址为ox7fff5d0d2508，其保存着指针变量值为0x7fbe7bc04c00. 好了，接下来看看正确实现及解析。 Python 12345678910111213141516171819202122232425# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # @param {ListNode} head # @return {ListNode} def deleteDuplicates(self, head): if head is None: return None dummy = ListNode(0) dummy.next = head node = dummy while node.next is not None and node.next.next is not None: if node.next.val == node.next.next.val: val_prev = node.next.val while node.next is not None and node.next.val == val_prev: node.next = node.next.next else: node = node.next return dummy.next C++ 123456789101112131415161718192021222324252627282930313233/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: ListNode* deleteDuplicates(ListNode* head) { if (head == NULL) return NULL; ListNode dummy(0); dummy.next = head; ListNode *node = &amp;dummy; while (node-&gt;next != NULL &amp;&amp; node-&gt;next-&gt;next != NULL) { if (node-&gt;next-&gt;val == node-&gt;next-&gt;next-&gt;val) { int val_prev = node-&gt;next-&gt;val; // remove ListNode node-&gt;next while (node-&gt;next != NULL &amp;&amp; val_prev == node-&gt;next-&gt;val) { ListNode *temp = node-&gt;next; node-&gt;next = node-&gt;next-&gt;next; delete temp; } } else { node = node-&gt;next; } } return dummy.next; }}; Java 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { public ListNode deleteDuplicates(ListNode head) { if (head == null) return null; ListNode dummy = new ListNode(0); dummy.next = head; ListNode node = dummy; while(node.next != null &amp;&amp; node.next.next != null) { if (node.next.val == node.next.next.val) { int val_prev = node.next.val; while (node.next != null &amp;&amp; node.next.val == val_prev) { node.next = node.next.next; } } else { node = node.next; } } return dummy.next; }} 源码分析 首先考虑异常情况，head 为 NULL 时返回 NULL new一个dummy变量，dummy-&gt;next指向原链表头。(C++中最好不要使用 new 的方式生成 dummy, 否则会有内存泄露) 使用新变量node并设置其为dummy头节点，遍历用。 当前节点和下一节点val相同时先保存当前值，便于while循环终止条件判断和删除节点。注意这一段代码也比较精炼。 最后返回dummy-&gt;next，即题目所要求的头节点。 Python 中也可不使用is not None判断，但是效率会低一点。 复杂度分析 两根指针(node.next 和 node.next.next)遍历，时间复杂度为 \\[O(2n)\\]. 使用了一个 dummy 和中间缓存变量，空间复杂度近似为 \\[O(1)\\]. Reference Remove Duplicates from Sorted List II | 九章","link":"/posts/1338754042.html"},{"title":"Remove Duplicates from Sorted List","text":"Question leetcode: Remove Duplicates from Sorted List lintcode: Remove Duplicates from Sorted List ### Problem Statement Given a sorted linked list, delete all duplicates such that each element appear only once. For example, Given 1-&gt;1-&gt;2, return 1-&gt;2. Given 1-&gt;1-&gt;2-&gt;3-&gt;3, return 1-&gt;2-&gt;3. 题解 遍历之，遇到当前节点和下一节点的值相同时，删除下一节点，并将当前节点next值指向下一个节点的next, 当前节点首先保持不变，直到相邻节点的值不等时才移动到下一节点。 Python 12345678910111213141516171819&quot;&quot;&quot;Definition of ListNodeclass ListNode(object): def __init__(self, val, next=None): self.val = val self.next = next&quot;&quot;&quot;class Solution: &quot;&quot;&quot; @param head: A ListNode @return: A ListNode &quot;&quot;&quot; def deleteDuplicates(self, head): curt = head while curt: while curt.next and curt.next.val == curt.val: curt.next = curt.next.next curt = curt.next return head C++ 123456789101112131415161718192021222324252627282930313233/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: head node */ ListNode *deleteDuplicates(ListNode *head) { ListNode *curr = head; while (curr != NULL) { while (curr-&gt;next != NULL &amp;&amp; curr-&gt;val == curr-&gt;next-&gt;val) { ListNode *temp = curr-&gt;next; curr-&gt;next = curr-&gt;next-&gt;next; delete(temp); temp = NULL; } curr = curr-&gt;next; } return head; }}; Java 12345678910111213141516171819202122232425262728/** * Definition for ListNode * public class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */public class Solution { /** * @param ListNode head is the head of the linked list * @return: ListNode head of linked list */ public static ListNode deleteDuplicates(ListNode head) { ListNode curr = head; while (curr != null) { while (curr.next != null &amp;&amp; curr.val == curr.next.val) { curr.next = curr.next.next; } curr = curr.next; } return head; }} 源码分析 首先进行异常处理，判断head是否为NULL 遍历链表，curr-&gt;val == curr-&gt;next-&gt;val时，保存curr-&gt;next，便于后面释放内存(非C/C++无需手动管理内存) 不相等时移动当前节点至下一节点，注意这个步骤必须包含在else中，否则逻辑较为复杂 while 循环处也可使用curr != null &amp;&amp; curr.next != null, 这样就不用单独判断head 是否为空了，但是这样会降低遍历的效率，因为需要判断两处。使用双重while循环可只在内循环处判断，避免了冗余的判断，谢谢 @xuewei4d 提供的思路。 复杂度分析 遍历链表一次，时间复杂度为 \\[O(n)\\], 使用了一个中间变量进行遍历，空间复杂度为 \\[O(1)\\]. Reference Remove Duplicates from Sorted List 参考程序 | 九章","link":"/posts/495674801.html"},{"title":"Remove Nth Node From End of List","text":"Problem source(lintcode): https://www.lintcode.com/problem/remove-nth-node-from-end-of-list/ source(leetcode): https://leetcode.com/problems/remove-nth-node-from-end-of-list/ ### Description Given a linked list, remove the nth node from the end of list and return its head. Notice The minimum number of nodes in list is n. Example Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;null, and n = 2. After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5-&gt;null. Challenge Can you do it without getting the length of the linked list? 题解 简单题，使用快慢指针解决此题，需要注意最后删除的是否为头节点。让快指针先走n步，直至快指针走到终点，找到需要删除节点之前的一个节点，改变node-&gt;next域即可。见基础数据结构部分的链表解析。 C++ 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @param n: An integer. * @return: The head of linked list. */ ListNode *removeNthFromEnd(ListNode *head, int n) { if (NULL == head || n &lt; 1) { return head; } ListNode dummy(0); dummy.next = head; ListNode *preDel = dummy; for (int i = 0; i != n; ++i) { if (NULL == head) { return NULL; } head = head-&gt;next; } while (head) { head = head-&gt;next; preDel = preDel-&gt;next; } preDel-&gt;next = preDel-&gt;next-&gt;next; return dummy.next; }}; Java 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { if (head == nul) return head; ListNode dummy = new ListNode(0); dummy.next = head; ListNode fast = head; ListNode slow = dummy; for (int i = 0; i &lt; n; i++) { fast = fast.next; } while(fast != null) { fast = fast.next; slow = slow.next; } // gc friendly // ListNode toBeDeleted = slow.next; slow.next = slow.next.next; // toBeDeleted.next = null; // toBeDeleted = null; return dummy.next; }} 源码分析 引入dummy节点后画个图分析下就能确定head和preDel的转移关系了。 注意 while 循环中和快慢指针初始化的关系，否则容易在顺序上错一。 复杂度分析 极限情况下遍历两遍链表，时间复杂度为 \\[O(n)\\].","link":"/posts/1445959543.html"},{"title":"Reorder List","text":"Question leetcode: Reorder List | LeetCode OJ lintcode: (99) Reorder List ### Problem Statement Given a singly linked list L: _L_0→_L_1→…→_L__n_-1→_L_n, reorder it to: _L_0→_L__n_→_L_1→_L__n_-1→_L_2→_L__n_-2→… You must do this in-place without altering the nodes' values. For example, Given {1,2,3,4}, reorder it to {1,4,2,3}. 题解1 - 链表长度(TLE) 直观角度来考虑，如果把链表视为数组来处理，那么我们要做的就是依次将下标之和为n的两个节点链接到一块儿，使用两个索引即可解决问题，一个索引指向i, 另一个索引则指向其之后的第n - 2*i个节点(对于链表来说实际上需要获取的是其前一个节点), 直至第一个索引大于第二个索引为止即处理完毕。 既然依赖链表长度信息，那么要做的第一件事就是遍历当前链表获得其长度喽。获得长度后即对链表进行遍历，小心处理链表节点的断开及链接。用这种方法会提示 TLE，也就是说还存在较大的优化空间！ C++ - TLE 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: void */ void reorderList(ListNode *head) { if (NULL == head || NULL == head-&gt;next || NULL == head-&gt;next-&gt;next) { return; } ListNode *last = head; int length = 0; while (NULL != last) { last = last-&gt;next; ++length; } last = head; for (int i = 1; i &lt; length - i; ++i) { ListNode *beforeTail = last; for (int j = i; j &lt; length - i; ++j) { beforeTail = beforeTail-&gt;next; } ListNode *temp = last-&gt;next; last-&gt;next = beforeTail-&gt;next; last-&gt;next-&gt;next = temp; beforeTail-&gt;next = NULL; last = temp; } }}; 源码分析 异常处理，对于节点数目在两个以内的无需处理。 遍历求得链表长度。 遍历链表，第一个索引处的节点使用last表示，第二个索引处的节点的前一个节点使用beforeTail表示。 处理链表的链接与断开，迭代处理下一个last。 复杂度分析 遍历整个链表获得其长度，时间复杂度为 \\[O(n)\\]. 双重for循环的时间复杂度为 \\[(n-2) + (n-4) + ... + 2 = O(\\frac{1}{2} \\cdot n^2)\\]. 总的时间复杂度可近似认为是 \\[O(n^2)\\], 空间复杂度为常数。 Warning 使用这种方法务必注意i和j的终止条件，若取i &lt; length + 1 - i, 则在处理最后两个节点时会出现环，且尾节点会被删掉。在对节点进行遍历时务必注意保留头节点的信息！ 题解2 - 反转链表后归并 既然题解1存在较大的优化空间，那我们该从哪一点出发进行优化呢？擒贼先擒王，题解1中时间复杂度最高的地方在于双重for循环，在对第二个索引进行遍历时，j每次都从i处开始遍历，要是j能从链表尾部往前遍历该有多好啊！这样就能大大降低时间复杂度了，可惜本题的链表只是单向链表... 有什么特技可以在单向链表中进行反向遍历吗？还真有——反转链表！一语惊醒梦中人。 C++ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: void */ void reorderList(ListNode *head) { if (head == NULL || head-&gt;next == NULL) return; // find middle ListNode *slow = head, *fast = head-&gt;next; while (fast != NULL &amp;&amp; fast-&gt;next != NULL) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; } ListNode *rHead = slow-&gt;next; slow-&gt;next = NULL; // reverse ListNode on the right side ListNode *prev = NULL; while (rHead != NULL) { ListNode *temp = rHead-&gt;next; rHead-&gt;next = prev; prev = rHead; rHead = temp; } // merge two list rHead = prev; ListNode *lHead = head; while (lHead != NULL &amp;&amp; rHead != NULL) { ListNode *temp1 = lHead-&gt;next; lHead-&gt;next = rHead; ListNode *temp2 = rHead-&gt;next; rHead-&gt;next = temp1; lHead = temp1; rHead = temp2; } }}; Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Definition for ListNode. * public class ListNode { * int val; * ListNode next; * ListNode(int val) { * this.val = val; * this.next = null; * } * } */ public class Solution { /** * @param head: The head of linked list. * @return: void */ public void reorderList(ListNode head) { if (head == null || head.next == null) return; // find middle ListNode slow = head, fast = head.next; while (fast != null &amp;&amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } ListNode rHead = slow.next; slow.next = null; // reverse ListNode on the right side ListNode prev = null; while (rHead != null) { ListNode temp = rHead.next; rHead.next = prev; prev = rHead; rHead = temp; } // merge two list rHead = prev; ListNode lHead = head; while (lHead != null &amp;&amp; rHead != null) { ListNode temp1 = lHead.next; lHead.next = rHead; rHead = rHead.next; lHead.next.next = temp1; lHead = temp1; } }} 源码分析 相对于题解1，题解2更多地利用了链表的常用操作如反转、找中点、合并。 找中点：我在九章算法模板的基础上增加了对head-&gt;next的异常检测，增强了鲁棒性。 反转：非常精炼的模板，记牢！ 合并：也可使用九章提供的模板，思想是一样的，需要注意left, right和dummy三者的赋值顺序，不能更改任何一步。 复杂度分析 找中点一次，时间复杂度近似为 \\[O(n)\\]. 反转链表一次，时间复杂度近似为 \\[O(n/2)\\]. 合并左右链表一次，时间复杂度近似为 \\[O(n/2)\\]. 故总的时间复杂度为 \\[O(n)\\]. Reference Reorder List | 九章算法","link":"/posts/3138734815.html"},{"title":"Reverse Linked List II","text":"Question leetcode: Reverse Linked List II | LeetCode OJ lintcode: (36) Reverse Linked List II ### Problem Statement Reverse a linked list from position m to n. Example Given 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL, m = 2 and n = 4, return 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;NULL. Note Given m, n satisfy the following condition: 1 ≤ m ≤ n ≤ length of list. Challenge Reverse it in-place and in one-pass 题解 此题在上题的基础上加了位置要求，只翻转指定区域的链表。由于链表头节点不确定，祭出我们的dummy杀器。此题边界条件处理特别tricky，需要特别注意。 由于只翻转指定区域，分析受影响的区域为第m-1个和第n+1个节点 找到第m个节点，使用for循环n-m次，使用上题中的链表翻转方法 处理第m-1个和第n+1个节点 返回dummy-&gt;next C++ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Definition of singly-linked-list: * * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The head of linked list. * @param m: The start position need to reverse. * @param n: The end position need to reverse. * @return: The new head of partial reversed linked list. */ ListNode *reverseBetween(ListNode *head, int m, int n) { if (head == NULL || m &gt; n) { return NULL; } ListNode *dummy = new ListNode(0); dummy-&gt;next = head; ListNode *node = dummy; for (int i = 1; i != m; ++i) { if (node == NULL) { return NULL; } else { node = node-&gt;next; } } ListNode *premNode = node; ListNode *mNode = node-&gt;next; ListNode *nNode = mNode, *postnNode = nNode-&gt;next; for (int i = m; i != n; ++i) { if (postnNode == NULL) { return NULL; } ListNode *temp = postnNode-&gt;next; postnNode-&gt;next = nNode; nNode = postnNode; postnNode = temp; } premNode-&gt;next = nNode; mNode-&gt;next = postnNode; return dummy-&gt;next; }}; Java 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Definition for ListNode * public class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */public class Solution { /** * @param ListNode head is the head of the linked list * @oaram m and n * @return: The head of the reversed ListNode */ public ListNode reverseBetween(ListNode head, int m , int n) { ListNode dummy = new ListNode(0); dummy.next = head; // find the mth node ListNode premNode = dummy; for (int i = 1; i &lt; m; i++) { premNode = premNode.next; } // reverse node between m and n ListNode prev = null, curr = premNode.next; while (curr != null &amp;&amp; (m &lt;= n)) { ListNode nextNode = curr.next; curr.next = prev; prev = curr; curr = nextNode; m++; } // join head and tail before m and after n premNode.next.next = curr; premNode.next = prev; return dummy.next; }} 源码分析 处理异常 使用dummy辅助节点 找到premNode——m节点之前的一个节点 以nNode和postnNode进行遍历翻转，注意考虑在遍历到n之前postnNode可能为空 连接premNode和nNode，premNode-&gt;next = nNode; 连接mNode和postnNode，mNode-&gt;next = postnNode; 务必注意node 和node-&gt;next的区别！！，node指代节点，而node-&gt;next指代节点的下一连接。","link":"/posts/1516300715.html"},{"title":"Rotate List","text":"Question leetcode: Rotate List | LeetCode OJ lintcode: (170) Rotate List ### Problem Statement Given a list, rotate the list to the right by k places, where k is non- negative. Example Given 1-&gt;2-&gt;3-&gt;4-&gt;5 and k = 2, return 4-&gt;5-&gt;1-&gt;2-&gt;3. 题解 旋转链表，链表类问题通常需要找到需要处理节点处的前一个节点。因此我们只需要找到旋转节点和最后一个节点即可。需要注意的细节是 k 有可能比链表长度还要大，此时需要取模，另一个 corner case 则是链表长度和 k 等长。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */public class Solution { /** * @param head: the List * @param k: rotate to the right k places * @return: the list after rotation */ public ListNode rotateRight(ListNode head, int k) { if (head == null) return head; ListNode fast = head, slow = head; int len = 1; for (len = 1; fast.next != null &amp;&amp; len &lt;= k; len++) { fast = fast.next; } // k mod len if k &gt; len if (len &lt;= k) { k = k % len; fast = head; for (int i = 0; i &lt; k; i++) { fast = fast.next; } } // forward slow and fast while (fast.next != null) { fast = fast.next; slow = slow.next; } // return new head fast.next = head; head = slow.next; slow.next = null; return head; }} 源码分析 由于需要处理的是节点的前一个节点，故最终的while 循环使用fast.next != null. k 与链表等长时包含在len &lt;= k中。 复杂度分析 时间复杂度 \\[O(n)\\], 空间复杂度 \\[O(1)\\].","link":"/posts/2664855015.html"},{"title":"Search a 2D Matrix","text":"Question leetcode: Search a 2D Matrix | LeetCode OJ lintcode: (28) Search a 2D Matrix Problem Statement Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties: Integers in each row are sorted from left to right. The first integer of each row is greater than the last integer of the previous row. #### Example Consider the following matrix: [ [1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 50] ] Given target = 3, return true. Challenge O(log(n) + log(m)) time 题解 - 一次二分搜索 V.S. 两次二分搜索 一次二分搜索 - 由于矩阵按升序排列，因此可将二维矩阵转换为一维问题。对原始的二分搜索进行适当改变即可(求行和列)。时间复杂度为 \\[O(log(mn))=O(log(m)+log(n))\\] 两次二分搜索 - 先按行再按列进行搜索，即两次二分搜索。时间复杂度相同。 一次二分搜索 Python 123456789101112131415161718class Solution: def search_matrix(self, matrix, target): # Find the first position of target if not matrix or not matrix[0]: return False m, n = len(matrix), len(matrix[0]) st, ed = 0, m * n - 1 while st + 1 &lt; ed: mid = (st + ed) / 2 if matrix[mid / n][mid % n] == target: return True elif matrix[mid / n][mid % n] &lt; target: st = mid else: ed = mid return matrix[st / n][st % n] == target or \\ matrix[ed / n][ed % n] == target C++ 12345678910111213141516171819class Solution {public: bool searchMatrix(vector&lt;vector&lt;int&gt;&gt;&amp; matrix, int target) { if (matrix.empty() || matrix[0].empty()) return false; int ROW = matrix.size(), COL = matrix[0].size(); int lb = -1, ub = ROW * COL; while (lb + 1 &lt; ub) { int mid = lb + (ub - lb) / 2; if (matrix[mid / COL][mid % COL] &lt; target) { lb = mid; } else { if (matrix[mid / COL][mid % COL] == target) return true; ub = mid; } } return false; }}; Java lower bound 二分模板。 12345678910111213141516171819202122232425262728public class Solution { /** * @param matrix, a list of lists of integers * @param target, an integer * @return a boolean, indicate whether matrix contains target */ public boolean searchMatrix(int[][] matrix, int target) { if (matrix == null || matrix.length == 0 || matrix[0] == null) { return false; } int ROW = matrix.length, COL = matrix[0].length; int lb = -1, ub = ROW * COL; while (lb + 1 &lt; ub) { int mid = lb + (ub - lb) / 2; if (matrix[mid / COL][mid % COL] &lt; target) { lb = mid; } else { if (matrix[mid / COL][mid % COL] == target) { return true; } ub = mid; } } return false; }} 源码分析 仍然可以使用经典的二分搜索模板(lower bound)，注意下标的赋值即可。 首先对输入做异常处理，不仅要考虑到matrix为null，还要考虑到matrix[0]的长度也为0。 由于 lb 的变化处一定小于 target, 故在 else 中判断。 复杂度分析 二分搜索，\\[O(\\log mn)\\]. 两次二分法 Python 123456789101112131415161718192021222324252627282930313233class Solution: def search_matrix(self, matrix, target): if not matrix or not matrix[0]: return False # first pos &gt;= target st, ed = 0, len(matrix) - 1 while st + 1 &lt; ed: mid = (st + ed) / 2 if matrix[mid][-1] == target: st = mid elif matrix[mid][-1] &lt; target: st = mid else: ed = mid if matrix[st][-1] &gt;= target: row = matrix[st] elif matrix[ed][-1] &gt;= target: row = matrix[ed] else: return False # binary search in row st, ed = 0, len(row) - 1 while st + 1 &lt; ed: mid = (st + ed) / 2 if row[mid] == target: return True elif row[mid] &lt; target: st = mid else: ed = mid return row[st] == target or row[ed] == target 源码分析 先找到first position的行， 这一行的最后一个元素大于等于target 再在这一行中找target 复杂度分析 二分搜索， \\[O(\\log m + \\log n)\\]","link":"/posts/3917559914.html"},{"title":"Sort List","text":"Question leetcode: Sort List | LeetCode OJ lintcode: (98) Sort List 1Sort a linked list in O(n log n) time using constant space complexity. 题解1 - 归并排序(链表长度求中间节点) 链表的排序操作，对于常用的排序算法，能达到 \\[O(n \\log n)\\]的复杂度有快速排序(平均情况)，归并排序，堆排序。快速排序不一定能保证其时间复杂度一定满足要求，归并排序和堆排序都能满足复杂度的要求。在数组排序中，归并排序通常需要使用 \\[O(n)\\] 的额外空间，也有原地归并的实现，代码写起来略微麻烦一点。但是对于链表这种非随机访问数据结构，所谓的「排序」不过是指针next值的变化而已，主要通过指针操作，故仅需要常数级别的额外空间，满足题意。堆排序通常需要构建二叉树，在这道题中不太适合。 既然确定使用归并排序，我们就来思考归并排序实现的几个要素。 按长度等分链表，归并虽然不严格要求等分，但是等分能保证线性对数的时间复杂度。由于链表不能随机访问，故可以先对链表进行遍历求得其长度。 合并链表，细节已在 Merge Two Sorted Lists | Data Structure and Algorithm 中详述。 在按长度等分链表时进行「后序归并」——先求得左半部分链表的表头，再求得右半部分链表的表头，最后进行归并操作。 由于递归等分链表的操作需要传入链表长度信息，故需要另建一辅助函数。新鲜出炉的源码如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: You should return the head of the sorted linked list, using constant space complexity. */ ListNode *sortList(ListNode *head) { if (NULL == head) { return NULL; } // get the length of List int len = 0; ListNode *node = head; while (NULL != node) { node = node-&gt;next; ++len; } return sortListHelper(head, len); }private: ListNode *sortListHelper(ListNode *head, const int length) { if ((NULL == head) || (0 &gt;= length)) { return head; } ListNode *midNode = head; int count = 1; while (count &lt; length / 2) { midNode = midNode-&gt;next; ++count; } ListNode *rList = sortListHelper(midNode-&gt;next, length - length / 2); midNode-&gt;next = NULL; ListNode *lList = sortListHelper(head, length / 2); return mergeList(lList, rList); } ListNode *mergeList(ListNode *l1, ListNode *l2) { ListNode *dummy = new ListNode(0); ListNode *lastNode = dummy; while ((NULL != l1) &amp;&amp; (NULL != l2)) { if (l1-&gt;val &lt; l2-&gt;val) { lastNode-&gt;next = l1; l1 = l1-&gt;next; } else { lastNode-&gt;next = l2; l2 = l2-&gt;next; } lastNode = lastNode-&gt;next; } lastNode-&gt;next = (NULL != l1) ? l1 : l2; return dummy-&gt;next; }}; 源码分析 归并子程序没啥好说的了，见 Merge Two Sorted Lists | Data Structure and Algorithm. 在递归处理链表长度时，分析方法和 Convert Sorted List to Binary Search Tree | Data Structure and Algorithm 一致，count表示遍历到链表中间时表头指针需要移动的节点数。在纸上分析几个简单例子后即可确定，由于这个题需要的是「左右」而不是二叉搜索树那道题需要三分——「左中右」，故将count初始化为1更为方便，左半部分链表长度为length / 2, 这两个值的确定最好是先用纸笔分析再视情况取初值，不可死记硬背。 找到中间节点后首先将其作为右半部分链表处理，然后将其next值置为NULL, 否则归并子程序无法正确求解。这里需要注意的是midNode是左半部分的最后一个节点，midNode-&gt;next才是链表右半部分的起始节点。 递归模型中左、右、合并三者的顺序可以根据分治思想确定，即先找出左右链表，最后进行归并(因为归并排序的前提是两个子链表各自有序)。 复杂度分析 遍历求得链表长度，时间复杂度为 \\[O(n)\\], 「折半取中」过程中总共有 \\[\\log(n)\\] 层，每层找中点需遍历 \\[n/2\\] 个节点，故总的时间复杂度为 \\[ n/2 \\cdot O(\\log n)\\] (折半取中), 每一层归并排序的时间复杂度介于 \\[O(n/2)\\] 和 \\[O(n)\\]之间，故总的时间复杂度为 \\[O(n \\log n)\\], 空间复杂度为常数级别，满足题意。 题解2 - 归并排序(快慢指针求中间节点) 除了遍历链表求得总长外，还可使用看起来较为巧妙的技巧如「快慢指针」，快指针每次走两步，慢指针每次走一步，最后慢指针所指的节点即为中间节点。使用这种特技的关键之处在于如何正确确定快慢指针的起始位置。 C++ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * Definition of ListNode * class ListNode { * public: * int val; * ListNode *next; * ListNode(int val) { * this-&gt;val = val; * this-&gt;next = NULL; * } * } */class Solution {public: /** * @param head: The first node of linked list. * @return: You should return the head of the sorted linked list, using constant space complexity. */ ListNode *sortList(ListNode *head) { if (NULL == head || NULL == head-&gt;next) { return head; } ListNode *midNode = findMiddle(head); ListNode *rList = sortList(midNode-&gt;next); midNode-&gt;next = NULL; ListNode *lList = sortList(head); return mergeList(lList, rList); }private: ListNode *findMiddle(ListNode *head) { if (NULL == head || NULL == head-&gt;next) { return head; } ListNode *slow = head, *fast = head-&gt;next; while(NULL != fast &amp;&amp; NULL != fast-&gt;next) { fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; } return slow; } ListNode *mergeList(ListNode *l1, ListNode *l2) { ListNode *dummy = new ListNode(0); ListNode *lastNode = dummy; while ((NULL != l1) &amp;&amp; (NULL != l2)) { if (l1-&gt;val &lt; l2-&gt;val) { lastNode-&gt;next = l1; l1 = l1-&gt;next; } else { lastNode-&gt;next = l2; l2 = l2-&gt;next; } lastNode = lastNode-&gt;next; } lastNode-&gt;next = (NULL != l1) ? l1 : l2; return dummy-&gt;next; }}; ###Java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Definition for ListNode. * public class ListNode { * int val; * ListNode next; * ListNode(int val) { * this.val = val; * this.next = null; * } * } */ public class Solution { /** * @param head: The head of linked list. * @return: You should return the head of the sorted linked list, using constant space complexity. */ public ListNode sortList(ListNode head) { // write your code here if (head == null || head.next == null) return head; ListNode mid = findMid(head); ListNode head1 = head; ListNode head2 = mid.next; mid.next = null; ListNode left = sortList(head1); ListNode right = sortList(head2); return merge(left, right); } // find mid public ListNode findMid(ListNode head) { if (head == null) return null; ListNode fast = head.next; ListNode slow = head; while (fast != null &amp;&amp; fast.next != null) { fast = fast.next.next; slow = slow.next; } return slow; } // merge public ListNode merge(ListNode head1, ListNode head2) { ListNode dummy = new ListNode(0); ListNode head = dummy; while (head1 != null || head2 != null) { int a = head1 == null ? Integer.MAX_VALUE : head1.val; int b = head2 == null ? Integer.MAX_VALUE : head2.val; if (a &lt; b) { head.next = new ListNode(a); if (head1 != null) head1 = head1.next; } else { head.next = new ListNode(b); if (head2 != null) head2 = head2.next; } head = head.next; } return dummy.next; } } 源码分析 异常处理不仅考虑了head, 还考虑了head-&gt;next, 可减少辅助程序中的异常处理。 使用快慢指针求中间节点时，将fast初始化为head-&gt;next可有效避免无法分割两个节点如1-&gt;2-&gt;null1。 求中点的子程序也可不做异常处理，但前提是主程序sortList中对head-&gt;next做了检测。 最后进行merge归并排序。 Note 在递归和迭代程序中，需要尤其注意终止条件的确定，以及循环语句中变量的自增，以防出现死循环或访问空指针。 复杂度分析 同上。 题解3 - 归并排序(自底向上) 归并排序，总的时间复杂度是（nlogn),但是递归的空间复杂度并不是常数（和递归的层数有着关；递归的过程是自顶向下，好理解；这里提供自底向上的非递归方法； C++ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: ListNode* sortList(ListNode* head) { int len_list = 0; ListNode *p=head; while(p){ p = p-&gt;next; len_list++; } ListNode *l_list,*r_list,**p_merge_list; for(int i = 1; i &lt; len_list; i &lt;&lt;= 1){ r_list = l_list = head; p_merge_list = &amp;head; for(int j = 0; j &lt; len_list - i ; j += i &lt;&lt; 1){ for(int k = 0; k &lt; i; ++k) r_list=r_list-&gt;next; int l_len=i,r_len=min(i, len_list - j - i); while(l_len || r_len ){ if(r_len &gt; 0 &amp;&amp; (l_len == 0 || r_list-&gt;val &lt;= l_list-&gt;val)){ *p_merge_list = r_list; p_merge_list=&amp;(r_list-&gt;next); r_list = r_list-&gt;next; --r_len; } else{ *p_merge_list = l_list; p_merge_list=&amp;(l_list-&gt;next); l_list = l_list-&gt;next; --l_len; } } l_list=r_list; } *p_merge_list = r_list; } return head; }}; 复杂度分析 归并排序，分解子问题的过程是O(logn),合并子问题的过程是O(n); Reference Sort List | 九章算法 LeetCode: Sort List 解题报告 - Yu's Garden - 博客园↩︎","link":"/posts/1260843745.html"},{"title":"Sqrt(x)","text":"Question leetcode: Sqrt(x) lintcode: Sqrt(x) Problem Statement Implement int sqrt(int x). Compute and return the square root of x. 题解 - 二分搜索 由于只需要求整数部分，故对于任意正整数 \\[x\\], 设其整数部分为 \\[k\\], 显然有 \\[1 \\leq k \\leq x\\], 求解 \\[k\\] 的值也就转化为了在有序数组中查找满足某种约束条件的元素，显然二分搜索是解决此类问题的良方。 Python 12345678910111213141516171819202122class Solution(object): def mySqrt(self, x): &quot;&quot;&quot; :type x: int :rtype: int &quot;&quot;&quot; if x &lt; 0: return -1 elif x == 0: return 0 lb, ub = 1, x while lb + 1 &lt; ub: mid = (lb + ub) / 2 if mid**2 == x: return mid elif mid**2 &lt; x: lb = mid else: ub = mid return lb C++ 12345678910111213141516171819202122class Solution {public: int mySqrt(int x) { if (x &lt; 0) return -1; if (x == 0) return 0; int lb = 1, ub = x; long long mid = 0; while (lb + 1 &lt; ub) { mid = lb + (ub - lb) / 2; if (mid * mid == x) { return mid; } else if (mid * mid &lt; x) { lb = mid; } else { ub = mid; } } return lb; }}; Java 123456789101112131415161718192021public class Solution { public int mySqrt(int x) { if (x &lt; 0) return -1; if (x == 0) return 0; int lb = 1, ub = x; long mid = 0; while (lb + 1 &lt; ub) { mid = lb + (ub - lb) / 2; if (mid * mid == x) { return (int)mid; } else if (mid * mid &lt; x) { lb = (int)mid; } else { ub = (int)mid; } } return (int)lb; }} 源码分析 异常检测，先处理小于等于0的值。 使用二分搜索的经典模板，注意不能使用lb &lt; ub, 否则在给定值1时产生死循环。 最后返回平方根的整数部分lb. C++ 代码 mid 需要定义为long long，否则计算平方时会溢出，定义 mid 放在循环体外部有助于提升效率。 二分搜索过程很好理解，关键是最后的返回结果还需不需要判断？比如是取 lb, ub, 还是 mid? 我们首先来分析下二分搜索的循环条件，由while循环条件lb + 1 &lt; ub可知，lb 和 ub 只可能有两种关系，一个是ub == 1 || ub ==2这一特殊情况，返回值均为1，另一个就是循环终止时lb恰好在ub前一个元素。设值 x 的整数部分为 k, 那么在执行二分搜索的过程中 \\[lb \\leq k \\leq ub\\] 关系一直存在，也就是说在没有找到 \\[mid^2 == x\\] 时，循环退出时有 \\[lb &lt; k &lt; ub\\], 取整的话显然就是lb了。 复杂度分析 经典的二分搜索，时间复杂度为 \\[O(\\log n)\\], 使用了lb, ub, mid变量，空间复杂度为 \\[O(1)\\]. 除了使用二分法求平方根近似解之外，还可使用牛顿迭代法进一步提高运算效率，欲知后事如何，请猛戳 求平方根sqrt()函数的底层算法效率问题 -- 简明现代魔法，不得不感叹算法的魔力！","link":"/posts/3144288927.html"},{"title":"Statistical Machine Translation：IBM Models 1 and 2","text":"Introduction 这部分讲机器翻译，尤其是在统计机器翻译（SMT）系统上，此部分关注IBM翻译模型。这里以翻译法语(源语言)为英语(目标语言)为例，用\\(f\\)表示法语句子，即\\(f_1,f_2...f_m\\)，其中m为句子的长度；用e表示英语句子，即\\(e_1,e_2...e_l\\)，其中l表示英语句子的长度。用\\((f^{(k)},e^{k})\\)表示第k个法语句子和英语句子。 The Noisy-Channel Approach IBM模型是一个噪声通道模型的例子，给出两个参数，用\\(p(e)\\)表示任意一个句子\\(e_1,e_2...e_l\\)在英语中的概率，用\\(p(f|e)\\)表示出现法语/英语对的概率，那么对于该模型，给定一个新的法语句子，其输出的结果是（即对应的英语句子）： \\[ e^{*}=arg \\,\\, \\max_{e \\in E} \\,\\,p(e)p(f|e) \\] 这个前面的章节中有讲到为什么，这里就直接用了。此时的重点在于如何定义模型\\(p(f|e)\\)，以及如何从训练集\\((f^{(k)},e^{(k)}) \\,\\,for\\,\\,k=1...n\\)中评估模型的参数？ The IBM Models 直接求解\\(p(f_1...f_m|e_1...e_l,m)\\)比较难，将其条件概率细化为\\(p(f_1...f_m,a_1...a_m|e_1...e_l,m)\\)，其中变量\\(a_1...a_m\\)的\\(a_i \\in \\{0,0,...,l\\}\\)表示法语的第i个单词对应英语的某个单词，这样再回到原条件概率： \\[ p(f_1...f_m|e_1...e_l)=\\sum_{a_1=0}^{l}\\sum_{a_2=0}^{l}...\\sum_{a_m=0}^{l}p(f_1...f_m,a_1...a_m|e_1...e_l) \\] IBM Model2 用一个有限集\\(\\varepsilon\\)表示英语单词集，用\\(F\\)表示法语集，用\\(M\\)和\\(L\\)分别表示法语的最大长度和英语的最大长度，下面给出两个参数： 一个是\\(t(f|e)\\)，表示从英语单词e生成法语单词f的条件概率，其中\\(f \\in F , e \\in \\varepsilon \\cup \\{NULL\\}\\) 一个是\\(q(j|i,l,m)\\)，表示在法语句子和英语句子长度分别为m和l的条件下，对齐变量\\(a_i\\)值为j的概率，其中\\(l \\in\\{1...L\\}，m \\in \\{1...M\\}，i \\in \\{1...m\\}，j \\in \\{0...l\\}\\) 前面讲到的条件概率有如下等价： \\[ p(f_1...f_m,a_1...a_m|e_1...e_l,m)=\\prod_{i=1}^mq(a_i|i,l,m)t(f_i|e_{a_i}) \\] 此处定义\\(e_0\\)为NULL。 上式为什么就等价呢，下面用随机变量来讲解： 定义\\(E_1...E_l\\)为对应英语句子中单词的随机变量序列，L为英语句子长度的随机变量，\\(F_1...F_m\\)为法语单词的随机变量序列，M为法语句子长度的随机变量，\\(A_1...A_m\\)为对齐变量，这样建立模型为: \\[ P(F_1=f_1...F_m=f_m,A_1=a_1...A_m=a_m|E_1=e_1...E_l=e_l,L=l,M=m) \\] 上式用条件概率展开如下： \\[ P(F_1=f_1...F_m=f_m,A_1=a_1...A_m=a_m|E_1=e_1...E_l=e_l,L=l,M=m)\\\\ =P(A_1=a_1...A_m=a_m|E_1=e_1...E_l=e_l,L=l,M=m)\\cdot \\\\ P(F_1=f_1...F_m=f_m|A_1=a_1...A_m=a_m,E_1=e_1...E_l=e_l,L=l,M=m) \\] 上面两部分分别做如下两个假设： 1、 \\[ P(A_1=a_1...A_m=a_m|E_1=e_1...E_l=e_l,L=l,M=m)\\\\ =\\prod_{i=1}^{m}P(A_i=a_i|A_1=a_1...A_{i-1}=a_{i-1},E_1=e_1...E_l=e_l,L=l,M=m)\\\\ =\\prod_{i=1}^{m}P(A_i=a_i|L=l,M=m) \\] 2、 \\[ P(F_1=f_1...F_m=f_m|A_1=a_1...A_m=a_m,E_1=e_1...E_l=e_l,L=l,M=m)\\\\ =\\prod_{i=1}^{m}P(F_i=f_i|F_1=f_1...F_{i-1}=f_{i-1},A_1=a_1...A_m=a_m,E_1=e_1...E_l=e_l,L=l,M=m)\\\\ =\\prod_{i=1}^{m}P(F_i=f_i|E_{a_i}=e_{a_i}) \\] 第二行假设随机变量\\(F_i\\)仅仅依赖于\\(E_{ai}\\) 【【此处假设有点强，Fi竟然和其它的法语变量没关系…有点不太懂？？？】】 Applying IBM Model 2 前面讲到了IBM Model 2 中的参数\\(q(j|i,l,m)\\)和\\(t(f|e)\\)，即我们知道了分布\\(p(f,a|e)\\)，而我们需要知道对于任意的\\(f,a,e\\)，得出如下分布： \\[ p(f|e)=\\sum_ap(f,a|e) \\] 最后，假定我们已经估计了语言模型\\(p(e)\\)，那么我们对任意一个法语句子f的翻译结果就是： \\[ arg\\,\\, \\max_ep(e)p(f|e) \\] IBM模型并不是一个好的翻译系统，但是仍然是一个关键的算法 ，这里说道的两种原因：\\(t(f|e)\\)被用到各种翻译系统，现代的翻译模型都是建立在IBM模型上的。 继续接上面最后的公式，对于由英语句子和法语句子对组成的训练集中，我们可以分析出一组对齐变量，使得下面的概率最大，也就是最符合的对齐变量： \\[ arg \\,\\, \\max_{a_1...a_l}p(a_1...a_m|f_1...f_m,e_1...e_l,m) \\] 继而求解如下子问题： \\[ a_i=arg\\,\\, \\max_{j \\in (0...l)}(q(j|i,l,m)t(f_i|e_j)) \\] Parameter Estimation 定义\\(c(e,f)\\)表示在训练集中单词e和单词l对齐的次数，\\(c(e)\\)表示e和任意一个法语单词对齐的次数，\\(c(j|i,l,m)\\)表示在看到长度为l的英语句子和长度为m的法语句子，且在看到单词i对应的是单词j的次数(就是\\(a_i=j\\)的次数)，\\(c(i,l,m)\\)表示长度为l的英语句子和长度为m的法语句子下标为i的个数。 上面的含义有点乱，而且下面给出的算法我也是看了好一会才理解什么意思，如下分别对全部语料库和部分语料库给出的算法，这里先分析对全部语料库的算法： 接上面定义的变量的含义，这里当\\(a_i^{(k)}=j\\)时，有\\(\\delta(k,i,j)=1\\)，否则为0；算法中每次都会一起加1，看着值一样，其实是有区别的，这里我举个例子： 训练集中的数据为\\(f^{(k)},e^{(k)},a^{(k)}\\)拿书中的一个句子为例： 1234l=6,m=7e= And the programma has been implementedf= Le programme a ete mis en applicationa={2,3,4,5,6,6,6} 这里\\(c(e)\\)就没什么好说的，就是统计如\\(c(And),c(the)\\)的个数，\\(c(e,f)\\)就是统计如\\(c(the,Le)\\)的个数，\\(c(j|i,l,m)\\)作如下解释，比如f中第一个单词Le对应e中第二个单词the，那么就将\\(c(2|1,6,7)\\)的值增加1，当然，对于\\(c(the),c(the,Le),c(1,6,7)\\)都会增加1，也就是说如果还有这样一个样本l=6，m=7，同时f中的第一个单词对应着e中第二个单词，那么\\(c(2|1,6,7),c(1,6,7)\\)也会增加1，但是\\(c(the),c(the,Le)\\)就不会增加了，这样就理解上面的算法了吧。接着看下面的算法： 观察两个算法的区别，主要在于\\(\\delta(k,i,j)=1\\)计算不同，\\(\\delta(k,i,j)=1\\)表示第k组平行预料（训练集中法文-英文句子）里的第i个法文词，第j个英文词。如果是上帝模式，那\\(\\delta(k,i,j)=1/0\\)分别表示这两个词之间应当/不应当对齐，其问题在于我们不可能有全部语料库，也就是说等于1或者0，没有人能够知道，所以采用最大似然估计来估计（EM算法），于是就采用如下公式： \\[ \\delta(k,i,j)=\\frac{q(j|i,l_k,m_k)t(f_i^{(k)}|e_j^{(k)})}{\\sum_{j=0}^{l_k}q(j|i,l_k,m_k)t(f_i^{(k)}|e_j^{(k)})} \\] More on the EM Algorithm: Maximum-likelihood Estimation 这部分好像没什么用额，功力不够，就不写了 Initialzation using IBM Model 1 EM算法对IBM模型2的初始化敏感，依赖初始值(随机数)，这里使用IBM模型1，主要区别在于将模型2开始对\\(q(j|i,l,m)\\)的概率设为定值： \\[ q(j|i,l,m)=\\frac{1}{l+1} \\] 注意这里的分母\\(l+1\\)表示的是j全部的取值个数，\\(j \\in \\{0,1,...,l\\}\\)，这样做的意思就是说，在长度分别为l和m的英语句子和法语句子中，\\(a_i\\)对应j的关系是同概率的，没有什么相关性。 那么句子预测结果的概条件率公式可重写如下： \\[ p(f_1...f_m,a_1...a_m|e_1...e_l,m)=\\prod \\frac{1}{l+1}t(f_i|e_{a_i})=\\frac{1}{(1+l)^m}\\prod_{i=1}^{m}t(f_i|e_{a_i}) \\] EM算法重写如下： \\[ \\delta(k,i,j)=\\frac{q(j|i,l_k,m_k)t(f_i^{(k)}|e_j^{(k)})}{\\sum_{j=0}^{l_k}q(j|i,l_k,m_k)t(f_i^{(k)}|e_j^{(k)})} =\\frac{t(f_i^{(k)}|e_j^{(k)}}{\\sum_{j=0}^{l_k}t(f_i^{(k)}|e_j^{(k)})} \\] 算法如下：","link":"/posts/3979220335.html"},{"title":"Swap Nodes in Pairs","text":"Question leetcode: Swap Nodes in Pairs | LeetCode OJ lintcode: (451) Swap Nodes in Pairs ### Problem Statement Given a linked list, swap every two adjacent nodes and return its head. Example Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Challenge Your algorithm should use only constant space. You may not modify the values in the list, only nodes itself can be changed. 题解1 - Iteration 直觉上我们能想到的是使用 dummy 处理不定头节点，但是由于这里是交换奇偶位置的链表节点，我们不妨首先使用伪代码来表示。大致可以分为如下几个步骤： 保存2.next 将2.next赋值为1 将1.next赋值为1中保存的2.next 将前一个链表节点的 next 指向1 更新前一个链表节点为1 更新当前的链表节点为1中保存的2.next 链表类题目看似容易，但要做到 bug-free 其实不容易，建议结合图像辅助分析，onsite 时不要急，把过程先用伪代码写出来。然后将伪代码逐行转化。 Java 123456789101112131415161718192021222324252627282930313233/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { /** * @param head a ListNode * @return a ListNode */ public ListNode swapPairs(ListNode head) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode prev = dummy, curr = head; while (curr != null &amp;&amp; curr.next != null) { ListNode after = curr.next; ListNode nextCurr = after.next; after.next = curr; curr.next = nextCurr; // link new node after prev prev.next = after; // update prev and curr prev = curr; curr = nextCurr; } return dummy.next; }} 源码分析 这里使用 dummy 处理不定头节点，首先将prev初始化为dummy, 然后按照题解中的几个步骤逐步转化，需要注意的是 while 循环中curr和curr.next都不能为null. 复杂度分析 遍历链表一遍，时间复杂度 \\[O(1)\\]. 使用了若干临时链表节点引用对象，空间复杂度 \\[O(1)\\]. 题解2 - Recursion 在题解1 的分析过程中我们发现比较难处理的是 prev和下一个头的连接，要是能直接得到链表后面新的头节点该有多好啊。首先我们可以肯定的是若head == null || head.next == null时应直接返回，如果不是则求得交换奇偶节点后的下一个头节点并链接到之前的奇数个节点。这种思想使用递归实现起来非常优雅！ Java 1234567891011121314151617181920212223/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { /** * @param head a ListNode * @return a ListNode */ public ListNode swapPairs(ListNode head) { if (head == null || head.next == null) return head; ListNode after = head.next; head.next = swapPairs(after.next); after.next = head; return after; }} 源码分析 这个递归实现非常优雅，需要注意的是递归步的退出条件==&gt;head == null || head.next == null). 复杂度分析 每个节点最多被遍历若干次，时间复杂度 \\[O(n)\\], 空间复杂度 \\[O(1)\\].","link":"/posts/2705356483.html"},{"title":"TF-IDF","text":"介绍 词频-逆文档频率法(Term frequency-inverse document frequency,TF-IDF)是在文本挖掘中广泛使用的特征向量化方法。 它反映语料中词对文档的重要程度。假设用t表示词，d表示文档，D表示语料。词频TF(t,d)表示词t在文档d中出现的次数。文档频率DF(t,D)表示语料中出现词t的文档的个数。 如果我们仅仅用词频去衡量重要程度，这很容易过分强调出现频繁但携带较少文档信息的词，如of、the等。如果一个词在语料中出现很频繁，这意味着它不携带特定文档的特殊信息。逆文档频率数值衡量一个词提供多少信息。 如果某个词出现在所有的文档中，它的IDF值为0。注意，上式有个平滑项，这是为了避免分母为0的情况发生。TF-IDF就是TF和IDF简单的相乘。 词频和文档频率的定义有很多种不同的变种。在Mllib中，分别提供了TF和IDF的实现，以便有更好的灵活性。 Mllib使用hashing trick实现词频。元素的特征应用一个hash函数映射到一个索引（即词），通过这个索引计算词频。这个方法避免计算全局的词-索引映射，因为全局的词-索引映射在大规模语料中花费较大。 但是，它会出现哈希冲突，这是因为不同的元素特征可能得到相同的哈希值。为了减少碰撞冲突，我们可以增加目标特征的维度，例如哈希表的桶数量。默认的特征维度是1048576。 实例 TF的计算 123456789import org.apache.spark.rdd.RDDimport org.apache.spark.SparkContextimport org.apache.spark.mllib.feature.HashingTFimport org.apache.spark.mllib.linalg.Vectorval sc: SparkContext = ...// Load documents (one per line).val documents: RDD[Seq[String]] = sc.textFile(&quot;...&quot;).map(_.split(&quot; &quot;).toSeq)val hashingTF = new HashingTF()val tf: RDD[Vector] = hashingTF.transform(documents) IDF的计算 12345678import org.apache.spark.mllib.feature.IDF// ... continue from the previous exampletf.cache()val idf = new IDF().fit(tf)val tfidf: RDD[Vector] = idf.transform(tf)//或者val idf = new IDF(minDocFreq = 2).fit(tf)val tfidf: RDD[Vector] = idf.transform(tf) 源码实现 下面分别分析HashingTF和IDF的实现。 HashingTF 12345678def transform(document: Iterable[_]): Vector = { val termFrequencies = mutable.HashMap.empty[Int, Double] document.foreach { term =&gt; val i = indexOf(term) termFrequencies.put(i, termFrequencies.getOrElse(i, 0.0) + 1.0) } Vectors.sparse(numFeatures, termFrequencies.toSeq) } 以上代码中，indexOf方法使用哈希获得索引。 123456//为了减少碰撞，将numFeatures设置为1048576def indexOf(term: Any): Int = Utils.nonNegativeMod(term.##, numFeatures)def nonNegativeMod(x: Int, mod: Int): Int = { val rawMod = x % mod rawMod + (if (rawMod &lt; 0) mod else 0) } 这里的term.##等价于term.hashCode，得到哈希值之后，作取余操作得到相应的索引。 IDF 我们先看IDF的fit方法。 12345678def fit(dataset: RDD[Vector]): IDFModel = { val idf = dataset.treeAggregate(new IDF.DocumentFrequencyAggregator( minDocFreq = minDocFreq))( seqOp = (df, v) =&gt; df.add(v), combOp = (df1, df2) =&gt; df1.merge(df2) ).idf() new IDFModel(idf) } 该函数使用treeAggregate处理数据集，生成一个DocumentFrequencyAggregator对象，它用于计算文档频率。重点看add和merge方法。 123456789101112131415161718192021222324252627282930def add(doc: Vector): this.type = { if (isEmpty) { df = BDV.zeros(doc.size) } //计算 doc match { case SparseVector(size, indices, values) =&gt; val nnz = indices.size var k = 0 while (k &lt; nnz) { if (values(k) &gt; 0) { df(indices(k)) += 1L } k += 1 } case DenseVector(values) =&gt; val n = values.size var j = 0 while (j &lt; n) { if (values(j) &gt; 0.0) { df(j) += 1L } j += 1 } case other =&gt; throw new UnsupportedOperationException } m += 1L this } df这个向量的每个元素都表示该索引对应的词出现的文档数。m表示文档总数。 123456789101112def merge(other: DocumentFrequencyAggregator): this.type = { if (!other.isEmpty) { m += other.m if (df == null) { df = other.df.copy } else { //简单的向量相加 df += other.df } } this } treeAggregate方法处理完数据之后，调用idf方法将文档频率低于给定值的词的idf置为0，其它的按照上面的公式计算。 12345678910111213 def idf(): Vector = { val n = df.length val inv = new Array[Double](n) var j = 0 while (j &lt; n) { if (df(j) &gt;= minDocFreq) { //计算得到idf inv(j) = math.log((m + 1.0) / (df(j) + 1.0)) } j += 1 } Vectors.dense(inv)} 最后使用transform方法计算tfidf值。 123456789101112131415161718192021222324252627282930//这里的dataset指tfdef transform(dataset: RDD[Vector]): RDD[Vector] = { val bcIdf = dataset.context.broadcast(idf) dataset.mapPartitions(iter =&gt; iter.map(v =&gt; IDFModel.transform(bcIdf.value, v))) }def transform(idf: Vector, v: Vector): Vector = { val n = v.size v match { case SparseVector(size, indices, values) =&gt; val nnz = indices.size val newValues = new Array[Double](nnz) var k = 0 while (k &lt; nnz) { //tf-idf = tf * idf newValues(k) = values(k) * idf(indices(k)) k += 1 } Vectors.sparse(n, indices, newValues) case DenseVector(values) =&gt; val newValues = new Array[Double](n) var j = 0 while (j &lt; n) { newValues(j) = values(j) * idf(j) j += 1 } Vectors.dense(newValues) case other =&gt; throw new UnsupportedOperationException } } Tokenizer Tokenization是一个将文本(如一个句子)转换为个体单元(如词)的处理过程。 一个简单的Tokenizer类就提供了这个功能。下面的例子展示了如何将句子转换为此序列。 RegexTokenizer基于正则表达式匹配提供了更高级的断词(tokenization)。默认情况下,参数pattern(默认是\\s+)作为分隔符, 用来切分输入文本。用户可以设置gaps参数为false用来表明正则参数pattern表示tokens而不是splitting gaps,这个类可以找到所有匹配的事件并作为结果返回。下面是调用的例子。 123456789101112131415161718import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}val sentenceDataFrame = spark.createDataFrame(Seq( (0, &quot;Hi I heard about Spark&quot;), (1, &quot;I wish Java could use case classes&quot;), (2, &quot;Logistic,regression,models,are,neat&quot;))).toDF(&quot;label&quot;, &quot;sentence&quot;)val tokenizer = new Tokenizer().setInputCol(&quot;sentence&quot;).setOutputCol(&quot;words&quot;)val regexTokenizer = new RegexTokenizer() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;words&quot;) .setPattern(&quot;\\\\W&quot;) // alternatively .setPattern(&quot;\\\\w+&quot;).setGaps(false)val tokenized = tokenizer.transform(sentenceDataFrame)tokenized.select(&quot;words&quot;, &quot;label&quot;).take(3).foreach(println)val regexTokenized = regexTokenizer.transform(sentenceDataFrame)regexTokenized.select(&quot;words&quot;, &quot;label&quot;).take(3).foreach(println)","link":"/posts/3621251491.html"},{"title":"Tagging Problems and Hidden Markov Models","text":"概述 对于一个句子，我们要做的是给每一个单词打上词性标记，比如句子the dog saw a cat对应的tag sequence是D N V D N，这个句子的长度是5，对应的输入\\(x_1=the,x_2=dog,x_3=saw,x_4=the,x_5=cat\\)，用\\(y_1y_2...y_n\\)来表示tagging model的output，对应上面的有\\(y_1=D,y_2=N,y_3=V,...\\)。匹配句子\\(x_1...x_n\\)的tag sequence \\(y_1...y_n\\)的问题叫做 sequence labeling problem 或者是 tagging problem。 POS Tagging and Named-Entity Recognition 前面讲到的就是POS Tagging，其有两大挑战，第一个是tagging的歧义问题，因为一个单词有可能是动词又有可能是名词；第二个是词库中的单词有限，导致训练例子中的单词可能在词库中没有出现，然后是Named-Entity Recognition，notes中提到的实体有PERSON,LOCATION,COMPANY。但是对这一块的讲解非常少，所以这里就不提了[后期如果有用会补上吧。] Generative Models 这里主要是讲解隐马尔可夫模型应用在tagging问题上，这里以\\(x^{(i)}\\)作为句子，\\(y^{(i)}\\)作为对应的标签，我们的目的是在语料库集合\\(Y,y \\in Y\\)中找到最符合句子\\(x^{(i)}\\)的标签\\(y^{(i)}\\)，即如下： \\[ f(x)=arg \\, \\max_{y\\in Y}\\,\\,p(y|x) \\] 那么如何对于给定的\\(x^{(i)}\\)，在Y中找到最优的y呢，即如何求解\\(p(y|x)\\)呢，这里我们将上式转换一下，用贝叶斯公式求解： \\[ p(y|x)=\\frac{p(y)p(x|y)}{p(x)} \\] 由于对于x而言，求解全部y的概率，因此p(x)可以看成常数，也就是求下面的最大值对应的y： \\[ f(x)=arg \\,\\, \\max_y p(y)p(x|y) \\] Generative Tagging Models 对于单词集\\(V\\)，标记集\\(K\\)，定义\\(S\\)为sequence/tag-sequence对\\(&lt;x_1...x_n,y_1...y_n&gt;\\)，对于\\(x_i \\in V,y_i \\in K\\)，有如下要求： \\[ for\\,\\,any\\,\\, &lt;x_1...x_n,y_1...y_n&gt; \\in S\\\\ p(x_1...x_n,y_1...y_n) &gt;=0\\\\ \\sum_{&lt;x_1...x_n,y_1...y_n&gt; \\in S} p(x_1...x_n,y_1...y_n)=1 \\] 那么对于给定的一个生成标记模型，从序列\\(x_1...x_n\\)中标记出序列\\(y_1...y_n\\)被定义如下： \\[ f(x_1...x_n)=arg\\,\\, \\max_{y_1...y_n}\\,\\,p(x_1...x_n,y_1...y_n) \\] Trigram Hidden Markov Models 这里是Trigram，也就是说序列中\\(y_i\\)只与\\(y_{i-1}\\)和\\(y_{i-2}\\)有关。 给出下面两个参数 ： 参数\\(q(s|u,v)\\)：对于任意的trigram\\((u,v,s)\\)，其中\\(s \\in K \\cup \\{STOP\\}, u,v \\in K \\cup \\{*\\}\\)，其概率\\(q(s|u,v)\\)可由在看到s在bigram \\((u,v)\\)后的概率。 参数\\(e(x|s)\\)：对于任意\\(x \\in V, s \\in K​\\)这个值可以表示为看见观测值x配对s的概率。 那么之前概率\\(p(x_1...x_n,y_1...y_{n+1})\\)可以表示如下： \\[ p(x_1...x_n,y_1...y_{n+1})=p(y_1...y_{n+1})p(x_1...x_n|y_1...y_{n+1})\\\\ =\\prod_{i=1}^{n+1} q(y_i|y_{i-2},y_{i-1}) \\prod_{i=1}^{n} e(x_i|y_i) \\] 这里有\\(y_{n+1}=STOP，y_0=y_{-1}=*\\)。 那么为什么上面这个公式成立呢，这里是有一些假设的，这里使用随机变量序列\\(X_1...X_n\\)和\\(Y_1...Y_n\\)，其中n为序列的长度，假设\\(X_i\\)可以取集合\\(V\\)中的任意的值，\\(Y_i\\)可以取集合\\(K\\)中任意一个标记，那么上式可以被定义为： \\[ P(X_1=x_1...X_n=x_n,Y_1=y_1...Y_{n+1}=y_{n+1})\\\\ =\\prod_{i=1}^{n+1}P(Y_i=y_i|Y_{i-2}=y_{i-2},Y_{i-1}=y_{i-1})\\prod_{i=1}^{n}P(X_i=x_i|Y_i=y_i) \\] 这里第一个连乘是没有问题的，但是第二个连乘是怎么得到的呢，下面一步步分析 这里先讲一下概率模型\\(P(X_1=x_1...X_n=x_n)\\)的求解，如下： \\[ P(X_1=x_1...X_n=x_n) =P(X_n=x_n|X_1=x_1...X_{n-1}=x_{n-1})P(X_1=x_1...X_{n-1}=x_{n-1}) \\] 其中\\(P(X_1=x_1...X_{n-1}=x_{n-1})\\) \\[ =P(X_{n-1}=x_{n-1}|X_1=x_1...X_{n-2}=x_{n-2})P(X_1=x_1...X_{n-2}=x_{n-2}) ... P(X_1=x_1, X_2=x_2)=P(X_2=x_2|X_1=x_1)P(X_1=x_1) \\] 这样上面可以表示如下： \\[ P(X_1=x_1...X_n=x_n)=\\prod_{i=1}^{n}P(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1}) \\] 那么下面这个公式就知道怎么变来的了吧 \\[ P(X_1=x_1...X_n=x_n|Y_1=y_1...Y_{n+1}=y_{n+1})\\\\ =\\prod_{i=1}^{n}P(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1},Y_1=y_1...Y_{n+1}=y_{n+1}) \\] 这里做了一个假设，变量\\(X_i\\)独立于之前的观测变量\\(X_1...X_{i-1}\\)和状态变量\\(Y_1...Y_{i-1},Y_{i+1}...Y_{n+1}\\) ，当被给出变量\\(Y_i\\)时。那么上面的式子就变为： \\[ \\prod_{i=1}^{n}P(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1},Y_1=y_1...Y_{n+1}=y_{n+1})\\\\ =\\prod_{i=1}^{n}P(X_i=x_i|Y_i=y_i) \\] Estimating the Parameters of a Trigram HMM 这里定义一下\\(c(s \\to x)\\)为在语料库中标签s和单词x匹配的次数，那么上述的两个参数在语料库中的最大似然估计如下： \\[ q(s|u,v)=\\frac{c(u,v,s)}{c(u,v)}\\\\ e(x|s)=\\frac{c(s \\to x)}{c(s)} \\] 同样对于在语料库中\\(q(s|u,v)\\)为0的情况，我们用上一章中讲到的解决，对于\\(e(x|s)\\)为0的情况，也就是说在语料库中没有出现该单词，这里notes中给出一种解决办法就是使用伪词(pseudo-words)[这块没有细看，后期用到再补] Decoding with HMMs: the Viterbi Algorithm 那么知道如何求解\\(q(s|u,v)\\)和\\(e(x|s)\\)，我们又知道 \\[ p(x_1...x_n,y_1...y_{n+1})== \\prod_{i=1}^{n+1} q(y_i|y_{i-2},y_{i-1}) \\prod_{i=1}^{n} e(x_i|y_i) \\] 而我们的原始问题是发现一组使概率值最大的序列\\(y_1...y_{n+1}\\)，即： \\[ arg\\,\\, \\max_{y_1...y_n}\\,\\,p(x_1...x_n,y_1...y_n) \\] 这里举个例子，对于输入的句子the dog barks ，假设标记集为\\(K=\\{D,N,V\\}\\)，那么可能的标记序列\\(y_1y_2y_3y_4\\)可以是下面任意一种： 12345D D D STOPD D N STOPD D V STOPD N D STOP... 对于\\(|K|=3\\)时，有27种可能，如果计算这\\(3^3=27\\)种情况中的每一种，找出概率最大的那个序列，显然是不太现实的，我们需要用一种更为简化的方法来解决这个问题。 下面的维特比算法就给出这样一种解决办法，可以将时间复杂度由\\(O(|K|^n)\\)缩小到\\(O(n|K|^3)\\)，使用的方法是动态规划，就是对一个长度为n的句子分解，给定初始条件，给出通项公式，即求长度为i的子句最大概率下对应的子标记和长度为i-1的子句最大概率下对应的子标记的关系，这样就可以求出长度为n的原句最大概率下对应的标记。 VITERBI ALGORITHM 输入的句子是\\(x_1...x_n\\)，对任意的\\(k\\in \\{1...n\\}\\) ，任意的序列\\(y_{-1},y_{0},y_{1}...y_{k}\\)，且\\(y_i \\in K , y_{-1}=y_0=*\\)，定义如下函数： \\[ r(y_{-1},y_0,y_1,...,y_k)=\\prod_{i=1}^{k}q(y_i|y_{i-2},y_{i-1})\\prod_{i=1}^{k}e(x_i|y_i) \\] 这里举个例子如下： \\[ p(x_1...x_n,y_1...y_{n+1})=r(*,*,y1,...,y_n) q(STOP|y_{n-1},y_n) \\] 继续，定义\\(K_{-1}=K_{0}=\\{*\\}\\)和\\(K_k=K\\,\\,for\\,\\,k \\in \\{1...n\\}\\),定义\\(S(k,u,v)\\)为序列\\(y_{-1},y_{0},y_1,...,y_{k}\\) 的集合，其中\\(u \\in K_{k-1}, v \\in K_{k},y_{k-1}=u,y_k=v\\)，定义： \\[ \\pi(k,u,v)= \\max_{&lt;y_{-1},y_0,y_1,...,y_k&gt; \\in S(k,u,v)} r(y_{-1},y_0,y_1,...,y_k) \\] 给定初始条件：\\(\\pi(0,*,*)=1\\) 给定通项公式： \\[ \\pi(k,u,v)=\\max_{w \\in K_{k-2}}\\big(\\pi(k-1,w,u)q(v|w,u)e(x_k|v)\\big) \\] 算法如下： 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# -*- coding: utf-8 -*-# state 存放隐藏序列，sunny 0 rainy 1# obser 存放观测序列 0 1 2 对应 walk shop clean# start_p 是初始概率，0元素对应sunny的初始概率 1元素对应rainy的概率# transition_p 转移概率矩阵 2*2 行为初始状态 列为新状态# emission_p 发射概率矩阵 2*3 行为隐藏状态 列为可观测状态# 迭代过程，每次只需要记录第t个时间点 每个节点的最大概率即可，后续计算时直接使用前序节点的最大概率即可def compute(obser, state, start_p, transition_p, emission_p): # max_p 记录每个时间点每个状态的最大概率，i行j列，（i,j）记录第i个时间点 j隐藏状态的最大概率 max_p = [[0 for col in range(len(state))] for row in range(len(obser))] # path 记录max_p 对应概率处的路径 i 行 j列 （i,j）记录第i个时间点 j隐藏状态最大概率的情况下 其前驱状态 path = [[0 for col in range(len(state))] for row in range(len(obser))] # 初始状态(1状态) for i in range(len(state)): # max_p[0][i]表示初始状态第i个隐藏状态的最大概率 # 概率 = start_p[i] * emission_p [state[i]][obser[0]] max_p[0][i] = start_p[i] * emission_p[state[i]][obser[0]] path[0][i] = i # 后续循环状态(2-t状态) # 此时max_p 中已记录第一个状态的两个隐藏状态概率 for i in range(1, len(obser)): # 循环t-1次，初始已计算 max_item = [0 for i in range(len(state))] for j in range(len(state)): # 循环隐藏状态数，计算当前状态每个隐藏状态的概率 item = [0 for i in state] for k in range(len(state)): # 再次循环隐藏状态数，计算选定隐藏状态的前驱状态为各种状态的概率 p = max_p[i - 1][k] * emission_p[state[j]][obser[i]] * transition_p[state[k]][state[j]] # k即代表前驱状态 k或state[k]均为前驱状态 item[state[k]] = p # 设置概率记录为最大情况 max_item[state[j]] = max(item) # 记录最大情况路径(下面语句的作用：当前时刻下第j个状态概率最大时，记录其前驱节点) # item.index(max(item))寻找item的最大值索引，因item记录各种前驱情况的概率 path[i][state[j]] = item.index(max(item)) # 将单个状态的结果加入总列表max_p max_p[i] = max_item #newpath记录最后路径 newpath = [] #判断最后一个时刻哪个状态的概率最大 p=max_p[len(obser)-1].index(max(max_p[len(obser)-1])) newpath.append(p) #从最后一个状态开始倒着寻找前驱节点 for i in range(len(obser) - 1, 0, -1): newpath.append(path[i][p]) p = path[i][p] newpath.reverse() return newpathif __name__ == '__main__': # 隐状态 hidden_state = ['rainy', 'sunny'] # 观测序列 obsevition = ['walk', 'shop', 'clean'] state_s = [0, 1] obser = [0, 1, 2] # 初始状态，测试集中，0.6概率观测序列以sunny开始 start_probability = [0.6, 0.4] # 转移概率，0.7：sunny下一天sunny的概率 transititon_probability = [[0.7, 0.3], [0.4, 0.6]] # 发射概率，0.4：sunny在0.4概率下为shop emission_probability = [[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]] result = compute(obser, state_s, start_probability, transititon_probability, emission_probability) for k in range(len(result)): print(hidden_state[int(result[k])])","link":"/posts/511902924.html"},{"title":"Temporal-Difference Learning","text":"时序差分（Temporal-Difference）简介 时序差分是强化学习的核心观点。 时序差分是DP和MC方法的结合。 MC要等一个完整的序列结束，比如玩21点扑克，直到玩完才能知道是胜是负；相反，时序差分每经历一步，都会更新价值函数，因为每一步都会观察到一个新的Reward，比如Grid World，每走一步都知道reward是什么。 TD往往比MC高效；TD和MC都使用经验（experience）来解决预测问题。 所谓差分就是下一个时刻的估计和当前时刻的估计的差。 什么是stationary？ stationary：环境不随时间变化而变化； non-stationary：环境会随时间变化而变化。 TD(0) \\(V(S_t)\\leftarrow V(S_t)+\\alpha[R_{t+1}+\\gamma V(S_{t+1})-V(S_t)]\\) 因为直接使用现有的估计取更新估计，因此这种方法被称为自举（bootstrap）。 TD error：\\(\\delta_t = R_{t+1}+\\gamma V(S_{t+1})-V(S_t)\\) 123456789101112def temporal_difference(values, alpha=0.1, batch=False): state = 3 trajectory = [state] rewards = [0] while True: ... # TD update if not batch: values[old_state] += alpha * (reward + values[state] - values[old_state]) ... rewards.append(reward) return trajectory, rewards Sarsa 一种on-policy的TD控制。 \\(Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]\\) 核心代码： 12345# Sarsa updateq_value[state[0], state[1], action] += \\ ALPHA * (REWARD + q_value[next_state[0], next_state[1], next_action] - q_value[state[0], state[1], action])state = next_stateaction = next_action Q-learning 一种off-policy的TD控制。 早期强化学习的一个突破。 \\(Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha[R_{t+1}+\\gamma \\underset{a}{max}Q(S_{t+1},a)-Q(S_t,A_t)]\\) 核心代码： 12345678910111213def q_learning(q_value, step_size=ALPHA): state = START rewards = 0.0 while state != GOAL: action = choose_action(state, q_value) next_state, reward = step(state, action) rewards += reward # Q-Learning update q_value[state[0], state[1], action] += step_size * ( reward + GAMMA * np.max(q_value[next_state[0], next_state[1], :]) - q_value[state[0], state[1], action]) state = next_state return rewards Expected Sarsa 一种off-policy的TD控制。 \\(Q(S_t,A_t)\\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1} + \\gamma\\sum_a\\pi(a|S_{t+1})Q(S_{t+1}, a)-Q(S_t,A_t)]\\) Double Learning 解决Q-learning的最大化偏差（maximization bias）问题 2011年提出。","link":"/posts/49571.html"},{"title":"Validate Binary Search Tree","text":"Question leetcode: Validate Binary Search Tree lintcode: Validate Binary Search Tree ### Problem Statement Given a binary tree, determine if it is a valid binary search tree (BST). Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node's key. The right subtree of a node contains only nodes with keys greater than the node's key. Both the left and right subtrees must also be binary search trees. A single node tree is a BST Example An example: 2 / \\ 1 4 / \\ 3 5 The above binary tree is serialized as {2,1,4,#,#,3,5} (in level order). 题解1 - recursion 按照题中对二叉搜索树所给的定义递归判断，我们从递归的两个步骤出发分析： 1. 基本条件/终止条件 - 返回值需斟酌。 2. 递归步/条件递归 - 能使原始问题收敛。 终止条件好确定——当前节点为空，或者不符合二叉搜索树的定义，返回值分别是什么呢？先别急，分析下递归步试试先。递归步的核心步骤为比较当前节点的key和左右子节点的key大小，和定义不符则返回false, 否则递归处理。从这里可以看出在节点为空时应返回true, 由上层的其他条件判断。但需要注意的是这里不仅要考虑根节点与当前的左右子节点，还需要考虑左子树中父节点的最小值和右子树中父节点的最大值。否则程序在[10,5,15,#,#,6,20] 这种 case 误判。 由于不仅需要考虑当前父节点，还需要考虑父节点的父节点... 故递归时需要引入上界和下界值。画图分析可知对于左子树我们需要比较父节点中最小值，对于右子树则是父节点中的最大值。又由于满足二叉搜索树的定义时，左子结点的值一定小于根节点，右子节点的值一定大于根节点，故无需比较所有父节点的值，使用递推即可得上界与下界，这里的实现非常巧妙。 C++ - long long 12345678910111213141516171819202122232425262728293031323334/** * Definition of TreeNode: * class TreeNode { * public: * int val; * TreeNode *left, *right; * TreeNode(int val) { * this-&gt;val = val; * this-&gt;left = this-&gt;right = NULL; * } * } */class Solution {public: /** * @param root: The root of binary tree. * @return: True if the binary tree is BST, or false */ bool isValidBST(TreeNode *root) { if (root == NULL) return true; return helper(root, LLONG_MIN, LLONG_MAX); } bool helper(TreeNode *root, long long lower, long long upper) { if (root == NULL) return true; if (root-&gt;val &lt;= lower || root-&gt;val &gt;= upper) return false; bool isLeftValidBST = helper(root-&gt;left, lower, root-&gt;val); bool isRightValidBST = helper(root-&gt;right, root-&gt;val, upper); return isLeftValidBST &amp;&amp; isRightValidBST; }}; C++ - without long long 12345678910111213141516171819202122232425262728293031323334353637383940/** * Definition of TreeNode: * class TreeNode { * public: * int val; * TreeNode *left, *right; * TreeNode(int val) { * this-&gt;val = val; * this-&gt;left = this-&gt;right = NULL; * } * } */class Solution {public: /** * @param root: The root of binary tree. * @return: True if the binary tree is BST, or false */ bool isValidBST(TreeNode *root) { if (root == NULL) return true; return helper(root, INT_MIN, INT_MAX); } bool helper(TreeNode *root, int lower, int upper) { if (root == NULL) return true; if (root-&gt;val &lt;= lower || root-&gt;val &gt;= upper) { bool right_max = root-&gt;val == INT_MAX &amp;&amp; root-&gt;right == NULL; bool left_min = root-&gt;val == INT_MIN &amp;&amp; root-&gt;left == NULL; if (!(right_max || left_min)) { return false; } } bool isLeftValidBST = helper(root-&gt;left, lower, root-&gt;val); bool isRightValidBST = helper(root-&gt;right, root-&gt;val, upper); return isLeftValidBST &amp;&amp; isRightValidBST; }}; Java 123456789101112131415161718192021222324252627282930313233/** * Definition of TreeNode: * public class TreeNode { * public int val; * public TreeNode left, right; * public TreeNode(int val) { * this.val = val; * this.left = this.right = null; * } * } */public class Solution { /** * @param root: The root of binary tree. * @return: True if the binary tree is BST, or false */ public boolean isValidBST(TreeNode root) { if (root == null) return true; return helper(root, Long.MIN_VALUE, Long.MAX_VALUE); } private boolean helper(TreeNode root, long lower, long upper) { if (root == null) return true; // System.out.println(&quot;root.val = &quot; + root.val + &quot;, lower = &quot; + lower + &quot;, upper = &quot; + upper); // left node value &lt; root node value &lt; right node value if (root.val &gt;= upper || root.val &lt;= lower) return false; boolean isLeftValidBST = helper(root.left, lower, root.val); boolean isRightValidBST = helper(root.right, root.val, upper); return isLeftValidBST &amp;&amp; isRightValidBST; }} 源码分析 为避免节点中出现整型的最大最小值，引入 long 型进行比较。有些 BST 的定义允许左子结点的值与根节点相同，此时需要更改比较条件为root.val &gt; upper. C++ 中 long 可能与 int 范围相同，故使用 long long. 如果不使用比 int 型更大的类型，那么就需要在相等时多加一些判断。 复杂度分析 递归遍历所有节点，时间复杂度为 \\[O(n)\\], 使用了部分额外空间，空间复杂度为 \\[O(1)\\]. 题解2 - iteration 联想到二叉树的中序遍历。 Java 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */public class Solution { public boolean isValidBST(TreeNode root) { Deque&lt;TreeNode&gt; st = new ArrayDeque&lt;&gt;(); long pre = Long.MIN_VALUE; // inorder traverse while (root != null || !st.isEmpty()) { if (root != null) { st.push(root); root = root.left; } else { root = st.pop(); if (root.val &gt; pre) pre = root.val; else return false; root = root.right; } } return true; }} Reference LeetCode: Validate Binary Search Tree 解题报告 - Yu's Garden - 博客园 - 提供了4种不同的方法，思路可以参考。","link":"/posts/1343700060.html"},{"title":"带权最小二乘","text":"原理 给定n个带权的观察样本\\((w_i,a_i,b_i)\\): \\(w_i\\)表示第i个观察样本的权重； \\(a_i\\)表示第i个观察样本的特征向量； \\(b_i\\)表示第i个观察样本的标签。 每个观察样本的特征数是m。我们使用下面的带权最小二乘公式作为目标函数： \\[minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\\] 其中\\(\\lambda\\)是正则化参数，\\(\\delta\\)是标签的总体标准差，\\(\\sigma_j\\)是第j个特征列的总体标准差。 这个目标函数有一个解析解法，它仅仅需要一次处理样本来搜集必要的统计数据去求解。与原始数据集必须存储在分布式系统上不同， 如果特征数相对较小，这些统计数据可以加载进单机的内存中，然后在driver端使用乔里斯基分解求解目标函数。 spark ml中使用WeightedLeastSquares求解带权最小二乘问题。WeightedLeastSquares仅仅支持L2正则化，并且提供了正则化和标准化 的开关。为了使正太方程（normal equation）方法有效，特征数不能超过4096。如果超过4096，用L-BFGS代替。下面从代码层面介绍带权最小二乘优化算法 的实现。 代码解析 我们首先看看WeightedLeastSquares的参数及其含义。 1234567891011121314private[ml] class WeightedLeastSquares( val fitIntercept: Boolean, //是否使用截距 val regParam: Double, //L2正则化参数，指上面公式中的lambda val elasticNetParam: Double, // alpha，控制L1和L2正则化 val standardizeFeatures: Boolean, // 是否标准化特征 val standardizeLabel: Boolean, // 是否标准化标签 val solverType: WeightedLeastSquares.Solver = WeightedLeastSquares.Auto, val maxIter: Int = 100, // 迭代次数 val tol: Double = 1e-6) extends Logging with Serializable sealed trait Solver case object Auto extends Solver case object Cholesky extends Solver case object QuasiNewton extends Solver 在上面的代码中，standardizeFeatures决定是否标准化特征，如果为真，则\\(\\sigma_j\\)是A第j个特征列的总体标准差，否则\\(\\sigma_j\\)为1。 standardizeLabel决定是否标准化标签，如果为真，则\\(\\delta\\)是标签b的总体标准差，否则\\(\\delta\\)为1。solverType指定求解的类型，有Auto，Cholesky 和QuasiNewton三种选择。tol表示迭代的收敛阈值，仅仅在solverType为QuasiNewton时可用。 求解过程 WeightedLeastSquares接收一个包含（标签，权重，特征）的RDD，使用fit方法训练，并返回WeightedLeastSquaresModel。 1def fit(instances: RDD[Instance]): WeightedLeastSquaresModel 训练过程分为下面几步。 1 统计样本信息 1val summary = instances.treeAggregate(new Aggregator)(_.add(_), _.merge(_)) 使用treeAggregate方法来统计样本信息。统计的信息在Aggregator类中给出了定义。通过展开上面的目标函数，我们可以知道这些统计信息的含义。 12345678910111213private class Aggregator extends Serializable { var initialized: Boolean = false var k: Int = _ // 特征数 var count: Long = _ // 样本数 var triK: Int = _ // 对角矩阵保存的元素个数 var wSum: Double = _ // 权重和 private var wwSum: Double = _ // 权重的平方和 private var bSum: Double = _ // 带权标签和 private var bbSum: Double = _ // 带权标签的平方和 private var aSum: DenseVector = _ // 带权特征和 private var abSum: DenseVector = _ // 带权特征标签相乘和 private var aaSum: DenseVector = _ // 带权特征平方和 } 方法add添加样本的统计信息，方法merge合并不同分区的统计信息。代码很简单，如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Adds an instance. */ def add(instance: Instance): this.type = { val Instance(l, w, f) = instance val ak = f.size if (!initialized) { init(ak) } assert(ak == k, s&quot;Dimension mismatch. Expect vectors of size $k but got $ak.&quot;) count += 1L wSum += w wwSum += w * w bSum += w * l bbSum += w * l * l BLAS.axpy(w, f, aSum) BLAS.axpy(w * l, f, abSum) BLAS.spr(w, f, aaSum) // wff^T this } /** * Merges another [[Aggregator]]. */ def merge(other: Aggregator): this.type = { if (!other.initialized) { this } else { if (!initialized) { init(other.k) } assert(k == other.k, s&quot;dimension mismatch: this.k = $k but other.k = ${other.k}&quot;) count += other.count wSum += other.wSum wwSum += other.wwSum bSum += other.bSum bbSum += other.bbSum BLAS.axpy(1.0, other.aSum, aSum) BLAS.axpy(1.0, other.abSum, abSum) BLAS.axpy(1.0, other.aaSum, aaSum) this } Aggregator类给出了以下一些统计信息： 1234567aBar: 特征加权平均数bBar: 标签加权平均数aaBar: 特征平方加权平均数bbBar: 标签平方加权平均数aStd: 特征的加权总体标准差bStd: 标签的加权总体标准差aVar: 带权的特征总体方差 计算出这些信息之后，将均值缩放到标准空间，即使每列数据的方差为1。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 缩放bBar和 bbBarval bBar = summary.bBar / bStdval bbBar = summary.bbBar / (bStd * bStd)val aStd = summary.aStdval aStdValues = aStd.values// 缩放aBarval aBar = { val _aBar = summary.aBar val _aBarValues = _aBar.values var i = 0 // scale aBar to standardized space in-place while (i &lt; numFeatures) { if (aStdValues(i) == 0.0) { _aBarValues(i) = 0.0 } else { _aBarValues(i) /= aStdValues(i) } i += 1 } _aBar}val aBarValues = aBar.values// 缩放 abBarval abBar = { val _abBar = summary.abBar val _abBarValues = _abBar.values var i = 0 // scale abBar to standardized space in-place while (i &lt; numFeatures) { if (aStdValues(i) == 0.0) { _abBarValues(i) = 0.0 } else { _abBarValues(i) /= (aStdValues(i) * bStd) } i += 1 } _abBar}val abBarValues = abBar.values// 缩放aaBarval aaBar = { val _aaBar = summary.aaBar val _aaBarValues = _aaBar.values var j = 0 var p = 0 // scale aaBar to standardized space in-place while (j &lt; numFeatures) { val aStdJ = aStdValues(j) var i = 0 while (i &lt;= j) { val aStdI = aStdValues(i) if (aStdJ == 0.0 || aStdI == 0.0) { _aaBarValues(p) = 0.0 } else { _aaBarValues(p) /= (aStdI * aStdJ) } p += 1 i += 1 } j += 1 } _aaBar}val aaBarValues = aaBar.values 2 处理L2正则项 123456789101112131415161718192021222324val effectiveRegParam = regParam / bStdval effectiveL1RegParam = elasticNetParam * effectiveRegParamval effectiveL2RegParam = (1.0 - elasticNetParam) * effectiveRegParam// 添加L2正则项到对角矩阵中var i = 0var j = 2while (i &lt; triK) { var lambda = effectiveL2RegParam if (!standardizeFeatures) { val std = aStdValues(j - 2) if (std != 0.0) { lambda /= (std * std) //正则项标准化 } else { lambda = 0.0 } } if (!standardizeLabel) { lambda *= bStd } aaBarValues(i) += lambda i += j j += 1} 3 选择solver WeightedLeastSquares实现了CholeskySolver和QuasiNewtonSolver两种不同的求解方法。当没有正则化项时， 选择CholeskySolver求解，否则用QuasiNewtonSolver求解。 123456789101112131415161718192021val solver = if ((solverType == WeightedLeastSquares.Auto &amp;&amp; elasticNetParam != 0.0 &amp;&amp; regParam != 0.0) || (solverType == WeightedLeastSquares.QuasiNewton)) { val effectiveL1RegFun: Option[(Int) =&gt; Double] = if (effectiveL1RegParam != 0.0) { Some((index: Int) =&gt; { if (fitIntercept &amp;&amp; index == numFeatures) { 0.0 } else { if (standardizeFeatures) { effectiveL1RegParam } else { if (aStdValues(index) != 0.0) effectiveL1RegParam / aStdValues(index) else 0.0 } } }) } else { None } new QuasiNewtonSolver(fitIntercept, maxIter, tol, effectiveL1RegFun) } else { new CholeskySolver } CholeskySolver和QuasiNewtonSolver的详细分析会在另外的专题进行描述。 4 处理结果 1234567891011121314151617181920212223242526val solution = solver match { case cholesky: CholeskySolver =&gt; try { cholesky.solve(bBar, bbBar, ab, aa, aBar) } catch { // if Auto solver is used and Cholesky fails due to singular AtA, then fall back to // Quasi-Newton solver. case _: SingularMatrixException if solverType == WeightedLeastSquares.Auto =&gt; logWarning(&quot;Cholesky solver failed due to singular covariance matrix. &quot; + &quot;Retrying with Quasi-Newton solver.&quot;) // ab and aa were modified in place, so reconstruct them val _aa = getAtA(aaBarValues, aBarValues) val _ab = getAtB(abBarValues, bBar) val newSolver = new QuasiNewtonSolver(fitIntercept, maxIter, tol, None) newSolver.solve(bBar, bbBar, _ab, _aa, aBar) } case qn: QuasiNewtonSolver =&gt; qn.solve(bBar, bbBar, ab, aa, aBar) } val (coefficientArray, intercept) = if (fitIntercept) { (solution.coefficients.slice(0, solution.coefficients.length - 1), solution.coefficients.last * bStd) } else { (solution.coefficients, 0.0) } 上面代码的异常处理需要注意一下。在AtA是奇异矩阵的情况下，乔里斯基分解会报错，这时需要用拟牛顿方法求解。 以上的结果是在标准空间中，所以我们需要将结果从标准空间转换到原来的空间。 1234567// convert the coefficients from the scaled space to the original spacevar q = 0val len = coefficientArray.lengthwhile (q &lt; len) { coefficientArray(q) *= { if (aStdValues(q) != 0.0) bStd / aStdValues(q) else 0.0 } q += 1}","link":"/posts/2300094874.html"},{"title":"Wide and Deep Learning for Recommender Systems","text":"Google Play 用的深度神经网络推荐系统，主要思路是将 Memorization(Wide Model) 和 Generalization(Deep Model) 取长补短相结合。论文见 Wide &amp; Deep Learning for Recommender Systems Overview of System 先来看一下推荐系统的整体架构，由两个部分组成，检索系统(或者说候选生成系统） 和 排序系统(排序网络)。首先，用 检索(retrieval) 的方法对大数据集进行初步筛选，返回最匹配 query 的一部分物品列表，这里的检索通常会结合采用 机器学习模型(machine-learned models) 和 人工定义规则(human-defined rules) 两种方法。从大规模样本中召回最佳候选集之后，再使用 排序系统 对每个物品进行算分、排序，分数 P(y|x)，y 是用户采取的行动(比如说下载行为)，x 是特征，包括 User features e.g., country, language, demographics Contextual features e.g., device, hour of the day, day of the week Impression features e.g., app age, historical statistics of an app WDL 就是用在排序系统中。 Wide and Deep Learning 简单来说，人脑就是一个不断记忆（memorization）并且归纳（generalization）的过程，而这篇论文的思想，就是将宽线性模型（Wide Model，用于记忆，下图左侧）和深度神经网络模型（Deep Model，用于归纳，下图右侧）结合，汲取各自优势形成了 Wide &amp; Deep 模型用于推荐排序（下图中间）。 Wide Model Memorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data. 要理解的概念是 Memorization，主要是学习特征的共性或者说相关性，产生的推荐是和已经有用户行为的物品直接相关的物品。 用的模型是 逻辑回归(logistic regression, LR)，LR 的优点就是简单(simple)、容易规模化(scalable)、可解释性强(interpretable)。LR 的特征往往是二值且稀疏的(binary and sparse)，这里同样采用 one-hot 编码，如 “user_installed_app=netflix”，如果用户安装了 Netflix，这个特征的值为 1，否则为 0。 为了达到 Memorization，我们对稀疏的特征采取 cross-product transformation，比如说 AND(user_installed_app=netflix, impression_app=pandora”) 这个特征，只有 Netflix 和 Pandora 两个条件都达到了，值才为 1，这类 feature 解释了 co-occurrence 和 target label 之间的关系。一个 cross-product transformation 的局限在于，对于在训练集里没有出现过的 query-item pair，它不能进行泛化(Generalization) 到此，总结一下，宽度模型的输入是用户安装应用(installation)和为用户展示（impression）的应用间的向量积（叉乘），模型通常训练 one-hot 编码后的二值特征，这种操作不会归纳出训练集中未出现的特征对。 Linear model 大家都很熟悉了 \\[y = w^Tx+b\\] $x = [x_1, x_2, …, x_d] \\(是包含了 d 个特征的向量，\\)w = [w_1, w_2, …, w_d] $是模型参数，b 是偏置。特征包括了原始的输入特征以及 cross-product transformation 特征，cross-product transformation 的式子如下： \\(\\\\varnothing\\_k(x)=\\\\prod^d\\_{i=1}x\\_i^{c\\_{ki}}\\) \\(ckjc_{kj}\\)是一个布尔变量，如果第 i 个特征是第 k 个 transformation φk 的一部分，那么值就为 1，否则为 0，作用： This captures the interactions between the binary features, and adds nonlinearity to the generalized linear model. Deep Model Generalization is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past. 要理解的概念是 Generalization，可以理解为相关性的传递(transitivity)，会学习新的特征组合，来提高推荐物品的多样性，或者说提供泛化能力(Generalization) 泛化往往是通过学习 low-dimensional dense embeddings 来探索过去从未或很少出现的新的特征组合来实现的，通常的 embedding-based model 有 Factorization Machines(FM) 和 Deep Neural Networks(DNN)。特殊兴趣或者小众爱好的用户，query-item matrix 非常稀疏，很难学习，然而 dense embedding 的方法还是可以得到对所有 query-item pair 非零的预测，这就会导致 over-generalize，推荐不怎么相关的物品。这点和 LR 正好互补，因为 LR 只能记住很少的特征组合。 为了达到 Generalization，我们会引入新的小颗粒特征，如类别特征（安装了视频类应用，展示的是音乐类应用，等等）AND(user_installed_category=video, impression_category=music)，这些高维稀疏的类别特征（如人口学特征和设备类别）映射为低纬稠密的向量后，与其他连续特征（用户年龄、应用安装数等）拼接在一起，输入 MLP 中，最后输入逻辑输出单元。 一开始嵌入向量(embedding vectors)被随机初始化，然后训练过程中通过最小化损失函数来优化模型。每一个隐层(hidden-layer)做这样的计算： \\[a^{(l+1)}=f(W^{(l)}a^{(l)}+b^{(l)})\\] f 是激活函数(通常用 ReLU)，l 是层数。 总结一下，基于 embedding 的深度模型的输入是 类别特征(产生embedding)+连续特征。 Joint Training 对两个模型的输出算 log odds ratio 然后加权求和，作为预测。 Joint Training vs Ensemble Joint Training 同时训练 wide &amp; deep 模型，优化的参数包括两个模型各自的参数以及 weights of sum Ensemble 中的模型是分别独立训练的，互不干扰，只有在预测时才会联系在一起 用 mini-batch stochastic optimization 来进行训练，可以看下这篇论文Efficient Mini-batch Training for Stochastic Optimization。 在论文提到的实验中，训练时 Wide Model 部分用了 Follow-the-regularized-learder(FTRL)+ L1 正则，Deep Model 用了 AdaGrad，对于逻辑回归，模型预测如下： System Implementation pipeline 如下图 Data Generation Label: 标准是 app acquisition，用户下载为 1，否则为 0 Vocabularies: 将类别特征(categorical features)映射为整型的 id，连续的实值先用累计分布函数CDF归一化到[0,1]，再划档离散化。 Continuous real-valued features are normalized to [0, 1] by mapping a feature value x to its cumulative distribution function P(X ≤ x), divided into nqnqn_q quantiles. The normalized value is \\(i−1 \\\\over n_q-1\\)for values in the i-th quantiles. Model Training 训练数据有 500 billion examples， Input layer 会同时产生稀疏(sparse)的和稠密(dense)的特征，具体的 Model 上面已经讨论过了。需要注意的是，当新的训练数据来临的时候，我们用的是热启动(warm-starting)方式，也就是从之前的模型中读取 embeddings 以及 linear model weights 来初始化一个新模型，而不是全部推倒重新训练。 Model Serving 当模型训练并且优化好之后，我们将它载入服务器，对每一个 request，排序系统从检索系统接收候选列表以及用户特征，来为每一个 app 算分排序，分数就是前向传播的值(forward inference)啦，可以并行训练提高 performance。 参考链接 《Wide &amp; Deep Learning for Recommender Systems 》笔记 深度学习第二课：个性化推荐","link":"/posts/1237613955.html"},{"title":"Word2Vector","text":"Word2Vector将词转换成分布式向量。分布式表示的主要优势是相似的词在向量空间距离较近，这使我们更容易泛化新的模式并且使模型估计更加健壮。 分布式的向量表示在许多自然语言处理应用（如命名实体识别、消歧、词法分析、机器翻译）中非常有用。 ## 模型 在MLlib中，Word2Vector使用skip-gram模型来实现。skip-gram的训练目标是学习词向量表示，这个表示可以很好的预测它在相同句子中的上下文。数学上，给定训练词w_1,w_2,...,w_T， skip-gram模型的目标是最大化下面的平均对数似然。 其中k是训练窗口的大小。在skip-gram模型中，每个词w和两个向量u_w和v_w相关联，这两个向量分别表示词和上下文。正确地预测给定词w_j的条件下w_i的概率使用softmax模型。 其中V表示词汇数量。在skip-gram模型中使用softmax是非常昂贵的，因为计算log p(w_i|w_j)与V是成比例的。为了加快Word2Vec的训练速度，MLlib使用了分层softmax,这样可以将计算的复杂度降低为O(log(V))。 实例 下面的例子展示了怎样加载文本数据、切分数据、构造Word2Vec实例、训练模型。最后，我们打印某个词的40个同义词。 1234567891011import org.apache.spark._import org.apache.spark.rdd._import org.apache.spark.SparkContext._import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}val input = sc.textFile(&quot;text8&quot;).map(line =&gt; line.split(&quot; &quot;).toSeq)val word2vec = new Word2Vec()val model = word2vec.fit(input)val synonyms = model.findSynonyms(&quot;china&quot;, 40)for((synonym, cosineSimilarity) &lt;- synonyms) { println(s&quot;$synonym $cosineSimilarity&quot;)} 源码分析 由于涉及神经网络相关的知识，这里先不作分析，后续会补上。要更详细了解Word2Vector可以阅读文献【2】。 参考文献 【1】哈夫曼树与哈夫曼编码 【2】Deep Learning 实战之 word2vec 【3】Word2Vector谷歌实现","link":"/posts/48642834.html"},{"title":"XGBoost算法原理小结","text":"在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 从GBDT到XGBoost 作为GBDT的高效实现，XGBoost是一个上限特别高的算法，因此在算法竞赛中比较受欢迎。简单来说，对比原算法GBDT，XGBoost主要从下面三个方面做了优化： 一是算法本身的优化：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以直接很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。算法本身的优化是我们后面讨论的重点。 二是算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。 三是算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。 在上面三方面的优化中，第一部分算法本身的优化是重点也是难点。现在我们就来看看算法本身的优化内容。 XGBoost损失函数 在看XGBoost本身的优化内容前，我们先回顾下GBDT的回归算法迭代的流程，详细算法流程见梯度提升树(GBDT)原理小结第三节，对于GBDT的第t颗决策树，主要是走下面4步： 1)对样本i=1,2，...m，计算负梯度 \\[ r_{ti} = -\\bigg[\\frac{\\partial L(y_i, f(x_i)))}{\\partial f(x_i)}\\bigg]_{f(x) = f_{t-1}\\;\\; (x)} \\] 2)利用\\((x_i,r_{ti})\\;\\; (i=1,2,..m)\\), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为\\(R_{tj}, j =1,2,..., J\\)。其中J为回归树t的叶子节点的个数。 对叶子区域j =1,2,..J,计算最佳拟合值 \\[ c_{tj} = \\underbrace{arg\\; min}_{c}\\sum\\limits_{x_i \\in R_{tj}} L(y_i,f_{t-1}(x_i) +c) \\] 更新强学习器 \\[ f_{t}(x) = f_{t-1}(x) + \\sum\\limits_{j=1}^{J}c_{tj}I(x \\in R_{tj}) \\] 上面第一步是得到负梯度，或者是泰勒展开式的一阶导数。第二步是第一个优化求解，即基于残差拟合一颗CART回归树，得到J个叶子节点区域。第三步是第二个优化求解，在第二步优化求解的结果上，对每个节点区域再做一次线性搜索，得到每个叶子节点区域的最优取值。最终得到当前轮的强学习器。 从上面可以看出，我们要求解这个问题，需要求解当前决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解\\(c_{tj}\\)。GBDT采样的方法是分两步走，先求出最优的所有J个叶子节点区域，再求出每个叶子节点区域的最优解。 对于XGBoost，它期望把第2步和第3步合并在一起做，即一次求解出决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解\\(c_{tj}\\)。在讨论如何求解前，我们先看看XGBoost的损失函数的形式。 在GBDT损失函数\\(L(y, f_{t-1}(x)+ h_t(x))\\)的基础上，我们加入正则化项如下： \\[ \\Omega(h_t) = \\gamma J + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^Jw_{tj}^2 \\] 这里的\\(J\\)是叶子节点的个数，而\\(w_{tj}\\)是第j个叶子节点的最优值。这里的\\(w_{tj}\\)和我们GBDT里使用的\\(c_{tj}\\)是一个意思，只是XGBoost的论文里用的是\\(w\\)表示叶子区域的值，因此这里和论文保持一致。 最终XGBoost的损失函数可以表达为： \\[ L_t=\\sum\\limits_{i=1}^mL(y_i, f_{t-1}(x_i)+ h_t(x_i)) + \\gamma J + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^Jw_{tj}^2 \\] 最终我们要极小化上面这个损失函数，得到第t个决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解\\(w_{tj}\\)。XGBoost没有和GBDT一样去拟合泰勒展开式的一阶导数，而是期望直接基于损失函数的二阶泰勒展开式来求解。现在我们来看看这个损失函数的二阶泰勒展开式： \\[ \\begin{align} L_t &amp; = \\sum\\limits_{i=1}^mL(y_i, f_{t-1}(x_i)+ h_t(x_i)) + \\gamma J + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^Jw_{tj}^2 \\\\ &amp; \\approx \\sum\\limits_{i=1}^m( L(y_i, f_{t-1}(x_i)) + \\frac{\\partial L(y_i, f_{t-1}(x_i) }{\\partial f_{t-1}(x_i)}h_t(x_i) + \\frac{1}{2}\\frac{\\partial^2 L(y_i, f_{t-1}(x_i) }{\\partial f_{t-1}^2(x_i)} h_t^2(x_i)) + \\gamma J + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^Jw_{tj}^2 \\end{align} \\] 为了方便，我们把第i个样本在第t个弱学习器的一阶和二阶导数分别记为 \\[ g_{ti} = \\frac{\\partial L(y_i, f_{t-1}(x_i) }{\\partial f_{t-1}(x_i)}, \\; h_{ti} = \\frac{\\partial^2 L(y_i, f_{t-1}(x_i) }{\\partial f_{t-1}^2(x_i)} \\] 则我们的损失函数现在可以表达为： \\[ L_t \\approx \\sum\\limits_{i=1}^m( L(y_i, f_{t-1}(x_i)) + g_{ti}h_t(x_i) + \\frac{1}{2} h_{ti} h_t^2(x_i)) + \\gamma J + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^Jw_{tj}^2 \\] 损失函数里面\\(L(y_i, f_{t-1}(x_i))\\)是常数，对最小化无影响，可以去掉，同时由于每个决策树的第j个叶子节点的取值最终会是同一个值\\(w_{tj}\\)因此我们的损失函数可以继续化简。 \\[ \\begin{align} L_t &amp; \\approx \\sum\\limits_{i=1}^m g_{ti}h_t(x_i) + \\frac{1}{2} h_{ti} h_t^2(x_i)) + \\gamma J + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^Jw_{tj}^2 \\\\ &amp; = \\sum\\limits_{j=1}^J (\\sum\\limits_{x_i \\in R_{tj}}g_{ti}w_{tj} + \\frac{1}{2} \\sum\\limits_{x_i \\in R_{tj}}h_{ti} w_{tj}^2) + \\gamma J + \\frac{\\lambda}{2}\\sum\\limits_{j=1}^Jw_{tj}^2 \\\\ &amp; = \\sum\\limits_{j=1}^J [(\\sum\\limits_{x_i \\in R_{tj}}g_{ti})w_{tj} + \\frac{1}{2}( \\sum\\limits_{x_i \\in R_{tj}}h_{ti}+ \\lambda) w_{tj}^2] + \\gamma J \\end{align} \\] 我们把每个叶子节点区域样本的一阶和二阶导数的和单独表示如下： \\[ G_{tj} = \\sum\\limits_{x_i \\in R_{tj}}g_{ti},\\; H_{tj} = \\sum\\limits_{x_i \\in R_{tj}}h_{ti} \\] 最终损失函数的形式可以表示为： \\[ L_t = \\sum\\limits_{j=1}^J [G_{tj}w_{tj} + \\frac{1}{2}(H_{tj}+\\lambda)w_{tj}^2] + \\gamma J \\] 现在我们得到了最终的损失函数，那么回到前面讲到的问题，我们如何一次求解出决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解\\(w_{tj}\\)呢？ XGBoost损失函数的优化求解 关于如何一次求解出决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解\\(w_{tj}\\)，我们可以把它拆分成2个问题： 如果我们已经求出了第t个决策树的J个最优的叶子节点区域，如何求出每个叶子节点区域的最优解\\(w_{tj}\\)？ 对当前决策树做子树分裂决策时，应该如何选择哪个特征和特征值进行分裂，使最终我们的损失函数\\(L_t\\)最小？ 对于第一个问题，其实是比较简单的，我们直接基于损失函数对\\(w_{tj}\\)求导并令导数为0即可。这样我们得到叶子节点区域的最优解\\(w_{tj}\\)表达式为： \\[ w_{tj} = - \\frac{G_{tj}}{H_{tj} + \\lambda} \\] 这个叶子节点的表达式不是XGBoost首创，实际上在GBDT的分类算法里，已经在使用了。大家在梯度提升树(GBDT)原理小结第4.1节中叶子节点区域值的近似解表达式为： \\[ c_{tj} = \\sum\\limits_{x_i \\in R_{tj}}r_{ti}\\bigg / \\sum\\limits_{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|) \\] 它其实就是使用了上式来计算最终的\\(c_{tj}\\)。注意到二元分类的损失函数是： \\[ L(y, f(x)) = log(1+ exp(-yf(x))) \\] 其每个样本的一阶导数为： \\[ g_i=-r_i= -y_i/(1+exp(y_if(x_i))) \\] 其每个样本的二阶导数为： \\[ h_i =\\frac{exp(y_if(x_i)}{(1+exp(y_if(x_i))^2} = |g_i|(1-|g_i|) \\] 由于没有正则化项，则\\(c_{tj} = -\\frac{g_i}{h_i}\\)，即可得到GBDT二分类叶子节点区域的近似值。 现在我们回到XGBoost，我们已经解决了第一个问题。现在来看XGBoost优化拆分出的第二个问题：如何选择哪个特征和特征值进行分裂，使最终我们的损失函数\\(L_t\\)最小？ 在GBDT里面，我们是直接拟合的CART回归树，所以树节点分裂使用的是均方误差。XGBoost这里不使用均方误差，而是使用贪心法，即每次分裂都期望最小化我们的损失函数的误差。 注意到在我们\\(w_{tj}\\)取最优解的时候，原损失函数对应的表达式为： \\[ L_t = -\\frac{1}{2}\\sum\\limits_{j=1}^J\\frac{G_{tj}^2}{H_{tj} + \\lambda} +\\gamma J \\] 如果我们每次做左右子树分裂时，可以最大程度的减少损失函数的损失就最好了。也就是说，假设当前节点左右子树的一阶二阶导数和为\\(G_L,H_L,G_R,H_L\\), 则我们期望最大化下式： \\[ -\\frac{1}{2}\\frac{(G_L+G_R)^2}{H_L+H_R+ \\lambda} +\\gamma J -( -\\frac{1}{2}\\frac{G_L^2}{H_L + \\lambda} -\\frac{1}{2}\\frac{G_{R}^2}{H_{R} + \\lambda}+ \\gamma (J+1) ) \\] 整理下上式后，我们期望最大化的是： \\[ \\max \\frac{1}{2}\\frac{G_L^2}{H_L + \\lambda} + \\frac{1}{2}\\frac{G_R^2}{H_R+\\lambda} - \\frac{1}{2}\\frac{(G_L+G_R)^2}{H_L+H_R+ \\lambda} - \\gamma \\] 也就是说，我们的决策树分裂标准不再使用CART回归树的均方误差，而是上式了。 具体如何分裂呢？举个简单的年龄特征的例子如下，假设我们选择年龄这个 特征的值a作为决策树的分裂标准，则可以得到左子树2个人，右子树三个人，这样可以分别计算出左右子树的一阶和二阶导数和，进而求出最终的上式的值。 然后我们使用其他的不是值a的划分标准，可以得到其他组合的一阶和二阶导数和，进而求出上式的值。最终我们找出可以使上式最大的组合，以它对应的特征值来分裂子树。 至此，我们解决了XGBoost的2个优化子问题的求解方法。 XGBoost算法主流程 这里我们总结下XGBoost的算法主流程，基于决策树弱分类器。不涉及运行效率的优化和健壮性优化的内容。 输入是训练集样本\\(I=\\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\\}\\)， 最大迭代次数T, 损失函数L， 正则化系数\\(\\lambda,\\gamma\\)。 输出是强学习器f(x) 对迭代轮数t=1,2,...T有： 计算第i个样本(i-1,2,..m)在当前轮损失函数L基于\\(f_{t-1}(x_i)\\)的一阶导数\\(g_{ti}\\)，二阶导数\\(h_{ti}\\),计算所有样本的一阶导数和\\(G_t = \\sum\\limits_{i=1}^mg_{ti}\\),二阶导数和\\(H_t = \\sum\\limits_{i=1}^mh_{ti}\\) 基于当前节点尝试分裂决策树，默认分数score=0 对特征序号 k=1,2...K: \\(G_L=0, H_L=0\\) b.1) 将样本按特征k从小到大排列，依次取出第i个样本，依次计算当前样本放入左子树后，左右子树一阶和二阶导数和： \\[ G_L = G_L+ g_{ti}, G_R=G-G_L \\] \\[ H_L = H_L+ h_{ti}, H_R=H-H_L \\] b.2) 尝试更新最大的分数： \\[ score = max(score, \\frac{1}{2}\\frac{G_L^2}{H_L + \\lambda} + \\frac{1}{2}\\frac{G_R^2}{H_R+\\lambda} - \\frac{1}{2}\\frac{(G_L+G_R)^2}{H_L+H_R+ \\lambda} -\\gamma ) \\] 基于最大score对应的划分特征和特征值分裂子树。 如果最大score为0，则当前决策树建立完毕，计算所有叶子区域的\\(w_{tj}\\), 得到弱学习器\\(h_t(x)\\)，更新强学习器\\(f_t(x)\\),进入下一轮弱学习器迭代.如果最大score不是0，则转到第2)步继续尝试分裂决策树。 XGBoost算法运行效率的优化 在第2,3,4节我们重点讨论了XGBoost算法本身的优化，在这里我们再来看看XGBoost算法运行效率的优化。 大家知道,Boosting算法的弱学习器是没法并行迭代的，但是单个弱学习器里面最耗时的是决策树的分裂过程，XGBoost针对这个分裂做了比较大的并行优化。对于不同的特征的特征划分点，XGBoost分别在不同的线程中并行选择分裂的最大增益。 同时，对训练的每个特征排序并且以块的的结构存储在内存中，方便后面迭代重复使用，减少计算量。计算量的减少参见上面第4节的算法流程，首先默认所有的样本都在右子树，然后从小到大迭代，依次放入左子树，并寻找最优的分裂点。这样做可以减少很多不必要的比较。 具体的过程如下图所示： 此外，通过设置合理的分块的大小，充分利用了CPU缓存进行读取加速（cache-aware access）。使得数据读取的速度更快。另外，通过将分块进行压缩（block compressoin）并存储到硬盘上，并且通过将分块分区到多个硬盘上实现了更大的IO。 XGBoost算法健壮性的优化 最后我们再来看看XGBoost在算法健壮性的优化，除了上面讲到的正则化项提高算法的泛化能力外，XGBoost还对特征的缺失值做了处理。 XGBoost没有假设缺失值一定进入左子树还是右子树，则是尝试通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向，这样处理起来更加的灵活和合理。 也就是说，上面第4节的算法的步骤a),b.1)和b.2)会执行2次，第一次假设特征k所有有缺失值的样本都走左子树，第二次假设特征k所有缺失值的样本都走右子树。然后每次都是针对没有缺失值的特征k的样本走上述流程，而不是所有的的样本。 如果是所有的缺失值走右子树，使用上面第4节的a),b.1)和b.2)即可。如果是所有的样本走左子树，则上面第4节的a)步要变成： \\[ G_R=0, H_R=0 \\] b.1)步要更新为： \\[ G_R = G_R+g_{ti}, G_L=G-G_R \\] \\[ H_R = H_R+h_{ti}, H_L=H-H_R \\] XGBoost小结 不考虑深度学习，则XGBoost是算法竞赛中最热门的算法，它将GBDT的优化走向了一个极致。当然，后续微软又出了LightGBM，在内存占用和运行速度上又做了不少优化，但是从算法本身来说，优化点则并没有XGBoost多。 何时使用XGBoost，何时使用LightGBM呢？个人建议是优先选择XGBoost，毕竟调优经验比较多一些，可以参考的资料也多一些。如果你使用XGBoost遇到的内存占用或者运行速度问题，那么尝试LightGBM是个不错的选择。","link":"/posts/2923522808.html"},{"title":"Add Binary","text":"Question leetcode: Add Binary | LeetCode OJ lintcode: (408) Add Binary 123456Given two binary strings, return their sum (also a binary string).For example,a = &quot;11&quot;b = &quot;1&quot;Return &quot;100&quot;. 题解 用字符串模拟二进制的加法，加法操作一般使用自后往前遍历的方法，不同位大小需要补零。 Java 12345678910111213141516171819202122232425262728293031public class Solution { /** * @param a a number * @param b a number * @return the result */ public String addBinary(String a, String b) { if (a == null || a.length() == 0) return b; if (b == null || b.length() == 0) return a; StringBuilder sb = new StringBuilder(); int aLen = a.length(), bLen = b.length(); int carry = 0; for (int ia = aLen - 1, ib = bLen - 1; ia &gt;= 0 || ib &gt;= 0; ia--, ib--) { // replace with 0 if processed int aNum = (ia &lt; 0) ? 0 : a.charAt(ia) - '0'; int bNum = (ib &lt; 0) ? 0 : b.charAt(ib) - '0'; int num = (aNum + bNum + carry) % 2; carry = (aNum + bNum + carry) / 2; sb.append(num); } if (carry == 1) sb.append(1); // important! sb.reverse(); String result = sb.toString(); return result; }} 源码分析 用到的技巧主要有两点，一是两个数位数大小不一时用0补上，二是最后需要判断最高位的进位是否为1。最后需要反转字符串，因为我们是从低位往高位迭代的。虽然可以使用 insert 避免最后的 reverse 操作，但如此一来时间复杂度就从 \\[O(n)\\] 变为 \\[O(n^2)\\] 了。 复杂度分析 遍历两个字符串，时间复杂度 \\[O(n)\\]. reverse 操作时间复杂度 \\[O(n)\\], 故总的时间复杂度 \\[O(n)\\]. 使用了 StringBuilder 作为临时存储对象，空间复杂度 \\[O(n)\\].","link":"/posts/2028932621.html"},{"title":"Add Two Numbers","text":"Question leetcode: Add Two Numbers | LeetCode OJ lintcode: Add Two Numbers Problem Statement You have two numbers represented by a linked list, where each node contains a single digit. The digits are stored in reverse order, such that the 1's digit is at the head of the list. Write a function that adds the two numbers and returns the sum as a linked list. Example Given 7-&gt;1-&gt;6 + 5-&gt;9-&gt;2. That is, 617 + 295. Return 2-&gt;1-&gt;9. That is 912. Given 3-&gt;1-&gt;5 and 5-&gt;9-&gt;2, return 8-&gt;0-&gt;8. 题解 一道看似简单的进位加法题，实则杀机重重，不信你不看答案自己先做做看。 首先由十进制加法可知应该注意进位的处理，但是这道题仅注意到这点就够了吗？还不够！因为两个链表长度有可能不等长！因此这道题的亮点在于边界和异常条件的处理，感谢 @wen 引入的 dummy 节点，处理起来更为优雅！ Python 1234567891011121314151617181920212223242526272829# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def add_two_numbers(self, l1, l2): ''' :type l1: ListNode :type l2: ListNode :rtype: ListNode ''' carry = 0 dummy = prev = ListNode(-1) while l1 or l2 or carry: v1 = l1.val if l1 else 0 v2 = l2.val if l2 else 0 val = (v1 + v2 + carry) % 10 carry = (v1 + v2 + carry) / 10 prev.next = ListNode(val) prev = prev.next if l1: l1 = l1.next if l2: l2 = l2.next return dummy.next C++ 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { ListNode dummy(0); ListNode *curr = &amp;dummy; int carry = 0; while ((l1 != NULL) || (l2 != NULL) || (carry != 0)) { int l1_val = (l1 != NULL) ? l1-&gt;val : 0; int l2_val = (l2 != NULL) ? l2-&gt;val : 0; int sum = carry + l1_val + l2_val; carry = sum / 10; curr-&gt;next = new ListNode(sum % 10); curr = curr-&gt;next; if (l1 != NULL) l1 = l1-&gt;next; if (l2 != NULL) l2 = l2-&gt;next; } return dummy.next; }}; Java 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */public class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode dummy = new ListNode(0); ListNode curr = dummy; int carry = 0; while ((l1 != null) || (l2 != null) || (carry != 0)) { int l1_val = (l1 != null) ? l1.val : 0; int l2_val = (l2 != null) ? l2.val : 0; int sum = carry + l1_val + l2_val; // update carry carry = sum / 10; curr.next = new ListNode(sum % 10); curr = curr.next; if (l1 != null) l1 = l1.next; if (l2 != null) l2 = l2.next; } return dummy.next; }} 源码分析 迭代能正常进行的条件为(NULL != l1) || (NULL != l2) || (0 != carry), 缺一不可。 对于空指针节点的处理可以用相对优雅的方式处理 - int l1_val = (NULL == l1) ? 0 : l1-&gt;val; 生成新节点时需要先判断迭代终止条件 - (NULL == l1) &amp;&amp; (NULL == l2) &amp;&amp; (0 == carry), 避免多生成一位数0。 使用 dummy 节点可避免这一情况。 复杂度分析 没啥好分析的，时间和空间复杂度均为 \\[O(max(L1, L2))\\]. Reference CC150 Chapter 9.2 题2.5，中文版 p123 Add two numbers represented by linked lists | Set 1 - GeeksforGeeks","link":"/posts/1690892303.html"},{"title":"Balanced Binary Tree","text":"Question leetcode: Balanced Binary Tree | LeetCode OJ lintcode: (93) Balanced Binary Tree Problem Statement Given a binary tree, determine if it is height-balanced. For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. Example Given binary tree A={3,9,20,#,#,15,7}, B={3,#,20,15,7} 12345A) 3 B) 3 / \\ \\ 9 20 20 / \\ / \\ 15 7 15 7 The binary tree A is a height-balanced binary tree, but B is not. 题解1 - 递归 根据题意，平衡树的定义是两子树的深度差最大不超过1，显然使用递归进行分析较为方便。既然使用递归，那么接下来就需要分析递归调用的终止条件。和之前的 Maximum Depth of Binary Tree | Algorithm 类似，NULL == root必然是其中一个终止条件，返回0；根据题意还需的另一终止条件应为「左右子树高度差大于1」，但对应此终止条件的返回值是多少？——INT_MAX or INT_MIN？想想都不合适，为何不在传入参数中传入bool指针或者bool引用咧？并以此变量作为最终返回值，此法看似可行，先来看看鄙人最开始想到的这种方法。 C++ Recursion with extra bool variable 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Definition of TreeNode: * class TreeNode { * public: * int val; * TreeNode *left, *right; * TreeNode(int val) { * this-&gt;val = val; * this-&gt;left = this-&gt;right = NULL; * } * } */class Solution {public: /** * @param root: The root of binary tree. * @return: True if this Binary tree is Balanced, or false. */ bool isBalanced(TreeNode *root) { if (NULL == root) { return true; } bool result = true; maxDepth(root, result); return result; }private: int maxDepth(TreeNode *root, bool &amp;isBalanced) { if (NULL == root) { return 0; } int leftDepth = maxDepth(root-&gt;left, isBalanced); int rightDepth = maxDepth(root-&gt;right, isBalanced); if (abs(leftDepth - rightDepth) &gt; 1) { isBalanced = false; // speed up the recursion process return INT_MAX; } return max(leftDepth, rightDepth) + 1; }}; 源码解析 如果在某一次子树高度差大于1时，返回INT_MAX以减少不必要的计算过程，加速整个递归调用的过程。 初看起来上述代码好像还不错的样子，但是在看了九章的实现后，瞬间觉得自己弱爆了... 首先可以确定abs(leftDepth - rightDepth) &gt; 1肯定是需要特殊处理的，如果返回-1呢？咋一看似乎在下一步返回max(leftDepth, rightDepth) + 1时会出错，再进一步想想，我们能否不让max...这一句执行呢？如果返回了-1，其接盘侠必然是leftDepth或者rightDepth中的一个，因此我们只需要在判断子树高度差大于1的同时也判断下左右子树深度是否为-1即可都返回-1，不得不说这种处理方法要精妙的多，赞！ C++ 123456789101112131415161718192021222324252627282930313233343536373839/** * forked from http://www.jiuzhang.com/solutions/balanced-binary-tree/ * Definition of TreeNode: * class TreeNode { * public: * int val; * TreeNode *left, *right; * TreeNode(int val) { * this-&gt;val = val; * this-&gt;left = this-&gt;right = NULL; * } * } */class Solution {public: /** * @param root: The root of binary tree. * @return: True if this Binary tree is Balanced, or false. */ bool isBalanced(TreeNode *root) { return (-1 != maxDepth(root)); }private: int maxDepth(TreeNode *root) { if (NULL == root) { return 0; } int leftDepth = maxDepth(root-&gt;left); int rightDepth = maxDepth(root-&gt;right); if (leftDepth == -1 || rightDepth == -1 || \\ abs(leftDepth - rightDepth) &gt; 1) { return -1; } return max(leftDepth, rightDepth) + 1; }}; Java 12345678910111213141516171819202122232425262728/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */public class Solution { public boolean isBalanced(TreeNode root) { return maxDepth(root) != -1; } private int maxDepth(TreeNode root) { if (root == null) return 0; int leftDepth = maxDepth(root.left); int rightDepth = maxDepth(root.right); if (leftDepth == -1 || rightDepth == -1 || Math.abs(leftDepth - rightDepth) &gt; 1) { return -1; } return 1 + Math.max(leftDepth, rightDepth); }} 源码分析 抓住两个核心：子树的高度以及高度之差，返回值应该包含这两种信息。 复杂度分析 遍历所有节点各一次，时间复杂度为 \\[O(n)\\], 使用了部分辅助变量，空间复杂度 \\[O(1)\\].","link":"/posts/1545358800.html"},{"title":"Bipartial Graph - Part I - 二分图一•二分图判定","text":"Question hihoCoder Problem Statement 时间限制:10000ms 单点时限:1000ms 内存限制:256MB 描述 大家好，我是小Hi和小Ho的小伙伴Nettle，从这个星期开始由我来完成我们的Weekly。 新年回家，又到了一年一度大龄剩男剩女的相亲时间。Nettle去姑姑家玩的时候看到了一张姑姑写的相亲情况表，上面都是姑姑介绍相亲的剩男剩女们。每行有2个名字， 表示这两个人有一场相亲。由于姑姑年龄比较大了记性不是太好，加上相亲的人很多，所以姑姑一时也想不起来其中有些人的性别。因此她拜托我检查一下相亲表里面有没有错误 的记录，即是否把两个同性安排了相亲。 OK，让我们愉快的暴力搜索吧！ 才怪咧。 对于拿到的相亲情况表，我们不妨将其转化成一个图。将每一个人作为一个点(编号1..N)，若两个人之间有一场相亲，则在对应的点之间连接一条无向边。(如下图) img1 因为相亲总是在男女之间进行的，所以每一条边的两边对应的人总是不同性别。假设表示男性的节点染成白色，女性的节点染色黑色。对于得到的无向图来说，即每一条边的两端 一定是一白一黑。如果存在一条边两端同为白色或者黑色，则表示这一条边所表示的记录有误。 由于我们并不知道每个人的性别，我们的问题就转化为判定是否存在一个合理的染色方案，使得我们所建立的无向图满足每一条边两端的顶点颜色都不相同。 那么，我们不妨将所有的点初始为未染色的状态。随机选择一个点，将其染成白色。再以它为起点，将所有相邻的点染成黑色。再以这些黑色的点为起点，将所有与其相邻未染色 的点染成白色。不断重复直到整个图都染色完成。(如下图) img2 在染色的过程中，我们应该怎样发现错误的记录呢？相信你一定发现了吧。对于一个已经染色的点，如果存在一个与它相邻的已染色点和它的颜色相同，那么就一定存在一条错误 的记录。(如上图的4，5节点) 到此我们就得到了整个图的算法： 选取一个未染色的点u进行染色 遍历u的相邻节点v：若v未染色，则染色成与u不同的颜色，并对v重复第2步；若v已经染色，如果 u和v颜色相同，判定不可行退出遍历。 若所有节点均已染色，则判定可行。 接下来就动手写写吧！ 输入 第1行：1个正整数T(1≤T≤10) 接下来T组数据，每组数据按照以下格式给出： 第1行：2个正整数N,M(1≤N≤10,000，1≤M≤40,000) 第2..M+1行：每行两个整数u,v表示u和v之间有一条边 输出 第1..T行：第i行表示第i组数据是否有误。如果是正确的数据输出”Correct”，否则输出”Wrong” 样例输入 2 5 5 1 2 1 3 3 4 5 2 1 5 5 5 1 2 1 3 3 4 5 2 3 5 样例输出 Wrong Correct 题解 二分图中最简单的题，思路原文中已提到，这里就不赘述了，简单实现的话可以使用二维数组，如果要模拟图的操作的话可以自定义类。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.util.*;import java.util.Queue;class UndirectedGraphNode { int label; int color; ArrayList&lt;UndirectedGraphNode&gt; neighbors; UndirectedGraphNode(int x) { this.label = x; this.color = 0; this.neighbors = new ArrayList&lt;UndirectedGraphNode&gt;(); }}public class Main { public static void main(String[] args) { Scanner in = new Scanner(System.in); int T = in.nextInt(); for (int i = 1; i &lt;= T; i++) { int N = in.nextInt(); int M = in.nextInt(); // initialize graph List&lt;UndirectedGraphNode&gt; graph = new ArrayList&lt;UndirectedGraphNode&gt;(); for (int n = 1; n &lt;= N; n++) { graph.add(new UndirectedGraphNode(n)); } // construct graph for (int j = 1; j &lt;= M; j++) { int u = in.nextInt(), v = in.nextInt(); graph.get(u - 1).neighbors.add(graph.get(v - 1)); graph.get(v - 1).neighbors.add(graph.get(u - 1)); } // solve if (solve(graph)) { System.out.println(&quot;Correct&quot;); } else { System.out.println(&quot;Wrong&quot;); } } } public static boolean solve(List&lt;UndirectedGraphNode&gt; graph) { // 1 for white, -1 for black, 0 for uncolored for (UndirectedGraphNode node : graph) { if (node.color == 0) { node.color = 1; Queue&lt;UndirectedGraphNode&gt; q = new LinkedList&lt;UndirectedGraphNode&gt;(); q.offer(node); while (!q.isEmpty()) { int qSize = q.size(); for (int i = 0; i &lt; qSize; i++) { UndirectedGraphNode qNode = q.poll(); for (UndirectedGraphNode neighbor : qNode.neighbors) { if (neighbor.color == 0) { neighbor.color = -1 * qNode.color; q.offer(neighbor); } else if (neighbor.color + qNode.color != 0) { // the color of qNode is the same with neighbor return false; } } } } } } return true; }} 源码分析 使用 BFS 不容易爆栈。 复杂度分析 时间复杂度 \\[O(V + E)\\].","link":"/posts/3795995408.html"},{"title":"二分&#96;k-means&#96;算法","text":"二分k-means算法是层次聚类（Hierarchical clustering）的一种，层次聚类是聚类分析中常用的方法。 层次聚类的策略一般有两种： 聚合。这是一种自底向上的方法，每一个观察者初始化本身为一类，然后两两结合 分裂。这是一种自顶向下的方法，所有观察者初始化为一类，然后递归地分裂它们 二分k-means算法是分裂法的一种。 ## 二分k-means的步骤 二分k-means算法是k-means算法的改进算法，相比k-means算法，它有如下优点： 二分k-means算法可以加速k-means算法的执行速度，因为它的相似度计算少了 能够克服k-means收敛于局部最小的缺点 二分k-means算法的一般流程如下所示： （1）把所有数据初始化为一个簇，将这个簇分为两个簇。 （2）选择满足条件的可以分解的簇。选择条件综合考虑簇的元素个数以及聚类代价（也就是误差平方和SSE），误差平方和的公式如下所示，其中\\(w_{(i)}\\)表示权重值，\\(y^{*}\\)表示该簇所有点的平均值。 （3）使用k-means算法将可分裂的簇分为两簇。 （4）一直重复（2）（3）步，直到满足迭代结束条件。 以上过程隐含着一个原则是：因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于它们的质心，聚类效果就越好。 所以我们就需要对误差平方和最大的簇进行再一次的划分，因为误差平方和越大，表示该簇聚类越不好，越有可能是多个簇被当成一个簇了，所以我们首先需要对这个簇进行划分。 二分k-means的源码分析 spark在文件org.apache.spark.mllib.clustering.BisectingKMeans中实现了二分k-means算法。在分步骤分析算法实现之前，我们先来了解BisectingKMeans类中参数代表的含义。 12345class BisectingKMeans private ( private var k: Int, private var maxIterations: Int, private var minDivisibleClusterSize: Double, private var seed: Long) 上面代码中，k表示叶子簇的期望数，默认情况下为4。如果没有可被切分的叶子簇，实际值会更小。maxIterations表示切分簇的k-means算法的最大迭代次数，默认为20。 minDivisibleClusterSize的值如果大于等于1，它表示一个可切分簇的最小点数量；如果值小于1，它表示可切分簇的点数量占总数的最小比例，该值默认为1。 BisectingKMeans的run方法实现了二分k-means算法，下面将一步步分析该方法的实现过程。 （1）初始化数据 123//计算输入数据的二范式并转化为VectorWithNormval norms = input.map(v =&gt; Vectors.norm(v, 2.0)).persist(StorageLevel.MEMORY_AND_DISK)val vectors = input.zip(norms).map { case (x, norm) =&gt; new VectorWithNorm(x, norm) } （2）将所有数据初始化为一个簇，并计算代价 123var assignments = vectors.map(v =&gt; (ROOT_INDEX, v))var activeClusters = summarize(d, assignments) //格式为Map[index,ClusterSummary]val rootSummary = activeClusters(ROOT_INDEX) 在上述代码中，第一行给每个向量加上一个索引，用以标明簇在最终生成的树上的深度，ROOT_INDEX的值为1。summarize方法计算误差平方和，我们来看看它的实现。 1234567891011private def summarize( d: Int, assignments: RDD[(Long, VectorWithNorm)]): Map[Long, ClusterSummary] = { assignments.aggregateByKey(new ClusterSummaryAggregator(d))( //分区内循环添加 seqOp = (agg, v) =&gt; agg.add(v), //分区间合并 combOp = (agg1, agg2) =&gt; agg1.merge(agg2) ).mapValues(_.summary) .collect().toMap} 这里的d表示特征维度，代码对assignments使用aggregateByKey操作，根据key值在分区内循环添加（add）数据，在分区间合并（merge）数据集，转换成最终ClusterSummaryAggregator对象，然后针对每个key，调用summary方法，计算。 ClusterSummaryAggregator包含三个很简单的方法，分别是add，merge以及summary。 1234567891011121314151617181920212223242526272829303132private class ClusterSummaryAggregator(val d: Int) extends Serializable { private var n: Long = 0L private val sum: Vector = Vectors.zeros(d) //向量和 private var sumSq: Double = 0.0 //向量的范数平方和 //添加一个VectorWithNorm对象到ClusterSummaryAggregator对象中 def add(v: VectorWithNorm): this.type = { n += 1L sumSq += v.norm * v.norm BLAS.axpy(1.0, v.vector, sum) this } //合并两个ClusterSummaryAggregator对象 def merge(other: ClusterSummaryAggregator): this.type = { n += other.n sumSq += other.sumSq //y += a * x BLAS.axpy(1.0, other.sum, sum) this } def summary: ClusterSummary = { //求平均值 val mean = sum.copy if (n &gt; 0L) { //x = a * x BLAS.scal(1.0 / n, mean) } val center = new VectorWithNorm(mean) //所有点的范数平方和减去n乘以中心点范数平方，得到误差平方和 val cost = math.max(sumSq - n * center.norm * center.norm, 0.0) new ClusterSummary(n, center, cost) } } 这里计算误差平方和与第一章的公式有所不同，但是效果一致。这里计算聚类代价函数的公式如下所示： 获取第一个簇之后，我们需要做的就是迭代分裂可分裂的簇，直到满足我们的要求。迭代停止的条件是activeClusters为空，或者numLeafClustersNeeded为0（即没有分裂的叶子簇）,或者迭代深度大于LEVEL_LIMIT。 1while (activeClusters.nonEmpty &amp;&amp; numLeafClustersNeeded &gt; 0 &amp;&amp; level &lt; LEVEL_LIMIT) 这里，LEVEL_LIMIT是一个较大的值，计算方法如下。 1private val LEVEL_LIMIT = math.log10(Long.MaxValue) / math.log10(2) （3）获取需要分裂的簇 在每一次迭代中，我们首先要做的是获取满足条件的可以分裂的簇。 1234567891011//选择需要分裂的簇var divisibleClusters = activeClusters.filter { case (_, summary) =&gt; (summary.size &gt;= minSize) &amp;&amp; (summary.cost &gt; MLUtils.EPSILON * summary.size)}// If we don't need all divisible clusters, take the larger ones.if (divisibleClusters.size &gt; numLeafClustersNeeded) { divisibleClusters = divisibleClusters.toSeq.sortBy { case (_, summary) =&gt; -summary.size }.take(numLeafClustersNeeded) .toMap} 这里选择分裂的簇用到了两个条件，即数据点的数量大于规定的最小数量以及代价小于等于MLUtils.EPSILON * summary.size。并且如果可分解的簇的个数多余我们规定的个数numLeafClustersNeeded即(k-1)， 那么我们取包含数量最多的numLeafClustersNeeded个簇用于分裂。 （4）使用k-means算法将可分裂的簇分解为两簇 我们知道，k-means算法分为两步，第一步是初始化中心点，第二步是迭代更新中心点直至满足最大迭代数或者收敛。下面就分两步来说明。 第一步，随机的选择中心点，将可分裂簇分为两簇 123456 //切分簇var newClusterCenters = divisibleClusters.flatMap { case (index, summary) =&gt; //随机切分簇为两簇，找到这两个簇的中心点 val (left, right) = splitCenter(summary.center, random) Iterator((leftChildIndex(index), left), (rightChildIndex(index), right))}.map(identity) 在上面的代码中，用splitCenter方法将簇随机地分为了两簇，并返回相应的中心点，它的实现如下所示。 1234567891011121314151617private def splitCenter( center: VectorWithNorm, random: Random): (VectorWithNorm, VectorWithNorm) = { val d = center.vector.size val norm = center.norm val level = 1e-4 * norm //随机的初始化一个点，并用这个点得到两个初始中心点 val noise = Vectors.dense(Array.fill(d)(random.nextDouble())) val left = center.vector.copy //y += a * x,left=left-level*noise BLAS.axpy(-level, noise, left) val right = center.vector.copy //right=right+level*noise BLAS.axpy(level, noise, right) //返回中心点 (new VectorWithNorm(left), new VectorWithNorm(right)) } 第二步，迭代更新中心点 123456789101112131415var newClusters: Map[Long, ClusterSummary] = nullvar newAssignments: RDD[(Long, VectorWithNorm)] = null//迭代获得中心点，默认迭代次数为20for (iter &lt;- 0 until maxIterations) { //根据更新的中心点，将数据点重新分类 newAssignments = updateAssignments(assignments, divisibleIndices, newClusterCenters) .filter { case (index, _) =&gt; divisibleIndices.contains(parentIndex(index)) } //计算中心点以及代价值 newClusters = summarize(d, newAssignments) newClusterCenters = newClusters.mapValues(_.center).map(identity)}val indices = updateAssignments(assignments, divisibleIndices, newClusterCenters).keys .persist(StorageLevel.MEMORY_AND_DISK) 这段代码中，updateAssignments会根据更新的中心点将数据分配给距离其最短的中心点所在的簇，即重新分配簇。代码如下 1234567891011121314151617private def updateAssignments(assignments: RDD[(Long, VectorWithNorm)],divisibleIndices: Set[Long], newClusterCenters: Map[Long, VectorWithNorm]): RDD[(Long, VectorWithNorm)] = { assignments.map { case (index, v) =&gt; if (divisibleIndices.contains(index)) { //leftChildIndex=2*index , rightChildIndex=2*index+1 val children = Seq(leftChildIndex(index), rightChildIndex(index)) //返回序列中第一个符合条件的最小的元素 val selected = children.minBy { child =&gt; KMeans.fastSquaredDistance(newClusterCenters(child), v) } //将v分配给中心点距离其最短的簇 (selected, v) } else { (index, v) } } } 重新分配簇之后，利用summarize方法重新计算中心点以及代价值。 （5）处理变量值为下次迭代作准备 123456//数节点中簇的index以及包含的数据点 assignments = indices.zip(vectors) inactiveClusters ++= activeClusters activeClusters = newClusters //调整所需簇的数量 numLeafClustersNeeded -= divisibleClusters.size","link":"/posts/2817666893.html"},{"title":"Combination Sum","text":"Question leetcode: Combination Sum | LeetCode OJ lintcode: (135) Combination Sum 123456789101112131415161718192021Given a set of candidate numbers (C) and a target number (T),find all unique combinations in C where the candidate numbers sums to T.The same repeated number may be chosen from C unlimited number of times.For example, given candidate set 2,3,6,7 and target 7,A solution set is:[7][2, 2, 3]Have you met this question in a real interview? YesExamplegiven candidate set 2,3,6,7 and target 7,A solution set is:[7][2, 2, 3]Note- All numbers (including target) will be positive integers.- Elements in a combination (a1, a2, … , ak) must be in non-descending order.(ie, a1 ≤ a2 ≤ … ≤ ak).- The solution set must not contain duplicate combinations. 题解 和 Permutations 十分类似，区别在于剪枝函数不同。这里允许一个元素被多次使用，故递归时传入的索引值不自增，而是由 for 循环改变。 Java 12345678910111213141516171819202122232425262728293031323334353637public class Solution { /** * @param candidates: A list of integers * @param target:An integer * @return: A list of lists of integers */ public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) { List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;List&lt;Integer&gt;&gt;(); List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if (candidates == null) return result; Arrays.sort(candidates); helper(candidates, 0, target, list, result); return result; } private void helper(int[] candidates, int pos, int gap, List&lt;Integer&gt; list, List&lt;List&lt;Integer&gt;&gt; result) { if (gap == 0) { // add new object for result result.add(new ArrayList&lt;Integer&gt;(list)); return; } for (int i = pos; i &lt; candidates.length; i++) { // cut invalid candidate if (gap &lt; candidates[i]) { return; } list.add(candidates[i]); helper(candidates, i, gap - candidates[i], list, result); list.remove(list.size() - 1); } }} 源码分析 对数组首先进行排序是必须的，递归函数中本应该传入 target 作为入口参数，这里借用了 Soulmachine 的实现，使用 gap 更容易理解。注意在将临时 list 添加至 result 中时需要 new 一个新的对象。 复杂度分析 按状态数进行分析，时间复杂度 \\[O(n!)\\], 使用了list 保存中间结果，空间复杂度 \\[O(n)\\]. Reference Soulmachine 的 leetcode 题解","link":"/posts/2987533676.html"},{"title":"Construct Binary Tree from Inorder and Postorder Traversal","text":"Question lintcode: (72) Construct Binary Tree from Inorder and Postorder Traversal 123456789Given inorder and postorder traversal of a tree, construct the binary tree.ExampleGiven inorder [1,2,3] and postorder [1,3,2], return a tree: 2 / \\ 1 3 Note You may assume that duplicates do not exist in the tree. 题解 和题 Construct Binary Tree from Preorder and Inorder Traversal 几乎一致，关键在于找到中序遍历中的根节点和左右子树，递归解决。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Definition of TreeNode: * public class TreeNode { * public int val; * public TreeNode left, right; * public TreeNode(int val) { * this.val = val; * this.left = this.right = null; * } * } */public class Solution { /** *@param inorder : A list of integers that inorder traversal of a tree *@param postorder : A list of integers that postorder traversal of a tree *@return : Root of a tree */ public TreeNode buildTree(int[] inorder, int[] postorder) { if (inorder == null || postorder == null) return null; if (inorder.length == 0 || postorder.length == 0) return null; if (inorder.length != postorder.length) return null; TreeNode root = helper(inorder, 0, inorder.length - 1, postorder, 0, postorder.length - 1); return root; } private TreeNode helper(int[] inorder, int instart, int inend, int[] postorder, int poststart, int postend) { // corner cases if (instart &gt; inend || poststart &gt; postend) return null; // build root TreeNode int root_val = postorder[postend]; TreeNode root = new TreeNode(root_val); // find index of root_val in inorder[] int index = findIndex(inorder, instart, inend, root_val); // build left subtree root.left = helper(inorder, instart, index - 1, postorder, poststart, poststart + index - instart - 1); // build right subtree root.right = helper(inorder, index + 1, inend, postorder, poststart + index - instart, postend - 1); return root; } private int findIndex(int[] nums, int start, int end, int target) { for (int i = start; i &lt;= end; i++) { if (nums[i] == target) return i; } return -1; }} 源码分析 找根节点的方法作为私有方法，辅助函数需要注意索引范围。 复杂度分析 找根节点近似 \\[O(n)\\], 递归遍历整个数组，嵌套找根节点的方法，故总的时间复杂度为 \\[O(n^2)\\].","link":"/posts/2270614219.html"},{"title":"卡方选择器","text":"特征选择试图识别相关的特征用于模型构建。它改变特征空间的大小，它可以提高速度以及统计学习行为。ChiSqSelector实现卡方特征选择，它操作于带有类别特征的标注数据。 ChiSqSelector根据独立的卡方测试对特征进行排序，然后选择排序最高的特征。下面是一个使用的例子。 1234567891011121314151617181920import org.apache.spark.SparkContext._import org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.mllib.util.MLUtilsimport org.apache.spark.mllib.feature.ChiSqSelector// 加载数据val data = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)// 卡方分布需要类别特征，所以对特征除一个整数。虽然特征是double类型，//但是ChiSqSelector将每个唯一的值当做一个类别val discretizedData = data.map { lp =&gt; LabeledPoint(lp.label, Vectors.dense(lp.features.toArray.map { x =&gt; (x / 16).floor } ) )}// Create ChiSqSelector that will select top 50 of 692 featuresval selector = new ChiSqSelector(50)// Create ChiSqSelector model (selecting features)val transformer = selector.fit(discretizedData)// Filter the top 50 features from each feature vectorval filteredData = discretizedData.map { lp =&gt; LabeledPoint(lp.label, transformer.transform(lp.features)) } 下面看看选择特征的实现，入口函数是fit。 123456789def fit(data: RDD[LabeledPoint]): ChiSqSelectorModel = { //计算数据卡方值 val indices = Statistics.chiSqTest(data) .zipWithIndex.sortBy { case (res, _) =&gt; -res.statistic } .take(numTopFeatures) .map { case (_, indices) =&gt; indices } .sorted new ChiSqSelectorModel(indices) } 这里通过Statistics.chiSqTest计算卡方检测的值。下面需要了解卡方检测的理论基础。 卡方检测 什么是卡方检测 卡方检验是一种用途很广的计数资料的假设检验方法。它属于非参数检验的范畴，主要是比较两个及两个以上样本率( 构成比）以及两个分类变量的关联性分析。 其根本思想就是在于比较理论频数和实际频数的吻合程度或拟合优度问题。 卡方检测的基本思想 卡方检验是以\\({X}^{2}\\)分布为基础的一种常用假设检验方法，它的无效假设H0是：观察频数与期望频数没有差别。 该检验的基本思想是：首先假设H0成立，基于此前提计算出\\({X}^{2}\\)值，它表示观察值与理论值之间的偏离程度。根据\\({X}^{2}\\)分布及自由度可以确定在H0假设成立的情况下获得当前统计量及更极端情况的概率P。 如果P值很小，说明观察值与理论值偏离程度太大，应当拒绝无效假设，表示比较资料之间有显著差异；否则就不能拒绝无效假设，尚不能认为样本所代表的实际情况和理论假设有差别。 卡方值的计算与意义 卡方值表示观察值与理论值之问的偏离程度。计算这种偏离程度的基本思路如下。 设A代表某个类别的观察频数，E代表基于H0计算出的期望频数，A与E之差称为残差。 残差可以表示某一个类别观察值和理论值的偏离程度，但如果将残差简单相加以表示各类别观察频数与期望频数的差别，则有一定的不足之处。 因为残差有正有负，相加后会彼此抵消，总和仍然为0，为此可以将残差平方后求和。 另一方面，残差大小是一个相对的概念，相对于期望频数为10时，期望频数为20的残差非常大，但相对于期望频数为1000时20的残差就很小了。 考虑到这一点，人们又将残差平方除以期望频数再求和，以估计观察频数与期望频数的差别。 进行上述操作之后，就得到了常用的\\({X}^{2}\\)统计量。其计算公式是： 当n比较大时，卡方统计量近似服从k-1(计算E_i时用到的参数个数)个自由度的卡方分布。由卡方的计算公式可知，当观察频数与期望频数完全一致时，卡方值为0；观察频数与期望频数越接近，两者之间的差异越小，卡方值越小； 反之，观察频数与期望频数差别越大，两者之间的差异越大，卡方值越大。 卡方检测的源码实现 在MLlib中，使用chiSquaredFeatures方法实现卡方检测。它为每个特征进行皮尔森独立检测。下面看它的代码实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def chiSquaredFeatures(data: RDD[LabeledPoint], methodName: String = PEARSON.name): Array[ChiSqTestResult] = { val maxCategories = 10000 val numCols = data.first().features.size val results = new Array[ChiSqTestResult](numCols) var labels: Map[Double, Int] = null // 某个时刻至少1000列 val batchSize = 1000 var batch = 0 while (batch * batchSize &lt; numCols) { val startCol = batch * batchSize val endCol = startCol + math.min(batchSize, numCols - startCol) val pairCounts = data.mapPartitions { iter =&gt; val distinctLabels = mutable.HashSet.empty[Double] val allDistinctFeatures: Map[Int, mutable.HashSet[Double]] = Map((startCol until endCol).map(col =&gt; (col, mutable.HashSet.empty[Double])): _*) var i = 1 iter.flatMap { case LabeledPoint(label, features) =&gt; if (i % 1000 == 0) { if (distinctLabels.size &gt; maxCategories) { throw new SparkException } allDistinctFeatures.foreach { case (col, distinctFeatures) =&gt; if (distinctFeatures.size &gt; maxCategories) { throw new SparkException } } } i += 1 distinctLabels += label features.toArray.view.zipWithIndex.slice(startCol, endCol).map { case (feature, col) =&gt; allDistinctFeatures(col) += feature (col, feature, label) } } }.countByValue() if (labels == null) { // Do this only once for the first column since labels are invariant across features. labels = pairCounts.keys.filter(_._1 == startCol).map(_._3).toArray.distinct.zipWithIndex.toMap } val numLabels = labels.size pairCounts.keys.groupBy(_._1).map { case (col, keys) =&gt; val features = keys.map(_._2).toArray.distinct.zipWithIndex.toMap val numRows = features.size val contingency = new BDM(numRows, numLabels, new Array[Double](numRows * numLabels)) keys.foreach { case (_, feature, label) =&gt; val i = features(feature) val j = labels(label) //带有标签的特征的出现次数 contingency(i, j) += pairCounts((col, feature, label)) } results(col) = chiSquaredMatrix(Matrices.fromBreeze(contingency), methodName) } batch += 1 } results } 上述代码主要对数据进行处理，获取带有标签的特征的出现次数，并用这个次数计算卡方值。真正获取卡方值的函数是chiSquaredMatrix。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def chiSquaredMatrix(counts: Matrix, methodName: String = PEARSON.name): ChiSqTestResult = { val method = methodFromString(methodName) val numRows = counts.numRows val numCols = counts.numCols // get row and column sums val colSums = new Array[Double](numCols) val rowSums = new Array[Double](numRows) val colMajorArr = counts.toArray val colMajorArrLen = colMajorArr.length var i = 0 while (i &lt; colMajorArrLen) { val elem = colMajorArr(i) if (elem &lt; 0.0) { throw new IllegalArgumentException(&quot;Contingency table cannot contain negative entries.&quot;) } //每列的总数 colSums(i / numRows) += elem //每行的总数 rowSums(i % numRows) += elem i += 1 } //所有元素的总和 val total = colSums.sum // second pass to collect statistic var statistic = 0.0 var j = 0 while (j &lt; colMajorArrLen) { val col = j / numRows val colSum = colSums(col) if (colSum == 0.0) { throw new IllegalArgumentException(&quot;Chi-squared statistic undefined for input matrix due to&quot; + s&quot;0 sum in column [$col].&quot;) } val row = j % numRows val rowSum = rowSums(row) if (rowSum == 0.0) { throw new IllegalArgumentException(&quot;Chi-squared statistic undefined for input matrix due to&quot; + s&quot;0 sum in row [$row].&quot;) } //期望值 val expected = colSum * rowSum / total //PEARSON statistic += method.chiSqFunc(colMajorArr(j), expected) j += 1 } //自由度 val df = (numCols - 1) * (numRows - 1) if (df == 0) { // 1 column or 1 row. Constant distribution is independent of anything. // pValue = 1.0 and statistic = 0.0 in this case. new ChiSqTestResult(1.0, 0, 0.0, methodName, NullHypothesis.independence.toString) } else { //计算累积概率 val pValue = 1.0 - new ChiSquaredDistribution(df).cumulativeProbability(statistic) new ChiSqTestResult(pValue, df, statistic, methodName, NullHypothesis.independence.toString) } } //上述代码中的method.chiSqFunc(colMajorArr(j), expected)，调用下面的代码 val PEARSON = new Method(&quot;pearson&quot;, (observed: Double, expected: Double) =&gt; { val dev = observed - expected dev * dev / expected }) 上述代码的实现和参考文献【2】中Test of independence的描述一致。 参考文献 【1】卡方检验 【2】Pearson's chi-squared test","link":"/posts/3485476636.html"},{"title":"数据类型","text":"MLlib既支持保存在单台机器上的本地向量和矩阵，也支持备份在一个或多个RDD中的分布式矩阵。本地向量和本地矩阵是简单的数据模型，作为公共接口提供。底层的线性代数操作通过Breeze和jblas提供。 在MLlib中，用于有监督学习的训练样本称为标注点(labeled point)。 # 本地向量(Local vector) 一个本地向量拥有从0开始的integer类型的索引以及double类型的值，它保存在单台机器上面。MLlib支持两种类型的本地向量：稠密(dense)向量和稀疏(sparse)向量。 一个稠密向量通过一个double类型的数组保存数据，这个数组表示向量的条目值(entry values)；一个稀疏向量通过两个并行的数组（indices和values）保存数据。例如，一个向量 (1.0, 0.0, 3.0)可以以稠密的格式保存为[1.0, 0.0, 3.0]或者以稀疏的格式保存为(3, [0, 2], [1.0, 3.0])，其中3表示数组的大小。 本地向量的基类是Vector，Spark提供了两种实现： DenseVector和SparseVector。 Spark官方推荐使用Vectors中实现的工厂方法去创建本地向量。下面是创建本地向量的例子。 1234567import org.apache.spark.mllib.linalg.{Vector, Vectors}// 创建一个dense vector (1.0, 0.0, 3.0).val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)// 创建一个sparse vector (1.0, 0.0, 3.0)并且指定它的索引和值val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))// 创建一个sparse vector (1.0, 0.0, 3.0)并且指定它的索引和值val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0))) 注意，Scala默认引入scala.collection.immutable.Vector，这里我们需要主动引入MLLib中的org.apache.spark.mllib.linalg.Vector来操作。我们可以看看Vectors对象的部分方法。 1234567891011121314151617def dense(firstValue: Double, otherValues: Double*): Vector = new DenseVector((firstValue +: otherValues).toArray)def dense(values: Array[Double]): Vector = new DenseVector(values)def sparse(size: Int, indices: Array[Int], values: Array[Double]): Vector = new SparseVector(size, indices, values)def sparse(size: Int, elements: Seq[(Int, Double)]): Vector = { require(size &gt; 0, &quot;The size of the requested sparse vector must be greater than 0.&quot;) val (indices, values) = elements.sortBy(_._1).unzip var prev = -1 indices.foreach { i =&gt; require(prev &lt; i, s&quot;Found duplicate indices: $i.&quot;) prev = i } require(prev &lt; size, s&quot;You may not write an element to index $prev because the declared &quot; + s&quot;size of your vector is $size&quot;) new SparseVector(size, indices.toArray, values.toArray) } 标注点(Labeled point) 一个标注点就是一个本地向量（或者是稠密的或者是稀疏的），这个向量和一个标签或者响应相关联。在MLlib中，标注点用于有监督学习算法。我们用一个double存储标签，这样我们就可以在回归和分类中使用标注点。 对于二分类，一个标签可能是0或者是1；对于多分类，一个标签可能代表从0开始的类别索引。 在MLlib中，一个标注点通过样本类LabeledPoint表示。 123456789@Since(&quot;0.8.0&quot;)@BeanInfocase class LabeledPoint @Since(&quot;1.0.0&quot;) ( @Since(&quot;0.8.0&quot;) label: Double, @Since(&quot;1.0.0&quot;) features: Vector) { override def toString: String = { s&quot;($label,$features)&quot; }} 下面是使用LabeledPoint的一个例子。 123456import org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPoint// Create a labeled point with a positive label and a dense feature vector.val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))// Create a labeled point with a negative label and a sparse feature vector.val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))) 在现实的应用中，训练数据是稀疏的情况非常常见，MLlib支持读取训练数据存储为LIBSVM格式。它是LIBSVM和LIBLINEAR默认的格式。 它是一种文本格式，每一行表示一个标注的稀疏特征向量，如下所示： 1label index1:value1 index2:value2 ... 本地矩阵（Local matrix） 一个本地矩阵拥有Integer类型的行和列索引以及Double类型的值。MLlib支持稠密矩阵和稀疏矩阵两种。稠密矩阵将条目(entry)值保存为单个double数组，这个数组根据列的顺序存储。 稀疏矩阵的非零条目值保存为压缩稀疏列（Compressed Sparse Column ，CSC）格式，这种格式也是以列顺序存储。例如下面的稠密矩阵： 这个稠密矩阵保存为一维数组[1.0, 3.0, 5.0, 2.0, 4.0, 6.0]，数组大小为(3,2)。 本地矩阵的基类是Matrix，它提供了两种实现：DenseMatrix和SparseMatrix。 推荐使用Matrices的工厂方法来创建本地矩阵。下面是一个实现的例子： 12345import org.apache.spark.mllib.linalg.{Matrix, Matrices}// Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))// Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))val sm: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), Array(0, 2, 1), Array(9, 6, 8)) 稠密矩阵的存储很简单，不赘述。稀疏矩阵的存储使用CSC。关于压缩矩阵的介绍，请参看文献【1】。 分布式矩阵(Distributed matrix) 一个分布式矩阵拥有long类型的行和列索引，以及double类型的值，分布式的存储在一个或多个RDD中。选择正确的格式存储大型分布式矩阵是非常重要的。将一个分布式矩阵转换为另外一个格式可能需要一个全局的shuffle，这是非常昂贵的。 到目前为止，已经实现了三种类型的分布式矩阵。 基本的类型是RowMatrix，RowMatrix是一个面向行的分布式矩阵，它没有有意义的行索引。它的行保存为一个RDD,每一行都是一个本地向量。我们假设一个RowMatrix的列的数量不是很巨大，这样单个本地向量可以方便的和driver通信，也可以被单个节点保存和操作。 IndexedRowMatrix和RowMatrix很像，但是它拥有行索引，行索引可以用于识别行和进行join操作。CoordinateMatrix是一个分布式矩阵，它使用COO格式存储。请参看文献【1】了解COO格式。 RowMatrix RowMatrix是一个面向行的分布式矩阵，它没有有意义的行索引。它的行保存为一个RDD,每一行都是一个本地向量。因为每一行保存为一个本地向量，所以列数限制在了整数范围。 一个RowMatrix可以通过RDD[Vector]实例创建。创建完之后，我们可以计算它的列的统计和分解。QR分解的形式为A=QR，其中Q是一个正交矩阵， R是一个上三角矩阵。下面是一个RowMatrix的例子。 12345678910import org.apache.spark.mllib.linalg.Vectorimport org.apache.spark.mllib.linalg.distributed.RowMatrixval rows: RDD[Vector] = ... // an RDD of local vectors// Create a RowMatrix from an RDD[Vector].val mat: RowMatrix = new RowMatrix(rows)// Get its size.val m = mat.numRows()val n = mat.numCols()// QR decomposition val qrResult = mat.tallSkinnyQR(true) IndexedRowMatrix IndexedRowMatrix和RowMatrix很像，但是它拥有行索引。索引的行保存为一个RDD[IndexedRow]，其中IndexedRow是一个参数为(Long, Vector)的样本类，所以每一行通过它的索引以及一个本地向量表示。 一个IndexedRowMatrix可以通过RDD[IndexedRow]实例创建，并且一个IndexedRowMatrix可以通过去掉它的行索引，转换成RowMatrix。下面是一个例子： 123456789import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix, RowMatrix}val rows: RDD[IndexedRow] = ... // an RDD of indexed rows// Create an IndexedRowMatrix from an RDD[IndexedRow].val mat: IndexedRowMatrix = new IndexedRowMatrix(rows)// Get its size.val m = mat.numRows()val n = mat.numCols()// Drop its row indices.val rowMat: RowMatrix = mat.toRowMatrix() IndexedRow这个样本类的代码如下： 1case class IndexedRow(index: Long, vector: Vector) CoordinateMatrix CoordinateMatrix是一个分布式矩阵，它的条目保存为一个RDD。每一个条目是一个(i: Long, j: Long, value: Double)格式的元组，其中i表示行索引，j表示列索引，value表示条目值。 CoordinateMatrix应该仅仅在矩阵维度很大并且矩阵非常稀疏的情况下使用。 CoordinateMatrix可以通过RDD[MatrixEntry]实例创建，其中MatrixEntry是(Long, Long, Double)的包装。CoordinateMatrix可以转换成IndexedRowMatrix。下面是一个例子： 123456789import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}val entries: RDD[MatrixEntry] = ... // an RDD of matrix entries// Create a CoordinateMatrix from an RDD[MatrixEntry].val mat: CoordinateMatrix = new CoordinateMatrix(entries)// Get its size.val m = mat.numRows()val n = mat.numCols()// Convert it to an IndexRowMatrix whose rows are sparse vectors.val indexedRowMatrix = mat.toIndexedRowMatrix() MatrixEntry这个样本类的代码如下： 1case class MatrixEntry(i: Long, j: Long, value: Double) BlockMatrix BlockMatrix是一个分布式矩阵，它的保存为一个MatrixBlocks的RDD。MatrixBlock是一个((Int, Int), Matrix)类型的元组，其中(Int, Int)代表块的索引，Matrix代表子矩阵。 BlockMatrix支持诸如add和multiply等方法。BlockMatrix还有一个帮助方法validate，用来判断一个BlockMatrix是否正确的创建。 可以轻松的通过调用toBlockMatrix从一个IndexedRowMatrix或者CoordinateMatrix创建一个BlockMatrix。toBlockMatrix默认创建1024 * 1024大小的块，用户可以手动修个块的大小。 下面是一个例子： 1234567891011import org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}val entries: RDD[MatrixEntry] = ... // an RDD of (i, j, v) matrix entries// Create a CoordinateMatrix from an RDD[MatrixEntry].val coordMat: CoordinateMatrix = new CoordinateMatrix(entries)// Transform the CoordinateMatrix to a BlockMatrixval matA: BlockMatrix = coordMat.toBlockMatrix().cache()// Validate whether the BlockMatrix is set up properly. Throws an Exception when it is not valid.// Nothing happens if it is valid.matA.validate()// Calculate A^T A.val ata = matA.transpose.multiply(matA) 参考文献 【1】稀疏矩阵存储格式总结+存储效率对比:COO,CSR,DIA,ELL,HYB","link":"/posts/3126418019.html"},{"title":"决策树","text":"决策树理论 什么是决策树 所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。 树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，从根节点到叶节点所经历的路径对应一个判定测试序列。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。 ### 决策树学习流程 决策树学习的主要目的是为了产生一棵泛化能力强的决策树。其基本流程遵循简单而直接的“分而治之”的策略。它的流程实现如下所示： 12345678910111213141516171819输入：训练集 D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}; 属性集 A={a_1,a_2,...,a_d}过程：函数GenerateTree(D,A)1: 生成节点node；2: if D中样本全属于同一类别C then3: 将node标记为C类叶节点，并返回4: end if5: if A为空 OR D中样本在A上取值相同 then6: 将node标记为叶节点，其类别标记为D中样本数量最多的类，并返回7: end if8: 从A中选择最优划分属性 a*； //每个属性包含若干取值，这里假设有v个取值9: for a* 的每个值a*_v do10: 为node生成一个分支，令D_v表示D中在a*上取值为a*_v的样本子集；11: if D_v 为空 then12: 将分支节点标记为叶节点，其类别标记为D中样本最多的类，并返回13: else14: 以GenerateTree(D_v,A\\{a*})为分支节点15: end if16: end for 决策树的生成是一个递归的过程。有三种情况会导致递归的返回：（1）当前节点包含的样本全属于同一个类别。（2）当前属性值为空，或者所有样本在所有属性上取相同的值。 （3）当前节点包含的样本集合为空。 在第（2）中情形下，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别；在第（3）中情形下，同样把当前节点标记为叶节点， 但是将其类别设定为其父节点所含样本最多的类别。这两种处理实质不同，前者利用当前节点的后验分布，后者则把父节点的样本分布作为当前节点的先验分布。 决策树的构造 构造决策树的关键步骤是分裂属性（即确定属性的不同取值，对应上面流程中的a_v）。所谓分裂属性就是在某个节点处按照某一属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。 尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况： 1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。 2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。 3、属性是连续值。此时确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支。 划分选择 在决策树算法中，如何选择最优划分属性是最关键的一步。一般而言，随着划分过程的不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度(purity)”越来越高。 有几种度量样本集合纯度的指标。在MLlib中，信息熵和基尼指数用于决策树分类，方差用于决策树回归。 信息熵 信息熵是度量样本集合纯度最常用的一种指标，假设当前样本集合D中第k类样本所占的比例为p_k，则D的信息熵定义为： Ent(D)的值越小，则D的纯度越高。 基尼系数 采用和上式相同的符号，基尼系数可以用来度量数据集D的纯度。 直观来说，Gini(D)反映了从数据集D中随机取样两个样本，其类别标记不一致的概率。因此，Gini(D)越小，则数据集D的纯度越高。 方差 MLlib中使用方差来度量纯度。如下所示 信息增益 假设切分大小为N的数据集D为两个数据集D_left和D_right，那么信息增益可以表示为如下的形式。 一般情况下，信息增益越大，则意味着使用属性a来进行划分所获得的纯度提升越大。因此我们可以用信息增益来进行决策树的划分属性选择。即流程中的第8步。 决策树的优缺点 决策树的优点： 1 决策树易于理解和解释； 2 能够同时处理数据型和类别型属性； 3 决策树是一个白盒模型，给定一个观察模型，很容易推出相应的逻辑表达式； 4 在相对较短的时间内能够对大型数据作出效果良好的结果； 5 比较适合处理有缺失属性值的样本。 决策树的缺点： 1 对那些各类别数据量不一致的数据，在决策树种，信息增益的结果偏向那些具有更多数值的特征； 2 容易过拟合； 3 忽略了数据集中属性之间的相关性。 实例与源码分析 实例 下面的例子用于分类。 12345678910111213141516171819202122232425import org.apache.spark.mllib.tree.DecisionTreeimport org.apache.spark.mllib.tree.model.DecisionTreeModelimport org.apache.spark.mllib.util.MLUtils// Load and parse the data file.val data = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)// Split the data into training and test sets (30% held out for testing)val splits = data.randomSplit(Array(0.7, 0.3))val (trainingData, testData) = (splits(0), splits(1))// Train a DecisionTree model.// Empty categoricalFeaturesInfo indicates all features are continuous.val numClasses = 2val categoricalFeaturesInfo = Map[Int, Int]()val impurity = &quot;gini&quot;val maxDepth = 5val maxBins = 32val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)// Evaluate model on test instances and compute test errorval labelAndPreds = testData.map { point =&gt; val prediction = model.predict(point.features) (point.label, prediction)}val testErr = labelAndPreds.filter(r =&gt; r._1 != r._2).count().toDouble / testData.count()println(&quot;Test Error = &quot; + testErr)println(&quot;Learned classification tree model:\\n&quot; + model.toDebugString) 下面的例子用于回归。 123456789101112131415161718192021222324import org.apache.spark.mllib.tree.DecisionTreeimport org.apache.spark.mllib.tree.model.DecisionTreeModelimport org.apache.spark.mllib.util.MLUtils// Load and parse the data file.val data = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)// Split the data into training and test sets (30% held out for testing)val splits = data.randomSplit(Array(0.7, 0.3))val (trainingData, testData) = (splits(0), splits(1))// Train a DecisionTree model.// Empty categoricalFeaturesInfo indicates all features are continuous.val categoricalFeaturesInfo = Map[Int, Int]()val impurity = &quot;variance&quot;val maxDepth = 5val maxBins = 32val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity, maxDepth, maxBins)// Evaluate model on test instances and compute test errorval labelsAndPredictions = testData.map { point =&gt; val prediction = model.predict(point.features) (point.label, prediction)}val testMSE = labelsAndPredictions.map{ case (v, p) =&gt; math.pow(v - p, 2) }.mean()println(&quot;Test Mean Squared Error = &quot; + testMSE)println(&quot;Learned regression tree model:\\n&quot; + model.toDebugString) 源码分析 在MLlib中，决策树的实现和随机森林的实现是在一起的。随机森林实现中，当树的个数为1时，它的实现即为决策树的实现。 123456def run(input: RDD[LabeledPoint]): DecisionTreeModel = { //树个数为1 val rf = new RandomForest(strategy, numTrees = 1, featureSubsetStrategy = &quot;all&quot;, seed = 0) val rfModel = rf.run(input) rfModel.trees(0) } 这里的strategy是Strategy的实例，它包含如下信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Stores all the configuration options for tree construction * @param algo Learning goal. Supported: * [[org.apache.spark.mllib.tree.configuration.Algo.Classification]], * [[org.apache.spark.mllib.tree.configuration.Algo.Regression]] * @param impurity Criterion used for information gain calculation. * Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]], * [[org.apache.spark.mllib.tree.impurity.Entropy]]. * Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]]. * @param maxDepth Maximum depth of the tree. * E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. * @param numClasses Number of classes for classification. * (Ignored for regression.) * Default value is 2 (binary classification). * @param maxBins Maximum number of bins used for discretizing continuous features and * for choosing how to split on features at each node. * More bins give higher granularity. * @param quantileCalculationStrategy Algorithm for calculating quantiles. Supported: * [[org.apache.spark.mllib.tree.configuration.QuantileStrategy.Sort]] * @param categoricalFeaturesInfo A map storing information about the categorical variables and the * number of discrete values they take. For example, an entry (n -&gt; * k) implies the feature n is categorical with k categories 0, * 1, 2, ... , k-1. It's important to note that features are * zero-indexed. * @param minInstancesPerNode Minimum number of instances each child must have after split. * Default value is 1. If a split cause left or right child * to have less than minInstancesPerNode, * this split will not be considered as a valid split. * @param minInfoGain Minimum information gain a split must get. Default value is 0.0. * If a split has less information gain than minInfoGain, * this split will not be considered as a valid split. * @param maxMemoryInMB Maximum memory in MB allocated to histogram aggregation. Default value is * 256 MB. * @param subsamplingRate Fraction of the training data used for learning decision tree. * @param useNodeIdCache If this is true, instead of passing trees to executors, the algorithm will * maintain a separate RDD of node Id cache for each row. * @param checkpointInterval How often to checkpoint when the node Id cache gets updated. * E.g. 10 means that the cache will get checkpointed every 10 updates. If * the checkpoint directory is not set in * [[org.apache.spark.SparkContext]], this setting is ignored. */class Strategy @Since(&quot;1.3.0&quot;) ( @Since(&quot;1.0.0&quot;) @BeanProperty var algo: Algo,//选择的算法，有分类和回归两种选择 @Since(&quot;1.0.0&quot;) @BeanProperty var impurity: Impurity,//纯度有熵、基尼系数、方差三种选择 @Since(&quot;1.0.0&quot;) @BeanProperty var maxDepth: Int,//树的最大深度 @Since(&quot;1.2.0&quot;) @BeanProperty var numClasses: Int = 2,//分类数 @Since(&quot;1.0.0&quot;) @BeanProperty var maxBins: Int = 32,//最大子树个数 @Since(&quot;1.0.0&quot;) @BeanProperty var quantileCalculationStrategy: QuantileStrategy = Sort, //保存类别变量以及相应的离散值。一个entry (n -&gt;k) 表示特征n属于k个类别，分别是0,1,...,k-1 @Since(&quot;1.0.0&quot;) @BeanProperty var categoricalFeaturesInfo: Map[Int, Int] = Map[Int, Int](), @Since(&quot;1.2.0&quot;) @BeanProperty var minInstancesPerNode: Int = 1, @Since(&quot;1.2.0&quot;) @BeanProperty var minInfoGain: Double = 0.0, @Since(&quot;1.0.0&quot;) @BeanProperty var maxMemoryInMB: Int = 256, @Since(&quot;1.2.0&quot;) @BeanProperty var subsamplingRate: Double = 1, @Since(&quot;1.2.0&quot;) @BeanProperty var useNodeIdCache: Boolean = false, @Since(&quot;1.2.0&quot;) @BeanProperty var checkpointInterval: Int = 10) extends Serializable 决策树的实现我们在随机森林专题介绍。这里我们只需要知道，当随机森林的树个数为1时，它即为决策树， 并且此时，树的训练所用的特征是全部特征，而不是随机选择的部分特征。即featureSubsetStrategy = \"all\"。","link":"/posts/2380035110.html"},{"title":"递归和动态规划","text":"动态规划可以理解为是查表的递归。那么什么是递归？ 递归 定义： 递归算法是一种直接或者间接调用自身函数或者方法的算法。 算法中使用递归可以很简单地完成一些用循环实现的功能，比如二叉树的左中右序遍历。递归在算法中有非常广泛的使用， 包括现在日趋流行的函数式编程。 纯粹的函数式编程中没有循环，只有递归。 接下来我们来讲解以下递归。通俗来说，递归算法的实质是把问题分解成规模缩小的同类问题的子问题，然后递归调用方法来表示问题的解 递归的三个要素 一个问题的解可以分解为几个子问题的解 子问题的求解思路除了规模之外，没有任何区别 有递归终止条件 我这里列举了几道算法题目，这几道算法题目都可以用递归轻松写出来： 递归实现 sum 二叉树的遍历 走楼梯问题 汉诺塔问题 动态规划 如果说递归是从问题的结果倒推，直到问题的规模缩小到寻常。 那么动态规划就是从寻常入手， 逐步扩大规模到最优子结构。 这句话需要一定的时间来消化, 如果不理解，可以过一段时间再来看。 递归的解决问题非常符合人的直觉，代码写起来比较简单。但是我们通过分析（可以尝试画一个递归树），可以看出递归在缩小问题规模的同时可能会 重复计算。 279.perfect-squares 中 我通过递归的方式来解决这个问题，同时内部维护了一个缓存 来存储计算过的运算，那么我们可以减少很多运算。 这其实和动态规划有着异曲同工的地方。 我们结合求和问题来讲解一下, 题目是给定一个数组，求出数组中所有项的和，要求使用递归实现。 代码： 123456function sum(nums) { if (nums.length === 0) return 0; if (nums.length === 1) return nums[0]; return nums[0] + sum(nums.slice(1));} 我们用递归树来直观地看一下。 dynamic-programming-1 这种做法本身没有问题，但是每次执行一个函数都有一定的开销，拿 JS 引擎执行 JS 来说， 每次函数执行都会进行入栈操作，并进行预处理和执行过程，所以对于内存来说是一个挑战。 很容易造成爆栈。 浏览器中的 JS 引擎对于代码执行栈的长度是有限制的，超过会爆栈，抛出异常。 我们再举一个更加明显的例子，问题描述： 一个人爬楼梯，每次只能爬 1 个或 2 个台阶，假设有 n 个台阶，那么这个人有多少种不同的爬楼梯方法？ 代码： 12345function climbStairs(n) { if (n === 1) return 1; if (n === 2) return 2; return climbStairs(n - 1) + climbStairs(n - 2);} 这道题和 fibnacci 数列一摸一样，我们继续用一个递归树来直观感受以下： dynamic-programming-2 可以看出这里面有很多重复计算，我们可以使用一个 hashtable 去缓存中间计算结果，从而省去不必要的计算。 那么动态规划是怎么解决这个问题呢？ 答案就是“查表”。 刚才我们说了递归是从问题的结果倒推，直到问题的规模缩小到寻常。 动态规划是从寻常入手， 逐步扩大规模到最优子结构。 从刚才的两个例子，我想大家可能对前半句话有了一定的理解，我们接下来讲解下后半句。 如果爬楼梯的问题，使用动态规划，代码是这样的： 12345678910111213141516function climbStairs(n) { if (n === 1) return 1; if (n === 2) return 2; let a = 1; let b = 2; let temp; for (let i = 3; i &lt;= n; i++) { temp = a + b; a = b; b = temp; } return temp;} 动态规划的查表过程如果画成图，就是这样的： dynamic-programming-3 虚线代表的是查表过程 这道题目是动态规划中最简单的问题了，因为设计到单个因素的变化，如果涉及到多个因素，就比较复杂了，比如著名的背包问题，挖金矿问题等。 对于单个因素的，我们最多只需要一个一维数组即可，对于如背包问题我们需要二维数组等更高纬度。 爬楼梯我们并没有使用一维数组，而是借助两个变量来实现的，空间复杂度是 O(1). 之所以能这么做，是因为爬楼梯问题的状态转移方程只和前两个有关，因此只需要存储这两个即可。 动态规划问题有时候有很多这种讨巧的方式，但并不是所有的 动态规划都可以这么讨巧，比如背包问题。 动态规划的两个要素 状态转移方程 临界条件 在上面讲解的爬楼梯问题中 123f(1) 与 f(2) 就是【边界】f(n) = f(n-1) + f(n-2) 就是【状态转移公式】 动态规划为什么要画表格 动态规划问题要画表格，但是有的人不知道为什么要画，就觉得这个是必然的，必要要画表格才是动态规划。 其实动态规划本质上是将大问题转化为小问题，然后大问题的解是和小问题有关联的，换句话说大问题可以由小问题进行计算得到。 这一点是和递归一样的， 但是动态规划是一种类似查表的方法来缩短时间复杂度和空间复杂度。 画表格的目的就是去不断推导，完成状态转移， 表格中的每一个cell都是一个小问题， 我们填表的过程其实就是在解决问题的过程， 我们先解决规模为寻常的情况，然后根据这个结果逐步推导，通常情况下，表格的右下角是问题的最大的规模，也就是我们想要求解的规模。 比如我们用动态规划解决背包问题， 其实就是在不断根据之前的小问题A[i - 1][j] A[i -1][w - wj]来询问： 我是应该选择它 还是不选择它 至于判断的标准很简单，就是价值最大，因此我们要做的就是对于选择和不选择两种情况分别求价值，然后取最大，最后更新cell即可。 相关问题 0091.decode-ways 0139.word-break 0198.house-robber 0309.best-time-to-buy-and-sell-stock-with-cooldown 0322.coin-change 0416.partition-equal-subset-sum 0518.coin-change-2 太多了，没有逐一列举 总结 本篇文章总结了算法中比较常用的两个方法 - 递归和动态规划。 如果你只能借助一句话，那么请记住：递归是从问题的结果倒推，直到问题的规模缩小到寻常。 动态规划是从寻常入手， 逐步扩大规模到最优子结构。","link":"/posts/4023985006.html"},{"title":"特征值分解","text":"假设向量v是方阵A的特征向量，可以表示成下面的形式： 这里lambda表示特征向量v所对应的特征值。并且一个矩阵的一组特征向量是一组正交向量。特征值分解是将一个矩阵分解为下面的形式： 其中Q是这个矩阵A的特征向量组成的矩阵。sigma是一个对角矩阵，每个对角线上的元素就是一个特征值。 特征值分解是一个提取矩阵特征很不错的方法，但是它只适合于方阵，对于非方阵，它不适合。这就需要用到奇异值分解。 源码分析 MLlib使用ARPACK来求解特征值分解。它的实现代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687def symmetricEigs( mul: BDV[Double] =&gt; BDV[Double], n: Int, k: Int, tol: Double, maxIterations: Int): (BDV[Double], BDM[Double]) = { val arpack = ARPACK.getInstance() // tolerance used in stopping criterion val tolW = new doubleW(tol) // number of desired eigenvalues, 0 &lt; nev &lt; n val nev = new intW(k) // nev Lanczos vectors are generated in the first iteration // ncv-nev Lanczos vectors are generated in each subsequent iteration // ncv must be smaller than n val ncv = math.min(2 * k, n) // &quot;I&quot; for standard eigenvalue problem, &quot;G&quot; for generalized eigenvalue problem val bmat = &quot;I&quot; // &quot;LM&quot; : compute the NEV largest (in magnitude) eigenvalues val which = &quot;LM&quot; var iparam = new Array[Int](11) // use exact shift in each iteration iparam(0) = 1 // maximum number of Arnoldi update iterations, or the actual number of iterations on output iparam(2) = maxIterations // Mode 1: A*x = lambda*x, A symmetric iparam(6) = 1 var ido = new intW(0) var info = new intW(0) var resid = new Array[Double](n) var v = new Array[Double](n * ncv) var workd = new Array[Double](n * 3) var workl = new Array[Double](ncv * (ncv + 8)) var ipntr = new Array[Int](11) // call ARPACK's reverse communication, first iteration with ido = 0 arpack.dsaupd(ido, bmat, n, which, nev.`val`, tolW, resid, ncv, v, n, iparam, ipntr, workd, workl, workl.length, info) val w = BDV(workd) // ido = 99 : done flag in reverse communication while (ido.`val` != 99) { if (ido.`val` != -1 &amp;&amp; ido.`val` != 1) { throw new IllegalStateException(&quot;ARPACK returns ido = &quot; + ido.`val` + &quot; This flag is not compatible with Mode 1: A*x = lambda*x, A symmetric.&quot;) } // multiply working vector with the matrix val inputOffset = ipntr(0) - 1 val outputOffset = ipntr(1) - 1 val x = w.slice(inputOffset, inputOffset + n) val y = w.slice(outputOffset, outputOffset + n) y := mul(x) // call ARPACK's reverse communication arpack.dsaupd(ido, bmat, n, which, nev.`val`, tolW, resid, ncv, v, n, iparam, ipntr, workd, workl, workl.length, info) } val d = new Array[Double](nev.`val`) val select = new Array[Boolean](ncv) // copy the Ritz vectors val z = java.util.Arrays.copyOfRange(v, 0, nev.`val` * n) // call ARPACK's post-processing for eigenvectors arpack.dseupd(true, &quot;A&quot;, select, d, z, n, 0.0, bmat, n, which, nev, tol, resid, ncv, v, n, iparam, ipntr, workd, workl, workl.length, info) // number of computed eigenvalues, might be smaller than k val computed = iparam(4) val eigenPairs = java.util.Arrays.copyOfRange(d, 0, computed).zipWithIndex.map { r =&gt; (r._1, java.util.Arrays.copyOfRange(z, r._2 * n, r._2 * n + n)) } // sort the eigen-pairs in descending order val sortedEigenPairs = eigenPairs.sortBy(- _._1) // copy eigenvectors in descending order of eigenvalues val sortedU = BDM.zeros[Double](n, computed) sortedEigenPairs.zipWithIndex.foreach { r =&gt; val b = r._2 * n var i = 0 while (i &lt; n) { sortedU.data(b + i) = r._1._2(i) i += 1 } } (BDV[Double](sortedEigenPairs.map(_._1)), sortedU) } 我们可以查看ARPACK的注释详细了解dsaupd和dseupd方法的作用。","link":"/posts/1540263390.html"},{"title":"Find Minimum in Rotated Sorted Array II","text":"Question leetcode: Find Minimum in Rotated Sorted Array II | LeetCode OJ lintcode: (160) Find Minimum in Rotated Sorted Array II Problem Statement Suppose a sorted array is rotated at some pivot unknown to you beforehand. (i.e., 0 1 2 4 5 6 7 might become 4 5 6 7 0 1 2). Find the minimum element. The array may contain duplicates. Example Given [4,4,5,6,7,0,1,2] return 0 题解 由于此题输入可能有重复元素，因此在num[mid] == num[end]时无法使用二分的方法缩小start或者end的取值范围。此时只能使用递增start/递减end逐步缩小范围。 C++ 1234567891011121314151617181920212223242526272829303132class Solution {public: /** * @param num: a rotated sorted array * @return: the minimum number in the array */ int findMin(vector&lt;int&gt; &amp;num) { if (num.empty()) { return -1; } vector&lt;int&gt;::size_type start = 0; vector&lt;int&gt;::size_type end = num.size() - 1; vector&lt;int&gt;::size_type mid; while (start + 1 &lt; end) { mid = start + (end - start) / 2; if (num[mid] &gt; num[end]) { start = mid; } else if (num[mid] &lt; num[end]) { end = mid; } else { --end; } } if (num[start] &lt; num[end]) { return num[start]; } else { return num[end]; } }}; Java 123456789101112131415161718192021222324252627public class Solution { /** * @param num: a rotated sorted array * @return: the minimum number in the array */ public int findMin(int[] num) { if (num == null || num.length == 0) return Integer.MIN_VALUE; int lb = 0, ub = num.length - 1; // case1: num[0] &lt; num[num.length - 1] // if (num[lb] &lt; num[ub]) return num[lb]; // case2: num[0] &gt; num[num.length - 1] or num[0] &lt; num[num.length - 1] while (lb + 1 &lt; ub) { int mid = lb + (ub - lb) / 2; if (num[mid] &lt; num[ub]) { ub = mid; } else if (num[mid] &gt; num[ub]){ lb = mid; } else { ub--; } } return Math.min(num[lb], num[ub]); }} 源码分析 注意num[mid] &gt; num[ub]时应递减 ub 或者递增 lb. 复杂度分析 最坏情况下 \\[O(n)\\], 平均情况下 \\[O(\\log n)\\].","link":"/posts/1551310537.html"},{"title":"Git 与 GitHub 入门实践","text":"git配置 优先级：--local &gt; --global &gt; --system 用了--global这个参数，表示你这台机器上所有的Git仓库都会使用这个配置 ### 配置git用户名和邮箱 1234git config --global user.name # 查看git config --global user.name 用户名 # 修改git config --global user.email # 查看git config --global user.email 邮箱 # 修改 仓库 创建git仓库 12git init 仓库名 #创建一个git仓库git init #将一个项目转化为使用git管理（创建.git目录） 示例： 目录结构： 12345678910project |------.git |--------branches |--------config #仓库的配置文件 |--------description |--------HEAD |--------hooks |--------info |--------objects |--------refs 隐藏目录.git不算工作区，而是Git的版本库 查看仓库状态 1git status 远程仓库 最早，肯定只有一台机器有一个原始版本库，此后，别的机器可以“克隆”这个原始版本库，而且每台机器的版本库其实都是一样的，并没有主次之分 实际情况往往是这样，找一台电脑充当服务器的角色，每天24小时开机，其他每个人都从这个“服务器”仓库克隆一份到自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交 GitHub就是提供Git仓库托管服务的，所以，只要注册一个GitHub账号，就可以免费获得Git远程仓库，即Github为我们的git仓库提供了一个远程仓库，有了这个远程仓库，妈妈再也不用担心我的硬盘了 为本地与GitHub的通信配置ssh 本地git仓库和GitHub上的远程仓库之间的传输是通过SSH加密的，所以，需要一点设置： 创建ssh key： 1ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 登录你的GitHub帐号，Settings -&gt; SSH and GPG keys -&gt; new SSH key ，将id_rsa.pub的内容复制进去 为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送 让本地git仓库和远程仓库同步 在有了本地git仓库后，还需创建对应的远程仓库 在GitHub上创建远程仓库（如果已有则省略） 为本地仓库设置远程仓库信息（如果同时需要为本地仓库添加多个远程仓库（如果github+码云），则可以将origin分别换成github和gitee，推送操作时也要修改origin。添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库） 1git remote add origin https://github.com/用户名/仓库名 删除本地仓库的远程仓库信息：git remote remove origin 修改远端地址：git remote set-url 新地址 查看远程仓库信息：git remote -v 将本地git仓库push到远程仓库 1234# 由于远程库是空的，我们第一次推送master分支时，加上了-u参数,Git不但会把本地的# master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master# 分支关联起来，在以后的推送或者拉取时就可以简化命令git push [-u] origin 分支名 并不是一定要把本地分支往远程推送。哪些分支需要推送、哪些不需要呢？ master：主分支，要时刻与远程同步 dev：开发分支，团队所有成员都需要在上面工作，所有也需要与远程同步 bug：只用于在本地修复bug，就没必要推送到远程了，除非老板要看看你每周修复了几个bug 协同工作 拉取分支： 1git pull git clone时，默认情况下只能看到本地的master分支。如果要在dev分支上开发，就必须创建远程origin的dev分支到本地，可以使用如下命令创建本地dev分支： 1git checkout -b dev 将本地dev分支与远程origin/dev分支关联起来： 1git branch --set-upstream dev origin/dev 使用GitHub Bootstrap的官方仓库twbs/bootstrap、你在GitHub上克隆的仓库my/bootstrap，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样： 如果你想修复bootstrap的一个bug，或者新增一个功能，立刻就可以开始干活，干完后，往自己的仓库推送 如果你希望bootstrap的官方库能接受你的修改，你就可以在GitHub上发起一个pull request。当然，对方是否接受你的pull request就不一定了 版本控制 隐藏目录.git不算工作区，而是Git的版本库。版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区。还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD 添加或删除修改 将修改添加到暂存区： 1git add 文件/目录 从暂存区删除修改： 1git rm --cached 文件/目录 以下命令可以将暂存区的修改重置，暂存区的改变会被移除到工作区： 1git reset HEAD [文件名] 以下命令可以丢弃工作区的修改： 1git checkout -- [文件名] 如果刚对一个文件进行了编辑，可以撤销文件的改变，回到编辑开始。命令其实起到“一键恢复”的作用，还可用于“误删恢复”。可以在 git reset HEAD [文件名] 后使用 提交版本 如果修改了readme.txt，添加了文件LICENSE，并将2者添加到暂存区后，暂存区的状态就变成这样： 使用commit提交修改，实际上就是把暂存区的所有内容提交到当前分支： 1git commit -m '信息' commit相当于游戏里面一次存档。对应一个版本 文件删除 rm做出的删除不会被暂存，git rm做出的改变会被暂存。如果使用rm删除掉，能使用git rm来暂存。git rm不在意文件已经不存在了 删除(暂存)单个文件 1git rm 删除(暂存)多个文件（一般情况下，更可能是对大量文件进行管理。可能同时会删除很多文件，不可能使用git rm一个个删除） 12# 它会变量当前目录，将所有删除暂存git add -u . 如果有文件被误删，可以使用git checkout -- 文件名恢复 工作现场保存与恢复 有时候在修复bug或某项任务还未完成，但是需要紧急处理另外一个问题。此时可以先保存工作现场，当问题处理完成后，再恢复bug或任务的进度 保存工作现场：git stash 查看保存的工作现场：git stash list 恢复工作现场：git stash apply 删除stash内容：git stash drop 恢复工作现场并删除stash内容（相当于上面2步合并）：git stash pop 改动查询 1234567git diff [选项] # 查看工作区中的修改git diff [选项] --staged # 查看已添加到暂存区的修改git diff [选项] HEAD # 查看当前所有未提交的修改选项： --color-words： 颜色 --stat： 不显示具体修改，只显示修改了的文件 版本回退 123456git reset --hard 版本ID/HEAD形式的版本git reset --hard HEAD # 当前版本git reset --hard HEAD^ # 上一个版本git reset --hard HEAD^^ # 上上个版本git reset --hard HEAD~n # 前n个版本 如果回到过去的版本，想要回到原来新的版本： 如果终端未关，可以找到新版本的id，通过上述命令回去新版本 如果终端已关，git reflog查看版本，再通过上述命令回去新版本 查看历史提交 1234567git log [选项]选项： --online：只显示提交提示信息 --stat：添加每次提交包含的文件信息 --path：查看每次提交改变的内容 --graph 加文件名可以显示具体文件相关的所有提交信息 分支管理 创建与合并分支 每次commit相当于一次存档，对应一个版本。Git都把它们串成一条时间线，这条时间线就是一个分支。master就是主分支。HEAD指向当前分支，而master指向主分支的最近提交。每次提交，master分支都会向前移动一步 当创建一个分支时，如dev，Git创建一个指针dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上： 从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变： 假如我们在dev上的工作完成了，就可以把dev合并到master上。最简单的方法，就是直接把master指向dev的当前提交，就完成了合并： 合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支： 上面的合并使用的是Fast forward。这种模式下，删除分支后，会丢掉分支信息。如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的提交，这样，从分支历史上就可以看出分支信息。通过在git merge命令中使用--no-ff选项禁用Fast forward模式。比如在合并dev时： 1git merge --no-ff -m &quot;merge with no-ff&quot; dev 由于会生成一个新的提交，所以需要使用-m指明新提交的信息。此时分支情况如下： 相关命令如下： (创建分支并)切换到新分支：git checkout -b 新分支 创建分支：git branch 新分支 切换分支：git checkout 欲切换到的分支 查看当前分支：git branch 合并某分支到当前分支：git merge 欲合并到当前分支的分支 查看历史分支情况：git log --graph --pretty=oneline --abbrev-commit 删除未合并的分支：git branch -D 分支 分支合并冲突 如果两个分支修改了同一文件，合并时会发生冲突。比如master分支和feature1分支都修改了readme.txt文件，各自都有新的提交： 这种情况下，Git无法执行“快速合并”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突。此时readme.txt文件会变成如下形式： 123456789Git is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADCreating a new branch is quick &amp; simple.=======Creating a new branch is quick AND simple.&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1 Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，此时需要手动修改后保存。然后再使用git commit进行一次提交。分支会变成如下： 分支管理策略 在实际开发中，我们应该按照几个基本原则进行分支管理 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活 干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本 你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了 所以，团队合作的分支看起来就像这样： 当你从远程仓库克隆时，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且，远程仓库的默认名称是origin 要查看远程库的信息，用git remote： 12$ git remoteorigin 或者，用git remote -v显示更详细的信息： 123$ git remote -vorigin git@github.com:michaelliao/learngit.git (fetch)origin git@github.com:michaelliao/learngit.git (push) 上面显示了可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址 推送分支 1git push origin 欲推送的分支 master分支是主分支，因此要时刻与远程同步 dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步 bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发","link":"/posts/1640691208.html"},{"title":"k近邻法","text":"学习笔记 k值的选择、距离度量及分类决策规则是k近邻法的三要素 三要素在算法之中完整体现出来： 算法 输入: \\(T=\\{(x_1,y_1),(x_2,y_2),\\dots,(x_N,y_N)\\}， x_i\\in X \\sube{\\bf{R}^n}, y_i\\in Y=\\{c_1,c_2,\\dots, c_k\\}\\); 实例特征向量\\(x\\) 输出: 实例所属的\\(y\\) 步骤: 1. 根据指定的距离度量，在\\(T\\)中查找\\(x\\)的最近邻的\\(k\\)个点，覆盖这\\(k\\)个点的\\(x\\)的邻域定义为\\(N_k(x)\\) 1. 在\\(N_k(x)\\)中应用分类决策规则决定\\(x\\)的类别\\(y\\) \\[ y=\\arg\\max_{c_j}\\sum_{x_i\\in N_k(x)}I(y_i=c_j), i=1,2,\\dots,N, j=1,2,\\dots,K \\] 距离度量 特征空间中的两个实例点的距离是两个实例点相似程度的反映。 \\(p=1\\) 对应 曼哈顿距离 \\(p=2\\) 对应 欧氏距离 任意\\(p\\) 对应 闵可夫斯基距离 \\[L_p(x_i, x_j)=\\left(\\sum_{l=1}^{n}{\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|^p}\\right)^{\\frac{1}{p}}\\] 范数是对向量或者矩阵的度量，是一个标量，这个里面两个点之间的\\(L_p\\)距离可以认为是两个点坐标差值的\\(p\\)范数 k值选择 k值选择会对算法结果产生重大影响。若选较小，只有与输入实例相似的训练实例才会对预测结果起作用，“学习”的近似误差会减小，但“学习”的估计误差会增大，会对近邻的实例点非常敏感，k值减少意味着整体模型变得复杂，容易发生过拟合；若选较大，与输入实例不相似的训练实例也对预测起作用，从而发生错误 通过交叉验证选取最优\\(k\\)，算是超参数，一般k值会取一个较小的数值 在二分类问题中，\\(k\\)选择奇数有助于避免平票 分类决策规则 决策规则往往是多数表决(Majority Voting Rule) 误分类率 \\[\\frac{1}{k}\\sum_{x_i\\in N_k(x)}{I(y_i\\ne c_i)}=1-\\frac{1}{k}\\sum_{x_i\\in N_k(x)}{I(y_i= c_i)}\\] 如果分类损失函数是0-1损失，误分类率最低即经验风险最小。 kd树 k近邻最简单的实现方法是线性扫描，这时候要计算输入实例与每一个训练实例的距离 为了提高k近邻的搜索效率，考虑使用树结构存储训练数据，以减少计算距离的次数 kd树是二叉树，表示对k维空间的一个划分，注意这里的k和k近邻的k意义并不相同，只是习惯上的一致 kdTree搜索时效率未必是最优的，这个和样本分布有关系。随机分布样本kdTree搜索(这里应该是最近邻搜索)的平均计算复杂度是\\(O(\\log N)\\)，空间维数\\(K\\)接近训练样本数\\(N\\)时，搜索效率急速下降，几乎\\(O(N)\\) kd树的构造：构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域；通过递归的方法，不断对k维空间进行切分，生成子节点；直到子区域内没有实例时终止。 1234567891011121314151617k = len(data[0]) # 数据维度def CreateNode(split, data_set): # 按第split维划分数据集exset创建KdNode if not data_set: # 数据集为空 return None data_set.sort(key=lambda x: x[split]) split_pos = len(data_set) // 2 # //为Python中的整数除法 median = data_set[split_pos] # 中位数分割点 split_next = (split + 1) % k # cycle coordinates # 递归的创建kd树 return KdNode(median, split, CreateNode(split_next, data_set[:split_pos]), # 创建左子树 CreateNode(split_next, data_set[split_pos + 1:])) # 创建右子树self.root = CreateNode(0, data) # 从第0维分量开始构建kd树,返回根节点 kd树最近邻搜索 算法 &gt;输入：已构造的\\(kd\\)树，目标点\\(x\\) &gt;输出：\\(x\\)的最近邻 &gt;1. 在\\(kd\\)树中找出包含目标点\\(x\\)的叶结点：从根结点出发，递归地向下访问\\(kd\\)树。若目标点\\(x\\)当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。 &gt;1. 以此叶结点为“当前最近点”。 &gt;1. 递归地向上回退，在每个结点进行以下操作： &gt; * 如果该点保存地实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。 &gt; * 当前最近点一定存在于该结点一个子结点对应地区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。 &gt; 如果相交，可能在另一子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点。接着递归地进行最近邻搜索； &gt; 如果不相交，向上回退。 &gt;4. 当回退到根结点时，搜索结束。最后的“当前最近点”即为\\(x\\)的最近邻点。 习题解答 3.1 参照图 3.1，在 二维空间中给出实例点，画出 k 为 1 和 2 时的 K 近邻法构成的空间划分，并对其进行比较，体会 K 值选择与模型复杂度及预测准确率的关系。 k为1时 k为2时 3.2 利用例题 3.2 构造的kd树求点 x=(3,4.5) 的最近邻点。 首先找到包含(3,4.5)的叶节点(4,7)，将叶节点作为“当前最近点”； 回退到(4,7)的父节点(5,4)，将(5,4)作为“当前最近点”。以(3,4.5)为圆心，到“当前最近点”(5,4)距离为半径的圆显然和(5,4)的另一个子节点(2,3)区域相交，因此移动到(2,3)； 移动到(2,3)后发现，距离(3,4.5)更近，因此将(2,3)作为“当前最近点”，由于(2,3)是叶节点，因此直接回退； 回到(5,4)的根节点(7,2)，到(7,2)距离大于到“当前最近点”距离，同时(3,4.5)和“当前最近点”距离构成的圆和根节点的另一个子节点的区域不相交，所以搜索结束，得到最近点(2，3)。 3.3 参照算法 3.3，写出输出为 x 的 K 近邻的算法。 在寻找最近邻节点的时候需要维护一个”当前最近点“，而寻找 K 近邻的时候，就需要维护一个”当前 K 近邻点集“。首先定义一个”当前 K 近邻点集“插入新点操作：如果”当前 K 近邻点集“元素数量小于K，那么直接将新点插入集合；如果”当前 K 近邻点集“元素数量等于K，那么将新节点替换原来集合中最远的节点。 （1）在 kd 树中找出包含目标点 x 的叶结点：从根结点出发，递归地向下访问树。若目标点 x 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止； （2）如果”当前 K 近邻点集“元素数量小于K或者叶节点距离小于”当前 K 近邻点集“中最远点距离，那么将叶节点插入”当前 K 近邻点集“； （3）递归地向上回退，在每个结点进行以下操作： 如果”当前 K 近邻点集“元素数量小于K或者当前节点距离小于”当前 K 近邻点集“中最远点距离，那么将该节点插入”当前 K 近邻点集“， 检查另一子结点对应的区域是否与以目标点为球心、以目标点与于”当前 K 近邻点集“中最远点间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点 . 接着，递归地进行最近邻搜索；如果不相交，向上回退； （4）当回退到根结点时，搜索结束，最后的”当前 K 近邻点集“即为 x 的 K 近邻点集。","link":"/posts/2313.html"},{"title":"线性支持向量机","text":"介绍 线性支持向量机是一个用于大规模分类任务的标准方法。它的目标函数线性模型中的公式（1）。它的损失函数是合页（hinge）损失，如下所示 默认情况下，线性支持向量机训练时使用L2正则化。线性支持向量机输出一个SVM模型。给定一个新的数据点x，模型通过w^Tx的值预测，当这个值大于0时，输出为正，否则输出为负。 线性支持向量机并不需要核函数，要详细了解支持向量机，请参考文献【1】。 源码分析 实例 1234567891011121314151617181920212223import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}import org.apache.spark.mllib.evaluation.BinaryClassificationMetricsimport org.apache.spark.mllib.util.MLUtils// Load training data in LIBSVM format.val data = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)// Split data into training (60%) and test (40%).val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)val training = splits(0).cache()val test = splits(1)// Run training algorithm to build the modelval numIterations = 100val model = SVMWithSGD.train(training, numIterations)// Clear the default threshold.model.clearThreshold()// Compute raw scores on the test set.val scoreAndLabels = test.map { point =&gt; val score = model.predict(point.features) (score, point.label)}// Get evaluation metrics.val metrics = new BinaryClassificationMetrics(scoreAndLabels)val auROC = metrics.areaUnderROC()println(&quot;Area under ROC = &quot; + auROC) 训练 和逻辑回归一样，训练过程均使用GeneralizedLinearModel中的run训练，只是训练使用的Gradient和Updater不同。在线性支持向量机中，使用HingeGradient计算梯度，使用SquaredL2Updater进行更新。 它的实现过程分为4步。参加逻辑回归了解这五步的详细情况。我们只需要了解HingeGradient和SquaredL2Updater的实现。 12345678910111213141516171819202122232425262728293031323334class HingeGradient extends Gradient { override def compute(data: Vector, label: Double, weights: Vector): (Vector, Double) = { val dotProduct = dot(data, weights) // 我们的损失函数是 max(0, 1 - (2y - 1) (f_w(x))) // 所以梯度是 -(2y - 1)*x val labelScaled = 2 * label - 1.0 if (1.0 &gt; labelScaled * dotProduct) { val gradient = data.copy scal(-labelScaled, gradient) (gradient, 1.0 - labelScaled * dotProduct) } else { (Vectors.sparse(weights.size, Array.empty, Array.empty), 0.0) } } override def compute( data: Vector, label: Double, weights: Vector, cumGradient: Vector): Double = { val dotProduct = dot(data, weights) // 我们的损失函数是 max(0, 1 - (2y - 1) (f_w(x))) // 所以梯度是 -(2y - 1)*x val labelScaled = 2 * label - 1.0 if (1.0 &gt; labelScaled * dotProduct) { //cumGradient -= labelScaled * data axpy(-labelScaled, data, cumGradient) //损失值 1.0 - labelScaled * dotProduct } else { 0.0 } }} 线性支持向量机的训练使用L2正则化方法。 123456789101112131415161718192021class SquaredL2Updater extends Updater { override def compute( weightsOld: Vector, gradient: Vector, stepSize: Double, iter: Int, regParam: Double): (Vector, Double) = { // w' = w - thisIterStepSize * (gradient + regParam * w) // w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient //表示步长，即负梯度方向的大小 val thisIterStepSize = stepSize / math.sqrt(iter) val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector //正则化，brzWeights每行数据均乘以(1.0 - thisIterStepSize * regParam) brzWeights :*= (1.0 - thisIterStepSize * regParam) //y += x * a，即brzWeights -= gradient * thisInterStepSize brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights) //正则化||w||_2 val norm = brzNorm(brzWeights, 2.0) (Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm) }} 该函数的实现规则是： 12w' = w - thisIterStepSize * (gradient + regParam * w)w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient 这里thisIterStepSize表示参数沿负梯度方向改变的速率，它随着迭代次数的增多而减小。 预测 1234567891011override protected def predictPoint( dataMatrix: Vector, weightMatrix: Vector, intercept: Double) = { //w^Tx val margin = weightMatrix.toBreeze.dot(dataMatrix.toBreeze) + intercept threshold match { case Some(t) =&gt; if (margin &gt; t) 1.0 else 0.0 case None =&gt; margin } } 参考文献 【1】支持向量机通俗导论（理解SVM的三层境界）","link":"/posts/3881616076.html"},{"title":"英语学习指南（五）","text":"如何构建一个识别英语的程序 现在我们不讨论你如何学英语，而是让你构建一个可以识别、交流英语的程序，你会如何设计？ 简单的需求分析 以中文为例，当你听到一个女人对一个男人说：“你是一个男人吗？”时，你会收集到哪些信息？你需要哪些信息来明确这个女人想表达的确切意思？ 首先是听力输入，你需要确保麦克风录入了音频，然后拿到的声波内容是 U#@&amp;!&amp;&amp;。之后我们需要将声波内容输入到一大堆分析器中进行分析，并得到比较精准的意图。 第一个可能是性别分析器，通过一定的规则识别出这是一个男性的声音还是女性的声音。因为这句话是男的说出来还是女的说出来表达的意思是不一样的。 第二个就是内容识别器，先是加载粤语匹配引擎和粤语语料库发现声波无法匹配解析，那么换成普通话引擎和语料库。此时如果你的语料库里有 “你”、“是”、“一个”、“男人”、“吗” 这些声音素材，那么就可以匹配解析出这句话：“你是一个男人吗？”。换言之，如果你没有粤语语料库和解析引擎，即便是给你一段粤语录音你也听不懂。如果这句话有一个生僻单词你语料库里没有，那么也是无法识别出来。 之后还有更多识别器，比如年龄、情绪识别器、重音和疑问语气识别句等，这些因素共同决定了这句话究竟想要传达什么意思。如果是一个女性的激动的感叹语气“你是一个男人吗！”，那么可以推测出这个女的跟男的有一定的关系，这个男的做出了一些伤天害理的事情导致这个女性在质问。如果是一个轻声细语的疑问句“你是一个男人吗？”，可能是一名女性想确认对方的性别。当然更准确表达这个意图的句子应该是“你是男性吗？”或者“男的女的？”。 比较基础的方案设计 上面需求分析只是简单的介绍了 声音 -&gt; 听力识别器 -&gt; 意图 的过程，实际上语言交流是听说读写，其中包含两个识别器（听力识别器和视力识别器），一个核心理解器，两个表达器（口语表达器和书写表达器）。通过对应的实际场景，我们可以简单的梳理出对应需要的功能。 听力识别器 听力能力 说明：要求可以输入声音并转换成一种可分析的信号。 训练：买个好麦克风，对应人类是保护好耳朵和听力。 口音识别器 说明：各类方言比如粤语，各种口音比如东北口音、广东口音、英式发音和美式发音。 语言特性识别器 说明：语言之间会有不同特性，比如中文没有略读，都是一个一个字念出来，而英文会为了说话省劲而略读或者连读，比如 “drink it” 并不是单个蹦的 “准克一特”，而是类似 “准kei特”。中文的 “喝它” 就是 “喝它”，不会有类似 “赫特” 之类的变化。 训练：扩充特殊语言引擎的匹配规则，扩充语料库，当听到 “准kei特” 可以识别出是 “drink it”。 音量调节和杂音处理器 说明：可以通过算法过滤无用杂音，并将小音量调大使其清晰。人类天然进化出这种能力，无需特殊训练。 语气、性别、身份、语速识别器 说明：人类天然进化出这种能力，无需特殊训练。 上下文缓存器 说明：交流过程要有上下文内容缓存，结合输入理解器。 视力识别器 视力能力 图形识别器 说明：不同字体、变形（英文大小写、中文繁简体等）都可以识别出来具体字符，同时需要识别标点符号等输入理解器。 训练：识别能力、精准度和速度。比如一眼看出 message 和 massage 是不一样的。 上下文缓存器 理解器 理解器可以说是最重要的部分了，也是最难的部分 语料库 说明：字母、单词、发音、多重语境含义、历史文化背景、不同形态，同义词反义词相近词。 训练：需要长期积累和扩充，需要大量训练。 识别引擎 说明：单词拼装起来的句型句式、语法、时态含义和规则、标点符号、单复数、惯用表达。 训练：单点突破，专项训练，逐步体系化积累。 思考和思维能力 说明：针对意图结合之前的记忆以及经验得出自己想要表达的意图。 训练：结构性表达，思维能力锻炼，思考和总结能力。这个与语言无关。 上下文缓存器 口语表达器 当理解器思考运算并得到想要表达的意图之后，就需要开始表达传递出去。 意图语料组装器 说明：将意图结合语料中的单词、句型句式、惯用表达进行匹配组合，挑选出最符合你意图的语句。 发音器 说明：根据组装出来的内容，结合特殊的连读、略读等语言特性，转换成发声信号。对应人类的话是控制舌头、声带和呼吸系统的肌肉使其变成对应形状，让气流通过声带发出对应声音。 书写表达器 意图语料组装器 书写表达器 说明：将语料组装结果以视觉的方式表达，对于人类是控制手部肌肉书写出对应形状。 从上面可以看出，口语听力相对于阅读写作更加困难，主要因为实时性的要求。你必须迅速反馈不经过思考，这要求你要无意识的去用英语表达，所以语言学习没有技巧，只有大量训练。","link":"/posts/3230888314.html"},{"title":"Longest Increasing","text":"leetcode: Longest Increasing Subsequence | LeetCode OJ lintcode: (76) Longest Increasing Subsequence Dynamic Programming | Set 3 (Longest Increasing Subsequence) - GeeksforGeeks Problem Statement Given an unsorted array of integers, find the length of longest increasing subsequence. For example, Given [10, 9, 2, 5, 3, 7, 101, 18], The longest increasing subsequence is [2, 3, 7, 101], therefore the length is 4. Note that there may be more than one LIS combination, it is only necessary for you to return the length. Your algorithm should run in \\[O(n^2)\\] complexity. Follow up: Could you improve it to \\[O(n \\log n)\\] time complexity? Credits: Special thanks to [@pbrother](https://leetcode.com/discuss/user/pbrother) for adding this problem and creating all test cases. 题解1 - 双重 for 循环 由题意知这种题应该是单序列动态规划题，结合四要素，可定义f[i]为前i个数字中的 LIC 数目，那么问题来了，接下来的状态转移方程如何写？似乎写不出来... 再仔细看看 LIS 的定义，状态转移的关键一环应该为数字本身而不是最后返回的结果(数目)，那么理所当然的，我们应定义f[i]为前i个数字中以第i个数字结尾的 LIS 长度，相应的状态转移方程为f[i] = {1 + max{f[j]} where j &lt; i, nums[j] &lt; nums[i]}, 该转移方程的含义为在所有满足以上条件的 j 中将最大的f[j] 赋予f[i], 如果上式不满足，则f[i] = 1. 具体实现时不能直接使用f[i] = 1 + max(f[j]), 应为若if f[i] &lt; 1 + f[j], f[i] = 1 + f[j]. 最后返回 max(f[]). 需要注意的是 LIS 的含义，序列中是否可以包含相等的值。如果包含，则改为 nums[j] &lt; nums[i]. Python 123456789101112131415class Solution(object): def lengthOfLIS(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; if nums is None or len(nums) == 0: return 0 lis = [1] * len(nums) for i in range(1, len(nums)): for j in range(i): if nums[j] &lt; nums[i] and lis[i] &lt; 1 + lis[j]: lis[i] = 1 + lis[j] return max(lis) C++ 1234567891011121314151617181920212223class Solution {public: /** * @param nums: The integer array * @return: The length of LIS (longest increasing subsequence) */ int longestIncreasingSubsequence(vector&lt;int&gt; nums) { if (nums.empty()) return 0; int len = nums.size(); vector&lt;int&gt; lis(len, 1); for (int i = 1; i &lt; len; ++i) { for (int j = 0; j &lt; i; ++j) { if (nums[j] &lt; nums[i] &amp;&amp; (lis[i] &lt; lis[j] + 1)) { lis[i] = 1 + lis[j]; } } } return *max_element(lis.begin(), lis.end()); }}; Java 123456789101112131415161718192021222324252627282930public class Solution { /** * @param nums: The integer array * @return: The length of LIS (longest increasing subsequence) */ public int longestIncreasingSubsequence(int[] nums) { if (nums == null || nums.length == 0) return 0; int[] lis = new int[nums.length]; Arrays.fill(lis, 1); for (int i = 1; i &lt; nums.length; i++) { for (int j = 0; j &lt; i; j++) { if (nums[j] &lt; nums[i] &amp;&amp; (lis[i] &lt; lis[j] + 1)) { lis[i] = lis[j] + 1; } } } // get the max lis int max_lis = 0; for (int i = 0; i &lt; lis.length; i++) { if (lis[i] &gt; max_lis) { max_lis = lis[i]; } } return max_lis; }} 源码分析 初始化数组，初始值为1 根据状态转移方程递推求得lis[i] 遍历lis 数组求得最大值 复杂度分析 使用了与 nums 等长的空间，空间复杂度 \\[O(n)\\]. 两重 for 循环时间复杂度为 \\[O(n^2)\\], 遍历求得最大值，时间复杂度为 \\[O(n)\\], 故总的时间复杂度为 \\[O(n^2)\\]. 题解2 - 巧用 lower_bound 谢谢 @mckelvin 补充！在题解1中我们每次更新 LIS 的值时均遍历了之前的值，那么这里面是否存在重复判断从而可以优化时间复杂度的方法呢？由 LIS 的定义可知，最终构成 LIS 的数列一定是一个递增有序数列，求 LIS 即在构造 LIS 递增数列，最终输出该数列长度即可。这种场景使用 lower_bound 十分合适，即首先将数组第一个元素置于lis 第一个元素，随后如果元素比 lis 中的最后一个元素还要大，则将该元素加入至 lis 末尾，反之则将其放入指定的位置。这里有个小小的问题就是最终构成的 lis 并不一定是题目要求的 lis, 只是在长度上和题目要求的结果一致。Binary Search - lower/upper bound 中对 lower_bound 的实现做了详述。 C++ 123456789101112131415161718class Solution {public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) { if (nums.empty()) return 0; vector&lt;int&gt; lis; for (int i = 0; i &lt; nums.size(); ++i) { vector&lt;int&gt;::iterator it = lower_bound(lis.begin(), lis.end(), nums[i]); if (it == lis.end()) { lis.push_back(nums[i]); } else { *it = nums[i]; } } return lis.size(); }}; 源码分析 需要注意的是 lower_bound 的使用，需要找 nums[index] &gt;= target, min(index). 复杂度分析 最坏空间复杂度为和 nums 等长，\\[O(n)\\]. for 循环加上二分查找最坏情况下时间复杂度为 \\[O(n \\log n)\\] Follow up 上述问题均只输出最大值，现在需要输出 LIS 中的每一个原始元素值。 题解1 - LIS 由于以上递归推导式只能返回最大值，如果现在需要返回 LIS 中的每个元素，直观来讲，构成 LIS 数组中的值对应的原数组值即为我们想要的结果。我们不妨从后往前考虑，依次移除 lis[i] 数组中的值(减一)和索引，遇到和 lis[i]的值相等的 LIS 时即加入到最终返回结果。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.*;public class Solution { /** * @param nums: The integer array * @return: LIS array */ public int[] longestIncreasingSubsequence(int[] nums) { if (nums == null || nums.length == 0) return null; int[] lis = new int[nums.length]; Arrays.fill(lis, 1); for (int i = 1; i &lt; nums.length; i++) { for (int j = 0; j &lt; i; j++) { if (nums[j] &lt;= nums[i] &amp;&amp; (lis[i] &lt; lis[j] + 1)) { lis[i] = lis[j] + 1; } } } // get the max lis int max_lis = 0, index = 0; for (int i = 0; i &lt; lis.length; i++) { if (lis[i] &gt; max_lis) { max_lis = lis[i]; index = i; } } // get result int[] result = new int[max_lis]; for (int i = index; i &gt;= 0; i--) { if (lis[i] == max_lis) { result[max_lis - 1] = nums[i]; max_lis--; } } return result; } public static void main(String[] args) { int[] nums = new int[]{5, 4, 1, 2, 3}; Solution sol = new Solution(); int[] result = sol.longestIncreasingSubsequence(nums); for (int i : result) { System.out.println(i); } }} 关于// get result 那一节中为何max_lis 自减一定是会得到最终想要的结果？假如有和其一样的lis如何破？根据 DP 中状态的定义可知正好为其逆过程，只不过答案不唯一，反向输出的答案输出的是最靠右的结果。","link":"/posts/599181926.html"},{"title":"Maximal Square to bottom","text":"Maximal Square Question leetcode: Maximal Square | LeetCode OJ lintcode: Maximal Square Problem Statement Given a 2D binary matrix filled with 0's and 1's, find the largest square containing all 1's and return its area. Example For example, given the following matrix: 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 Return 4. 题解 第一次遇到这个题是在嘀嘀打车现场面试中，首先把题意理解错了，而且动态规划的状态定义错了，没搞出来... 所以说明确题意非常重要！ 题意是问矩阵中子正方形（不是长方形）的最大面积。也就是说我们的思路应该是去判断正方形这一子状态以及相应的状态转移方程。正方形的可能有边长为1，2，3等等... 边长为2的可由边长为1 的转化而来，边长为3的可由边长为2的转化而来。那么问题来了，边长的转化是如何得到的？边长由1变为2容易得知，即左上、左边以及上边的值均为1，边长由2变为3这一状态转移方程不容易直接得到。直观上来讲，我们需要边长为3的小正方形内格子中的数均为1. 抽象来讲也可以认为边长为3的正方形是由若干个边长为2的正方形堆叠得到的，这就是这道题的核心状态转移方程。 令状态dp[i][j]表示为从左上角(不一定是(0,0))到矩阵中坐标(i,j)为止能构成正方形的最大边长。那么有如下状态转移方程： 12dp[i][j] = min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) + 1; if matrix[i][j] == 1dp[i][j] = 0; if matrix[i][j] = 0 初始化直接用第一行和第一列即可。 Java 123456789101112131415161718192021222324252627282930313233343536373839public class Solution { /** * @param matrix: a matrix of 0 and 1 * @return: an integer */ public int maxSquare(int[][] matrix) { int side = 0; if (matrix == null || matrix.length == 0 || matrix[0].length == 0) { return side; } final int ROW = matrix.length, COL = matrix[0].length; int[][] dp = new int[ROW][COL]; for (int i = 0; i &lt; ROW; i++) { dp[i][0] = matrix[i][0]; side = 1; } for (int i = 0; i &lt; COL; i++) { dp[0][i] = matrix[0][i]; side = 1; } for (int i = 1; i &lt; ROW; i++) { side = Math.max(side, matrix[i][0]); for (int j = 1; j &lt; COL; j++) { if (matrix[i][j] == 1) { dp[i][j] = 1 + minTri(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]); side = Math.max(side, dp[i][j]); } } } return side * side; } private int minTri(int a, int b, int c) { return Math.min(a, Math.min(b, c)); }} 源码分析 经典的动规实现三步走。先初始化，后转移方程，最后对结果做必要的处理（边长 side 的更新）。 复杂度分析 使用了二维矩阵，空间复杂度 \\[O(mn)\\]. 遍历一次原矩阵，时间复杂度 \\[O(mn)\\]. Follow up 题目问的是子正方形，如果问的是矩形呢？ 转移方程仍然可以不变，但是遍历完之后需要做进一步处理，比如如果不是正方形的话可能会出现多个相同的边长值，此时需要对相同的边长值递增(按行或者按列)，相乘后保存，最后取最大输出。 Reference Maximum size square sub-matrix with all 1s - GeeksforGeeks maximal-square/ 参考程序 Java/C++/Python - 空间复杂度可进一步优化(只保存最近的两行即可)","link":"/posts/1312102250.html"},{"title":"n-step Bootstrapping","text":"n-step Bootstrapping简介 是MC方法和一步TD方法的结合。 是资格痕迹（eligibility traces，具体见第12章）的基础先验知识。 n-step TD Prediction MC对应的回报：\\(G_t = R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{T-t-1}R_T\\) 一步TD对应的回报：\\(G_{t:t+1} = R_{t+1} + \\gamma V_t(S_{t+1})\\) n-step的回报：\\(G_{t:t+n} = R_{t+1} + \\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^nV_{t+n+1}(S_{t+n})\\) 核心代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def temporal_difference(value, n, alpha): ... # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # choose an action randomly if np.random.binomial(1, 0.5) == 1: next_state = state + 1 else: next_state = state - 1 if next_state == 0: reward = -1 elif next_state == 20: reward = 1 else: reward = 0 # store new state and new reward states.append(next_state) rewards.append(reward) if next_state in END_STATES: T = time # get the time of the state to update update_time = time - n if update_time &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(update_time + 1, min(T, update_time + n) + 1): returns += pow(GAMMA, t - update_time - 1) * rewards[t] # add state value to the return if update_time + n &lt;= T: returns += pow(GAMMA, n) * value[states[(update_time + n)]] state_to_update = states[update_time] # update the state value if not state_to_update in END_STATES: value[state_to_update] += alpha * (returns - value[state_to_update]) if update_time == T - 1: break state = next_state n-step Sarsa n-step Sarsa: 回报：\\(G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{n-1} R_{t+n} + \\gamma^nQ_{t+n-1}(S_{t+n}, A_{t+n})\\) 更新动作价值函数：\\(Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \\alpha[G_{t:t+n} - Q_{t+n-1}(S_t,A_t)]\\) n-step Expected Sarsa: 回报：\\(G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{n-1} R_{t+n} + \\gamma^n \\bar V_{t+n-1}(S_{t+n})\\) 更新价值函数：\\(\\bar V_t(s) = \\sum_a \\pi(a|s)Q_t(s,a)\\) n-step off-policy with Importance Sampling 更新动作价值函数：\\(Q_{t+n}(S_t， A_t) = Q_{t+n-1}(S_t, A_t) + \\alpha \\rho_{t:t+n-1}[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]\\) 其中，重要性采样比例：\\(\\rho_{t:h} = \\prod_{k=t}^{min(h,T-1)}\\frac{\\pi(A_k|S_k)}{b(A_k, S_k)}\\) n-step Tree Backup Algorithm 不需要重要性采样。 使用所有叶子节点的动作价值函数去更新动作价值函数。 回报：\\(G_{t:t+n} = R_{t+1} + \\gamma \\sum_{a\\neq A_{t+1}}\\pi(a|S_{t+1})Q_{t+n-1}(S_{t+1}, a) + \\gamma\\pi(A_{t+1}|S_{t+1})G_{t+1:t+n}\\) n-step \\(Q(\\sigma)\\) \\(\\sigma\\)代表是否使用全采样。 回报：\\(G_{t:h} = R_{t+1} + \\gamma(\\sigma_{t+1}\\rho_{t+1}+(1-\\sigma_{t+1})\\pi(A_{t+1}|S_{t+1}))(G_{t+1:h}-Q_{h-1}(S_{t+1}, A_{t+1})) + \\gamma \\bar V_{h-1}(S_{t+1})\\)","link":"/posts/9305.html"},{"title":"Nuts and Bolts Problem","text":"Question lintcode: (399) Nuts &amp; Bolts Problem 12345678910111213141516171819202122232425262728Given a set of n nuts of different sizes and n bolts of different sizes.There is a one-one mapping between nuts and bolts.Comparison of a nut to another nut or a bolt to another bolt is not allowed.It means nut can only be compared with bolt and bolt can onlybe compared with nut to see which one is bigger/smaller.We will give you a compare function to compare nut with bolt.ExampleGiven nuts = ['ab','bc','dd','gg'], bolts = ['AB','GG', 'DD', 'BC'].Your code should find the matching bolts and nuts.one of the possible return:nuts = ['ab','bc','dd','gg'], bolts = ['AB','BC','DD','GG'].we will tell you the match compare function.If we give you another compare function.the possible return is the following:nuts = ['ab','bc','dd','gg'], bolts = ['BC','AA','DD','GG'].So you must use the compare function that we give to do the sorting.The order of the nuts or bolts does not matter.You just need to find the matching bolt for each nut. 题解 首先结合例子读懂题意，本题为 nuts 和 bolts 的配对问题，但是需要根据题目所提供的比较函数，且 nuts 与 nuts 之间的元素无法直接比较，compare 仅能在 nuts 与 bolts 之间进行。首先我们考虑若没有比较函数的限制，那么我们可以分别对 nuts 和 bolts 进行排序，由于是一一配对，故排完序后即完成配对。那么在只能通过比较对方元素得知相对大小时怎么完成排序呢？ 我们容易通过以一组元素作为参考进行遍历获得两两相等的元素，这样一来在最坏情况下时间复杂度为 \\[O(n^2)\\], 相当于冒泡排序。根据排序算法理论可知基于比较的排序算法最好的时间复杂度为 \\[O(n \\log n)\\], 也就是说这道题应该是可以进一步优化。回忆一些基于比较的排序算法，能达到 \\[O(n \\log n)\\] 时间复杂度的有堆排、归并排序和快速排序，由于这里只能通过比较得到相对大小的关系，故可以联想到快速排序。 快速排序的核心即为定基准，划分区间。由于这里只能以对方的元素作为基准，故一趟划分区间后仅能得到某一方基准元素排序后的位置，那通过引入 \\[O(n)\\] 的额外空间来对已处理的基准元素进行标记如何呢？这种方法实现起来较为困难，因为只能对一方的元素划分区间，而对方的元素无法划分区间进而导致递归无法正常进行。 山穷水尽疑无路，柳暗花明又一村。由于只能通过对方进行比较，故需要相互配合进行 partition 操作(这个点确实难以想到)。核心在于：首先使用 nuts 中的某一个元素作为基准对 bolts 进行 partition 操作，随后将 bolts 中得到的基准元素作为基准对 nuts 进行 partition 操作。 Python 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# class Comparator:# def cmp(self, a, b)# You can use Compare.cmp(a, b) to compare nuts &quot;a&quot; and bolts &quot;b&quot;,# if &quot;a&quot; is bigger than &quot;b&quot;, it will return 1, else if they are equal,# it will return 0, else if &quot;a&quot; is smaller than &quot;b&quot;, it will return -1.# When &quot;a&quot; is not a nut or &quot;b&quot; is not a bolt, it will return 2, which is not valid.class Solution: # @param nuts: a list of integers # @param bolts: a list of integers # @param compare: a instance of Comparator # @return: nothing def sortNutsAndBolts(self, nuts, bolts, compare): if nuts is None or bolts is None: return if len(nuts) != len(bolts): return self.qsort(nuts, bolts, 0, len(nuts) - 1, compare) def qsort(self, nuts, bolts, l, u, compare): if l &gt;= u: return # find the partition index for nuts with bolts[l] part_inx = self.partition(nuts, bolts[l], l, u, compare) # partition bolts with nuts[part_inx] self.partition(bolts, nuts[part_inx], l, u, compare) # qsort recursively self.qsort(nuts, bolts, l, part_inx - 1, compare) self.qsort(nuts, bolts, part_inx + 1, u, compare) def partition(self, alist, pivot, l, u, compare): m = l i = l + 1 while i &lt;= u: if compare.cmp(alist[i], pivot) == -1 or \\ compare.cmp(pivot, alist[i]) == 1: m += 1 alist[i], alist[m] = alist[m], alist[i] i += 1 elif compare.cmp(alist[i], pivot) == 0 or \\ compare.cmp(pivot, alist[i]) == 0: # swap nuts[l]/bolts[l] with pivot alist[i], alist[l] = alist[l], alist[i] else: i += 1 # move pivot to proper index alist[l], alist[m] = alist[m], alist[l] return m C++ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * class Comparator { * public: * int cmp(string a, string b); * }; * You can use compare.cmp(a, b) to compare nuts &quot;a&quot; and bolts &quot;b&quot;, * if &quot;a&quot; is bigger than &quot;b&quot;, it will return 1, else if they are equal, * it will return 0, else if &quot;a&quot; is smaller than &quot;b&quot;, it will return -1. * When &quot;a&quot; is not a nut or &quot;b&quot; is not a bolt, it will return 2, which is not valid.*/class Solution {public: /** * @param nuts: a vector of integers * @param bolts: a vector of integers * @param compare: a instance of Comparator * @return: nothing */ void sortNutsAndBolts(vector&lt;string&gt; &amp;nuts, vector&lt;string&gt; &amp;bolts, Comparator compare) { if (nuts.empty() || bolts.empty()) return; if (nuts.size() != bolts.size()) return; qsort(nuts, bolts, compare, 0, nuts.size() - 1); }private: void qsort(vector&lt;string&gt;&amp; nuts, vector&lt;string&gt;&amp; bolts, Comparator compare, int l, int u) { if (l &gt;= u) return; // find the partition index for nuts with bolts[l] int part_inx = partition(nuts, bolts[l], compare, l, u); // partition bolts with nuts[part_inx] partition(bolts, nuts[part_inx], compare, l, u); // qsort recursively qsort(nuts, bolts, compare, l, part_inx - 1); qsort(nuts, bolts, compare, part_inx + 1, u); } int partition(vector&lt;string&gt;&amp; str, string&amp; pivot, Comparator compare, int l, int u) { int m = l; for (int i = l + 1; i &lt;= u; ++i) { if (compare.cmp(str[i], pivot) == -1 || compare.cmp(pivot, str[i]) == 1) { ++m; std::swap(str[m], str[i]); } else if (compare.cmp(str[i], pivot) == 0 || compare.cmp(pivot, str[i]) == 0) { // swap nuts[l]/bolts[l] with pivot std::swap(str[i], str[l]); --i; } } // move pivot to proper index std::swap(str[m], str[l]); return m; }}; Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * public class NBCompare { * public int cmp(String a, String b); * } * You can use compare.cmp(a, b) to compare nuts &quot;a&quot; and bolts &quot;b&quot;, * if &quot;a&quot; is bigger than &quot;b&quot;, it will return 1, else if they are equal, * it will return 0, else if &quot;a&quot; is smaller than &quot;b&quot;, it will return -1. * When &quot;a&quot; is not a nut or &quot;b&quot; is not a bolt, it will return 2, which is not valid.*/public class Solution { /** * @param nuts: an array of integers * @param bolts: an array of integers * @param compare: a instance of Comparator * @return: nothing */ public void sortNutsAndBolts(String[] nuts, String[] bolts, NBComparator compare) { if (nuts == null || bolts == null) return; if (nuts.length != bolts.length) return; qsort(nuts, bolts, compare, 0, nuts.length - 1); } private void qsort(String[] nuts, String[] bolts, NBComparator compare, int l, int u) { if (l &gt;= u) return; // find the partition index for nuts with bolts[l] int part_inx = partition(nuts, bolts[l], compare, l, u); // partition bolts with nuts[part_inx] partition(bolts, nuts[part_inx], compare, l, u); // qsort recursively qsort(nuts, bolts, compare, l, part_inx - 1); qsort(nuts, bolts, compare, part_inx + 1, u); } private int partition(String[] str, String pivot, NBComparator compare, int l, int u) { // int m = l; for (int i = l + 1; i &lt;= u; i++) { if (compare.cmp(str[i], pivot) == -1 || compare.cmp(pivot, str[i]) == 1) { // m++; swap(str, i, m); } else if (compare.cmp(str[i], pivot) == 0 || compare.cmp(pivot, str[i]) == 0) { // swap nuts[l]/bolts[l] with pivot swap(str, i, l); i--; } } // move pivot to proper index swap(str, m, l); return m; } private void swap(String[] str, int l, int r) { String temp = str[l]; str[l] = str[r]; str[r] = temp; }} 源码分析 难以理解的可能在partition部分，不仅需要使用compare.cmp(alist[i], pivot), 同时也需要使用compare.cmp(pivot, alist[i]), 否则答案有误。第二个在于alist[i] == pivot时，需要首先将其和alist[l]交换，因为i是从l+1开始处理的，将alist[l]换过来后可继续和 pivot 进行比较。在 while 循环退出后在将当前遍历到的小于 pivot 的元素 alist[m] 和 alist[l] 交换，此时基准元素正确归位。对这一块不是很清楚的举个例子就明白了。 复杂度分析 快排的思路，时间复杂度为 \\[O(2n \\log n)\\], 使用了一些临时变量，空间复杂度 \\[O(1)\\]. Reference LintCode/Nuts &amp; Bolts Problem.py at master · algorhythms/LintCode","link":"/posts/1954136665.html"},{"title":"optimize water distribution in a village","text":"题目地址 https://leetcode.com/problems/optimize-water-distribution-in-a-village/ ## 题目描述 1234567891011121314151617181920212223There are n houses in a village. We want to supply water for all the houses by building wells and laying pipes.For each house i, we can either build a well inside it directly with cost wells[i], or pipe in water from another well to it. The costs to lay pipes between houses are given by the array pipes, where each pipes[i] = [house1, house2, cost] represents the cost to connect house1 and house2 together using a pipe. Connections are bidirectional.Find the minimum total cost to supply water to all houses.Example 1:Input: n = 3, wells = [1,2,2], pipes = [[1,2,1],[2,3,1]]Output: 3Explanation: The image shows the costs of connecting houses using pipes.The best strategy is to build a well in the first house with cost 1 and connect the other houses to it with cost 2 so the total cost is 3.Constraints:1 &lt;= n &lt;= 10000wells.length == n0 &lt;= wells[i] &lt;= 10^51 &lt;= pipes.length &lt;= 100001 &lt;= pipes[i][0], pipes[i][1] &lt;= n0 &lt;= pipes[i][2] &lt;= 10^5pipes[i][0] != pipes[i][1] example 1 pic: 思路 题意，在每个城市打井需要一定的花费，也可以用其他城市的井水，城市之间建立连接管道需要一定的花费，怎么样安排可以花费最少的前灌溉所有城市。 这是一道连通所有点的最短路径/最小生成树问题，把城市看成图中的点，管道连接城市看成是连接两个点之间的边。这里打井的花费是直接在点上，而且并不是所有 点之间都有边连接，为了方便，我们可以假想一个点（root）0，这里自身点的花费可以与 0 连接，花费可以是 0-i 之间的花费。这样我们就可以构建一个连通图包含所有的点和边。 那在一个连通图中求最短路径/最小生成树的问题. 参考延伸阅读中，维基百科针对这类题给出的几种解法。 解题步骤： 1. 创建 POJO EdgeCost(node1, node2, cost) - 节点1 和 节点2 连接边的花费。 2. 假想一个root 点 0，构建图 3. 连通所有节点和 0，[0,i] - i 是节点 [1,n]，0-1 是节点 0 和 1 的边，边的值是节点 i 上打井的花费 wells[i]; 4. 把打井花费和城市连接点转换成图的节点和边。 5. 对图的边的值排序（从小到大） 6. 遍历图的边，判断两个节点有没有连通 （Union-Find）， - 已连通就跳过，继续访问下一条边 - 没有连通，记录花费，连通节点 7. 若所有节点已连通，求得的最小路径即为最小花费，返回 8. 对于每次union, 节点数 n-1, 如果 n==0 说明所有节点都已连通，可以提前退出，不需要继续访问剩余的边。 这里用加权Union-Find 判断两个节点是否连通，和连通未连通的节点。 举例：n = 5, wells=[1,2,2,3,2], pipes=[[1,2,1],[2,3,1],[4,5,7]] 如图： 从图中可以看到，最后所有的节点都是连通的。 复杂度分析 时间复杂度: O(ElogE) - E 是图的边的个数 空间复杂度: O(E) 一个图最多有 n(n-1)/2 - n 是图中节点个数 条边 （完全连通图） 关键点分析 构建图，得出所有边 对所有边排序 遍历所有的边（从小到大） 对于每条边，检查是否已经连通，若没有连通，加上边上的值，连通两个节点。若已连通，跳过。 代码 (Java/Python3) Java code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class OptimizeWaterDistribution { public int minCostToSupplyWater(int n, int[] wells, int[][] pipes) { List&lt;EdgeCost&gt; costs = new ArrayList&lt;&gt;(); for (int i = 1; i &lt;= n; i++) { costs.add(new EdgeCost(0, i, wells[i - 1])); } for (int[] p : pipes) { costs.add(new EdgeCost(p[0], p[1], p[2])); } Collections.sort(costs); int minCosts = 0; UnionFind uf = new UnionFind(n); for (EdgeCost edge : costs) { int rootX = uf.find(edge.node1); int rootY = uf.find(edge.node2); if (rootX == rootY) continue; minCosts += edge.cost; uf.union(edge.node1, edge.node2); // for each union, we connnect one node n--; // if all nodes already connected, terminate early if (n == 0) { return minCosts; } } return minCosts; } class EdgeCost implements Comparable&lt;EdgeCost&gt; { int node1; int node2; int cost; public EdgeCost(int node1, int node2, int cost) { this.node1 = node1; this.node2 = node2; this.cost = cost; } @Override public int compareTo(EdgeCost o) { return this.cost - o.cost; } } class UnionFind { int[] parent; int[] rank; public UnionFind(int n) { parent = new int[n + 1]; for (int i = 0; i &lt;= n; i++) { parent[i] = i; } rank = new int[n + 1]; } public int find(int x) { return x == parent[x] ? x : find(parent[x]); } public void union(int x, int y) { int px = find(x); int py = find(y); if (px == py) return; if (rank[px] &gt;= rank[py]) { parent[py] = px; rank[px] += rank[py]; } else { parent[px] = py; rank[py] += rank[px]; } } }} Pythong3 code 1234567891011121314151617181920212223class Solution: def minCostToSupplyWater(self, n: int, wells: List[int], pipes: List[List[int]]) -&gt; int: union_find = {i: i for i in range(n + 1)} def find(x): return x if x == union_find[x] else find(union_find[x]) def union(x, y): px = find(x) py = find(y) union_find[px] = py graph_wells = [[cost, 0, i] for i, cost in enumerate(wells, 1)] graph_pipes = [[cost, i, j] for i, j, cost in pipes] min_costs = 0 for cost, x, y in sorted(graph_wells + graph_pipes): if find(x) == find(y): continue union(x, y) min_costs += cost n -= 1 if n == 0: return min_costs 延伸阅读 最短路径问题 Dijkstra算法 Floyd-Warshall算法 Bellman-Ford算法 Kruskal算法 Prim's 算法 最小生成树","link":"/posts/171901588.html"},{"title":"Palindrome Partitioning II","text":"Question leetcode: Palindrome Partitioning II | LeetCode OJ lintcode: (108) Palindrome Partitioning II 12345678910Given a string s, cut s into some substrings such thatevery substring is a palindrome.Return the minimum cuts needed for a palindrome partitioning of s.ExampleFor example, given s = &quot;aab&quot;,Return 1 since the palindrome partitioning [&quot;aa&quot;,&quot;b&quot;] could be producedusing 1 cut. 题解1 - 仅对最小切割数使用动态规划 此题为难题，费了我九牛二虎之力才bug-free :( 求最小切分数，非常明显的动规暗示。由问题出发可建立状态f[i] 表示到索引i 处时需要的最少切割数(即切割前 i 个字符组成的字符串)，状态转移方程为f[i] = min{1 + f[j]}, where j &lt; i and substring [j, i] is palindrome, 判断区间[j, i] 是否为回文简单的方法可反转后比较。 Python 123456789101112131415class Solution: # @param s, a string # @return an integer def minCut(self, s): if not s: print 0 cut = [i - 1 for i in xrange(1 + len(s))] for i in xrange(1 + len(s)): for j in xrange(i): # s[j:i] is palindrome if s[j:i] == s[j:i][::-1]: cut[i] = min(cut[i], 1 + cut[j]) return cut[-1] 源码分析 当 s 为 None 或者列表为空时返回0 初始化切割数数组 子字符串的索引位置可为[0, len(s) - 1], 内循环 j 比外循环 i 小，故可将 i 的最大值设为1 + len(s) 较为便利。 回文的判断使用了[::-1] 对字符串进行反转 最后返回数组最后一个元素 复杂度分析 两重循环，遍历的总次数为 \\[1/2 \\cdots n^2)\\], 每次回文的判断时间复杂度为 \\[O(len(s))\\], 故总的时间复杂度近似为 \\[O(n^3)\\]. 在 s 长度较长时会 TLE. 使用了与 s 等长的辅助切割数数组，空间复杂度近似为 \\[O(n)\\]. 题解2 - 使用动态规划计算子字符串回文状态 切割数部分使用的是动态规划，优化的空间不大，仔细瞅瞅可以发现在判断字符串是否为回文的部分存在大量重叠计算，故可引入动态规划进行优化，时间复杂度可优化至到平方级别。 定义状态 PaMat[i][j] 为区间 [i,j] 是否为回文的标志, 对应此状态的子问题可从回文的定义出发，如果字符串首尾字符相同且在去掉字符串首尾字符后字符串仍为回文，则原字符串为回文，相应的状态转移方程 PaMat[i][j] = s[i] == s[j] &amp;&amp; PaMat[i+1][j-1], 由于状态转移方程中依赖比i大的结果，故实现中需要从索引大的往索引小的递推，另外还需要考虑一些边界条件和初始化方式，做到 bug-free 需要点时间。 Python 12345678910111213141516171819202122232425262728class Solution: # @param s, a string # @return an integer def minCut(self, s): if not s: print 0 cut = [i - 1 for i in xrange(1 + len(s))] PaMatrix = self.getMat(s) for i in xrange(1 + len(s)): for j in xrange(i): if PaMatrix[j][i - 1]: cut[i] = min(cut[i], cut[j] + 1) return cut[-1] def getMat(self, s): PaMat = [[True for i in xrange(len(s))] for j in xrange(len(s))] for i in xrange(len(s), -1, -1): for j in xrange(i, len(s)): if j == i: PaMat[i][j] = True # not necessary if init with True # elif j == i + 1: # PaMat[i][j] = s[i] == s[j] else: PaMat[i][j] = s[i] == s[j] and PaMat[i + 1][j - 1] return PaMat C++ 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution {public: int minCut(string s) { if (s.empty()) return 0; int len = s.size(); vector&lt;int&gt; cut; for (int i = 0; i &lt; 1 + len; ++i) { cut.push_back(i - 1); } vector&lt;vector&lt;bool&gt; &gt; mat = getMat(s); for (int i = 1; i &lt; 1 + len; ++i) { for (int j = 0; j &lt; i; ++j) { if (mat[j][i - 1]) { cut[i] = min(cut[i], 1 + cut[j]); } } } return cut[len]; } vector&lt;vector&lt;bool&gt; &gt; getMat(string s) { int len = s.size(); vector&lt;vector&lt;bool&gt; &gt; mat = vector&lt;vector&lt;bool&gt; &gt;(len, vector&lt;bool&gt;(len, true)); for (int i = len; i &gt;= 0; --i) { for (int j = i; j &lt; len; ++j) { if (j == i) { mat[i][j] = true; } else if (j == i + 1) { mat[i][j] = (s[i] == s[j]); } else { mat[i][j] = ((s[i] == s[j]) &amp;&amp; mat[i + 1][j - 1]); } } } return mat; }}; Java 1234567891011121314151617181920212223242526272829303132333435363738394041public class Solution { public int minCut(String s) { if (s == null || s.length() == 0) return 0; int len = s.length(); int[] cut = new int[1 + len]; for (int i = 0; i &lt; 1 + len; ++i) { cut[i] = i - 1; } boolean[][] mat = paMat(s); for (int i = 1; i &lt; 1 + len; i++) { for (int j = 0; j &lt; i; j++) { if (mat[j][i - 1]) { cut[i] = Math.min(cut[i], 1 + cut[j]); } } } return cut[len]; } private boolean[][] paMat(String s) { int len = s.length(); boolean[][] mat = new boolean[len][len]; for (int i = len - 1; i &gt;= 0; i--) { for (int j = i; j &lt; len; j++) { if (j == i) { mat[i][j] = true; } else if (j == i + 1) { mat[i][j] = (s.charAt(i) == s.charAt(j)); } else { mat[i][j] = (s.charAt(i) == s.charAt(j)) &amp;&amp; mat[i + 1][j - 1]; } } } return mat; }} 源码分析 初始化 cut 长度为1 + len(s), cut[0] = -1 便于状态转移方程实现。在执行mat[i][j] == ... mat[i + 1][j - 1]时前提是j - 1 &gt; i + 1, 所以才需要分情况赋值。使用getMat 得到字符串区间的回文矩阵，由于cut 的长度为1+len(s), 两重 for 循环时需要注意索引的取值，这个地方非常容易错。 复杂度分析 最坏情况下每次 for 循环都遍历 n 次，时间复杂度近似为 \\[O(n^2)\\], 使用了二维回文矩阵保存记忆化搜索结果，空间复杂度为 \\[O(n^2)\\]. Reference Palindrome Partitioning II 参考程序 Java/C++/Python soulmachine 的 leetcode 题解","link":"/posts/3965210520.html"},{"title":"Permutation Sequence","text":"Question leetcode: Permutation Sequence | LeetCode OJ lintcode: (388) Permutation Sequence Problem Statement Given n and k, return the k-th permutation sequence. Example For n = 3, all permutations are listed as follows: &quot;123&quot; &quot;132&quot; &quot;213&quot; &quot;231&quot; &quot;312&quot; &quot;321&quot; If k = 4, the fourth permutation is \"231\" Note n will be between 1 and 9 inclusive. Challenge O(n*k) in time complexity is easy, can you do it in O(n^2) or less? 题解 和题 Permutation Index 正好相反，这里给定第几个排列的相对排名，输出排列值。和不同进制之间的转化类似，这里的『进制』为1!, 2!..., 以n=3, k=4为例，我们从高位到低位转化，直觉应该是用 k/(n-1)!, 但以 n=3,k=5 和 n=3,k=6 代入计算后发现边界处理起来不太方便，故我们可以尝试将 k 减1进行运算，后面的基准也随之变化。第一个数可以通过(k-1)/(n-1)!进行计算，那么第二个数呢？联想不同进制数之间的转化，我们可以通过求模运算求得下一个数的k-1, 那么下一个数可通过(k2 - 1)/(n-2)!求得，这里不理解的可以通过进制转换类比进行理解。和减掉相应的阶乘值是等价的。 Python 12345678910111213141516171819202122class Solution: &quot;&quot;&quot; @param n: n @param k: the k-th permutation @return: a string, the k-th permutation &quot;&quot;&quot; def getPermutation(self, n, k): # generate factorial list factorial = [1] for i in xrange(1, n + 1): factorial.append(factorial[-1] * i) nums = range(1, n + 1) perm = [] for i in xrange(n): rank = (k - 1) / factorial[n - i - 1] k = (k - 1) % factorial[n - i - 1] + 1 # append and remove nums[rank] perm.append(nums[rank]) nums.remove(nums[rank]) # combine digits return &quot;&quot;.join([str(digit) for digit in perm]) C++ 12345678910111213141516171819202122232425262728293031323334class Solution {public: /** * @param n: n * @param k: the kth permutation * @return: return the k-th permutation */ string getPermutation(int n, int k) { // generate factorial list vector&lt;int&gt; factorial = vector&lt;int&gt;(n + 1, 1); for (int i = 1; i &lt; n + 1; ++i) { factorial[i] = factorial[i - 1] * i; } // generate digits ranging from 1 to n vector&lt;int&gt; nums; for (int i = 1; i &lt; n + 1; ++i) { nums.push_back(i); } vector&lt;int&gt; perm; for (int i = 0; i &lt; n; ++i) { int rank = (k - 1) / factorial[n - i - 1]; k = (k - 1) % factorial[n - i - 1] + 1; // append and remove nums[rank] perm.push_back(nums[rank]); nums.erase(std::remove(nums.begin(), nums.end(), nums[rank]), nums.end()); } // transform a vector&lt;int&gt; to a string std::stringstream result; std::copy(perm.begin(), perm.end(), std::ostream_iterator&lt;int&gt;(result, &quot;&quot;)); return result.str(); }}; Java 1234567891011121314151617181920212223242526272829303132class Solution { /** * @param n: n * @param k: the kth permutation * @return: return the k-th permutation */ public String getPermutation(int n, int k) { if (n &lt;= 0 &amp;&amp; k &lt;= 0) return &quot;&quot;; int fact = 1; // generate nums 1 to n List&lt;Integer&gt; nums = new ArrayList&lt;Integer&gt;(); for (int i = 1; i &lt;= n; i++) { fact *= i; nums.add(i); } // get the permutation digit StringBuilder sb = new StringBuilder(); for (int i = n; i &gt;= 1; i--) { fact /= i; // take care of rank and k int rank = (k - 1) / fact; k = (k - 1) % fact + 1; // ajust the mapping of rank to num sb.append(nums.get(rank)); nums.remove(rank); } return sb.toString(); }} 源码分析 源码结构分为三步走， 建阶乘数组 生成排列数字数组 从高位到低位计算排列数值 复杂度分析 几个 for 循环，时间复杂度为 \\[O(n)\\], 用了与 n 等长的一些数组，空间复杂度为 \\[O(n)\\]. Reference Permutation Sequence 解题报告 Permutation Sequence 参考程序 Java/C++/Python c++ - How to transform a vector into a string? - Stack Overflow","link":"/posts/2505100657.html"},{"title":"主成分分析","text":"主成分分析原理 主成分分析是最常用的一种降维方法。我们首先考虑一个问题：对于正交矩阵空间中的样本点，如何用一个超平面对所有样本进行恰当的表达。容易想到，如果这样的超平面存在，那么他大概应该具有下面的性质。 最近重构性：样本点到超平面的距离都足够近 最大可分性：样本点在这个超平面上的投影尽可能分开 基于最近重构性和最大可分性，能分别得到主成分分析的两种等价推导。 ### 最近重构性 假设我们对样本点进行了中心化，即所有样本的和为0。再假设投影变换后得到的新坐标系为： 若丢弃新坐标系中的部分坐标，将维度降到d'，则样本点\\(x_{i}\\)在低位坐标系中的投影是\\(z_{i}\\) ： 这里\\(z_{ij}\\)是\\(x_{i}\\)在低维坐标系下第j维的坐标。若基于\\(z_{i}\\)来重构\\(x_{i}\\) ，那么可以得到 考虑整个训练集，原样本点和基于投影重构的样本点之间的距离为 根据最近重构性，最小化上面的式子，就可以得到主成分分析的优化目标 最大可分性 从最大可分性出发，我们可以得到主成分分析的另一种解释。我们知道，样本点\\(x_{i}\\)在新空间中超平面上的投影是\\(W^{T}x_{i}\\) ， 若所有样本点的投影能尽可能分开，则应该使投影后样本点的方差最大化。投影后样本点的方差是 于是优化目标可以写为 这个优化目标和上文的优化目标是等价的。对优化目标使用拉格朗日乘子法可得 于是，只需要对协方差矩阵进行特征值分解，将得到的特征值排序，在取前d'个特征值对应的特征向量，即得到主成分分析的解。 源码分析 实例 1234567import org.apache.spark.mllib.linalg.Matriximport org.apache.spark.mllib.linalg.distributed.RowMatrixval mat: RowMatrix = ...// Compute the top 10 principal components.val pc: Matrix = mat.computePrincipalComponents(10) // Principal components are stored in a local dense matrix.// Project the rows to the linear space spanned by the top 10 principal components.val projected: RowMatrix = mat.multiply(pc) 实现代码 主成分分析的实现代码在RowMatrix中实现。源码如下： 123456789101112def computePrincipalComponents(k: Int): Matrix = { val n = numCols().toInt //计算协方差矩阵 val Cov = computeCovariance().toBreeze.asInstanceOf[BDM[Double]] //特征值分解 val brzSvd.SVD(u: BDM[Double], _, _) = brzSvd(Cov) if (k == n) { Matrices.dense(n, k, u.data) } else { Matrices.dense(n, k, Arrays.copyOfRange(u.data, 0, n * k)) } } 这段代码首先会计算样本的协方差矩阵，然后在通过breeze的svd方法进行奇异值分解。这里由于协方差矩阵是方阵，所以奇异值分解等价于特征值分解。下面是计算协方差的代码 12345678910111213141516171819202122232425262728293031def computeCovariance(): Matrix = { val n = numCols().toInt checkNumColumns(n) val (m, mean) = rows.treeAggregate[(Long, BDV[Double])]((0L, BDV.zeros[Double](n)))( seqOp = (s: (Long, BDV[Double]), v: Vector) =&gt; (s._1 + 1L, s._2 += v.toBreeze), combOp = (s1: (Long, BDV[Double]), s2: (Long, BDV[Double])) =&gt; (s1._1 + s2._1, s1._2 += s2._2) ) updateNumRows(m) mean :/= m.toDouble // We use the formula Cov(X, Y) = E[X * Y] - E[X] E[Y], which is not accurate if E[X * Y] is // large but Cov(X, Y) is small, but it is good for sparse computation. // TODO: find a fast and stable way for sparse data. val G = computeGramianMatrix().toBreeze.asInstanceOf[BDM[Double]] var i = 0 var j = 0 val m1 = m - 1.0 var alpha = 0.0 while (i &lt; n) { alpha = m / m1 * mean(i) j = i while (j &lt; n) { val Gij = G(i, j) / m1 - alpha * mean(j) G(i, j) = Gij G(j, i) = Gij j += 1 } i += 1 } Matrices.fromBreeze(G) } 参考文献 【1】 机器学习.周志华","link":"/posts/120024664.html"},{"title":"Permutation Index II","text":"Question lintcode: (198) Permutation Index II Problem Statement Given a permutation which may contain repeated numbers, find its index in all the permutations of these numbers, which are ordered in lexicographical order. The index begins at 1. Example Given the permutation [1, 4, 2, 2], return 3. 题解 题 Permutation Index 的扩展，这里需要考虑重复元素，有无重复元素最大的区别在于原来的1!, 2!, 3!...等需要除以重复元素个数的阶乘，颇有点高中排列组合题的味道。记录重复元素个数同样需要动态更新，引入哈希表这个万能的工具较为方便。 Python 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: # @param {int[]} A an integer array # @return {long} a long integer def permutationIndexII(self, A): if A is None or len(A) == 0: return 0 index = 1 factor = 1 for i in xrange(len(A) - 1, -1, -1): hash_map = {A[i]: 1} rank = 0 for j in xrange(i + 1, len(A)): if A[j] in hash_map.keys(): hash_map[A[j]] += 1 else: hash_map[A[j]] = 1 # get rank if A[i] &gt; A[j]: rank += 1 index += rank * factor / self.dupPerm(hash_map) factor *= (len(A) - i) return index def dupPerm(self, hash_map): if hash_map is None or len(hash_map) == 0: return 0 dup = 1 for val in hash_map.values(): dup *= self.factorial(val) return dup def factorial(self, n): r = 1 for i in xrange(1, n + 1): r *= i return r C++ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Solution {public: /** * @param A an integer array * @return a long integer */ long long permutationIndexII(vector&lt;int&gt;&amp; A) { if (A.empty()) return 0; long long index = 1; long long factor = 1; for (int i = A.size() - 1; i &gt;= 0; --i) { int rank = 0; unordered_map&lt;int, int&gt; hash; ++hash[A[i]]; for (int j = i + 1; j &lt; A.size(); ++j) { ++hash[A[j]]; if (A[i] &gt; A[j]) { ++rank; } } index += rank * factor / dupPerm(hash); factor *= (A.size() - i); } return index; }private: long long dupPerm(unordered_map&lt;int, int&gt; hash) { if (hash.empty()) return 1; long long dup = 1; for (auto it = hash.begin(); it != hash.end(); ++it) { dup *= fact(it-&gt;second); } return dup; } long long fact(int num) { long long val = 1; for (int i = 1; i &lt;= num; ++i) { val *= i; } return val; }}; Java 12345678910111213141516171819202122232425262728293031public class Solution { /** * @param A an integer array * @return a long integer */ public long permutationIndexII(int[] A) { if (A == null || A.length == 0) return 0L; Map&lt;Integer, Integer&gt; hashmap = new HashMap&lt;Integer, Integer&gt;(); long index = 1, fact = 1, multiFact = 1; for (int i = A.length - 1; i &gt;= 0; i--) { // collect its repeated times and update multiFact if (hashmap.containsKey(A[i])) { hashmap.put(A[i], hashmap.get(A[i]) + 1); multiFact *= hashmap.get(A[i]); } else { hashmap.put(A[i], 1); } // get rank every turns int rank = 0; for (int j = i + 1; j &lt; A.length; j++) { if (A[i] &gt; A[j]) rank++; } // do divide by multiFact index += rank * fact / multiFact; fact *= (A.length - i); } return index; }} 源码分析 在计算重复元素个数的阶乘时需要注意更新multiFact的值即可，不必每次都去计算哈希表中的值。对元素A[i]需要加入哈希表 - hash.put(A[i], 1);，设想一下2, 2, 1, 1的计算即可知。 复杂度分析 双重 for 循环，时间复杂度为 \\[O(n^2)\\], 使用了哈希表，空间复杂度为 \\[O(n)\\].","link":"/posts/3423813505.html"},{"title":"快速迭代聚类","text":"谱聚类算法的原理 在分析快速迭代聚类之前，我们先来了解一下谱聚类算法。谱聚类算法是建立在谱图理论的基础上的算法，与传统的聚类算法相比，它能在任意形状的样本空间上聚类且能够收敛到全局最优解。 谱聚类算法的主要思想是将聚类问题转换为无向图的划分问题。 - 首先，数据点被看做一个图的顶点v，两数据的相似度看做图的边，边的集合由\\(E=A_{ij}\\)表示，由此构造样本数据集的相似度矩阵A，并求出拉普拉斯矩阵L。 - 其次，根据划分准则使子图内部相似度尽量大，子图之间的相似度尽量小，计算出L的特征值和特征向量 - 最后，选择k个不同的特征向量对数据点聚类 那么如何求拉普拉斯矩阵呢？ 将相似度矩阵A的每行元素相加就可以得到该顶点的度，我们定义以度为对角元素的对角矩阵称为度矩阵D。可以通过A和D来确定拉普拉斯矩阵。拉普拉斯矩阵分为规范和非规范两种，规范的拉普拉斯矩阵 表示为L=D-A，非规范的拉普拉斯矩阵表示为\\(L=I-D^{-1}A\\) 。 谱聚类算法的一般过程如下： （1）输入待聚类的数据点集以及聚类数k； （2）根据相似性度量构造数据点集的拉普拉斯矩阵L； （3）选取L的前k个（默认从小到大,这里的k和聚类数可以不一样）特征值和特征向量，构造特征向量空间（这实际上是一个降维的过程）； （4）使用传统方法对特征向量聚类，并对应于原始数据的聚类。 谱聚类算法和传统的聚类方法（例如K-means）比起来有不少优点： 和K-medoids类似，谱聚类只需要数据之间的相似度矩阵就可以了，而不必像K-means那样要求数据必须是N维欧氏空间中的向量。 由于抓住了主要矛盾，忽略了次要的东西，因此比传统的聚类算法更加健壮一些，对于不规则的误差数据不是那么敏感，而且性能也要好一些。 计算复杂度比K-means要小，特别是在像文本数据或者平凡的图像数据这样维度非常高的数据上运行的时候。 快速迭代算法和谱聚类算法都是将数据点嵌入到由相似矩阵推导出来的低维子空间中，然后直接或者通过k-means算法产生聚类结果，但是快速迭代算法有不同的地方。下面重点了解快速迭代算法的原理。 快速迭代算法的原理 在快速迭代算法中，我们构造另外一个矩阵\\(W=D^{-1}A\\) ,同第一章做比对，我们可以知道W的最大特征向量就是拉普拉斯矩阵L的最小特征向量。 我们知道拉普拉斯矩阵有一个特性：第二小特征向量（即第二小特征值对应的特征向量）定义了图最佳划分的一个解，它可以近似最大化划分准则。更一般的，k个最小的特征向量所定义的子空间很适合去划分图。 因此拉普拉斯矩阵第二小、第三小直到第k小的特征向量可以很好的将图W划分为k个部分。 注意，矩阵L的k个最小特征向量也是矩阵W的k个最大特征向量。计算一个矩阵最大的特征向量可以通过一个简单的方法来求得，那就是快速迭代（即PI）。 PI是一个迭代方法，它以任意的向量\\(v^{0}\\)作为起始，依照下面的公式循环进行更新。 在上面的公式中，c是标准化常量，是为了避免\\(v^{t}\\)产生过大的值，这里\\(c=||Wv^{t}||_{1}\\) 。在大多数情况下，我们只关心第k（k不为1）大的特征向量，而不关注最大的特征向量。 这是因为最大的特征向量是一个常向量：因为W每一行的和都为1。 快速迭代的收敛性在文献【1】中有详细的证明，这里不再推导。 快速迭代算法的一般步骤如下： 在上面的公式中，输入矩阵W根据\\(W=D^{-1}A\\)来计算。 快速迭代算法的源码实现 在spark中，文件org.apache.spark.mllib.clustering.PowerIterationClustering实现了快速迭代算法。我们从官方给出的例子出发来分析快速迭代算法的实现。 1234567891011121314151617import org.apache.spark.mllib.clustering.{PowerIterationClustering, PowerIterationClusteringModel}import org.apache.spark.mllib.linalg.Vectors// 加载和切分数据val data = sc.textFile(&quot;data/mllib/pic_data.txt&quot;)val similarities = data.map { line =&gt; val parts = line.split(' ') (parts(0).toLong, parts(1).toLong, parts(2).toDouble)}// 使用快速迭代算法将数据分为两类val pic = new PowerIterationClustering() .setK(2) .setMaxIterations(10)val model = pic.run(similarities)//打印出所有的簇model.assignments.foreach { a =&gt; println(s&quot;${a.id} -&gt; ${a.cluster}&quot;)} 在上面的例子中，我们知道数据分为三列，分别是起始id，目标id，以及两者的相似度，这里的similarities代表前面章节提到的矩阵A。有了数据之后，我们通过PowerIterationClustering的run方法来训练模型。 PowerIterationClustering类有三个参数： k：聚类数 maxIterations：最大迭代数 initMode：初始化模式。初始化模式分为Random和Degree两种，针对不同的模式对数据做不同的初始化操作 下面分步骤介绍run方法的实现。 （1）标准化相似度矩阵A到矩阵W 123456789101112131415161718192021222324252627282930def normalize(similarities: RDD[(Long, Long, Double)]): Graph[Double, Double] = { //获得所有的边 val edges = similarities.flatMap { case (i, j, s) =&gt; //相似度值必须非负 if (s &lt; 0.0) { throw new SparkException(&quot;Similarity must be nonnegative but found s($i, $j) = $s.&quot;) } if (i != j) { Seq(Edge(i, j, s), Edge(j, i, s)) } else { None } } //根据edges信息构造图，顶点的特征值默认为0 val gA = Graph.fromEdges(edges, 0.0) //计算从顶点的出发的边的相似度之和，在这里称为度 val vD = gA.aggregateMessages[Double]( sendMsg = ctx =&gt; { ctx.sendToSrc(ctx.attr) }, mergeMsg = _ + _, TripletFields.EdgeOnly) //计算得到W , W=A/D GraphImpl.fromExistingRDDs(vD, gA.edges) .mapTriplets( //gAi/vDi //使用边的权重除以起始点的度 e =&gt; e.attr / math.max(e.srcAttr, MLUtils.EPSILON), TripletFields.Src) } 上面的代码首先通过边集合构造图gA,然后使用aggregateMessages计算每个顶点的度（即所有从该顶点出发的边的相似度之和），构造出VertexRDD。最后使用现有的VertexRDD和EdgeRDD， 相继通过fromExistingRDDs和mapTriplets方法计算得到最终的图W。在mapTriplets方法中，对每一个EdgeTriplet，使用相似度除以出发顶点的度（为什么相除？对角矩阵的逆矩阵是各元素取倒数，\\(W=D^{-1}A\\)就可以通过元素相除得到）。 下面举个例子来说明这个步骤。假设有v1,v2,v3,v4四个点，它们之间的关系如下图所示，并且假设点与点之间的相似度均设为1。 通过该图，我们可以得到相似度矩阵A和度矩阵D，他们分别如下所示。 通过mapTriplets的计算，我们可以得到从点v1到v2,v3,v4的边的权重分别为1/3,1/3,1/3;从点v2到v1,v3,v4的权重分别为1/3,1/3,1/3;从点v3到v1,v2的权重分别为1/2,1/2;从点v4到v1,v2的权重分别为1/2,1/2。 将这个图转换为矩阵的形式，可以得到如下矩阵W。 通过代码计算的结果和通过矩阵运算得到的结果一致。因此该代码实现了\\(W=D^{-1}A\\) 。 （2）初始化\\(v^{0}\\) 根据选择的初始化模式的不同，我们可以使用不同的方法初始化\\(v^{0}\\) 。一种方式是随机初始化，一种方式是度（degree）初始化，下面分别来介绍这两种方式。 随机初始化 123456789101112131415def randomInit(g: Graph[Double, Double]): Graph[Double, Double] = { //给每个顶点指定一个随机数 val r = g.vertices.mapPartitionsWithIndex( (part, iter) =&gt; { val random = new XORShiftRandom(part) iter.map { case (id, _) =&gt; (id, random.nextGaussian()) } }, preservesPartitioning = true).cache() //所有顶点的随机值的绝对值之和 val sum = r.values.map(math.abs).sum() //取平均值 val v0 = r.mapValues(x =&gt; x / sum) GraphImpl.fromExistingRDDs(VertexRDD(v0), g.edges) } 度初始化 1234567def initDegreeVector(g: Graph[Double, Double]): Graph[Double, Double] = { //所有顶点的度之和 val sum = g.vertices.values.sum() //取度的平均值 val v0 = g.vertices.mapValues(_ / sum) GraphImpl.fromExistingRDDs(VertexRDD(v0), g.edges) } 通过初始化之后，我们获得了向量\\(v^{0}\\) 。它包含所有的顶点，但是顶点特征值发生了改变。随机初始化后，特征值为随机值；度初始化后，特征为度的平均值。 在这里，度初始化的向量我们称为“度向量”。度向量会给图中度大的节点分配更多的初始化权重，使其值可以更平均和快速的分布，从而更快的局部收敛。详细情况请参考文献【1】。 （3）快速迭代求最终的v 1234567891011121314151617181920for (iter &lt;- 0 until maxIterations if math.abs(diffDelta) &gt; tol) { val msgPrefix = s&quot;Iteration $iter&quot; // 计算w*vt val v = curG.aggregateMessages[Double]( //相似度与目标点的度相乘 sendMsg = ctx =&gt; ctx.sendToSrc(ctx.attr * ctx.dstAttr), mergeMsg = _ + _, TripletFields.Dst).cache() // 计算||Wvt||_1，即第二章公式中的c val norm = v.values.map(math.abs).sum() val v1 = v.mapValues(x =&gt; x / norm) // 计算v_t+1和v_t的不同 val delta = curG.joinVertices(v1) { case (_, x, y) =&gt; math.abs(x - y) }.vertices.values.sum() diffDelta = math.abs(delta - prevDelta) // 更新v curG = GraphImpl.fromExistingRDDs(VertexRDD(v1), g.edges) prevDelta = delta } 在上述代码中，我们通过aggregateMessages方法计算\\(Wv^{t}\\) 。我们仍然以第（1）步的举例来说明这个方法。假设我们以度来初始化\\(v^{0}\\) ， 在第一次迭代中，我们可以得到v1（注意这里的v1是上面举例的顶点）的特征值为(1/3)*(3/10)+(1/3)*(1/5)+(1/3)*(1/5)=7/30，v2的特征值为7/30，v3的特征值为3/10,v4的特征值为3/10。即满足下面的公式。 （4）使用k-means算法对v进行聚类 123456789def kMeans(v: VertexRDD[Double], k: Int): VertexRDD[Int] = { val points = v.mapValues(x =&gt; Vectors.dense(x)).cache() val model = new KMeans() .setK(k) .setRuns(5) .setSeed(0L) .run(points.values) points.mapValues(p =&gt; model.predict(p)).cache() } 如果对graphX不太了解，可以阅读spark graph使用和源码解析 参考文献 【1】Frank Lin,William W. Cohen.Power Iteration Clustering 【2】漫谈 Clustering (4): Spectral Clustering","link":"/posts/3118787290.html"},{"title":"Route Between Two Nodes in Graph","text":"Question lintcode: (176) Route Between Two Nodes in Graph Find if there is a path between two vertices in a directed graph - GeeksforGeeks Problem Statement Given a directed graph, design an algorithm to find out whether there is a route between two nodes. Example Given graph: A-----&gt;B-----&gt;C \\ | \\ | \\ | \\ v -&gt;D-----&gt;E for s = B and t = E, return true for s = D and t = C, return false 题解1 - DFS 检测图中两点是否通路，图搜索的简单问题，DFS 或者 BFS 均可，注意检查是否有环即可。这里使用哈希表记录节点是否被处理较为方便。深搜时以起点出发，递归处理其邻居节点，需要注意的是处理邻居节点的循环时不是直接 return, 而只在找到路径为真时才返回 true, 否则会过早返回 false 而忽略后续可能满足条件的路径。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Definition for Directed graph. * class DirectedGraphNode { * int label; * ArrayList&lt;DirectedGraphNode&gt; neighbors; * DirectedGraphNode(int x) { * label = x; * neighbors = new ArrayList&lt;DirectedGraphNode&gt;(); * } * } */public class Solution { /** * @param graph: A list of Directed graph node * @param s: the starting Directed graph node * @param t: the terminal Directed graph node * @return: a boolean value */ public boolean hasRoute(ArrayList&lt;DirectedGraphNode&gt; graph, DirectedGraphNode s, DirectedGraphNode t) { Set&lt;DirectedGraphNode&gt; visited = new HashSet&lt;DirectedGraphNode&gt;(); return dfs(graph, s, t, visited); } public boolean dfs(ArrayList&lt;DirectedGraphNode&gt; graph, DirectedGraphNode s, DirectedGraphNode t, Set&lt;DirectedGraphNode&gt; visited) { if (s == t) { return true; } else { // corner cases if (s == null || t == null) return false; // flag visited node, avoid cylic visited.add(s); // compare unvisited neighbor nodes recursively if (s.neighbors.size() &gt; 0) { for (DirectedGraphNode node : s.neighbors) { if (visited.contains(node)) continue; if (dfs(graph, node, t, visited)) return true; } } } return false; }} 源码分析 根据构造函数的实现，Java 中判断是否有邻居节点时使用.size，而不是null. 注意深搜前检测是否被处理过。行 1if (dfs(graph, node, t, visited)) return true; 中注意不是直接 return, 只在为 true 时返回。 复杂度分析 遍历所有点及边，时间复杂度为 \\[O(V+E)\\]. 题解2 - BFS 除了深搜处理邻居节点，我们也可以采用 BFS 结合队列处理，优点是不会爆栈，缺点是空间复杂度稍高和实现复杂点。 Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Definition for Directed graph. * class DirectedGraphNode { * int label; * ArrayList&lt;DirectedGraphNode&gt; neighbors; * DirectedGraphNode(int x) { * label = x; * neighbors = new ArrayList&lt;DirectedGraphNode&gt;(); * } * } */public class Solution { /** * @param graph: A list of Directed graph node * @param s: the starting Directed graph node * @param t: the terminal Directed graph node * @return: a boolean value */ public boolean hasRoute(ArrayList&lt;DirectedGraphNode&gt; graph, DirectedGraphNode s, DirectedGraphNode t) { if (graph == null || s == null || t == null) return false; Queue&lt;DirectedGraphNode&gt; q = new LinkedList&lt;DirectedGraphNode&gt;(); Set&lt;DirectedGraphNode&gt; visited = new HashSet&lt;DirectedGraphNode&gt;(); q.offer(s); while (!q.isEmpty()) { int qLen = q.size(); for (int i = 0; i &lt; qLen; i++) { DirectedGraphNode node = q.poll(); visited.add(node); if (node == t) return true; // push neighbors into queue if (node.neighbors.size() &gt; 0) { for (DirectedGraphNode n : node.neighbors) { // avoid cylic if (visited.contains(n)) continue; q.offer(n); } } } } return false; }} 源码分析 同题解一。 复杂度分析 时间复杂度同题解一，也是 \\[O(V+E)\\], 空间复杂度最坏情况下为两层多叉树，为 \\[O(V+E)\\].","link":"/posts/1031106346.html"},{"title":"流式&#96;k-means&#96;算法","text":"当数据是以流的方式到达的时候，我们可能想动态的估计（estimate）聚类的簇，通过新的到达的数据来更新聚类。spark.mllib支持流式k-means聚类，并且可以通过参数控制估计衰减（decay）(或“健忘”(forgetfulness))。 这个算法使用一般地小批量更新规则来更新簇。 ## 流式k-means算法原理 对每批新到的数据，我们首先将点分配给距离它们最近的簇，然后计算新的数据中心，最后更新每一个簇。使用的公式如下所示： 在上面的公式中，\\(c_{t}\\)表示前一个簇中心，\\(n_{t}\\)表示分配给这个簇的点的数量， \\(x_{t}\\)表示从当前批数据的簇中心，\\(m_{t}\\)表示当前批数据的点数量。 当评价新的数据时，把衰减因子alpha当做折扣加权应用到当前的点上，用以衡量当前预测的簇的贡献度量。当alpha等于1时，所有的批数据赋予相同的权重，当alpha等于0时，数据中心点完全通过当前数据确定。 衰减因子alpha也可以通过halfLife参数联合时间单元（time unit）来确定，时间单元可以是一批数据也可以是一个数据点。假如数据从t时刻到来并定义了halfLife为h， 在t+h时刻，应用到t时刻的数据的折扣（discount）为0.5。 流式k-means算法的步骤如下所示： （1）分配新的数据点到离其最近的簇； （2）根据时间单元（time unit）计算折扣（discount）值，并更新簇权重； （3）应用更新规则； （4）应用更新规则后，有些簇可能消失了，那么切分最大的簇为两个簇。 流式k-means算法源码分析 在分步骤分析源码之前，我们先了解一下StreamingKMeans参数表达的含义。 12345class StreamingKMeans( var k: Int, //簇个数 var decayFactor: Double,//衰减因子 var timeUnit: String //时间单元) 在上述定义中，k表示我们要聚类的个数，decayFactor表示衰减因子，用于计算折扣，timeUnit表示时间单元，时间单元既可以是一批数据（StreamingKMeans.BATCHES）也可以是单条数据（StreamingKMeans.POINTS）。 由于我们处理的是流式数据，所以我们在流式数据来之前要先初始化模型。有两种初始化模型的方法，一种是直接指定初始化中心点及簇权重，一种是随机初始化中心点以及簇权重。 12345678910111213//直接初始化中心点及簇权重def setInitialCenters(centers: Array[Vector], weights: Array[Double]): this.type = { model = new StreamingKMeansModel(centers, weights) this}//随机初始化中心点以及簇权重def setRandomCenters(dim: Int, weight: Double, seed: Long = Utils.random.nextLong): this.type = { val random = new XORShiftRandom(seed) val centers = Array.fill(k)(Vectors.dense(Array.fill(dim)(random.nextGaussian()))) val weights = Array.fill(k)(weight) model = new StreamingKMeansModel(centers, weights) this} 初始化中心点以及簇权重之后，对于新到的流数据，我们使用更新规则修改中心点和权重，调整聚类情况。更新过程在update方法中实现，下面我们分步骤分析该方法。 （1）分配新到的数据到离其最近的簇，并计算更新后的簇的向量和以及点数量 123456789101112131415//选择离数据点最近的簇val closest = data.map(point =&gt; (this.predict(point), (point, 1L)))def predict(point: Vector): Int = { //返回和给定点相隔最近的中心 KMeans.findClosest(clusterCentersWithNorm, new VectorWithNorm(point))._1}// 获得更新的簇的向量和以及点数量val mergeContribs: ((Vector, Long), (Vector, Long)) =&gt; (Vector, Long) = (p1, p2) =&gt; { // y += a * x,向量相加 BLAS.axpy(1.0, p2._1, p1._1) (p1._1, p1._2 + p2._2)}val pointStats: Array[(Int, (Vector, Long))] = closest .aggregateByKey((Vectors.zeros(dim), 0L))(mergeContribs, mergeContribs) .collect() （2）获取折扣值，并用折扣值作用到权重上 1234567891011121314// 折扣val discount = timeUnit match { case StreamingKMeans.BATCHES =&gt; decayFactor case StreamingKMeans.POINTS =&gt; //所有新增点的数量和 val numNewPoints = pointStats.view.map { case (_, (_, n)) =&gt; n }.sum // x^y math.pow(decayFactor, numNewPoints)}//将折扣应用到权重上//x = a * xBLAS.scal(discount, Vectors.dense(clusterWeights)) 上面的代码更加时间单元的不同获得不同的折扣值。当时间单元为StreamingKMeans.BATCHES时，折扣就为衰减因子；当时间单元为StreamingKMeans.POINTS时，折扣由新增数据点的个数n和衰减因子decay共同决定。 折扣值为n个decay相乘。 （3）实现更新规则 12345678910111213// 实现更新规则pointStats.foreach { case (label, (sum, count)) =&gt; //获取中心点 val centroid = clusterCenters(label) //更新权重 val updatedWeight = clusterWeights(label) + count val lambda = count / math.max(updatedWeight, 1e-16) clusterWeights(label) = updatedWeight //x = a * x,即（1-lambda）*centroid BLAS.scal(1.0 - lambda, centroid) // y += a * x，即centroid +=sum*lambda/count BLAS.axpy(lambda / count, sum, centroid)} 上面的代码对每一个簇，首先更新簇的权重，权重值为原有的权重加上新增数据点的个数。然后计算lambda，通过lambda更新中心点。lambda为新增数据的个数和更新权重的商。 假设更新之前的中心点为c1，更新之后的中心点为c2，那么c2=(1-lambda)*c1+sum/count，其中sum/count为所有点的平均值。 （4）调整权重最小和最大的簇 12345678910111213141516171819202122val weightsWithIndex = clusterWeights.view.zipWithIndex//获取权重值最大的簇val (maxWeight, largest) = weightsWithIndex.maxBy(_._1)//获取权重值最小的簇val (minWeight, smallest) = weightsWithIndex.minBy(_._1)//判断权重最小的簇是否过小，如果过小，就将这两个簇重新划分为两个新的簇，权重为两者的均值if (minWeight &lt; 1e-8 * maxWeight) { logInfo(s&quot;Cluster $smallest is dying. Split the largest cluster $largest into two.&quot;) val weight = (maxWeight + minWeight) / 2.0 clusterWeights(largest) = weight clusterWeights(smallest) = weight val largestClusterCenter = clusterCenters(largest) val smallestClusterCenter = clusterCenters(smallest) var j = 0 while (j &lt; dim) { val x = largestClusterCenter(j) val p = 1e-14 * math.max(math.abs(x), 1.0) largestClusterCenter.toBreeze(j) = x + p smallestClusterCenter.toBreeze(j) = x - p j += 1 } }","link":"/posts/2667793592.html"},{"title":"奇异值分解","text":"奇异值分解 在了解特征值分解之后，我们知道，矩阵A不一定是方阵。为了得到方阵，可以将矩阵A的转置乘以该矩阵。从而可以得到公式： 现在假设存在M*N矩阵A，我们的目标是在n维空间中找一组正交基，使得经过A变换后还是正交的。假设已经找到这样一组正交基： A矩阵可以将这组正交基映射为如下的形式。 要使上面的基也为正交基，即使它们两两正交，那么需要满足下面的条件。 如果正交基v选择为\\(A^{T}A\\)的特征向量的话，由于\\(A^{T}A\\)是对称阵，v之间两两正交，那么 由于下面的公式成立 所以取单位向量 可以得到(下面的公式有误，delta_i 应该等于sqrt(lamda_i)) 奇异值分解是一个能适用于任意的矩阵的一种分解的方法，它的形式如下： 其中，U是一个M*M的方阵，它包含的向量是正交的，称为左奇异向量（即上文的u）。sigma是一个M*N的对角矩阵，每个对角线上的元素就是一个奇异值。V是一个N*N的矩阵，它包含的向量是正交的，称为右奇异向量（即上文的v）。 源码分析 MLlib在RowMatrix类中实现了奇异值分解。下面是一个使用奇异值分解的例子。 123456789import org.apache.spark.mllib.linalg.Matriximport org.apache.spark.mllib.linalg.distributed.RowMatriximport org.apache.spark.mllib.linalg.SingularValueDecompositionval mat: RowMatrix = ...// Compute the top 20 singular values and corresponding singular vectors.val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(20, computeU = true)val U: RowMatrix = svd.U // The U factor is a RowMatrix.val s: Vector = svd.s // The singular values are stored in a local dense vector.val V: Matrix = svd.V // The V factor is a local dense matrix. 性能 我们假设n比m小。奇异值和右奇异值向量可以通过方阵\\(A^{T}A\\)的特征值和特征向量得到。左奇异向量通过\\(AVS^{-1}\\)求得。 ml实际使用的方法方法依赖计算花费。 当n很小（n&lt;100）或者k比n大(k&gt;n/2)，我们会首先计算方阵\\(A^{T}A\\) ，然后在driver本地计算它的top特征值和特征向量。它的空间复杂度是O(n*n)，时间复杂度是O(n*n*k)。 否则，我们用分布式的方式先计算\\(A^{T}Av\\),然后把它传给ARPACK在driver上计算top特征值和特征向量。它需要传递O(k)的数据，每个executor的空间复杂度是O(n),driver的空间复杂度是O(nk) 代码实现 12345678910def computeSVD( k: Int, computeU: Boolean = false, rCond: Double = 1e-9): SingularValueDecomposition[RowMatrix, Matrix] = { // 迭代次数 val maxIter = math.max(300, k * 3) // 阈值 val tol = 1e-10 computeSVD(k, computeU, rCond, maxIter, tol, &quot;auto&quot;)} computeSVD(k, computeU, rCond, maxIter, tol, \"auto\")的实现分为三步。分别是选择计算模式，\\(A^{T}A\\)的特征值分解，计算V,U,Sigma。 下面分别介绍这三步。 1 选择计算模式 1234567891011121314151617181920val computeMode = mode match { case &quot;auto&quot; =&gt; if (k &gt; 5000) { logWarning(s&quot;computing svd with k=$k and n=$n, please check necessity&quot;) } if (n &lt; 100 || (k &gt; n / 2 &amp;&amp; n &lt;= 15000)) { // 满足上述条件，首先计算方阵，然后本地计算特征值，避免数据传递 if (k &lt; n / 3) { SVDMode.LocalARPACK } else { SVDMode.LocalLAPACK } } else { // 分布式实现 SVDMode.DistARPACK } case &quot;local-svd&quot; =&gt; SVDMode.LocalLAPACK case &quot;local-eigs&quot; =&gt; SVDMode.LocalARPACK case &quot;dist-eigs&quot; =&gt; SVDMode.DistARPACK} 2 特征值分解 12345678910111213141516val (sigmaSquares: BDV[Double], u: BDM[Double]) = computeMode match { case SVDMode.LocalARPACK =&gt; val G = computeGramianMatrix().toBreeze.asInstanceOf[BDM[Double]] EigenValueDecomposition.symmetricEigs(v =&gt; G * v, n, k, tol, maxIter) case SVDMode.LocalLAPACK =&gt; // breeze (v0.10) svd latent constraint, 7 * n * n + 4 * n &lt; Int.MaxValue val G = computeGramianMatrix().toBreeze.asInstanceOf[BDM[Double]] val brzSvd.SVD(uFull: BDM[Double], sigmaSquaresFull: BDV[Double], _) = brzSvd(G) (sigmaSquaresFull, uFull) case SVDMode.DistARPACK =&gt; if (rows.getStorageLevel == StorageLevel.NONE) { logWarning(&quot;The input data is not directly cached, which may hurt performance if its&quot; + &quot; parent RDDs are also uncached.&quot;) } EigenValueDecomposition.symmetricEigs(multiplyGramianMatrixBy, n, k, tol, maxIter) } 当计算模式是SVDMode.LocalARPACK和SVDMode.LocalLAPACK时，程序实现的步骤是先获取方阵\\(A^{T}A\\) ，在计算其特征值和特征向量。 获取方阵无需赘述，我们只需要注意它无法处理列大于65535的矩阵。我们分别看这两种模式下，如何获取特征值和特征向量。 在SVDMode.LocalARPACK模式下，使用EigenValueDecomposition.symmetricEigs(v =&gt; G * v, n, k, tol, maxIter)计算特征值和特征向量。在SVDMode.LocalLAPACK模式下，直接使用breeze的方法计算。 在SVDMode.DistARPACK模式下，不需要先计算方阵，但是传入EigenValueDecomposition.symmetricEigs方法的函数不同。 1234567891011121314151617private[mllib] def multiplyGramianMatrixBy(v: BDV[Double]): BDV[Double] = { val n = numCols().toInt //v作为广播变量 val vbr = rows.context.broadcast(v) rows.treeAggregate(BDV.zeros[Double](n))( seqOp = (U, r) =&gt; { val rBrz = r.toBreeze val a = rBrz.dot(vbr.value) rBrz match { //计算y += x * a case _: BDV[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[BDV[Double]], U) case _: BSV[_] =&gt; brzAxpy(a, rBrz.asInstanceOf[BSV[Double]], U) case _ =&gt; throw new UnsupportedOperationException } U }, combOp = (U1, U2) =&gt; U1 += U2) } 特征值分解的具体分析在特征值分解中有详细分析，请参考该文了解详情。 3 计算U,V以及Sigma 1234567891011121314151617181920212223242526272829303132333435363738//获取特征值向量val sigmas: BDV[Double] = brzSqrt(sigmaSquares)val sigma0 = sigmas(0)val threshold = rCond * sigma0var i = 0// sigmas的长度可能会小于k// 所以使用 i &lt; min(k, sigmas.length) 代替 i &lt; k.if (sigmas.length &lt; k) { logWarning(s&quot;Requested $k singular values but only found ${sigmas.length} converged.&quot;)}while (i &lt; math.min(k, sigmas.length) &amp;&amp; sigmas(i) &gt;= threshold) { i += 1}val sk = iif (sk &lt; k) { logWarning(s&quot;Requested $k singular values but only found $sk nonzeros.&quot;)}//计算s，也即sigmaval s = Vectors.dense(Arrays.copyOfRange(sigmas.data, 0, sk))//计算Vval V = Matrices.dense(n, sk, Arrays.copyOfRange(u.data, 0, n * sk))//计算U// N = Vk * Sk^{-1}val N = new BDM[Double](n, sk, Arrays.copyOfRange(u.data, 0, n * sk))var i = 0var j = 0while (j &lt; sk) { i = 0 val sigma = sigmas(j) while (i &lt; n) { //对角矩阵的逆即为倒数 N(i, j) /= sigma i += 1 } j += 1}//U=A * Nval U = this.multiply(Matrices.fromBreeze(N)) 参考文献 【1】强大的矩阵奇异值分解(SVD)及其应用 【2】奇异值分解(SVD)原理详解及推导 【3】A Singularly Valuable Decomposition: The SVD of a Matrix","link":"/posts/1511864039.html"},{"title":"Top K Frequent Words","text":"Problem Metadata tags: Pocket Gems, Hash Table, Amazon, Priority Queue, Bloomberg, Yelp, Heap, Uber, EditorsChoice difficulty: Medium source(lintcode): https://www.lintcode.com/problem/top-k-frequent-words/ source(leetcode): https://leetcode.com/problems/top-k-frequent-words/ Description Given a list of words and an integer k, return the top k frequent words in the list. Notice You should order the words by the frequency of them in the return list, the most frequent one comes first. If two words has the same frequency, the one with lower alphabetical order come first. Example Given [ &quot;yes&quot;, &quot;lint&quot;, &quot;code&quot;, &quot;yes&quot;, &quot;code&quot;, &quot;baby&quot;, &quot;you&quot;, &quot;baby&quot;, &quot;chrome&quot;, &quot;safari&quot;, &quot;lint&quot;, &quot;code&quot;, &quot;body&quot;, &quot;lint&quot;, &quot;code&quot; ] for k = 3, return [\"code\", \"lint\", \"baby\"]. for k = 4, return [\"code\", \"lint\", \"baby\", \"yes\"], Challenge Do it in O(nlogk) time and O(n) extra space. 题解 输出出现频率最高的 K 个单词并对相同频率的单词按照字典序排列。如果我们使用大根堆维护，那么我们可以在输出结果时依次移除根节点即可。这种方法虽然可行，但不可避免会产生不少空间浪费，理想情况下，我们仅需要维护 K 个大小的堆即可。所以接下来的问题便是我们怎么更好地维护这种 K 大小的堆，并且在新增元素时剔除的是最末尾(最小)的节点。 Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class Solution { /** * @param words: an array of string * @param k: An integer * @return: an array of string */ public String[] topKFrequentWords(String[] words, int k) { // write your code here if (words == null || words.length == 0) return words; if (k &lt;= 0) return new String[0]; Map&lt;String, Integer&gt; wordFreq = new HashMap&lt;&gt;(); for (String word : words) { wordFreq.putIfAbsent(word, 0); wordFreq.put(word, wordFreq.get(word) + 1); } PriorityQueue&lt;KeyFreq&gt; pq = new PriorityQueue&lt;KeyFreq&gt;(k); for (Map.Entry&lt;String, Integer&gt; entry : wordFreq.entrySet()) { KeyFreq kf = new KeyFreq(entry.getKey(), entry.getValue()); if (pq.size() &lt; k) { pq.offer(kf); } else { KeyFreq peek = pq.peek(); if (peek.compareTo(kf) &lt;= 0) { pq.poll(); pq.offer(kf); } } } int topKSize = Math.min(k, pq.size()); String[] topK = new String[topKSize]; for (int i = 0; i &lt; k &amp;&amp; !pq.isEmpty(); i++) { topK[i] = pq.poll().key; } // reverse array for (int i = 0, j = topKSize - 1; i &lt; j; i++, j--) { String temp = topK[i]; topK[i] = topK[j]; topK[j] = temp; } return topK; } class KeyFreq implements Comparable&lt;KeyFreq&gt; { String key; int freq; public KeyFreq(String key, int freq) { this.key = key; this.freq = freq; } @Override public int compareTo(KeyFreq kf) { if (this.freq != kf.freq) { return this.freq - kf.freq; } return kf.key.compareTo(this.key); } }} 源码分析 使用 Java 自带的 PriorityQueue 来实现堆，由于需要定制大小比较，所以这里自定义类中实现了 Comparable 的 compareTo 接口，另外需要注意的是这里原生使用了小根堆，所以我们在覆写 compareTo 时需要注意字符串的比较，相同频率的按照字典序排序，即优先保留字典序较小的字符串，所以正好和 freq 的比较相反。最后再输出答案时，由于是小根堆，所以还需要再转置一次。此题的 Java 实现中，使用的 PriorityQueue 并非线程安全，实际使用中需要注意是否需要用到线程安全的 PriorityBlockingQueue 对于 Java, 虽然标准库中暂未有定长的 PriorityQueue 实现，但是我们常用的 Google guava 库中其实已有类似实现，见 MinMaxPriorityQueue 不必再自己造轮子了。 复杂度分析 堆的插入删除操作，定长为 K, n 个元素，故时间复杂度约 \\[O(n \\log K)\\], 空间复杂度为 \\[O(n)\\].","link":"/posts/3788562168.html"},{"title":"Top k Largest Numbers","text":"Problem source(lintcode): https://www.lintcode.com/problem/top-k-largest-numbers/ Description Given an integer array, find the top k largest numbers in it. Example Given [3,10,1000,-99,4,100] and k = 3. Return [1000, 100, 10]. 题解 简单题，使用堆即可。 Java 12345678910111213141516171819202122public class Solution { /** * @param nums: an integer array * @param k: An integer * @return: the top k largest numbers in array */ public int[] topk(int[] nums, int k) { if (nums == null || nums.length &lt;= 1) return nums; PriorityQueue&lt;Integer&gt; pq = new PriorityQueue&lt;Integer&gt;(nums.length, Collections.reverseOrder()); for (int num : nums) { pq.offer(num); } int[] maxK = new int[k]; for (int i = 0; i &lt; k; i++) { maxK[i] = pq.poll(); } return maxK; }} 源码分析 略 复杂度分析 略","link":"/posts/512628613.html"},{"title":"Triangle - Find the minimum path sum from top to bottom","text":"Triangle - Find the minimum path sum from top to bottom Question leetcode: Triangle | LeetCode OJ lintcode: (109) Triangle 123456789101112131415Given a triangle, find the minimum path sum from top to bottom. Each step you may move to adjacent numbers on the row below.NoteBonus point if you are able to do this using only O(n) extra space, where n is the total number of rows in the triangle.ExampleFor example, given the following triangle[ [2], [3,4], [6,5,7], [4,1,8,3]]The minimum path sum from top to bottom is 11 (i.e., 2 + 3 + 5 + 1 = 11). 题解 题中要求最短路径和，每次只能访问下行的相邻元素，将triangle视为二维坐标。此题方法较多，下面分小节详述。 Method 1 - Traverse without hashmap 首先考虑最容易想到的方法——递归遍历，逐个累加所有自上而下的路径长度，最后返回这些不同的路径长度的最小值。由于每个点往下都有2条路径，使用此方法的时间复杂度约为 \\[O(2^n)\\], 显然是不可接受的解，不过我们还是先看看其实现思路。 C++ Traverse without hashmap 12345678910111213141516171819202122232425262728293031class Solution {public: /** * @param triangle: a list of lists of integers. * @return: An integer, minimum path sum. */ int minimumTotal(vector&lt;vector&lt;int&gt; &gt; &amp;triangle) { if (triangle.empty()) { return -1; } int result = INT_MAX; dfs(0, 0, 0, triangle, result); return result; }private: void dfs(int x, int y, int sum, vector&lt;vector&lt;int&gt; &gt; &amp;triangle, int &amp;result) { const int n = triangle.size(); if (x == n) { if (sum &lt; result) { result = sum; } return; } dfs(x + 1, y, (sum + triangle[x][y]), triangle, result); dfs(x + 1, y + 1, (sum + triangle[x][y]), triangle, result); }}; 源码分析 dfs()的循环终止条件为x == n，而不是x == n - 1，主要是方便在递归时sum均可使用sum + triangle[x][y]，而不必根据不同的y和y+1改变，代码实现相对优雅一些。理解方式则变为从第x行走到第x+1行时的最短路径和，也就是说在此之前并不将第x行的元素值计算在内。 这种遍历的方法时间复杂度如此之高的主要原因是因为在n较大时递归计算了之前已经得到的结果，而这些结果计算一次后即不再变化，可再次利用。因此我们可以使用hashmap记忆已经计算得到的结果从而对其进行优化。 Method 2 - Divide and Conquer without hashmap 既然可以使用递归遍历，当然也可以使用「分治」的方法来解。「分治」与之前的遍历区别在于「分治」需要返回每次「分治」后的计算结果，下面看代码实现。 C++ Divide and Conquer without hashmap 1234567891011121314151617181920212223242526class Solution {public: /** * @param triangle: a list of lists of integers. * @return: An integer, minimum path sum. */ int minimumTotal(vector&lt;vector&lt;int&gt; &gt; &amp;triangle) { if (triangle.empty()) { return -1; } int result = dfs(0, 0, triangle); return result; }private: int dfs(int x, int y, vector&lt;vector&lt;int&gt; &gt; &amp;triangle) { const int n = triangle.size(); if (x == n) { return 0; } return min(dfs(x + 1, y, triangle), dfs(x + 1, y + 1, triangle)) + triangle[x][y]; }}; 使用「分治」的方法代码相对简洁一点，接下来我们使用hashmap保存triangle中不同坐标的点计算得到的路径和。 Method 3 - Divide and Conquer with hashmap 新建一份大小和triangle一样大小的hashmap，并对每个元素赋以INT_MIN以做标记区分。 C++ Divide and Conquer with hashmap 12345678910111213141516171819202122232425262728293031323334353637383940class Solution {public: /** * @param triangle: a list of lists of integers. * @return: An integer, minimum path sum. */ int minimumTotal(vector&lt;vector&lt;int&gt; &gt; &amp;triangle) { if (triangle.empty()) { return -1; } vector&lt;vector&lt;int&gt; &gt; hashmap(triangle); for (int i = 0; i != hashmap.size(); ++i) { for (int j = 0; j != hashmap[i].size(); ++j) { hashmap[i][j] = INT_MIN; } } int result = dfs(0, 0, triangle, hashmap); return result; }private: int dfs(int x, int y, vector&lt;vector&lt;int&gt; &gt; &amp;triangle, vector&lt;vector&lt;int&gt; &gt; &amp;hashmap) { const int n = triangle.size(); if (x == n) { return 0; } // INT_MIN means no value yet if (hashmap[x][y] != INT_MIN) { return hashmap[x][y]; } int x1y = dfs(x + 1, y, triangle, hashmap); int x1y1 = dfs(x + 1, y + 1, triangle, hashmap); hashmap[x][y] = min(x1y, x1y1) + triangle[x][y]; return hashmap[x][y]; }}; 由于已经计算出的最短路径值不再重复计算，计算复杂度由之前的 \\[O(2^n)\\]，变为 \\[O(n^2)\\], 每个坐标的元素仅计算一次，故共计算的次数为 \\[1+2+...+n \\approx O(n^2)\\]. Method 4 - Dynamic Programming 从主章节中对动态规划的简介我们可以知道使用动态规划的难点和核心在于状态的定义及转化方程的建立。那么问题来了，到底如何去找适合这个问题的状态及转化方程呢？ 我们仔细分析题中可能的状态和转化关系，发现从triangle中坐标为 \\[triangle[x][y]\\] 的元素出发，其路径只可能为 \\[triangle[x][y]-&gt;triangle[x+1][y]\\] 或者 \\[triangle[x][y]-&gt;triangle[x+1][y+1]\\]. 以点 \\[(x,y)\\] 作为参考，那么可能的状态 \\[f(x,y)\\] 就可以是： 从 \\[(x,y)\\] 出发走到最后一行的最短路径和 从 \\[(0,0)\\] 走到 \\[(x,y)\\]的最短路径和 如果选择1作为状态，则相应的状态转移方程为： \\[f_1(x,y) = min\\{f_1(x+1, y), f_1(x+1, y+1)\\} + triangle[x][y]\\] 如果选择2作为状态，则相应的状态转移方程为： \\[f_2(x,y) = min\\{f_2(x-1, y), f_2(x-1, y-1)\\} + triangle[x][y]\\] 两个状态所对应的初始状态分别为 \\[f_1(n-1, y), 0 \\leq y \\leq n-1\\] 和 \\[f_2(0,0)\\]. 在代码中应注意考虑边界条件。下面分别就这种不同的状态进行动态规划。 C++ From Bottom to Top 12345678910111213141516171819202122232425262728class Solution {public: /** * @param triangle: a list of lists of integers. * @return: An integer, minimum path sum. */ int minimumTotal(vector&lt;vector&lt;int&gt; &gt; &amp;triangle) { if (triangle.empty()) { return -1; } vector&lt;vector&lt;int&gt; &gt; hashmap(triangle); // get the total row number of triangle const int N = triangle.size(); for (int i = 0; i != N; ++i) { hashmap[N-1][i] = triangle[N-1][i]; } for (int i = N - 2; i &gt;= 0; --i) { for (int j = 0; j &lt; i + 1; ++j) { hashmap[i][j] = min(hashmap[i + 1][j], hashmap[i + 1][j + 1]) + triangle[i][j]; } } return hashmap[0][0]; }}; 源码分析 异常处理 使用hashmap保存结果 初始化hashmap[N-1][i], 由于是自底向上，故初始化时保存最后一行元素 使用自底向上的方式处理循环 最后返回结果hashmap[0][0] 从空间利用角度考虑也可直接使用triangle替代hashmap，但是此举会改变triangle的值，不推荐。 C++ From Top to Bottom 1234567891011121314151617181920212223242526272829303132333435363738class Solution {public: /** * @param triangle: a list of lists of integers. * @return: An integer, minimum path sum. */ int minimumTotal(vector&lt;vector&lt;int&gt; &gt; &amp;triangle) { if (triangle.empty()) { return -1; } vector&lt;vector&lt;int&gt; &gt; hashmap(triangle); // get the total row number of triangle const int N = triangle.size(); //hashmap[0][0] = triangle[0][0]; for (int i = 1; i != N; ++i) { for (int j = 0; j &lt;= i; ++j) { if (j == 0) { hashmap[i][j] = hashmap[i - 1][j]; } if (j == i) { hashmap[i][j] = hashmap[i - 1][j - 1]; } if ((j &gt; 0) &amp;&amp; (j &lt; i)) { hashmap[i][j] = min(hashmap[i - 1][j], hashmap[i - 1][j - 1]); } hashmap[i][j] += triangle[i][j]; } } int result = INT_MAX; for (int i = 0; i != N; ++i) { result = min(result, hashmap[N - 1][i]); } return result; }}; 源码解析 自顶向下的实现略微有点复杂，在寻路时需要考虑最左边和最右边的边界，还需要在最后返回结果时比较最小值。 Java From Top to Bottom 1234567891011121314151617181920212223242526272829303132public class Solution { /** * @param triangle: a list of lists of integers. * @return: An integer, minimum path sum. */ public int minimumTotal(int[][] triangle) { // write your code here if (triangle == null || triangle.length == 0) return 0; int[] last = new int[triangle.length]; int[] current = new int[triangle.length]; last[0] = triangle[0][0]; current[0] = last[0]; for (int i = 1; i &lt; triangle.length; i++) { for (int j = 0; j &lt; i + 1; j++) { int sum = Integer.MAX_VALUE; if (j != 0) { sum = triangle[i][j] + last[j - 1]; } if (j != i) { sum = Math.min(sum, triangle[i][j] + last[j]); } current[j] = sum; } for (int k = 0; k &lt; i + 1; k++) last[k] = current[k]; } int min = Integer.MAX_VALUE; for (int n : current) { min = Math.min(n, min); } return min; }} 源码解析 思路基本和上个解法一样，但是在数组last中保留上一层的最短和的，因此不用hashmap，空间复杂度是O(n)","link":"/posts/4225844301.html"},{"title":"Unique Binary Search Trees II","text":"Question leetcode: Unique Binary Search Trees II | LeetCode OJ lintcode: (164) Unique Binary Search Trees II 1234567891011Given n, generate all structurally unique BST's(binary search trees) that store values 1...n.ExampleGiven n = 3, your program should return all 5 unique BST's shown below. 1 3 3 2 1 \\ / / / \\ \\ 3 2 1 1 3 2 / / \\ \\ 2 1 2 3 题解 题 Unique Binary Search Trees 的升级版，这道题要求的不是二叉搜索树的数目，而是要构建这样的树。分析方法仍然是可以借鉴的，核心思想为利用『二叉搜索树』的定义，如果以 i 为根节点，那么其左子树由[1, i - 1]构成，右子树由[i + 1, n] 构成。要构建包含1到n的二叉搜索树，只需遍历1到n中的数作为根节点，以i为界将数列分为左右两部分，小于i的数作为左子树，大于i的数作为右子树，使用两重循环将左右子树所有可能的组合链接到以i为根节点的节点上。 容易看出，以上求解的思路非常适合用递归来处理，接下来便是设计递归的终止步、输入参数和返回结果了。由以上分析可以看出递归严重依赖数的区间和i，那要不要将i也作为输入参数的一部分呢？首先可以肯定的是必须使用『数的区间』这两个输入参数，又因为i是随着『数的区间』这两个参数的，故不应该将其加入到输入参数中。分析方便，不妨设『数的区间』两个输入参数分别为start和end. 接下来谈谈终止步的确定，由于根据i拆分左右子树的过程中，递归调用的方法中入口参数会缩小，且存在start &lt;= i &lt;= end, 故终止步为start &gt; end. 那要不要对start == end返回呢？保险起见可以先写上，后面根据情况再做删改。总结以上思路，简单的伪代码如下： 123456789101112131415161718192021222324252627helper(start, end) { result; if (start &gt; end) { result.push_back(NULL); return; } else if (start == end) { result.push_back(TreeNode(i)); return; } // dfs for (int i = start; i &lt;= end; ++i) { leftTree = helper(start, i - 1); rightTree = helper(i + 1, end); // link left and right sub tree to the root i for (j in leftTree ){ for (k in rightTree) { root = TreeNode(i); root-&gt;left = leftTree[j]; root-&gt;right = rightTree[k]; result.push_back(root); } } } return result;} 大致的框架如上所示，我们来个简单的数据验证下，以[1, 2, 3]为例，调用堆栈图如下所示： helper(1,3) ---loop i = 2--- ---loop i = 3--- ... 简单验证后可以发现这种方法的核心为递归地构造左右子树并将其链接到相应的根节点中。对于start和end相等的情况的，其实不必单独考虑，因为start == end时其左右子树均返回空，故在for循环中返回根节点。当然单独考虑可减少递归栈的层数，但实际测下来后发现运行时间反而变长了不少 :( Python 1234567891011121314151617181920212223242526272829303132&quot;&quot;&quot;Definition of TreeNode:class TreeNode: def __init__(self, val): this.val = val this.left, this.right = None, None&quot;&quot;&quot;class Solution: # @paramn n: An integer # @return: A list of root def generateTrees(self, n): return self.helper(1, n) def helper(self, start, end): result = [] if start &gt; end: result.append(None) return result for i in xrange(start, end + 1): # generate left and right sub tree leftTree = self.helper(start, i - 1) rightTree = self.helper(i + 1, end) # link left and right sub tree to root(i) for j in xrange(len(leftTree)): for k in xrange(len(rightTree)): root = TreeNode(i) root.left = leftTree[j] root.right = rightTree[k] result.append(root) return result C++ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Definition of TreeNode: * class TreeNode { * public: * int val; * TreeNode *left, *right; * TreeNode(int val) { * this-&gt;val = val; * this-&gt;left = this-&gt;right = NULL; * } * } */class Solution {public: /** * @paramn n: An integer * @return: A list of root */ vector&lt;TreeNode *&gt; generateTrees(int n) { return helper(1, n); }private: vector&lt;TreeNode *&gt; helper(int start, int end) { vector&lt;TreeNode *&gt; result; if (start &gt; end) { result.push_back(NULL); return result; } for (int i = start; i &lt;= end; ++i) { // generate left and right sub tree vector&lt;TreeNode *&gt; leftTree = helper(start, i - 1); vector&lt;TreeNode *&gt; rightTree = helper(i + 1, end); // link left and right sub tree to root(i) for (int j = 0; j &lt; leftTree.size(); ++j) { for (int k = 0; k &lt; rightTree.size(); ++k) { TreeNode *root = new TreeNode(i); root-&gt;left = leftTree[j]; root-&gt;right = rightTree[k]; result.push_back(root); } } } return result; }}; Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition of TreeNode: * public class TreeNode { * public int val; * public TreeNode left, right; * public TreeNode(int val) { * this.val = val; * this.left = this.right = null; * } * } */public class Solution { /** * @paramn n: An integer * @return: A list of root */ public List&lt;TreeNode&gt; generateTrees(int n) { return helper(1, n); } private List&lt;TreeNode&gt; helper(int start, int end) { List&lt;TreeNode&gt; result = new ArrayList&lt;TreeNode&gt;(); if (start &gt; end) { result.add(null); return result; } for (int i = start; i &lt;= end; i++) { // generate left and right sub tree List&lt;TreeNode&gt; leftTree = helper(start, i - 1); List&lt;TreeNode&gt; rightTree = helper(i + 1, end); // link left and right sub tree to root(i) for (TreeNode lnode: leftTree) { for (TreeNode rnode: rightTree) { TreeNode root = new TreeNode(i); root.left = lnode; root.right = rnode; result.add(root); } } } return result; }} 源码分析 异常处理，返回None/NULL/null. 遍历start-&gt;end, 递归得到左子树和右子树。 两重for循环将左右子树的所有可能组合添加至最终返回结果。 注意 DFS 辅助方法helper中左右子树及返回根节点的顺序。 复杂度分析 递归调用，一个合理的数组区间将生成新的左右子树，时间复杂度为指数级别，使用的临时空间最后都被加入到最终结果，空间复杂度(堆)近似为 \\[O(1)\\], 栈上的空间较大。 Reference Code Ganker: Unique Binary Search Trees II -- LeetCode 水中的鱼: [LeetCode] Unique Binary Search Trees II, Solution Accepted Iterative Java solution. - Leetcode Discuss Unique Binary Search Trees II 参考程序 Java/C++/Python","link":"/posts/3070505727.html"},{"title":"Unique Subsets","text":"Question leetcode: Subsets II | LeetCode OJ lintcode: (18) Unique Subsets Problem Statement Given a list of numbers that may has duplicate numbers, return all possible subsets. Example If S = [1,2,2], a solution is: [ [2], [1], [1,2,2], [2,2], [1,2], [] ] Note Each element in a subset must be in non-descending order. The ordering between two subsets is free. The solution set must not contain duplicate subsets. 题解 此题在上一题的基础上加了有重复元素的情况，因此需要对回溯函数进行一定的剪枝，对于排列组合的模板程序，剪枝通常可以从两个地方出发，一是在返回结果result.add之前进行剪枝，另一个则是在list.add处剪枝，具体使用哪一种需要视情况而定，哪种简单就选谁。 由于此题所给数组不一定有序，故首先需要排序。有重复元素对最终结果的影响在于重复元素最多只能出现n次(重复个数为n时)。具体分析过程如下(此分析过程改编自 九章算法)。 以 \\[[1, 2_1, 2_2]\\] 为例，若不考虑重复，组合有 \\[[], [1], [1, 2_1], [1, 2_1, 2_2], [1, 2_2], [2_1], [2_1, 2_2], [2_2]\\]. 其中重复的有 \\[[1, 2_2], [2_2]\\]. 从中我们可以看出只能从重复元素的第一个持续往下添加到列表中，而不能取第二个或之后的重复元素。参考上一题Subsets的模板，能代表「重复元素的第一个」即为 for 循环中的pos变量，i == pos时，i处所代表的变量即为某一层遍历中得「第一个元素」，因此去重时只需判断i != pos &amp;&amp; s[i] == s[i - 1](不是 i + 1, 可能索引越界，而i 不等于 pos 已经能保证 i &gt;= 1). C++ 123456789101112131415161718192021222324252627282930313233343536class Solution {public: /** * @param S: A set of numbers. * @return: A list of lists. All valid subsets. */ vector&lt;vector&lt;int&gt; &gt; subsetsWithDup(const vector&lt;int&gt; &amp;S) { vector&lt;vector&lt;int&gt; &gt; result; if (S.empty()) { return result; } vector&lt;int&gt; list; vector&lt;int&gt; source(S); sort(source.begin(), source.end()); backtrack(result, list, source, 0); return result; }private: void backtrack(vector&lt;vector&lt;int&gt; &gt; &amp;ret, vector&lt;int&gt; &amp;list, vector&lt;int&gt; &amp;s, int pos) { ret.push_back(list); for (int i = pos; i != s.size(); ++i) { if (i != pos &amp;&amp; s[i] == s[i - 1]) { continue; } list.push_back(s[i]); backtrack(ret, list, s, i + 1); list.pop_back(); } }}; Java 123456789101112131415161718192021222324252627282930class Solution { /** * @param S: A set of numbers. * @return: A list of lists. All valid subsets. */ public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; subsetsWithDup(ArrayList&lt;Integer&gt; S) { ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); if (S == null) return result; // Collections.sort(S); List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); dfs(S, 0, list, result); return result; } private void dfs(ArrayList&lt;Integer&gt; S, int pos, List&lt;Integer&gt; list, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result) { result.add(new ArrayList&lt;Integer&gt;(list)); for (int i = pos; i &lt; S.size(); i++) { // exlude duplicate if (i != pos &amp;&amp; S.get(i) == S.get(i - 1)) { continue; } list.add(S.get(i)); dfs(S, i + 1, list, result); list.remove(list.size() - 1); } }} 源码分析 相比前一道题多了去重的判断。 复杂度分析 和前一道题差不多，最坏情况下时间复杂度为 \\[2^n\\]. 空间复杂度为 \\[O(n)\\]. Reference Subsets II | 九章算法","link":"/posts/3965380778.html"},{"title":"英语学习指南（三）","text":"资料、工具推荐和扩展阅读 单词量测试工具 首推 http://testyourvocab.com/ 其次欧路词典手机版中也有测试程序，测试方式很合理，可以尝试 ## 善用 Google 前面大篇幅提到了 Google 的关键词推荐、图片以及语音识别，其实还有很多可以利用的。强烈建议大家用英文关键词去搜索，并注意搜索结果。 比如前段时间我的 Mac 的 PDF Preview 在拖动鼠标选中文本的时候，没有背景颜色，导致看不出选中的是什么文本。所以我就堆砌关键词 macbook preview pdf text selection background color 试图解决这个问题。Google 搜索给我推了 Missing text highlighting Color when selecting it in Preview 这个搜索结果，成功解决了我的问题。 后面这句对比前面自己写的学到了很多，首先是 missing 的用法，其次这种背景颜色不是 background color 而是 highlighting color，然后这个问题句式结构也非常工整，简洁又准确。 写作辅助工具 写作最头疼的就是害怕语法、语义有问题。 语法方面我推荐 Grammarly 这个工具，在英语国家也十分流行，安装它的 Chrome 插件之后，会自动检测网页文本框里的内容是否有语法问题。购买了付费会员，会对内容进行更深度优化，会通过海量内容库对你的内容进行对比，做出一些改进（比如同义词替换、表达方式等），当然基础语法监测功能免费使用。 语义方面我推荐 Google Translate，用它将你写的英文翻译成中文，来看看意思是否跟自己想表达的一样。 这俩可以配合一起使用： 群友也贡献了一个自己开发的工具，可以横向对比多个翻译平台的结果，有兴趣的也可以使用： http://translation.education/paratrans/index.php 。 当然，机器毕竟机器，肯定会存在误差，使用时也注意甄别是否正确。不过人也有误解的时候。 Youtube 优秀老师 Rachel's English 美语发音，很优秀的老师。 engVid 综合性的免费英语学习网站，非常强大，很多老师很多视频，官方网站 https://www.engvid.com/ 。我比较喜欢下面三个老师，各有风格： JamesESL 自带 Rap。 Ronnie 肢体动作很丰富。 Emma Tarle Speech &amp; Language Services 很多单词发音对比视频。 SPEAK ENGLISH with SOZO-X 的也不错，发音注重口型，最后有小 quiz。 影子跟读素材和软件 寻找训练素材需要注意难度，假设你现在水平是 1，要找的素材难度应该介于 1.2 - 1.5 之间最有效果。太简单和太难都不是很好的资料。 学术类我用过两个素材，一个是老托93，不算太难，托福考试训练素材，每篇有个主题，由专业播音读，内容比较老。另一个是科学 60s，基本在一两分钟内讲述一个科学相关的知识，有自述也有对话，难度偏高，一是快，二是有些科学相关单词不会，经常更新。 此外也有推荐 BBC 广播、VOA 慢速英语，但是我用的比较少。还有推荐经济学人，我有影子跟读过，偏英式，有的主播语速飞快，感觉挺难。由于我选择美式发音，放弃。 影子跟读和听力软件首推《每日英语听力》，跟欧路词典同一家公司，上面资料全都有，单句重放以及跟读录音识别打分功能都有，最关键的是支持 Mac。其次 Aboboo 也非常好，可以自己导入音频帮你切割进行复读，方便影子跟读，但是只支持 Windows 平台。 英语流利说 懂你英语 英语流利说 App 的《懂你英语》课程还可以，它的训练模式非常有效。首先给你场景图片，播放音频让你跟读，之后会有一些题问你刚听到的内容并且让你跟着读出来，然后用人工智能识别打分协助纠音。 整个过程没有文本，这样可以让你专注模仿声音、解析声音。有时候如果听的过程可以看到文本，会更容易复述，但其实会降低听力识别的能力。 但并不是完美的 ，生词往往会卡住你的跟读，如果不显示文本靠听和读很难学到这个单词。其次 语音识别引擎不是特别准 ，有时候漏词加词也会标记你读对了，而且 在跟读过程由于不显示文本，不会告诉你具体哪个单词、哪个地方错了 ，这样会导致你跟读时，一直读错也不知道，只知道自己读的还行。 此外懂你英语并非免费课程，还需要付费购买，使用效果也因人而异，详情可以参考这个 Issue ，本人亦无利益相关，请自行体验、判断是否购买。 实用 Chrome 插件 我曾安装过非常多英语相关插件，这是我唯一保留至今常用的插件： Cambridge Dictionary：对准单词右击就打开 Cambridge Dictionary 看到对应的词义，用以重度查询 Google Dictionary (by Google)：双击即可弹出非常简单的弹窗，附带一个英文发音和简短的英文释义，用以轻度查询 Grammarly：语法检测。 其他优秀英语教程 这是一份 2004 的资料，台湾一大学英语老师做的网站 http://chifenchen.tripod.com/ 虽然很老土，但是知识不过时，比较系统、简洁明了的介绍了发音的几个点，而且发音非常准。 优秀的经验技巧扩展阅读 如何有效积累主动词汇？ 建立表达素材库，方便提升理解器能力。 英语好的人是怎样背单词的？ 强调输入英文学习单词，在语境中补充语料库，提升英文思维。 国内英语培训机构捧上天的「自然拼读法」，真的那么神奇吗？ 强调音标的重要性。 怎么练好英语口语？ 一位语音识别专业人士做的视频，强调输入英语学习，以及训练到无意识输出。 纠正/练习英语发音的好材料 一份纠正发音的经典材料，正文就是很松散地把一系列形似音不似、音似形不似的单词串起来，让你仔细体会其中的差别。练好可以磨嘴皮子同时对音标之间的区别更容易分辨。","link":"/posts/820836322.html"},{"title":"Word Break","text":"Question leetcode: Word Break | LeetCode OJ lintcode: (107) Word Break 12345678Given a string s and a dictionary of words dict, determine if s can besegmented into a space-separated sequence of one or more dictionary words.For example, givens = &quot;leetcode&quot;,dict = [&quot;leet&quot;, &quot;code&quot;].Return true because &quot;leetcode&quot; can be segmented as &quot;leet code&quot;. 题解 单序列(DP_Sequence) DP 题，由单序列动态规划的四要素可大致写出： State: f[i] 表示前i个字符能否根据词典中的词被成功分词。 Function: f[i] = or{f[j], j &lt; i, letter in [j+1, i] can be found in dict}, 含义为小于i的索引j中只要有一个f[j]为真且j+1到i中组成的字符能在词典中找到时，f[i]即为真，否则为假。具体实现可分为自顶向下或者自底向上。 Initialization: f[0] = true, 数组长度为字符串长度 + 1，便于处理。 Answer: f[s.length] 考虑到单词长度通常不会太长，故在s较长时使用自底向上效率更高。 Python 12345678910111213141516171819202122class Solution: # @param s, a string # @param wordDict, a set&lt;string&gt; # @return a boolean def wordBreak(self, s, wordDict): if not s: return True if not wordDict: return False max_word_len = max([len(w) for w in wordDict]) can_break = [True] for i in xrange(len(s)): can_break.append(False) for j in xrange(i, -1, -1): # optimize for too long interval if i - j + 1 &gt; max_word_len: break if can_break[j] and s[j:i + 1] in wordDict: can_break[i + 1] = True break return can_break[-1] C++ 123456789101112131415161718192021222324252627282930313233class Solution {public: bool wordBreak(string s, unordered_set&lt;string&gt;&amp; wordDict) { if (s.empty()) return true; if (wordDict.empty()) return false; // get the max word length of wordDict int max_word_len = 0; for (unordered_set&lt;string&gt;::iterator it = wordDict.begin(); it != wordDict.end(); ++it) { max_word_len = max(max_word_len, (*it).size()); } vector&lt;bool&gt; can_break(s.size() + 1, false); can_break[0] = true; for (int i = 1; i &lt;= s.size(); ++i) { for (int j = i - 1; j &gt;= 0; --j) { // optimize for too long interval if (i - j &gt; max_word_len) break; if (can_break[j] &amp;&amp; wordDict.find(s.substr(j, i - j)) != wordDict.end()) { can_break[i] = true; break; } } } return can_break[s.size()]; }}; Java 1234567891011121314151617181920212223242526272829public class Solution { public boolean wordBreak(String s, Set&lt;String&gt; wordDict) { if (s == null || s.length() == 0) return true; if (wordDict == null || wordDict.isEmpty()) return false; // get the max word length of wordDict int max_word_len = 0; for (String word : wordDict) { max_word_len = Math.max(max_word_len, word.length()); } boolean[] can_break = new boolean[s.length() + 1]; can_break[0] = true; for (int i = 1; i &lt;= s.length(); i++) { for (int j = i - 1; j &gt;= 0; j--) { // optimize for too long interval if (i - j &gt; max_word_len) break; String word = s.substring(j, i); if (can_break[j] &amp;&amp; wordDict.contains(word)) { can_break[i] = true; break; } } } return can_break[s.length()]; }} 源码分析 Python 之类的动态语言无需初始化指定大小的数组，使用时下标i比 C++和 Java 版的程序少1。使用自底向上的方法求解状态转移，首先遍历一次词典求得单词最大长度以便后续优化。 复杂度分析 求解词典中最大单词长度，时间复杂度为词典长度乘上最大单词长度 \\[O(L_D \\cdot L_w)\\] 词典中找单词的时间复杂度为 \\[O(1)\\](哈希表结构) 两重 for 循环，内循环在超出最大单词长度时退出，故最坏情况下两重 for 循环的时间复杂度为 \\[O(n L_w)\\]. 故总的时间复杂度近似为 \\[O(n L_w)\\]. 使用了与字符串长度几乎等长的布尔数组和临时单词word，空间复杂度近似为 \\[O(n)\\].","link":"/posts/2787625180.html"},{"title":"Word Ladder","text":"Question leetcode: Word Ladder | LeetCode OJ lintcode: (120) Word Ladder Problem Statement Given two words (start and end), and a dictionary, find the length of shortest transformation sequence from start to end, such that: Only one letter can be changed at a time Each intermediate word must exist in the dictionary Example Given: start = \"hit\" end = \"cog\" dict = [\"hot\",\"dot\",\"dog\",\"lot\",\"log\"] As one shortest transformation is \"hit\" -&gt; \"hot\" -&gt; \"dot\" -&gt; \"dog\" -&gt; \"cog\", return its length 5. Note Return 0 if there is no such transformation sequence. All words have the same length. All words contain only lowercase alphabetic characters. 题解 咋一看还以为是 Edit Distance 的变体，仔细审题后发现和动态规划没啥关系。题中有两大关键点：一次只能改动一个字符；改动的中间结果必须出现在词典中。那么大概总结下来共有四种情形： start 和 end 相等。 end 在 dict 中，且 start 可以转换为 dict 中的一个单词。 end 不在 dict 中，但可由 start 或者 dict 中的一个单词转化而来。 end 无法由 start 转化而来。 由于中间结果也必须出现在词典中，故此题相当于图搜索问题，将 start, end, dict 中的单词看做图中的节点，节点与节点（单词与单词）可通过一步转化得到，可以转换得到的节点相当于边的两个节点，边的权重为1（都是通过1步转化）。到这里问题就比较明确了，相当于搜索从 start 到 end 两点间的最短距离，即 Dijkstra 最短路径算法。通过 BFS 和哈希表实现。 首先将 start 入队，随后弹出该节点，比较其和 end 是否相同；再从 dict 中选出所有距离为1的单词入队，并将所有与当前节点距离为1且未访问过的节点（需要使用哈希表）入队，方便下一层遍历时使用，直至队列为空。 Java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Solution { /** * @param start, a string * @param end, a string * @param dict, a set of string * @return an integer */ public int ladderLength(String start, String end, Set&lt;String&gt; dict) { if (start == null &amp;&amp; end == null) return 0; if (start.length() == 0 &amp;&amp; end.length() == 0) return 0; assert(start.length() == end.length()); if (dict == null || dict.size() == 0) { return 0; } int ladderLen = 1; dict.add(end); // add end to dict, important! Queue&lt;String&gt; q = new LinkedList&lt;String&gt;(); Set&lt;String&gt; hash = new HashSet&lt;String&gt;(); q.offer(start); hash.add(start); while (!q.isEmpty()) { ladderLen++; int qLen = q.size(); for (int i = 0; i &lt; qLen; i++) { String strTemp = q.poll(); for (String nextWord : getNextWords(strTemp, dict)) { if (nextWord.equals(end)) return ladderLen; // filter visited word in the dict if (hash.contains(nextWord)) continue; q.offer(nextWord); hash.add(nextWord); } } } return 0; } private Set&lt;String&gt; getNextWords(String curr, Set&lt;String&gt; dict) { Set&lt;String&gt; nextWords = new HashSet&lt;String&gt;(); for (int i = 0; i &lt; curr.length(); i++) { char[] chars = curr.toCharArray(); for (char c = 'a'; c &lt;= 'z'; c++) { chars[i] = c; String temp = new String(chars); if (dict.contains(temp)) { nextWords.add(temp); } } } return nextWords; }} 源码分析 getNextWords的实现 首先分析给定单词curr并从 dict 中选出所有距离为1 的单词。常规的思路可能是将curr与 dict 中的单词逐个比较，并遍历每个字符串，返回距离为1的单词组。这种找距离为1的节点的方法复杂度为 \\[l(length\\ of\\ word) \\times n(size\\ of\\ dict)\\times m(queue\\ length) = O(lmn)\\]. 在 dict 较长时会 TLE. 其实根据 dict 的数据结构特点，比如查找任一元素的时间复杂度可认为是 \\[O(1)\\]. 根据哈希表和单个单词长度通常不会太长这一特点，我们就可以根据给定单词构造到其距离为一的单词变体，然后查询其是否在 dict 中，这种实现的时间复杂度为 \\[O(26(a\\ to\\ z) \\times l \\times m) = O(lm)\\], 与 dict 长度没有太大关系，大大优化了时间复杂度。 经验教训：根据给定数据结构特征选用合适的实现，遇到哈希表时多用其查找的 \\[O(1)\\] 特性。 BFS 和哈希表的配合使用 BFS 用作搜索，哈希表用于记录已经访问节点。在可以改变输入数据的前提下，需要将 end 加入 dict 中，否则对于不在 dict 中出现的 end 会有问题。 复杂度分析 主要在于getNextWords方法的时间复杂度，时间复杂度 \\[O(lmn)\\]。使用了队列存储中间处理节点，空间复杂度平均条件下应该是常量级别，当然最坏条件下可能恶化为 \\[O(n)\\], 即 dict 中某个点与其他点距离均为1. Reference Word Ladder 参考程序 Java/C++/Python Java Solution using Dijkstra's algorithm, with explanation - Leetcode Discuss","link":"/posts/3732161160.html"},{"title":"Word Search","text":"Question leetcode: Word Search | LeetCode OJ lintcode: (123) Word Search Problem Statement Given a 2D board and a word, find if the word exists in the grid. The word can be constructed from letters of sequentially adjacent cell, where \"adjacent\" cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once. Example Given board = 12345[ &quot;ABCE&quot;, &quot;SFCS&quot;, &quot;ADEE&quot;] word = \"ABCCED\", -&gt; returns true, word = \"SEE\", -&gt; returns true, word = \"ABCB\", -&gt; returns false. 题解 典型的 DFS 实现，这里有上下左右四个方向，往四个方向递归之前需要记录坐标处是否被访问过，并且在不满足条件时要重置该标记变量。该题的一大难点是如何处理起始点和字符串的第一个字符不等的情况，我最开始尝试在一个 DFS 中解决，发现很难 bug-free, 而且程序逻辑支离破碎。后来看了下其他题解发现简单粗暴的方法就是双重循环嵌套 DFS... Java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Solution { /** * @param board: A list of lists of character * @param word: A string * @return: A boolean */ public boolean exist(char[][] board, String word) { if (board == null || board.length == 0 || board[0].length == 0) return false; if (word == null || word.length() == 0) return false; boolean[][] visited = new boolean[board.length][board[0].length]; for (int i = 0; i &lt; board.length; i++) { for (int j = 0; j &lt; board[0].length; j++) { if (dfs(board, word, visited, i, j, 0)) { return true; } } } return false; } public boolean dfs(char[][] board, String word, boolean[][] visited, int row, int col, int wi) { // out of index if (row &lt; 0 || row &gt; board.length - 1 || col &lt; 0 || col &gt; board[0].length - 1) { return false; } if (!visited[row][col] &amp;&amp; board[row][col] == word.charAt(wi)) { // return instantly if (wi == word.length() - 1) return true; // traverse unvisited row and col visited[row][col] = true; boolean down = dfs(board, word, visited, row + 1, col, wi + 1); boolean right = dfs(board, word, visited, row, col + 1, wi + 1); boolean up = dfs(board, word, visited, row - 1, col, wi + 1); boolean left = dfs(board, word, visited, row, col - 1, wi + 1); // reset with false if none of above is true visited[row][col] = up || down || left || right; return up || down || left || right; } return false; }} 源码分析 注意处理好边界退出条件及visited在上下左右四个方向均为false时需要重置。判断字符串字符和board中字符是否相等前需要去掉已访问坐标。如果不引入visited二维矩阵，也可以使用特殊字符替换的方法，这样的话空间复杂度就大大降低了，细节见下面参考链接。 复杂度分析 DFS 最坏情况下遍历所有坐标点，二重 for 循环最坏情况下也全部执行完，故时间复杂度最差情况下为 \\[O(m^2n^2)\\], 使用了visited矩阵，空间复杂度为 \\[O(mn)\\], 当然这个可以优化到 \\[O(1)\\].(原地更改原 board 数组字符内容)。 Reference LeetCode – Word Search (Java)","link":"/posts/1564026122.html"},{"title":"决策树","text":"笔记摘要 决策树可以认为是if-then规则的集合，也可以认为是定义在特征空间上的条件概率分布 根据损失函数最小化的原则建立决策树模型 决策树的路径或其对应的if-then规则集合具有一个重要性质：互斥且完备 决策树的学习算法包含特征选择、决策树的生成与决策树的剪枝 决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择 熵 \\[ H(p)=H(X)=-\\sum_{i=1}^{n}p_i\\log p_i \\] 熵只与\\(X\\)的分布有关，与\\(X\\)取值无关 定义\\(0\\log0=0\\)，熵是非负的 表示随机变量不确定性的度量 条件熵 随机变量\\((X,Y)\\)的联合概率分布为 \\[P(X=x_i,Y=y_j)=p_{ij}, i=1,2,\\dots ,n;j=1,2,\\dots ,m\\] 条件熵\\(H(Y|X)\\)表示在已知随机变量\\(X\\)的条件下随机变量\\(Y\\)的不确定性 \\[ H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i) \\] 其中\\(p_i=P(X=x_i),i=1,2,\\dots ,n\\) 经验熵， 经验条件熵 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵 信息增益 特征\\(A\\)对训练数据集\\(D\\)的信息增益\\(g(D|A)\\)，定义为集合\\(D\\)的经验熵\\(H(D)\\)与特征\\(A\\)给定的条件下\\(D\\)的经验条件熵\\(H(D|A)\\)之差 \\[ g(D,A)=H(D)-H(D|A) \\] 熵与条件熵的差称为互信息 决策树中的信息增益等价于训练数据集中的类与特征的互信息 考虑ID这种特征， 本身是唯一的。按照ID做划分， 得到的经验条件熵为0, 会得到最大的信息增益。所以， 按照信息增益的准则来选择特征， 可能会倾向于取值比较多的特征 信息增益比 使用信息增益比可以对上面倾向取值较多的特征的问题进行校正 \\[ g_R(D,A)=\\frac{g(D,A)}{H_A(D)}\\\\ H_A(D)=-\\sum_{i=1}^n\\frac{D_i}{D}log_2\\frac{D_i}{D} \\] ID3算法 输入：训练数据集\\(D\\), 特征集\\(A\\)，阈值\\(\\epsilon\\) 输出：决策树\\(T\\) 如果\\(D\\)中所有实例属于同一类\\(C_k\\)，则\\(T\\)为单节点树，并将类\\(C_k\\)作为该节点的类标记，返回\\(T\\) 如果\\(A\\)是空集，则\\(T\\)为单节点树，并将实例数最多的类作为该节点类标记，返回\\(T\\) 计算\\(g\\), 选择信息增益最大的特征\\(A_g\\) 如果\\(A_g\\)的信息增益小于\\(\\epsilon\\)，则置\\(T\\)为单节点树，\\(D\\)中实例数最大的类\\(C_k\\)作为类标记，返回\\(T\\) 否则，依\\(A_g=a_i\\)将D划分若干非空子集\\(D_i\\)，\\(D_i\\)中实例数最大的类\\(C_k\\)作为类标记，构建子结点，由结点及其子结点 构成树\\(T\\)，返回\\(T\\) \\(D_i\\)训练集，\\(A-A_g\\)为特征集，递归调用前面步骤，得到\\(T_i\\)，返回\\(T_i\\) C4.5的生成算法 改用信息增益比来选择特征 &gt; 输入：训练数据集\\(D\\), 特征集\\(A\\)，阈值\\(\\epsilon\\) &gt; 输出：决策树\\(T\\) &gt; &gt; 1. 如果\\(D\\)属于同一类\\(C_k\\)，\\(T\\)为单节点树，类\\(C_k\\)作为该节点的类标记，返回\\(T\\) &gt; 1. 如果\\(A\\)是空集, 置\\(T\\)为单节点树，实例数最多的作为该节点类标记，返回\\(T\\) &gt; 1. 计算\\(g\\), 选择信息增益比最大的特征\\(A_g\\) &gt; 1. 如果\\(A_g\\)的信息增益比小于\\(\\epsilon\\)，\\(T\\)为单节点树，\\(D\\)中实例数最大的类\\(C_k\\)作为类标记，返回\\(T\\) &gt; 1. 否则，依\\(A_g=a_i\\)将D划分若干非空子集\\(D_i\\)，\\(D_i\\)中实例数最大的类\\(C_k\\)作为类标记，构建子结点，由结点及其子结点 构成树\\(T\\)，返回\\(T\\) &gt; 1. \\(D_i\\)训练集，\\(A-A_g\\)为特征集，递归调用前面步骤，得到\\(T_i\\)，返回\\(T_i\\) ID3和C4.5在生成上，差异只在准则的差异 树的剪枝 决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现的 树\\(T\\)的叶结点个数为\\(|T|\\)，\\(t\\)是树\\(T\\)的叶结点，该结点有\\(N_t\\)个样本点，其中\\(k\\)类的样本点有\\(N_{tk}\\)个，\\(H_t(T)\\)为叶结点\\(t\\)上的经验熵， \\(\\alpha\\geqslant 0\\)为参数，决策树学习的损失函数可以定义为 \\[ C_\\alpha(T)=\\sum_{i=1}^{|T|}N_tH_t(T)+\\alpha|T| \\] 其中 \\[ H_t(T)=-\\sum_k\\frac{N_{tk}}{N_t}\\log \\frac{N_{tk}}{N_t} \\] \\[ C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)\\color{black}=-\\sum_{t=1}^{|T|}\\sum_{k=1}^KN_{tk}\\log\\frac{N_{tk}}{N_t} \\] 这时有 \\[ C_\\alpha(T)=C(T)+\\alpha|T| \\] 其中\\(C(T)\\)表示模型对训练数据的误差，\\(|T|\\)表示模型复杂度，参数\\(\\alpha \\geqslant 0\\)控制两者之间的影响 剪枝算法 输入：生成算法生成的整个树\\(T\\)，参数\\(\\alpha\\) 输出：修剪后的子树\\(T_\\alpha\\) 计算每个结点的经验熵 递归地从树的叶结点向上回缩 假设一组叶结点回缩到其父结点之前与之后的整体树分别是\\(T_B\\)和\\(T_A\\)，其对应的损失函数分别是\\(C_\\alpha(T_A)\\)和\\(C_\\alpha(T_B)\\)，如果\\(C_\\alpha(T_A)\\leqslant C_\\alpha(T_B)\\)则进行剪枝，即将父结点变为新的叶结点 返回2，直至不能继续为止，得到损失函数最小的子树\\(T_\\alpha\\) 决策树的剪枝算法可以由一种动态规划的算法实现 CART 决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，并进行特征选择，生成二叉树 最小二乘回归树生成算法 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上地输出值，构建二叉决策树 输入：训练数据集\\(D\\) 输出：回归树\\(f(x)\\) 步骤： 1. 遍历变量\\(j\\)，对固定的切分变量\\(j\\)扫描切分点\\(s\\)，得到满足下面式子的\\((j,s)\\) \\[ \\min\\limits_{j,s}\\left[\\min\\limits_{c_1}\\sum\\limits_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min\\limits_{c_2}\\sum\\limits_{x_i\\in R_2(j,s)}(y_i-c_2)^2\\right] \\] 1. 用选定的\\((j,s)\\), 划分区域并决定相应的输出值 \\[ R_1(j,s)=\\{x|x^{(j)}\\leq s\\}, R_2(j,s)=\\{x|x^{(j)}&gt; s\\} \\\\ \\hat{c}_m= \\frac{1}{N}\\sum\\limits_{x_i\\in R_m(j,s)} y_j, x\\in R_m, m=1,2 \\] 1. 对两个子区域调用(1)(2)步骤， 直至满足停止条件 1. 将输入空间划分为\\(M\\)个区域\\(R_1, R_2,\\dots,R_M\\)，生成决策树： \\[ f(x)=\\sum_{m=1}^M\\hat{c}_mI(x\\in R_m) \\] * 课后题有详细例子 CART分类树的生成 分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点 概率分布的基尼指数定义 \\[ Gini(p) = \\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2 \\] * 如果样本集合\\(D\\)根据特征\\(A\\)是否取某一可能值\\(a\\)被分割成\\(D_1\\)和\\(D_2\\)两部分，则在特征\\(A\\)的条件下，集合\\(D\\)的基尼指数定义为 \\[Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)\\] &gt;输入：训练数据集\\(D\\)，停止计算的条件 &gt;输出：CART决策树 &gt;根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树： &gt;1. 设结点地训练数据集为\\(D\\)，计算现有特征对该数据集的基尼指数。此时，对每一个特征\\(A\\)，对其可能取得每个值\\(a\\)，根据样本点对\\(A=a\\)的测试为“是”或“否”将\\(D\\)分成\\(D_1\\)和\\(D_2\\)两部分，计算\\(A=a\\)时的基尼指数 &gt;1. 在所有可能的特征\\(A\\)以及它们所有可能的切分点\\(a\\)中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依照最优特征和最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 &gt;1. 对两个子结点递归地调用1、2，直至满足停止条件 &gt;1. 生成CART决策树 * 算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于指定阈值，或者没有更多特征 习题解答 5.1 根据表 5.1 所给的训练数据集，利用信息增益比（C4.5 算法）生成决策树 编写程序计算信息增益比，并利用C4.5算法生成决策树，得到结果如下 特征（年龄）的信息增益比为： 0.052 特征（有工作）的信息增益比为： 0.352 特征（有自己的房子）的信息增益比为： 0.433 特征（信贷情况）的信息增益比为： 0.232 特征（年龄）的信息增益比为： 0.164 特征（有工作）的信息增益比为： 1.000 特征（有自己的房子）的信息增益比为： 0.340 决策树 {'label:': None, 'feature': 2, 'tree': {'否': {'label:': None, 'feature': 1, 'tree': {'否': {'label:': '否', 'feature': None, 'tree': {}}, '是': {'label:': '是', 'feature': None, 'tree': {}}}}, '是': {'label:': '是', 'feature': None, 'tree': {}}}} 结果与ID3算法完全相同 5.2 已知如表 5.2 所示的训练数据，试用平方误差损失准则生成一个二叉回归树 5.3 证明 CART 剪枝算法中，当\\(α\\)确定时，存在唯一的最小子树 \\(T_α\\)使损失函数 \\(C_α(T)\\)最小 利用反证法，假设存在两个最小子树使损失函数最小 设这两棵最小子树 为\\(T_1\\)​ 和 \\(T_2\\)​ ，其剪枝位置分别是 \\(t_1\\)​ 和 \\(t_2\\)​ ，两者都能使得损失函数最小，即两者拥有相等的 \\(C_\\alpha(T)​\\) 又有： \\[C_\\alpha(t_1)&lt;C_\\alpha(T_{t1})\\\\ C_\\alpha(t_2)&lt;C_\\alpha(T_{t2})\\] 即剪枝\\(t_1\\),\\(t_2\\) ，总能使得整体损失函数减小，因此对于子树 \\(T_1\\), \\(T_2\\) ，总存在进一步的剪枝，使得损失函数进一步减小（在 \\(T_1\\) 中剪枝 \\(t_2\\),在 \\(T_2\\) 中剪枝 \\(t_1\\) ），因此 \\(T_1\\), \\(T_2\\) 不是最优的子树。 即不可能存在两棵及以上的最优子树 5.4 证明 CART 剪枝算法中求出的子树序列\\(\\{T_0,T_1,⋅⋅⋅,T_n\\}\\)分别是区间 \\(α∈[α_i,α_{i+1})\\)的最优子树\\(T_\\alpha\\) ，这里\\(i=0,1,⋅⋅⋅,n,0=\\alpha_0&lt;\\alpha_1&lt;\\cdot\\cdot\\cdot&lt;\\alpha_n&lt;+\\infty\\) 参见 Breiman的著作","link":"/posts/51507.html"},{"title":"使用UnblockNeteaseMusic播放网易云音乐客户端无版权歌曲","text":"&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;&lt;code&gt;UnblockNeteaseMusic&lt;/code&gt;是一款可以给你解除网易云音乐海外限制并解锁变灰(无版权)歌曲的工具，让你可以听取客户端任何的歌曲，而不受到限制，此前该类似工具挺多的，不过貌似都不维护或者失效了，这里提到的项目更新很频繁，支持&lt;code&gt;Windows&lt;/code&gt;、&lt;code&gt;MacOS&lt;/code&gt;、安卓、&lt;code&gt;IOS&lt;/code&gt;等设备，所以就分享一下使用方法。&lt;/p&gt; 截图 特性 使用网易云旧链/QQ/虾米/百度/酷狗/酷我/咕咪/JOOX音源替换变灰歌曲链接(默认仅启用前四)。 为请求增加X-Real-IP参数解锁海外限制，支持指定网易云服务器IP，支持设置上游HTTP/HTTPS代理。 完整的流量代理功能(HTTP/HTTPS)，可直接作为系统代理(同时支持PAC)。 安装 Github地址：https://github.com/nondanee/UnblockNeteaseMusic 1、安装Nodejs #Debian/Ubuntu系统 curl -sL https://deb.nodesource.com/setup_10.x | bash - apt install -y nodejs git #CentOS系统 curl -sL https://rpm.nodesource.com/setup_10.x | bash - yum install nodejs git -y 2、运行UnblockNeteaseMusic git clone https://github.com/nondanee/UnblockNeteaseMusic.git cd UnblockNeteaseMusic node app.js 运行端口默认为8080，需要想修改可以编辑app.js文件的config.port对应的数值，或者使用命令： #这里默认修改4567，自行调整 node app.js -p 4567 这里还可以加一个-s参数限制代理范围防止被滥用，也就是仅放行网易云音乐所属域名的请求，不过使用起来的话就受限了，仅支持PAC或Hosts。 node app.js -s 3、开机自启这里使用Systemd进程守护，只适用于CentOS 7、Debian 8+、Ubuntu 16+等。 #修改下ExecStartPre源码路径即可，然后一起复制到SSH运行 cat &gt; /etc/systemd/system/UnblockNeteaseMusic.service &lt;&lt;EOF [Unit] Description=UnblockNeteaseMusic After=network.target Wants=network.target [Service] Type=simple PIDFile=/var/run/UnblockNeteaseMusic.pid WorkingDirectory=/root/UnblockNeteaseMusic ExecStart=/usr/bin/node app.js -p 4567 RestartPreventExitStatus=23 Restart=always [Install] WantedBy=multi-user.target EOF 启动并开机自启： systemctl start UnblockNeteaseMusic systemctl enable UnblockNeteaseMusic 如果没有Systemd的，比如CentOS 6，Debian 7等，直接使用rc.local，使用命令： #路径和命令自行修改 sed -i '/exit/d' /etc/rc.local echo \"cd /root/UnblockNeteaseMusic &amp;&amp; node app.js -p 4567\" &gt;&gt; /etc/rc.local Docker安装 1、安装Docker #CentOS 6 rpm -iUvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm yum update -y yum -y install docker-io service docker start chkconfig docker on #CentOS 7、Debian、Ubuntu curl -sSL https://get.docker.com/ | sh systemctl start docker systemctl enable docker 2、运行镜像 docker run --name unmusic -d -p 4567:8080 nondanee/unblockneteasemusic 运行端口4567，自行修改。 使用 提示：使用方法可以选择修改Hosts或者设置代理的方式，自己挑选一种比较会的吧。 1、修改Hosts找到Hosts路径，大致位置： Windows系统：位于C:\\Windows\\System32\\drivers\\etc\\hosts Android系统：位于/system/etc/hosts Mac系统：位于/etc/hosts IOS系统：位于/etc/hosts Linux系统：位于/etc/hosts 向hosts文件添加两条规则 #将前面参数修改为服务器IP &lt;Server IP&gt; music.163.com &lt;Server IP&gt; interface.music.163.com 使用此方法必须监听80端口，也就是使用-p 80参数指定80端口运行。 2、设置代理安卓系统： #设置方法 手机设置 &gt; WLAN &gt; 网络设置 &gt; 高级选项 &gt; 代理 一般安卓手机设置代理类型的时候分2种，手动代理和自动代理，有的手机可能只有手动代理，大致设置如下： 手动代理 &gt; 只需要填上你的服务器IP和端口就可以了。 自动代理 &gt; 填上你的PAC地址就行了，地址：http://&lt;Server Name:PORT&gt;/proxy.pac，修改为你的ip、运行端口。 如果你发现设置后不能使用或者出现其它网络问题，带了-s参数的不妨去掉后再试试。 Windows系统： #设置方法 网易云客户端设置 &gt; 工具 &gt; 自定义代理 &gt; HTTP代理 这里只需要填上你的服务器ip和端口，保存即可。 IOS系统：这里很久前有人要博主详细说下IOS使用方法，对于不会设置的是挺复杂的，所以就截图说下，需要借助Shadowrocket工具。 #设置方法 打开小火箭 &gt; 添加节点(类型选择HTTP，输入你的ip和端口，随便备注下，右上角完成即可) &gt; 编辑配置 &gt; 添加规则(类型选择USER-AGENT，选项选择网易云，用户代理输入NeteaseMusic*)，保存即可。 像macOS啥的，没设备就不测试了，直接列举下作者提供方法。 UWP Windows设置 &gt; 网络和Internet &gt; 代理 Linux 系统设置 &gt; 网络 &gt; 网络代理 macOS 系统偏好设置 &gt; 网络 &gt; 高级 &gt; 代理 设置好了后，你会发现客户端之前因为版权问题不能听的都可以听了，然后建议搭建的时候，选择的服务器网络延迟尽量好点，不然播放会很慢。 问题补充 1、如果设置好了后，打开客户端提示网络问题的话，稍等片刻就好了，如果过会还是不行，就检查下操作啥的，比如防火墙。 一般情况下CentOS防火墙还需要开启运行端口，比如博主用的4567，使用命令： #CentOS 6 iptables -I INPUT -p tcp --dport 4567 -j ACCEPT service iptables save service iptables restart #CentOS 7 firewall-cmd --zone=public --add-port=4567/tcp --permanent firewall-cmd --reload 宝塔的话可以直接面板左侧-安全-开启对应的端口即可。 2、上面只提到了安卓WLAN下的使用方法，这里再补充个手机数据下的使用方法，建议配合Xndroid工具，最新版APP下载地址→传送门，大致设置如下： 打开Xndroid &gt; 点击左侧Fqrouter &gt; 添加个人代理(只需要填服务器ip和端口即可) &gt; 代理列表配置项(将高级设置里面全部关掉) &gt; 软件右上角代理设置(只代理网易云音乐) 设置完成后你还可以在右上角启动设置中，关掉xx-net和fqrouter里面的所有启动项，这样开启软件速度也会快很多。 最后貌似很多妹子都喜欢用网易云，小伙伴们可以去喜欢的妹子面前装个逼，自己搭建好了，然后教她们点亮下灰色无版权歌曲。 &lt;/div&gt;","link":"/posts/3051665138.html"},{"title":"提升方法","text":"笔记摘要 在PAC（概率近似正确(PAC, Probably approximately correct)）学习框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 提升方法的两个问题 在每一轮如何改变训练数据的权值或概率分布 如何将弱分类器组合成一个强分类器 Adaboost的解决方案： 提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类的样本的权值 加权多数表决的方法，加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值 AdaBoost算法 输入：弱学习算法和训练数据集 \\(T=\\{(x_1,y_1), (x_2,y_2),...,(x_N,y_N)\\}, x\\in X \\subseteq R^n\\) 输出：最终分类器\\(G(x)\\) 步骤 初始化训练数据的权值分布 \\(D_1=(w_{11},\\cdots,w_{1i},\\cdots,w_{1N},w_{1i}=\\frac{1}{N})​\\) m = 1,2, \\(\\cdots\\),M ( a ) 使用具有权值分布\\(D_m\\)的训练数据集学习，得到基本的分类器 \\[G_m(x):X→\\{-1,+1\\}\\] ( b ) 计算\\(G_m(x)\\)在训练集上的分类误差率 \\[e_m=\\sum_{i=1}^{N}P(G_m(x_i)\\not= y_i)=\\sum_{i=1}^{N}w_{mi}I(G_m(x_i)\\not=y_i)\\] ( c ) 计算\\(G_m(x)\\)的系数 \\[\\alpha_m=\\frac{1}{2}log\\frac{1-e_m}{e_m}\\] ( d ) 更新训练数据集的权值分布 \\[w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_my_iG_m(x_i))​\\] \\[Z_m=\\sum_{i=1}^Nw_{mi}exp(-\\alpha_my_iG_m(x_i))\\] \\(f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)\\) 最终分类器\\(G(x)=sign(f(x))=sign(\\sum_{m=1}^M\\alpha_mG_m(x))\\) 误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用， 这是AdaBoost的一个特点 利用基本分类器的线性组合构建最终分类器使AdaBoost的另一特点 AdaBoost算法的训练误差分析 AdaBoost算法最终分类器的训练误差界为 \\[ \\frac{1}{N}\\sum\\limits_{i=1}\\limits^N I(G(x_i)\\neq y_i)\\le\\frac{1}{N}\\sum\\limits_i\\exp(-y_i f(x_i))=\\prod\\limits_m Z_m \\] 这个的意思就是说指数损失是0-1损失的上界，这个上界使通过递推得到的，是归一化系数的连乘 AdaBoost算法的解释 模型为加法模型， 损失函数为指数函数， 学习算法为前向分步算法时的二分类学习方法。根据这些条件可以推导出AdaBoost 前向分步算法 输入：训练数据集\\(T={(x_1,y_1),(x_2,y_2),...,(x_N, y_N)}, x_i \\in X \\sube \\R^n, y_i\\in \\{-1, 1\\}\\)， 损失函数\\(L(y, f(x))\\); 基函数集合\\(\\{b(x;\\gamma)\\}\\) 输出：加法模型\\(f(x)\\) 步骤： 初始化\\(f_0(x)=0\\) 对\\(m=1,2,\\cdots,M\\), 极小化损失函数 \\[ (\\beta_m,\\gamma_m)=\\arg\\min \\limits_ {\\beta,\\gamma}\\sum_{i=1}^NL(y_i, f_{m-1}(x_i)+\\beta b(x_i;\\gamma)) \\] 更新 \\[ f_m(x)=f_{m-1}(x)+\\beta _mb(x;\\gamma_m) \\] 得到加法模型 \\[ f(x)=f_M(x)=\\sum_{m=1}^M\\beta_m b(x;\\gamma_m) \\] 提升树 提升树是以分类树或回归树为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一 提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法 提升树模型 以决策树为基函数的提升方法称为提升树 提升树模型可以表示成决策树的加法模型 \\[ f_M(x)=\\sum_{m=1}^MT(x;\\Theta_m) \\] 提升树算法 针对不同问题的提升树学习算法， 其主要区别在于使用的损失函数不同： 平方误差损失函数用于回归问题 指数损失函数用于分类问题 一般损失函数的一般决策问题 回归问题的提升树算法 输入：训练数据集\\(T={(x_1,y_1),(x_2,y_2),...,(x_N, y_N)}, x_i \\in X \\sube \\R^n,y_i \\in Y \\sube R\\) 输出：提升树\\(f_M(x)\\) 步骤： 初始化\\(f_0(x)=0\\) 对\\(m=1,2,\\dots,M\\) 计算残差 \\[ r_{mi}=y_i-f_{m-1}(x_i), i=1,2,\\dots,N \\] 拟合残差\\(r_{mi}\\)学习一个回归树，得到\\(T(x;\\Theta_m)\\) 更新\\(f_m(x)=f_{m-1}(x)+T(x;\\Theta_m)\\) 得到回归问题提升树 \\[ f(x)=f_M(x)=\\sum_{m=1}^MT(x;\\Theta_m) \\] 梯度提升(GBDT) 输入： 训练数据集\\(T={(x_1,y_1),(x_2,y_2),\\dots,(x_N,y_N)}, x_i \\in x \\sube \\R^n, y_i \\in y \\sube \\R\\)；损失函数\\(L(y,f(x))\\) 输出：回归树\\(\\hat{f}(x)\\) 步骤： 1. 初始化 \\[ f_0(x)=\\arg\\min\\limits_c\\sum_{i=1}^NL(y_i, c) \\] 对\\(m=1,2,\\cdots,M\\) （ a ）对\\(i=1,2,\\cdots,N\\),计算 \\[ r_{mi}=-\\left[\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f(x)=f_{m-1}(x)} \\] （ b ）对\\(r_{mi}\\)拟合一个回归树，得到第\\(m\\)棵树的叶节点区域\\(R_{mj}, j=1,2,\\dots,J\\) （ c ） 对\\(j=1,2,\\dots,J\\)，计算 \\[ c_{mj}=\\arg\\min_c\\sum_{xi\\in R_{mj}}L(y_i,f_{m-1}(x_i)+c) \\] 更新 \\[ f_m(x)=f_{m-1}(x)+\\sum_{j=1}^Jc_{mj}I(x\\in R_{mj}) \\] 得到回归树 \\[ \\hat{f}(x)=f_M(x)=\\sum_{m=1}^M\\sum_{j=1}^Jc_{mj}I(x\\in R_{mj}) \\] 习题解答 某公司招聘职员考查身体、业务能力、发展潜力这 3 项。身体分为合格1、不合格0两级，业务能力和发展潜力分为上1、中2、下3三级分类为合格1 、不合格-1两类。已知10个人的数据，如下表所示。假设弱分类器为决策树桩。.试用AdaBoost算法学习一个强分类器。 代码传送门 比较支持向量机、 AdaBoost 、逻辑斯谛回归模型的学习策略与算法。 支持向量机的学习策略是当训练数据近似线性可分时，通过软间隔最大化，学习一个线性分类器，其学习算法是SMO序列最小最优化算法 AdaBoost的学习策略是通过极小化加法模型的指数损失，得到一个强分类器，其学习算法是前向分步算法 逻辑斯谛回归模型的学习策略是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计，其学习算法可以是改进的迭代尺度算法（IIS），梯度下降法，牛顿法以及拟牛顿法","link":"/posts/55282.html"},{"title":"支持向量机","text":"笔记摘要 SVM的基本模型是定义在特征空间上的间隔最大的线性分类器 线性可分支持向量机和线性支持向量机假设输入空间和特征空间的元素一一对应，并将输入空间中的输入映射为特征空间的特征向量；非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量 支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数最小化问题 仿射变换是保凸变换 通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机 函数间隔 对于给定数据集\\(T\\)和超平面\\((w,b)\\)，定义超平面\\((w,b)\\)关于样本点\\((x_i,y_i)\\)的函数间隔为 \\[ \\hat \\gamma_i=y_i(w\\cdot x_i+b) \\] 定义超平面\\((w,b)\\)关于训练数据集\\(T\\)的函数间隔为超平面\\((w,b)\\)关于\\(T\\)中所有样本点\\((x_i,y_i)\\)的函数间隔之最小值，即 \\[ \\hat \\gamma=\\min_{i=1,\\cdots,N}\\hat\\gamma_i \\] 函数间隔可以表示分类预测的正确性及确信度 几何间隔 对于给定数据集\\(T\\)和超平面\\((w,b)\\)，定义超平面\\((w,b)\\)关于样本点\\((x_i,y_i)\\)的几何间隔为 \\[ \\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||}) \\] 定义超平面\\((w,b)\\)关于训练数据集\\(T\\)的函数间隔为超平面\\((w,b)\\)关于\\(T\\)中所有样本点\\((x_i,y_i)\\)的几何间隔之最小值，即 \\[ \\gamma=\\min_{i=1,\\cdots,N}\\hat\\gamma_i \\] 超平面\\((w,b)\\)关于样本点\\((x_i,y_i)\\)的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离 如果超平面参数成比例地改变，此时超平面没有发生改变，但函数间隔按此比例改变，而几何间隔不变 线性可分支持向量机 问题描述 \\[ \\begin{aligned} &amp;\\min_{w,b}\\frac{1}{2}||w||^2\\\\ &amp;s.t.\\ \\ \\ y_i(w\\cdot x_i+b)-1\\geqslant0,i=1,2,\\dots,N\\\\ \\end{aligned} \\] 这是个凸二次规划问题，如果求出了上述方程的解\\(w^*, b^*\\)，就可得到分离超平面 \\[ w^*\\cdot x+b^*=0 \\] 以及相应的分类决策函数 \\[ f(x)=sign(w^*\\cdot x+b^*) \\] 对偶算法 通过求解对偶问题得到原始问题地最优解的优点 对偶问题往往更容易求解 自然引入核函数，进而推广到非线性分类问题 针对每个不等式约束，定义拉格朗日乘子\\(\\alpha_i\\ge0​\\)，定义拉格朗日函数 \\[ \\begin{aligned} L(w,b,\\alpha)&amp;=\\frac{1}{2}w\\cdot w-\\left[\\sum_{i=1}^N\\alpha_i[y_i(w\\cdot x_i+b)-1]\\right]\\\\ &amp;=\\frac{1}{2}\\left\\|w\\right\\|^2-\\left[\\sum_{i=1}^N\\alpha_i[y_i(w\\cdot x_i+b)-1]\\right]\\\\ &amp;=\\frac{1}{2}\\left\\|w\\right\\|^2-\\sum_{i=1}^N\\alpha_iy_i(w\\cdot x_i+b)+\\sum_{i=1}^N\\alpha_i \\end{aligned} \\] \\[ \\alpha_i \\geqslant0, i=1,2,\\dots,N \\] 其中\\(\\alpha=(\\alpha_1,\\alpha_2,\\dots,\\alpha_N)^T​\\)为拉格朗日乘子向量 原始问题是极小极大问题，根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题: \\[ \\max\\limits_\\alpha\\min\\limits_{w,b}L(w,b,\\alpha) \\] 转换后的对偶问题 \\[ \\min\\limits_\\alpha \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i\\\\ s.t. \\ \\ \\ \\sum_{i=1}^N\\alpha_iy_i=0\\\\ \\alpha_i\\geqslant0, i=1,2,\\dots,N \\] 根据KKT条件求解，其中\\(\\alpha\\)不为零的点对应的实例为支持向量，通过支持向量可以求得\\(b\\)值 \\[ \\begin{aligned} w^*&amp;=\\sum_{i=1}^{N}\\alpha_i^*y_ix_i\\\\ b^*&amp;=y_j\\color{black}-\\sum_{i=1}^{N}\\alpha_i^*y_i(x_i\\cdot x_j\\color{black}) \\end{aligned} \\] * \\(b^*\\)的求解，通过\\(\\arg\\max \\alpha^*\\)实现，因为支持向量共线，所以通过任意支持向量求解都可以 线性支持向量机 问题描述 \\[ \\begin{aligned} \\min_{w,b,\\xi} &amp;\\frac{1}{2}\\left\\|w\\right\\|^2+C\\sum_{i=1}^N\\xi_i\\\\ s.t. \\ \\ \\ &amp;y_i(w\\cdot x_i+b)\\geqslant1-\\xi_i, i=1,2,\\dots,N\\\\ &amp;\\xi_i\\geqslant0,i=1,2,\\dots,N \\end{aligned} \\] 对偶问题描述 原始问题里面有两部分约束，涉及到两个拉格朗日乘子向量 \\[ \\begin{aligned} \\min_\\alpha\\ &amp;\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\sum_{i=1}^N\\alpha_i\\\\ s.t.\\ \\ \\ &amp;\\sum_{i=1}^N\\alpha_iy_i=0\\\\ &amp;0\\leqslant \\alpha_i \\leqslant C,i=1,2,\\dots,N \\end{aligned} \\] 通过求解对偶问题， 得到\\(\\alpha\\)，然后求解\\(w,b\\)的过程和之前一样 线性支持向量机的解\\(w^*\\)唯一但\\(b^*\\)不一定唯一 线性支持向量机是线性可分支持向量机的超集 合页损失 最小化目标函数 \\[\\min\\limits_{w,b} \\sum\\limits_{i=1}^N\\left[1-y_i(w\\cdot x+b)\\right]_++\\lambda\\left\\|w\\right\\|^2\\] 其中 第一项是经验损失或经验风险，函数\\(L(y(w\\cdot x+b))=[1-y(w\\cdot x+b)]_+\\)称为合页损失，可以表示成\\(L = \\max(1-y(w\\cdot x+b), 0)\\) 第二项是系数为\\(\\lambda\\)的\\(w\\)的\\(L_2\\)范数的平方，是正则化项 书中通过定理7.4说明了用合页损失表达的最优化问题和线性支持向量机原始最优化问题的关系 \\[ \\begin{aligned} \\min_{w,b,\\xi} &amp;\\frac{1}{2}\\left\\|w\\right\\|^2+C\\sum_{i=1}^N\\xi_i\\\\ s.t. \\ \\ \\ &amp;y_i(w\\cdot x_i+b)\\geqslant1-\\xi_i, i=1,2,\\dots,N\\\\ &amp;\\xi_i\\geqslant0,i=1,2,\\dots,N \\end{aligned} \\] 等价于 \\[ \\min\\limits_{w,b} \\sum\\limits_{i=1}^N\\left[1-y_i(w\\cdot x+b)\\right]_++\\lambda\\left\\|w\\right\\|^2 \\] 证明如下 令合页损失\\(\\left[1-y_i(w\\cdot x+b)\\right]_+=\\xi_i\\)，合页损失非负，所以有\\(\\xi_i\\ge0\\)，这个对应了原始最优化问题中的第二个约束 还是根据合页损失非负，当\\(1-y_i(w\\cdot x+b)\\leq\\color{red}0​\\)的时候，有\\(\\left[1-y_i(w\\cdot x+b)\\right]_+=\\color{red}\\xi_i=0​\\)，所以有\\(1-y_i(w\\cdot x+b)\\leq\\color{red}0=\\xi_i\\)，这对应了原始最优化问题中的第一个约束 所以，在满足这两个约束的情况下，有 \\[ \\begin{aligned} \\min\\limits_{w,b} &amp;\\sum\\limits_{i=1}^N\\left[1-y_i(w\\cdot x+b)\\right]_++\\lambda\\left\\|w\\right\\|^2\\\\ \\min\\limits_{w,b} &amp;\\sum\\limits_{i=1}^N\\xi_i+\\lambda\\left\\|w\\right\\|^2\\\\ \\min\\limits_{w,b} &amp;\\frac{1}{C}\\left(\\frac{1}{2}\\left\\|w\\right\\|^2+C\\sum\\limits_{i=1}^N\\xi_i\\right), with \\ \\lambda=\\frac{1}{2C}\\\\ \\end{aligned} \\] 合页损失函数 ### 非线性支持向量机 核技巧的想法是在学习和预测中只定义核函数\\(K(x,z)\\)，而不是显式的定义映射函数\\(\\phi\\) 通常，直接计算\\(K(x,z)\\)比较容易， 而通过\\(\\phi(x)\\)和\\(\\phi(z)\\)计算\\(K(x,z)\\)并不容易。 \\[ W(\\alpha)=\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum_{i=1}^N\\alpha_i\\\\ \\] \\[ f(x)=sign\\left(\\sum_{i=1}^{N_s}\\alpha_i^*y_i\\phi(x_i)\\cdot \\phi(x)+b^*\\right)=sign\\left(\\sum_{i=1}^{N_s}\\alpha_i^*y_iK(x_i,x)+b^*\\right) \\] 学习是隐式地在特征空间进行的，不需要显式的定义特征空间和映射函数 核函数 对于给定的核\\(K(x,z)\\)，特征空间\\(\\mathcal H\\)和映射函数\\(\\phi(x)\\)的取法并不唯一，可以取不同的特征空间，即便是同一特征空间里也可以取不同的映射 下面这个例子里面\\(\\phi(x)\\)实现了从低维空间到高维空间的映射 \\[ K(x,z)=(x\\cdot z)^2\\\\ {X}=\\R^2, x=(x^{(1)},x^{(2)})^T\\\\ {H}=\\R^3, \\phi(x)=((x^{(1)})^2, \\sqrt2x^{(1)}x^{(2)}, (x^{(2)})^2)^T\\\\ {H}=\\R^4, \\phi(x)=((x^{(1)})^2, x^{(1)}x^{(2)}, x^{(1)}x^{(2)}, (x^{(2)})^2)^T\\\\ \\] 核具有再生性，即满足下面条件的核称为再生核 \\[ K(\\cdot,x)\\cdot f=f(x)\\\\ K(\\cdot,x)\\cdot K(\\cdot, z)=K(x,z) \\] 通常所说的核函数就是正定核函数 问题描述 将向量内积替换成了核函数，而SMO算法求解的问题正是该问题 构建最优化问题： \\[ \\begin{aligned} \\min_\\alpha\\ &amp;\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum_{i=1}^N\\alpha_i\\\\ s.t.\\ \\ \\ &amp;\\sum_{i=1}^N\\alpha_iy_i=0\\\\ &amp;0\\leqslant \\alpha_i \\leqslant C,i=1,2,\\dots,N \\end{aligned} \\] 求解得到\\(\\alpha^*=(\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_N^*)^T\\) 选择\\(\\alpha^*\\)的一个正分量计算 \\[ b^*=y_j-\\sum_{i=1}^N\\alpha_i^*y_iK(x_i,x_j) \\] 构造决策函数 \\[ f(x)=sign\\left(\\sum_{i=1}^N\\alpha_i^*y_iK(x,x_i)+b^*\\right) \\] SMO算法 问题描述 \\[ \\begin{aligned} \\min_\\alpha\\ &amp;\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)-\\sum_{i=1}^N\\alpha_i\\\\ s.t.\\ \\ \\ &amp;\\sum_{i=1}^N\\alpha_iy_i=0\\\\ &amp;0\\leqslant \\alpha_i \\leqslant C,i=1,2,\\dots,N \\end{aligned} \\] 这个问题中，变量是\\(\\alpha\\)，一个变量\\(\\alpha_i\\)对应一个样本点\\((x_i,y_i)\\)，变量总数等于\\(N\\) KKT 条件 KKT条件是该最优化问题的充分必要条件 简单来说，约束最优化问题包含\\(\\leqslant0\\)，和\\(=0\\)两种约束条件 \\[ \\begin{aligned} \\min_{x \\in R^n}\\quad &amp;f(x) \\\\ s.t.\\quad&amp;c_i(x) \\leqslant 0 , i=1,2,\\ldots,k\\\\ &amp;h_j(x) = 0 , j=1,2,\\ldots,l \\end{aligned} \\] 引入广义拉格朗日函数 \\[ L(x,\\alpha,\\beta) = f(x) + \\sum_{i=0}^k \\alpha_ic_i(x) + \\sum_{j=1}^l \\beta_jh_j(x) \\] 在KKT的条件下，原始问题和对偶问题的最优值相等 \\[ ∇_xL(x^∗,α^∗,β^∗)=0\\\\ ∇_αL(x^∗,α^∗,β^∗)=0\\\\ ∇_βL(x^∗,α^∗,β^∗)=0\\\\ α_i^∗c_i(x^*)=0,i=1,2,…,k\\\\ c_i(x^*)≤0,i=1,2,…,k\\\\ α^∗_i≥0,i=1,2,…,k\\\\ h_j(x^∗)=0,j=1,2,…,l \\] 前面三个条件是由解析函数的知识，对于各个变量的偏导数为0，后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束,第四个条件是KKT的对偶互补条件 算法内容 整个SMO算法包括两部分： 求解两个变量二次规划的解析方法 选择变量的启发式方法 \\[ \\begin{aligned} \\min_\\alpha\\ &amp;\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i, x_j)-\\sum_{i=1}^N\\alpha_i\\\\ s.t.\\ \\ \\ &amp;\\sum_{i=1}^N\\alpha_iy_i=0\\\\ &amp;0\\leqslant \\alpha_i \\leqslant C,i=1,2,\\dots,N \\end{aligned} \\] Part I 两个变量二次规划求解 选择两个变量\\(\\alpha_1,\\alpha_2​\\)，由等式约束可以得到\\(\\alpha_1=-y_1\\sum\\limits_{i=2}^N\\alpha_iy_i​\\)，所以这个问题等价于一个单变量优化问题 \\[ \\begin{aligned} \\min_{\\alpha_1,\\alpha_2} W(\\alpha_1,\\alpha_2)=&amp;\\frac{1}{2}K_{11}\\alpha_1^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_1y_2K_{12}\\alpha_1\\alpha_2\\\\ &amp;-(\\alpha_1+\\alpha_2)+y_1\\alpha_1\\sum_{i=3}^Ny_i\\alpha_iK_{il}+y_2\\alpha_2\\sum_{i=3}^Ny_i\\alpha_iK_{i2}\\\\ s.t. \\ \\ \\ &amp;\\alpha_1y_1+\\alpha_2y_2=-\\sum_{i=3}^Ny_i\\alpha_i=\\varsigma\\\\ &amp;0\\leqslant\\alpha_i\\leqslant C, i=1,2 \\end{aligned} \\] 上面存在两个约束： 线性等式约束 边界约束 根据简单的线性规划可以得出等式约束使得\\((\\alpha_1,\\alpha_2)\\)在平行于盒子\\([0,C]\\times [0,C]\\)的对角线的直线上 * 首先求沿着约束方向未经剪辑，即不考虑约束\\(0\\leqslant\\alpha_i\\leqslant C\\)时\\(\\alpha_2\\)的最优解，然后再求剪辑后的解 \\[E_i=g(x_i)-y_i=(\\sum_{j=1}^N\\alpha_jy_jK(x_i, x_j)+b)-y_i,i=1,2\\] \\(E_i\\)为函数\\(g(x)\\)对输入的预测值与真实输出\\(y_i\\)的差 #### Part II 变量的选择方法 第一个变量\\(\\alpha_1\\)外层循环，寻找违反KKT条件最严重的样本点 第二个变量\\(\\alpha_2\\)内层循环，希望能使\\(\\alpha_2\\)有足够大的变化 计算阈值\\(b\\)和差值\\(E_i\\) 输入：训练数据集\\(T={(x_1,y_1),(x_2,y_2),\\dots, (x_N,y_N)}\\)，其中\\(x_i\\in\\mathcal X=\\bf R^n, y_i\\in\\mathcal Y=\\{-1,+1\\}, i=1,2,\\dots,N\\),精度\\(\\epsilon\\) 输出：近似解\\(\\hat\\alpha\\) 取初值\\(\\alpha_0=0\\)，令\\(k=0\\) 选取优化变量\\(\\alpha_1^{(k)},\\alpha_2^{(k)}\\)，解析求解两个变量的最优化问题，求得最优解\\(\\alpha_1^{(k+1)},\\alpha_2^{(k+1)}\\)，更新\\(\\alpha\\)为\\(\\alpha^{k+1}\\) 若在精度\\(\\epsilon\\)范围内满足停止条件 \\[ \\sum_{i=1}^{N}\\alpha_iy_i=0\\\\ 0\\leqslant\\alpha_i\\leqslant C,i=1,2,\\dots,N\\\\ y_i\\cdot g(x_i)= \\begin{cases} \\geqslant1,\\{x_i|\\alpha_i=0\\}\\\\ =1,\\{x_i|0&lt;\\alpha_i&lt;C\\}\\\\ \\leqslant1,\\{x_i|\\alpha_i=C\\} \\end{cases}\\\\ g(x_i)=\\sum_{j=1}^{N}\\alpha_jy_jK(x_j,x_i)+b \\] 则转4,否则，\\(k=k+1\\)转2 取\\(\\hat\\alpha=\\alpha^{(k+1)}\\) 习题解答 1.比较感知机的对偶形式与线性可分支持向量机的对偶形式 感知机的对偶形式 \\(f(x)=sign\\left(\\sum_{j=1}^N\\alpha_jy_jx_j\\cdot x+b\\right), \\alpha=(\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T\\) 线性可分支持向量机的对偶形式 \\(f(x)=sign\\left(\\sum_{i=1}^N\\alpha_i^*y_ix_i\\cdot x+b^*\\right), \\alpha^*=(\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_N^*)^T\\) 感知机学习算法的原始形式和对偶形式与线性可分支持向量机学习算法的原始形式和对偶形式相对应。在线性可分支持向量机的对偶形式中,\\(w\\)也是被表示为实例 \\(x_i\\)和标记\\(y_i\\)的线性组合的形式 \\[w=\\sum_{i=1}^{N}\\alpha_iy_ix_i\\] 而它们的偏置\\(b\\)的形式不同，前者\\(b=\\sum_{i=1}^{N}\\alpha_iy_i\\),而后者\\(b^*=y_j\\color{black}-\\sum_{i=1}^{N}\\alpha_i^*y_i(x_i\\cdot x_j)\\) 2.已知正例点\\(x_1=(1,2)^T\\)，\\(x_2=(2,3)^T\\)，\\(x_3=(3,3)^T\\),负例点\\(x_4=(2,1)^T\\)，\\(x_5=(3,2)^T\\)， 试求最大间隔分离超平面和分类决策函数，并在图上画出分离超平面、间隔边界及支持向量 根据书中算法，计算可得\\(w_1=-1\\),\\(w_2=2\\),\\(b=-2\\),即最大间隔分离超平面为 \\[-x^{(1)}+2x^{(2)}-2=0\\] 分类决策函数为 \\[f(x)=sign(-x^{(1)}+2x^{(2)}-2)\\] 3.线性支持向量机还可以定义为以下形式： \\[\\min_{w,b,\\xi}{\\frac{1}{2}\\|w\\|^2}+C\\sum^N_{i=1}\\xi_i^2\\\\s.t.{\\quad}y_i(w{\\cdot}x_i+b)\\ge1-\\xi_i,\\,i=1,2,\\cdots,N\\\\\\xi_i\\ge0,\\,i=1,2,\\cdots,N\\] 试求其对偶形式 * 首先求得原始化最优问题的拉格朗日函数是： \\(L(w,b,\\alpha,\\xi,μ)=\\frac{1}{2}\\left\\|w\\right\\|^2+C\\sum_{i=1}^N\\xi_i^2-\\sum_{i=1}^N\\alpha_i(y_i(w\\cdot x_i+b-1)+\\xi_i)-\\sum_{i=1}^Nμ_i\\xi_i\\) * 对偶问题是拉格朗日的极大极小问题。首先求\\(L(w,b,\\alpha,\\xi,μ)\\)对\\(w,b,\\xi\\)的极小,即对该三项求偏导，得 \\[ w=\\sum_{i=1}^{N}\\alpha_iy_ix_i\\\\ \\sum_{i=1}^N\\alpha_iy_i=0\\\\ 2C\\xi_i-\\alpha_i-μ_i=0 \\] 将上述带入拉格朗日函数，得 \\[-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-C\\sum_{i=1}^N\\xi_i^2+\\sum_{i=1}^N\\alpha_i\\\\ -\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j(x_i\\cdot x_j)-\\frac{1}{4C}\\sum_{i=1}^N(\\alpha_i+μ_i)^2+\\sum_{i=1}^N\\alpha_i\\] 4.证明内积的正整数幂函数\\(K(x,z)=(x{\\cdot}z)^p\\)是正定核函数，这里\\(p\\)是正整数，\\(x,z{\\in}R^n\\) 要证明正整数幂函数是正定核函数，只需证明其对应得Gram矩阵\\(K=[K(x_i,x_j)]_{m\\times m}\\)是半正定矩阵 对任意\\(c_1,c_2…c_m\\in R\\),有 \\[ \\begin{aligned} \\sum_{i,j=1}^{m}c_ic_jK(x_i,x_j)\\\\ =&amp;\\sum_{i,j=1}^{m}c_ic_j(x_i\\cdot x_j)^p\\\\ =&amp;(\\sum_{i=1}^{m}c_ix_i)(\\sum_{j=1}^{m}c_jx_j)(x_ix_j)^{p-1}\\\\ =&amp;||\\sum_{i=1}^{m}c_ix_i||^2(x_ix_j)^{p-1} \\end{aligned} \\] 由于p大于等于1，该式子也大于等于0，即Gram矩阵半正定，所以正整数的幂函数是正定核函数","link":"/posts/49107.html"},{"title":"文本挖掘的分词原理","text":"在做文本挖掘的时候，首先要做的预处理就是分词。英文单词天然有空格隔开容易按照空格分词，但是也有时候需要把多个单词做为一个分词，比如一些名词如“New York”，需要做为一个词看待。而中文由于没有空格，分词就是一个需要专门去解决的问题了。无论是英文还是中文，分词的原理都是类似的，本文就对文本挖掘时的分词原理做一个总结。 分词的基本原理 现代分词都是基于统计的分词，而统计的样本内容来自于一些标准的语料库。假如有一个句子：“小明来到荔湾区”，我们期望语料库统计后分词的结果是：\"小明/来到/荔湾/区\"，而不是“小明/来到/荔/湾区”。那么如何做到这一点呢？ 从统计的角度，我们期望\"小明/来到/荔湾/区\"这个分词后句子出现的概率要比“小明/来到/荔/湾区”大。如果用数学的语言来说说，如果有一个句子\\(s\\),它有m种分词选项如下： \\[ A_{11}A_{12}...A_{1n_1} \\] \\[ A_{21}A_{22}...A_{2n_2} \\] \\[ ...... ...... \\] \\[ A_{m1}A_{m2}...A_{mn_m} \\] 其中下标\\(n_i\\)代表第\\(i\\)种分词的词个数。如果我们从中选择了最优的第𝑟种分词方法，那么这种分词方法对应的统计分布概率应该最大，即： \\[ r = \\underbrace{arg\\;max}_iP(A_{i1},A_{i2},...,A_{in_i}) \\] 但是我们的概率分布\\(P(A_{i1},A_{i2},...,A_{in_i})\\)并不好求出来，因为它涉及到\\(n_i\\)个分词的联合分布。在NLP中，为了简化计算，我们通常使用马尔科夫假设，即每一个分词出现的概率仅仅和前一个分词有关，即： \\[ P(A_{ij}|A_{i1},A_{i2},...,A_{i(j-1)}) = P(A_{ij}|A_{i(j-1)}) \\] 在前面我们讲MCMC采样时，也用到了相同的假设来简化模型复杂度。使用了马尔科夫假设，则我们的联合分布就好求了，即： \\[ P(A_{i1},A_{i2},...,A_{in_i}) = P(A_{i1})P(A_{i2}|A_{i1})P(A_{i3}|A_{i2})...P(A_{in_i}|A_{i(n_i-1)}) \\] 而通过我们的标准语料库，我们可以近似的计算出所有的分词之间的二元条件概率，比如任意两个词\\(w_1,w_2\\)，它们的条件概率分布可以近似的表示为： \\[ P(w_2|w_1) = \\frac{P(w_1,w_2)}{P(w_1)} \\approx \\frac{freq(w_1,w_2)}{freq(w_1)} \\] \\[ P(w_1|w_2) = \\frac{P(w_2,w_1)}{P(w_2)} \\approx \\frac{freq(w_1,w_2)}{freq(w_2)} \\] 其中\\(freq(w_1,w_2)\\)表示\\(w_1,w_2\\)在语料库中相邻一起出现的次数，而其中\\(freq(w_1),freq(w_2)\\)分别表示\\(w_1,w_2\\)在语料库中出现的统计次数。 利用语料库建立的统计概率，对于一个新的句子，我们就可以通过计算各种分词方法对应的联合分布概率，找到最大概率对应的分词方法，即为最优分词。 N元模型 当然，你会说，只依赖于前一个词太武断了，我们能不能依赖于前两个词呢？即： \\[ P(A_{i1},A_{i2},...,A_{in_i}) = P(A_{i1})P(A_{i2}|A_{i1})P(A_{i3}|A_{i1}，A_{i2})...P(A_{in_i}|A_{i(n_i-2)}，A_{i(n_i-1)}) \\] 这样也是可以的，只不过这样联合分布的计算量就大大增加了。我们一般称只依赖于前一个词的模型为二元模型(Bi-Gram model)，而依赖于前两个词的模型为三元模型。以此类推，我们可以建立四元模型，五元模型,...一直到通用的𝑁元模型。越往后，概率分布的计算复杂度越高。当然算法的原理是类似的。 在实际应用中，\\(N\\)一般都较小，一般都小于4，主要原因是N元模型概率分布的空间复杂度为\\(O(|V|^N)\\)，其中\\(|V|\\)为语料库大小，而\\(N\\)为模型的元数，当𝑁增大时，复杂度呈指数级的增长。 \\(N\\)元模型的分词方法虽然很好，但是要在实际中应用也有很多问题，首先，某些生僻词，或者相邻分词联合分布在语料库中没有，概率为0。这种情况我们一般会使用拉普拉斯平滑，即给它一个较小的概率值，这个方法在朴素贝叶斯算法原理小结也有讲到。第二个问题是如果句子长，分词有很多情况，计算量也非常大，这时我们可以用下一节维特比算法来优化算法时间复杂度。 维特比算法与分词 为了简化原理描述，我们本节的讨论都是以二元模型为基础。 对于一个有很多分词可能的长句子，我们当然可以用暴力方法去计算出所有的分词可能的概率，再找出最优分词方法。但是用维特比算法可以大大简化求出最优分词的时间。 大家一般知道维特比算法是用于隐式马尔科夫模型HMM解码算法的，但是它是一个通用的求序列最短路径的方法，不光可以用于HMM，也可以用于其他的序列最短路径算法，比如最优分词。 维特比算法采用的是动态规划来解决这个最优分词问题的，动态规划要求局部路径也是最优路径的一部分，很显然我们的问题是成立的。首先我们看一个简单的分词例子：\"人生如梦境\"。它的可能分词可以用下面的概率图表示： 图中的箭头为通过统计语料库而得到的对应的各分词位置BEMS（开始位置，结束位置，中间位置，单词）的条件概率。比如P(生|人)=0.17。有了这个图，维特比算法需要找到从Start到End之间的一条最短路径。对于在End之前的任意一个当前局部节点，我们需要得到到达该节点的最大概率\\(\\delta\\)，和记录到达当前节点满足最大概率的前一节点位置\\(\\Psi\\)。 我们先用这个例子来观察维特比算法的过程。首先我们初始化有： \\[ \\delta(人) = 0.26\\;\\;\\Psi(人)=Start\\;\\;\\delta(人生) = 0.44\\;\\;\\Psi(人生)=Start \\] 对于节点\"生\"，它只有一个前向节点，因此有： \\[ \\delta(生) = \\delta(人)P(生|人) = 0.0442 \\;\\; \\Psi(生)=人 \\] 对于节点\"如\"，就稍微复杂一点了，因为它有多个前向节点，我们要计算出到“如”概率最大的路径： \\[ \\delta(如) = max\\{\\delta(生)P(如|生)，\\delta(人生)P(如|人生)\\} = max\\{0.01680, 0.3168\\} = 0.3168 \\;\\; \\Psi(如) = 人生 \\] 类似的方法可以用于其他节点如下： \\[ \\delta(如梦) = \\delta(人生)P(如梦|人生) = 0.242 \\;\\; \\Psi(如梦)=人生 \\] \\[ \\delta(梦) = \\delta(如)P(梦|如) = 0.1996 \\;\\; \\Psi(梦)=如 \\] \\[ \\delta(境) = max\\{\\delta(梦)P(境|梦) ,\\delta(如梦)P(境|如梦)\\}= max\\{0.0359, 0.0315\\} = 0.0359 \\;\\; \\Psi(境)=梦 \\] \\[ \\delta(梦境) = \\delta(如)P(梦境|如) = 0.1585 \\;\\; \\Psi(梦境)=如 \\] 最后我们看看最终节点End: \\[ \\delta(End) = max\\{\\delta(梦境)P(End|梦境), \\delta(境)P(End|境)\\} = max\\{0.0396, 0.0047\\} = 0.0396\\;\\;\\Psi(End)=梦境 \\] 由于最后的最优解为“梦境”，现在我们开始用\\(\\Psi\\)反推: \\[ \\Psi(End)=梦境 \\to \\Psi(梦境)=如 \\to \\Psi(如)=人生 \\to \\Psi(人生)=start \\] 从而最终的分词结果为\"人生/如/梦境\"。是不是很简单呢。 由于维特比算法我会在后面讲隐式马尔科夫模型HMM解码算法时详细解释，这里就不归纳了。 常用分词工具 对于文本挖掘中需要的分词功能，一般我们会用现有的工具。简单的英文分词不需要任何工具，通过空格和标点符号就可以分词了，而进一步的英文分词推荐使用nltk。对于中文分词，则推荐用结巴分词（jieba）。这些工具使用都很简单。你的分词没有特别的需求直接使用这些分词工具就可以了。 结语 分词是文本挖掘的预处理的重要的一步，分词完成后，我们可以继续做一些其他的特征工程，比如向量化（vectorize），TF-IDF以及Hash trick，这些我们后面再讲。","link":"/posts/2830654865.html"},{"title":"朴素贝叶斯法","text":"笔记摘要 条件概率分布\\(P(X=x|Y=c_k)\\)有指数级数量的参数，其实际估计是不可行的 指数级数量的参数 \\(K\\prod_{j=1}^nS_j\\)，实际估计不可行是实际上没有那么多样本 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法 贝叶斯定理 \\[P(B_i|A)=\\frac{P(B_i)P(A|B_i)}{\\sum _{j=1}^nP(B_j)P(A|B_j)}\\] 条件独立假设 independent and identically distributed 求\\(P(Y|X)\\)，其中\\(X\\in\\{X_1,X_2,\\dots,X_n\\}\\)，条件独立假设这里给定\\(Y\\)的情况下： 每一个\\(X_i\\)和其他的每个\\(X_k\\)是条件独立的 每一个\\(X_i\\)和其他的每个\\(X_k\\)的子集是条件独立的 条件独立性假设是: \\[ \\begin{aligned} P(X=x|Y=c_k)&amp;=P(X^{(1)},\\dots,X^{(n)}|Y=c_k)\\\\ &amp;=\\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k) \\end{aligned} \\] 条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的 参数估计 极大似然估计 为了估计状态变量的条件分布，利用贝叶斯法则，有 \\[ \\underbrace{P(X|Y)}_{posterior}=\\frac{\\overbrace{P(Y|X)}^{likelihood}\\overbrace{P(X)}^{prior}}{\\underbrace{P(Y)}_{evidence}}=\\frac{\\overbrace{P(Y|X)}^{likelihood}\\overbrace{P(X)}^{prior}}{\\underbrace{\\sum\\limits_x P(Y|X)P(X)}_{evidence}} \\] 其中\\(P(X|Y)\\)为给定\\(Y\\)下\\(X\\)的后验概率(Posterior)， \\(P(Y|X)\\)称为似然(Likelyhood)，\\(P(X)\\)称为先验(Prior)。 后验概率最大化的含义 朴素贝叶斯法将实例分到后验概率最大的类中， 这等价于期望风险最小化。 后验是指观察到\\(Y\\)之后，对\\(X\\)的信念 贝叶斯估计 对于\\(x\\)的某个特征的取值没有在先验中出现的情况 ，如果用极大似然估计就会出现所要估计的概率值为0的情况。这样会影响后验概率的计算结果，使分类产生偏差 但是出现这种情况的原因通常是因为数据集不能全覆盖样本空间，出现未知的情况处理的策略就是做平滑 \\[ P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum\\limits_{i=1}^NI(x_i^{j}=a_{jl},y_j=c_k)+\\lambda}{\\sum\\limits_{i=1}^NI(y_j=c_k)+S_j\\lambda} \\] 当\\(\\lambda = 0\\)的时候，就是极大似然估计 当\\(\\lambda=1\\)的时候，这个平滑方案叫做Laplace Smoothing。拉普拉斯平滑相当于给未知变量给定了先验概率 习题解答 4.1 用极大似然估计法推出朴素贝叶斯法中的概率估计公式(4.8)及公式 (4.9) 由于朴素贝叶斯法假设Y是定义在输出空间上的随机变量，因此可以定义\\(P(Y=c_k)=p\\),令\\(m=\\sum _{i=1}^NI(y_i=c_k)\\) 得出似然函数 \\(L(p)=p^m(1-p)^{N-m}\\) 求导求最值：\\(mp^{m-1}(1-p)^{N-m}-(N-m)p^m(1-p)^{N-m-1}=0\\) \\(p^{m-1}(1-p)^{N-m-1}(m-Np)=0\\),易得\\(p=\\frac mN\\),即为公式（4.8） 公式（4.9）的证明与公式（4.8）完全相同，定义\\(P(X^{(j)}=a_{jl}{\\mid}Y=c_k)=p\\)，令\\(m=\\sum_{i=1}^NI(y_i=c_k)\\)，\\(q=\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)\\)即可 4.2 用贝叶斯估计法推出朴素贝叶斯法中的慨率估计公式(4.10)及公式(4.11) 贝叶斯估计和传统的极大似然估计的区别就是，参数值是固定的还是也当做随机变量。传统的极大似然估计，把参数\\(\\theta\\)当做固定的一个值，不变的，只是目前还不知道，通过最大化\\(L\\)求出\\(\\theta\\)；贝叶斯估计认为参数\\(\\theta\\)也是随机变量，它也服从一个分布（β分布） 设\\(P(Y=c_k)=p\\),\\(m=\\sum _{i=1}^NI(y_i=c_k)\\),加入先验概率,认为是均匀的\\(p=\\frac{1}{K}\\)，对照上题极大似然概率下的条件概率约束 得到\\(\\lambda (pK-1)+pN-m=0\\),从而解出\\(P(Y=c_k)=\\frac{m+\\lambda}{N+K\\lambda}\\),即为公式（4.11）","link":"/posts/62831.html"},{"title":"条件随机场CRF(一)从随机场到线性链条件随机场","text":"条件随机场(Conditional Random Fields, 以下简称CRF)是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然语言处理中得到了广泛应用。本系列主要关注于CRF的特殊形式：线性链(Linear chain) CRF。本文关注与CRF的模型基础。 什么样的问题需要CRF模型 和HMM类似，在讨论CRF之前，我们来看看什么样的问题需要CRF模型。这里举一个简单的例子： 假设我们有Bob一天从早到晚的一系列照片，Bob想考考我们，要我们猜这一系列的每张照片对应的活动，比如: 工作的照片，吃饭的照片，唱歌的照片等等。一个比较直观的办法就是，我们找到Bob之前的日常生活的一系列照片，然后找Bob问清楚这些照片代表的活动标记，这样我们就可以用监督学习的方法来训练一个分类模型，比如逻辑回归，接着用模型去预测这一天的每张照片最可能的活动标记。 这种办法虽然是可行的，但是却忽略了一个重要的问题，就是这些照片之间的顺序其实是有很大的时间顺序关系的，而用上面的方法则会忽略这种关系。比如我们现在看到了一张Bob闭着嘴的照片，那么这张照片我们怎么标记Bob的活动呢？比较难去打标记。但是如果我们有Bob在这一张照片前一点点时间的照片的话，那么这张照片就好标记了。如果在时间序列上前一张的照片里Bob在吃饭，那么这张闭嘴的照片很有可能是在吃饭咀嚼。而如果在时间序列上前一张的照片里Bob在唱歌，那么这张闭嘴的照片很有可能是在唱歌。 为了让我们的分类器表现的更好，可以在标记数据的时候，可以考虑相邻数据的标记信息。这一点，是普通的分类器难以做到的。而这一块，也是CRF比较擅长的地方。 在实际应用中，自然语言处理中的词性标注(POS Tagging)就是非常适合CRF使用的地方。词性标注的目标是给出一个句子中每个词的词性（名词，动词，形容词等）。而这些词的词性往往和上下文的词的词性有关，因此，使用CRF来处理是很适合的，当然CRF不是唯一的选择，也有很多其他的词性标注方法。 从随机场到马尔科夫随机场 首先，我们来看看什么是随机场。“随机场”的名字取的很玄乎，其实理解起来不难。随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做随机场。还是举词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词...)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。 了解了随机场，我们再来看看马尔科夫随机场。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。继续举十个词的句子词性标注的例子： 如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，只与第二个词和第四个词的词性有关。 从马尔科夫随机场到条件随机场 理解了马尔科夫随机场，再理解CRF就容易了。CRF是马尔科夫随机场的特例，它假设马尔科夫随机场中只有𝑋和𝑌两种变量，\\(X\\)一般是给定的，而𝑌一般是在给定\\(X\\)的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。在我们十个词的句子词性标注的例子中，\\(X\\)是词，\\(Y\\)是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个CRF。 对于CRF，我们给出准确的数学语言描述： 设\\(X\\)与\\(Y\\)是随机变量，\\(P(Y|X)\\)是给定𝑋时𝑌的条件概率分布，若随机变量\\(Y\\)构成的是一个马尔科夫随机场，则称条件概率分布\\(P(Y|X)\\)是条件随机场。 从条件随机场到线性链条件随机场 注意在CRF的定义中，我们并没有要求\\(X\\)和\\(Y\\)有相同的结构。而实现中，我们一般都假设𝑋和𝑌有相同的结构，即: \\[ X =(X_1,X_2,...X_n),\\;\\;Y=(Y_1,Y_2,...Y_n) \\] 我们一般考虑如下图所示的结构：𝑋和𝑌有相同的结构的CRF就构成了线性链条件随机场(Linear chain Conditional Random Fields,以下简称 linear-CRF)。 在我们的十个词的句子的词性标记中，词有十个，词性也是十个，因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个linear-CRF。 我们再来看看 linear-CRF的数学定义： 设\\(X =(X_1,X_2,...X_n),\\;\\;Y=(Y_1,Y_2,...Y_n)\\)均为线性链表示的随机变量序列，在给定随机变量序列\\(X\\)的情况下，随机变量𝑌的条件概率分布\\(P(Y|X)\\)构成条件随机场，即满足马尔科夫性： \\[ P(Y_i|X,Y_1,Y_2,...Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1}) \\] 线性链条件随机场的参数化形式 对于上一节讲到的linear-CRF，我们如何将其转化为可以学习的机器学习模型呢？这是通过特征函数和其权重系数来定义的。什么是特征函数呢？ 在linear-CRF中，特征函数分为两类，第一类是定义在𝑌节点上的节点特征函数，这类特征函数只和当前节点有关，记为： \\[ s_l(y_i, x,i),\\;\\; l =1,2,...L \\] 其中\\(L\\)是定义在该节点的节点特征函数的总个数，\\(i\\)是当前节点在序列的位置。 第二类是定义在\\(Y\\)上下文的局部特征函数，这类特征函数只和当前节点和上一个节点有关，记为： \\[ t_k(y_{i-1},y_i, x,i),\\;\\; k =1,2,...K \\] 其中\\(K\\)是定义在该节点的局部特征函数的总个数，\\(i\\)是当前节点在序列的位置。之所以只有上下文相关的局部特征函数，没有不相邻节点之间的特征函数，是因为我们的linear-CRF满足马尔科夫性。 无论是节点特征函数还是局部特征函数，它们的取值只能是0或者1。即满足特征条件或者不满足特征条件。同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度。假设\\(t_k\\)的权重系数是\\(\\lambda_k\\),\\(s_l\\)的权重系数是𝜇𝑙,则linear-CRF由我们所有的\\(t_k, \\lambda_k, s_l, \\mu_l\\)共同决定。 此时我们得到了linear-CRF的参数化形式如下： \\[ P(y|x) = \\frac{1}{Z(x)}exp\\Big(\\sum\\limits_{i,k} \\lambda_kt_k(y_{i-1},y_i, x,i) +\\sum\\limits_{i,l}\\mu_ls_l(y_i, x,i)\\Big) \\] 其中，\\(Z(x)\\)为规范化因子： \\[ Z(x) =\\sum\\limits_{y} exp\\Big(\\sum\\limits_{i,k} \\lambda_kt_k(y_{i-1},y_i, x,i) +\\sum\\limits_{i,l}\\mu_ls_l(y_i, x,i)\\Big) \\] 回到特征函数本身，每个特征函数定义了一个linear-CRF的规则，则其系数定义了这个规则的可信度。所有的规则和其可信度一起构成了我们的linear-CRF的最终的条件概率分布。 线性链条件随机场实例 这里我们给出一个linear-CRF用于词性标注的实例，为了方便，我们简化了词性的种类。假设输入的都是三个词的句子，即\\(X=(X_1,X_2,X_3)\\),输出的词性标记为\\(Y=(Y_1,Y_2,Y_3)\\),其中\\(Y \\in \\{1(名词)，2(动词)\\}\\) 这里只标记出取值为1的特征函数如下： \\[ t_1 =t_1(y_{i-1} = 1, y_i =2,x,i), i =2,3,\\;\\;\\lambda_1=1 \\\\ t_2 =t_2(y_1=1,y_2=1,x,2)\\;\\;\\lambda_2=0.5 \\\\ t_3 =t_3(y_2=2,y_3=1,x,3)\\;\\;\\lambda_3=1 \\\\ t_4 =t_4(y_1=2,y_2=1,x,2)\\;\\;\\lambda_4=1 \\\\ t_5 =t_5(y_2=2,y_3=2,x,3)\\;\\;\\lambda_5=0.2 \\\\ s_1 =s_1(y_1=1,x,1)\\;\\;\\mu_1 =1 \\\\ s_2 =s_2( y_i =2,x,i), i =1,2,\\;\\;\\mu_2=0.5 \\\\ s_3 =s_3( y_i =1,x,i), i =2,3,\\;\\;\\mu_3=0.8 \\\\ s_4 =s_4(y_3=2,x,3)\\;\\;\\mu_4 =0.5 \\] 求标记(1,2,2)的非规范化概率。 利用linear-CRF的参数化公式，我们有： \\[ P(y|x) \\propto exp\\Big[\\sum\\limits_{k=1}^5\\lambda_k\\sum\\limits_{i=2}^3t_k(y_{i-1},y_i, x,i) + \\sum\\limits_{l=1}^4\\mu_l\\sum\\limits_{i=1}^3s_l(y_i, x,i) \\Big] \\] 带入(1,2,2)我们有： \\[ P(y_1=1,y_2=2,y_3=2|x) \\propto exp(3.2) \\] ## 线性链条件随机场的简化形式 在上几节里面，我们用\\(s_l\\)表示节点特征函数，用\\(t_k\\)表示局部特征函数，同时也用了不同的符号表示权重系数，导致表示起来比较麻烦。其实我们可以对特征函数稍加整理，将其统一起来。 假设我们在某一节点我们有\\(K_1\\)个局部特征函数和\\(K_2\\)个节点特征函数，总共有\\(K=K_1+K_2\\)个特征函数。我们用一个特征函数\\(f_k(y_{i-1},y_i, x,i)\\)来统一表示如下: \\[ f_k(y_{i-1},y_i, x,i)= \\begin{cases} t_k(y_{i-1},y_i, x,i) &amp; {k=1,2,...K_1}\\\\ s_l(y_i, x,i)&amp; {k=K_1+l,\\; l=1,2...,K_2} \\end{cases} \\] 对\\(f_k(y_{i-1},y_i, x,i)\\)在各个序列位置求和得到： \\[ f_k(y,x) = \\sum\\limits_{i=1}^nf_k(y_{i-1},y_i, x,i) \\] 同时我们也统一\\(f_k(y_{i-1},y_i, x,i)\\)对应的权重系数\\(w_k\\)如下： \\[ w_k= \\begin{cases} \\lambda_k &amp; {k=1,2,...K_1}\\\\ \\mu_l &amp; {k=K_1+l,\\; l=1,2...,K_2} \\end{cases} \\] 这样，我们的linear-CRF的参数化形式简化为： \\[ P(y|x) = \\frac{1}{Z(x)}exp\\sum\\limits_{k=1}^Kw_kf_k(y,x) \\] 其中，\\(Z(x)\\)为规范化因子： \\[ Z(x) =\\sum\\limits_{y}exp\\sum\\limits_{k=1}^Kw_kf_k(y,x) \\] 如果将上两式中的\\(w_k\\)与\\(f_k\\)的用向量表示，即: \\[ w=(w_1,w_2,...w_K)^T\\;\\;\\; F(y,x) =(f_1(y,x),f_2(y,x),...f_K(y,x))^T \\] 则linear-CRF的参数化形式简化为内积形式如下： \\[ P_w(y|x) = \\frac{exp(w \\bullet F(y,x))}{Z_w(x)} = \\frac{exp(w \\bullet F(y,x))}{\\sum\\limits_{y}exp(w \\bullet F(y,x))} \\] 线性链条件随机场的矩阵形式 将上一节统一后的linear-CRF公式加以整理，我们还可以将linear-CRF的参数化形式写成矩阵形式。为此我们定义一个\\(m \\times m\\)的矩阵\\(M\\)，\\(m\\)为\\(y\\)所有可能的状态的取值个数。\\(M\\)定义如下： \\[ M_i(x) = \\Big[ M_i(y_{i-1},y_i |x)\\Big] = \\Big[ exp(W_i(y_{i-1},y_i |x))\\Big] = \\Big[ exp(\\sum\\limits_{k=1}^Kw_kf_k(y_{i-1},y_i, x,i))\\Big] \\] 我们引入起点和终点标记\\(y_0 =start, y_{n+1} = stop\\), 这样，标记序列\\(y\\)的规范化概率可以通过\\(n+1\\)个矩阵元素的乘积得到，即： \\[ P_w(y|x) = \\frac{1}{Z_w(x)}\\prod_{i=1}^{n+1}M_i(y_{i-1},y_i |x) \\] 其中\\(Z_w(x)\\)为规范化因子。 以上就是linear-CRF的模型基础，后面我们会讨论linear-CRF和HMM类似的三个问题的求解方法。","link":"/posts/2620909474.html"},{"title":"条件随机场CRF(三) 模型学习与维特比算法解码","text":"在CRF系列的前两篇，我们总结了CRF的模型基础与第一个问题的求解方法，本文我们关注于linear-CRF的第二个问题与第三个问题的求解。第二个问题是模型参数学习的问题，第三个问题是维特比算法解码的问题。 linear-CRF模型参数学习思路 在linear-CRF模型参数学习问题中，我们给定训练数据集\\(X\\)和对应的标记序列\\(Y\\)，\\(K\\)个特征函数\\(f_k(x,y)\\)，需要学习linear-CRF的模型参数\\(w_k\\)和条件概率\\(P_w(y|x)\\)，其中条件概率\\(P_w(y|x)\\)和模型参数\\(w_k\\)满足一下关系： \\[ P_w(y|x) = P(y|x) = \\frac{1}{Z_w(x)}exp\\sum\\limits_{k=1}^Kw_kf_k(x,y) = \\frac{exp\\sum\\limits_{k=1}^Kw_kf_k(x,y)}{\\sum\\limits_{y}exp\\sum\\limits_{k=1}^Kw_kf_k(x,y)} \\] 所以我们的目标就是求出所有的模型参数\\(w_k\\)，这样条件概率\\(P_w(y|x)\\)可以从上式计算出来。 求解这个问题有很多思路，比如梯度下降法，牛顿法，拟牛顿法。同时，这个模型中\\(P_w(y|x)\\)的表达式和最大熵模型原理小结中的模型一样，也可以使用最大熵模型中使用的改进的迭代尺度法(improved iterative scaling, IIS)来求解。 下面我们只简要介绍用梯度下降法的求解思路。 linear-CRF模型参数学习之梯度下降法求解 在使用梯度下降法求解模型参数之前，我们需要定义我们的优化函数，一般极大化条件分布\\(P_w(y|x)\\)的对数似然函数如下： \\[ L(w)= log\\prod_{x,y}P_w(y|x)^{\\overline{P}(x,y)} = \\sum\\limits_{x,y}\\overline{P}(x,y)logP_w(y|x) \\] 其中\\(\\overline{P}(x,y)\\)为经验分布，可以从先验知识和训练集样本中得到,这点和最大熵模型类似。为了使用梯度下降法，我们现在极小化\\(f(w) = -L(P_w)\\)如下： 对\\(w\\)求导可以得到： \\[ \\frac{\\partial f(w)}{\\partial w} = \\sum\\limits_{x,y}\\overline{P}(x)P_w(y|x)f(x,y) - \\sum\\limits_{x,y}\\overline{P}(x,y)f(x,y) \\] 有了\\(w\\)的导数表达书，就可以用梯度下降法来迭代求解最优的\\(w\\)了。注意在迭代过程中，每次更新\\(w\\)后，需要同步更新\\(P_w(x,y)\\),以用于下一次迭代的梯度计算。 梯度下降法的过程这里就不累述了，如果不熟悉梯度下降算法过程建议阅读之前写的梯度下降（Gradient Descent）小结。以上就是linear-CRF模型参数学习之梯度下降法求解思路总结。 linear-CRF模型维特比算法解码思路 现在我们来看linear-CRF的第三个问题：解码。在这个问题中，给定条件随机场的条件概率\\(P(y|x)\\)和一个观测序列\\(x\\),要求出满足\\(P(y|x)\\)最大的序列\\(y\\)。 这个解码算法最常用的还是和HMM解码类似的维特比算法。到目前为止，我已经在三个地方讲到了维特比算法，第一个是文本挖掘的分词原理中用于中文分词，第二个是隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列中用于HMM解码。第三个就是这一篇了。 维特比算法本身是一个动态规划算法，利用了两个局部状态和对应的递推公式，从局部递推到整体，进而得解。对于具体不同的问题，仅仅是这两个局部状态的定义和对应的递推公式不同而已。由于在之前已详述维特比算法，这里就是做一个简略的流程描述。 对于我们linear-CRF中的维特比算法，我们的第一个局部状态定义为\\(\\delta_i(l)\\),表示在位置\\(i\\)标记\\(l\\)各个可能取值(1,2...m)对应的非规范化概率的最大值。之所以用非规范化概率是，规范化因子\\(Z(x)\\)不影响最大值的比较。根据\\(\\delta_i(l)\\)的定义，我们递推在位置\\(i+1\\)标记\\(l\\)的表达式为： \\[ \\delta_{i+1}(l) = \\max_{1 \\leq j \\leq m}\\{\\delta_i(j) + \\sum\\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\\}\\;, l=1,2,...m \\] 和HMM的维特比算法类似，我们需要用另一个局部状态\\(\\Psi_{i+1}(l)\\)来记录使\\(\\delta_{i+1}(l)\\)达到最大的位置\\(i\\)的标记取值,这个值用来最终回溯最优解，\\(\\delta_{i+1}(l)\\)的递推表达式为： \\[ \\Psi_{i+1}(l) = arg\\;\\max_{1 \\leq j \\leq m}\\{\\delta_i(j) + \\sum\\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\\}\\; ,l=1,2,...m \\] linear-CRF模型维特比算法流程 现在我们总结下 linear-CRF模型维特比算法流程： 输入：模型的\\(K\\)个特征函数，和对应的K个权重。观测序列\\(x=(x_1,x_2,...x_n)\\),可能的标记个数\\(m\\) 输出：最优标记序列\\(y^* =(y_1^*,y_2^*,...y_n^*)\\) 初始化： \\[ \\delta_{1}(l) = \\sum\\limits_{k=1}^Kw_kf_k(y_{0} =start,y_{1} = l,x,i)\\}\\;, l=1,2,...m \\] \\[ \\Psi_{1}(l) = start\\;, l=1,2,...m \\] 对于\\(i=1,2...n-1\\),进行递推： \\[ \\delta_{i+1}(l) = \\max_{1 \\leq j \\leq m}\\{\\delta_i(j) + \\sum\\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\\}\\;, l=1,2,...m \\] \\[ \\Psi_{i+1}(l) = arg\\;\\max_{1 \\leq j \\leq m}\\{\\delta_i(j) + \\sum\\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\\}\\; ,l=1,2,...m \\] 终止： \\[ y_n^* = arg\\;\\max_{1 \\leq j \\leq m}\\delta_n(j) \\] 4)回溯： \\[ y_i^* = \\Psi_{i+1}(y_{i+1}^*)\\;, i=n-1,n-2,...1 \\] 最终得到最优标记序列\\(y^* =(y_1^*,y_2^*,...y_n^*)\\) linear-CRF模型维特比算法实例 下面用一个具体的例子来描述 linear-CRF模型维特比算法，例子的模型和CRF系列第一篇中一样，都来源于《统计学习方法》。 假设输入的都是三个词的句子，即\\(X=(X_1,X_2,X_3)\\),输出的词性标记为\\(Y=(Y_1,Y_2,Y_3)\\),其中\\(Y \\in \\{1(名词)，2(动词)\\}\\) 这里只标记出取值为1的特征函数如下： \\[ t_1 =t_1(y_{i-1} = 1, y_i =2,x,i), i =2,3,\\;\\;\\lambda_1=1 \\\\ t_2 =t_2(y_1=1,y_2=1,x,2)\\;\\;\\lambda_2=0.6 \\\\ t_3 =t_3(y_2=2,y_3=1,x,3)\\;\\;\\lambda_3=1 \\\\ t_4 =t_4(y_1=2,y_2=1,x,2)\\;\\;\\lambda_4=1 \\\\ t_5 =t_5(y_2=2,y_3=2,x,3)\\;\\;\\lambda_5=0.2 \\\\ s_1 =s_1(y_1=1,x,1)\\;\\;\\mu_1 =1 \\\\ s_2 =s_2( y_i =2,x,i), i =1,2,\\;\\;\\mu_2=0.5 \\\\ s_3 =s_3( y_i =1,x,i), i =2,3,\\;\\;\\mu_3=0.8 \\\\ s_4 =s_4(y_3=2,x,3)\\;\\;\\mu_4 =0.5 \\] 求标记(1,2,2)的最可能的标记序列。 首先初始化: \\[ \\delta_1(1) = \\mu_1s_1 = 1\\;\\;\\;\\delta_1(2) = \\mu_2s_2 = 0.5\\;\\;\\;\\Psi_{1}(1) =\\Psi_{1}(2) = start \\] 接下来开始递推，先看位置2的： \\[ \\delta_2(1) = max\\{\\delta_1(1) + t_2\\lambda_2+\\mu_3s_3, \\delta_1(2) + t_4\\lambda_4+\\mu_3s_3 \\} = max\\{1+0.6+0.8,0.5+1+0.8\\} =2.4\\;\\;\\;\\Psi_{2}(1) =1 \\] \\[ \\delta_2(2) = max\\{\\delta_1(1) + t_1\\lambda_1+\\mu_2s_2, \\delta_1(2) + \\mu_2s_2\\} = max\\{1+1+0.5,0.5+0.5\\} =2.5\\;\\;\\;\\Psi_{2}(2) =1 \\] 再看位置3的： \\[ \\delta_3(1) = max\\{\\delta_2(1) +\\mu_3s_3, \\delta_2(2) + t_3\\lambda_3+\\mu_3s_3\\} = max\\{2.4+0.8,2.5+1+0.8\\} =4.3 \\] \\[ \\Psi_{3}(1) =2 \\] \\[ \\delta_3(2) = max\\{\\delta_2(1) +t_1\\lambda_1 + \\mu_4s_4, \\delta_2(2) + t_5\\lambda_5+\\mu_4s_4\\} = max\\{2.4+1+0.5,2.5+0.2+0.5\\} =3.9 \\] \\[ \\Psi_{3}(2) =1 \\] 最终得到\\(y_3^* =\\arg\\;max\\{\\delta_3(1), \\delta_3(2)\\}\\),递推回去，得到： \\[ y_2^* = \\Psi_3(1) =2\\;\\;y_1^* = \\Psi_2(2) =1 \\] 即最终的结果为(1,2,1),即标记为(名词，动词，名词)。 linear-CRF vs HMM linear-CRF模型和HMM模型有很多相似之处，尤其是其三个典型问题非常类似，除了模型参数学习的问题求解方法不同以外，概率估计问题和解码问题使用的算法思想基本也是相同的。同时，两者都可以用于序列模型，因此都广泛用于自然语言处理的各个方面。 现在来看看两者的不同点。最大的不同点是linear-CRF模型是判别模型，而HMM是生成模型，即linear-CRF模型要优化求解的是条件概率\\(P(y|x)\\),则HMM要求解的是联合分布\\(P(x,y)\\)。第二，linear-CRF是利用最大熵模型的思路去建立条件概率模型，对于观测序列并没有做马尔科夫假设。而HMM是在对观测序列做了马尔科夫假设的前提下建立联合分布的模型。 最后想说的是，只有linear-CRF模型和HMM模型才是可以比较讨论的。但是linear-CRF是CRF的一个特例，CRF本身是一个可以适用于很复杂条件概率的模型，因此理论上CRF的使用范围要比HMM广泛的多。 以上就是CRF系列的所有内容。","link":"/posts/1796031326.html"},{"title":"条件随机场CRF(二) 前向后向算法评估标记序列概率","text":"在条件随机场CRF(一)中我们总结了CRF的模型，主要是linear-CRF的模型原理。本文就继续讨论linear-CRF需要解决的三个问题：评估，学习和解码。这三个问题和HMM是非常类似的，本文关注于第一个问题：评估。第二个和第三个问题会在下一篇总结。 linear-CRF的三个基本问题 在隐马尔科夫模型HMM中，我们讲到了HMM的三个基本问题，而linear-CRF也有三个类似的的基本问题。不过和HMM不同，在linear-CRF中，我们对于给出的观测序列\\(x\\)是一直作为一个整体看待的，也就是不会拆开看\\((x_1,x_2,...)\\)，因此linear-CRF的问题模型要比HMM简单一些，如果你很熟悉HMM，那么CRF的这三个问题的求解就不难了。 linear-CRF第一个问题是评估，即给定 linear-CRF的条件概率分布\\(P(y|x)\\), 在给定输入序列\\(x\\)和输出序列\\(y\\)时，计算条件概率\\(P(y_i|x)\\)和\\(P(y_{i-1}，y_i|x)\\)以及对应的期望. 本文接下来会详细讨论问题一。 linear-CRF第二个问题是学习，即给定训练数据集\\(X\\)和\\(Y\\)，学习linear-CRF的模型参数\\(w_k\\)和条件概率\\(P_w(y|x)\\)，这个问题的求解比HMM的学习算法简单的多，普通的梯度下降法，拟牛顿法都可以解决。 linear-CRF第三个问题是解码，即给定 linear-CRF的条件概率分布\\(P(y|x)\\),和输入序列\\(x\\), 计算使条件概率最大的输出序列\\(y\\)。类似于HMM，使用维特比算法可以很方便的解决这个问题。 linear-CRF的前向后向概率概述 要计算条件概率\\(P(y_i|x)\\)和\\(P(y_{i-1}，y_i|x)\\)，我们也可以使用和HMM类似的方法，使用前向后向算法来完成。首先我们来看前向概率的计算。 我们定义\\(\\alpha_i(y_i|x)\\)表示序列位置𝑖的标记是\\(y_i\\) 时，在位置\\(i\\)之前的部分标记序列的非规范化概率。之所以是非规范化概率是因为我们不想加入一个不影响结果计算的规范化因子\\(Z(x)\\)在分母里面。 在条件随机场CRF(一)第八节中，我们定义了下式： \\[ M_i(y_{i-1},y_i |x) = exp(\\sum\\limits_{k=1}^Kw_kf_k(y_{i-1},y_i, x,i)) \\] 这个式子定义了在给定\\(y_{i-1}\\)时，从\\(y_{i-1}\\)转移到\\(y_i\\)的非规范化概率。 这样，我们很容易得到序列位置\\(i+1\\)的标记是\\(y_{i+1}\\)时，在位置\\(i+1\\)之前的部分标记序列的非规范化概率\\(\\alpha_{i+1}(y_{i+1}|x)\\)的递推公式： \\[ \\alpha_{i+1}(y_{i+1}|x) = \\alpha_i(y_i|x)M_{i+1}(y_{i+1},y_i|x) \\;\\; i=1,2,...,n+1 \\] 在起点处，我们定义： \\[ \\alpha_0(y_0|x)= \\begin{cases} 1 &amp; {y_0 =start}\\\\ 0 &amp; {else} \\end{cases} \\] 假设我们可能的标记总数是\\(m\\), 则\\(y_i\\)的取值就有\\(m\\)个，我们用\\(\\alpha_i(x)\\)表示这\\(m\\)个值组成的前向向量如下： \\[ \\alpha_i(x) = (\\alpha_i(y_i=1|x), \\alpha_i(y_i=2|x), ... \\alpha_i(y_i=m|x))^T \\] 同时用矩阵\\(M_i(x)\\)表示由$M_i(y_{i-1},y_i |x) \\(形成的\\)m m$阶矩阵： \\[ M_i(x) = \\Big[ M_i(y_{i-1},y_i |x)\\Big] \\] 这样递推公式可以用矩阵乘积表示： \\[ \\alpha_{i+1}^T(x) = \\alpha_i^T(x)M_{i+1}(x) \\] 同样的。我们定义\\(\\beta_i(y_i|x)\\)表示序列位置\\(i\\)的标记是\\(y_i\\)时，在位置\\(i\\)之后的从\\(i+1\\)到\\(n\\)的部分标记序列的非规范化概率。 这样，我们很容易得到序列位置\\(i+1\\)的标记是\\(y_{i+1}\\)时，在位置\\(i\\)之后的部分标记序列的非规范化概率\\(\\beta_{i}(y_{i}|x)\\)的递推公式： \\[ \\beta_{i}(y_{i}|x) = M_{i+1}(y_i,y_{i+1}|x)\\beta_{i+1}(y_{i+1}|x) \\] 在终点处，我们定义： \\[ \\beta_{n+1}(y_{n+1}|x)= \\begin{cases} 1 &amp; {y_{n+1} =stop}\\\\ 0 &amp; {else} \\end{cases} \\] 如果用向量表示，则有： \\[ \\beta_i(x) = M_{i+1}(x)\\beta_{i+1}(x) \\] 由于规范化因子\\(Z(x)\\)的表达式是： \\[ Z(x) = \\sum\\limits_{c=1}^m\\alpha_{n}(y_c|x) = \\sum\\limits_{c=1}^m\\beta_{1}(y_c|x) \\] 也可以用向量来表示\\(Z(x)\\): \\[ Z(x) = \\alpha_{n}^T(x) \\bullet \\mathbf{1} = \\mathbf{1}^T \\bullet \\beta_{1}(x) \\] 其中，\\(\\mathbf{1}\\)是\\(m\\)维全1向量。 linear-CRF的前向后向概率计算 有了前向后向概率的定义和计算方法，我们就很容易计算序列位置\\(i\\)的标记是\\(y_i\\)时的条件概率\\(P(y_i|x)\\): \\[ P(y_i|x) = \\frac{\\alpha_i^T(y_i|x)\\beta_i(y_i|x)}{Z(x)} = \\frac{\\alpha_i^T(y_i|x)\\beta_i(y_i|x)}{ \\alpha_{n}^T(x) \\bullet \\mathbf{1}} \\] 也容易计算序列位置𝑖的标记是\\(y_i\\)，位置\\(i-1\\)的标记是\\(y_{i-1}\\)时的条件概率\\(P(y_{i-1},y_i|x)\\): \\[ P(y_{i-1},y_i|x) = \\frac{\\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\\beta_i(y_i|x)}{Z(x)} = \\frac{\\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\\beta_i(y_i|x)}{ \\alpha_{n}^T(x) \\bullet \\mathbf{1}} \\] linear-CRF的期望计算 有了上一节计算的条件概率，我们也可以很方便的计算联合分布\\(P(x,y)\\)与条件分布\\(P(y|x)\\)的期望。 特征函数\\(f_k(x,y)\\)关于条件分布\\(P(y|x)\\)的期望表达式是： 同样可以计算联合分布\\(P(x,y)\\)的期望： 假设一共有\\(K\\)个特征函数，则\\(k=1,2,...K\\) linear-CRF前向后向算法总结 以上就是linear-CRF的前向后向算法，个人觉得比HMM简单的多，因此大家如果理解了HMM的前向后向算法，这一篇是很容易理解的。 注意到我们上面的非规范化概率\\(M_{i+1}(y_{i+1},y_i|x)\\)起的作用和HMM中的隐藏状态转移概率很像。但是这儿的概率是非规范化的，也就是不强制要求所有的状态的概率和为1。而HMM中的隐藏状态转移概率也规范化的。从这一点看，linear-CRF对序列状态转移的处理要比HMM灵活。","link":"/posts/2481245056.html"},{"title":"梯度提升树(GBDT)原理小结","text":"在集成学习之Adaboost算法原理小结中，我们对Boosting家族的Adaboost算法做了总结，本文就对Boosting家族中另一个重要的算法梯度提升树(Gradient Boosting Decison Tree, 以下简称GBDT)做一个总结。GBDT有很多简称，有GBT（Gradient Boosting Tree）, GTB（Gradient Tree Boosting ）， GBRT（Gradient Boosting Regression Tree）, MART(Multiple Additive Regression Tree)，其实都是指的同一种算法，本文统一简称GBDT。GBDT在BAT大厂中也有广泛的应用，假如要选择3个最重要的机器学习算法的话，个人认为GBDT应该占一席之地。 GBDT概述 GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，我们是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。 在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是\\(f_{t-1}(x)\\), 损失函数是\\(L(y, f_{t-1}(x))\\), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器\\(h_t(x)\\)，让本轮的损失函数\\(L(y, f_{t}(x) =L(y, f_{t-1}(x)+ h_t(x))\\)最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 从上面的例子看这个思想还是蛮简单的，但是有个问题是这个损失的拟合不好度量，损失函数各种各样，怎么找到一种通用的拟合方法呢？ GBDT的负梯度拟合 在上一节中，我们介绍了GBDT的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，大牛Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为 \\[ r_{ti} = -\\bigg[\\frac{\\partial L(y_i, f(x_i)))}{\\partial f(x_i)}\\bigg]_{f(x) = f_{t-1}\\;\\; (x)} \\] 利用\\((x_i,r_{ti})\\;\\; (i=1,2,..m)\\),我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域\\(R_{tj}, j =1,2,..., J\\)。其中J为叶子节点的个数。 针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值\\(c_{tj}\\)如下： \\[ c_{tj} = \\underbrace{arg\\; min}_{c}\\sum\\limits_{x_i \\in R_{tj}} L(y_i,f_{t-1}(x_i) +c) \\] 这样我们就得到了本轮的决策树拟合函数如下： \\[ h_t(x) = \\sum\\limits_{j=1}^{J}c_{tj}I(x \\in R_{tj}) \\] 从而本轮最终得到的强学习器的表达式如下： \\[ f_{t}(x) = f_{t-1}(x) + \\sum\\limits_{j=1}^{J}c_{tj}I(x \\in R_{tj}) \\] 通过损失函数的负梯度来拟合，我们找到了一种通用的拟合损失误差的办法，这样无轮是分类问题还是回归问题，我们通过其损失函数的负梯度的拟合，就可以用GBDT来解决我们的分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 GBDT回归算法 好了，有了上面的思路，下面我们总结下GBDT的回归算法。为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们在下一节讲。 输入是训练集样本\\(T=\\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\\}\\)， 最大迭代次数T, 损失函数L。 输出是强学习器f(x) 初始化弱学习器 \\[ f_0(x) = \\underbrace{arg\\; min}_{c}\\sum\\limits_{i=1}^{m}L(y_i, c) \\] 对迭代轮数t=1,2,...T有： a)对样本i=1,2，...m，计算负梯度 \\[ r_{ti} = -\\bigg[\\frac{\\partial L(y_i, f(x_i)))}{\\partial f(x_i)}\\bigg]_{f(x) = f_{t-1}\\;\\; (x)} \\] b)利用\\((x_i,r_{ti})\\;\\; (i=1,2,..m)\\), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为\\(R_{tj}, j =1,2,..., J\\)。其中J为回归树t的叶子节点的个数。 对叶子区域j =1,2,..J,计算最佳拟合值 \\[ c_{tj} = \\underbrace{arg\\; min}_{c}\\sum\\limits_{x_i \\in R_{tj}} L(y_i,f_{t-1}(x_i) +c) \\] 更新强学习器 \\[ f_{t}(x) = f_{t-1}(x) + \\sum\\limits_{j=1}^{J}c_{tj}I(x \\in R_{tj}) \\] 得到强学习器f(x)的表达式 \\[ f(x) = f_T(x) =f_0(x) + \\sum\\limits_{t=1}^{T}\\sum\\limits_{j=1}^{J}c_{tj}I(x \\in R_{tj}) \\] ## GBDT分类算法 这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。 为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。 二元GBDT分类算法 对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为： \\[ L(y, f(x)) = log(1+ exp(-yf(x))) \\] 其中\\(y \\in\\{-1, +1\\}\\)。则此时的负梯度误差为 \\[ r_{ti} = -\\bigg[\\frac{\\partial L(y, f(x_i)))}{\\partial f(x_i)}\\bigg]_{f(x) = f_{t-1}\\;\\; (x)} = y_i/(1+exp(y_if(x_i))) \\] 对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为 \\[ c_{tj} = \\underbrace{arg\\; min}_{c}\\sum\\limits_{x_i \\in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c))) \\] 由于上式比较难优化，我们一般使用近似值代替 \\[ c_{tj} = \\sum\\limits_{x_i \\in R_{tj}}r_{ti}\\bigg / \\sum\\limits_{x_i \\in R_{tj}}|r_{ti}|(1-|r_{ti}|) \\] 除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。 多元GBDT分类算法 多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： \\[ L(y, f(x)) = - \\sum\\limits_{k=1}^{K}y_klog\\;p_k(x) \\] 其中如果样本输出类别为k，则\\(y_k=1\\)。第k类的概率$p_k(x) $的表达式为： \\[ p_k(x) = exp(f_k(x)) \\bigg / \\sum\\limits_{l=1}^{K} exp(f_l(x)) \\] 集合上两式，我们可以计算出第\\(t\\)轮的第\\(i\\)个样本对应类别𝑙的负梯度误差为 \\[ r_{til} = -\\bigg[\\frac{\\partial L(y_i, f(x_i)))}{\\partial f(x_i)}\\bigg]_{f_k(x) = f_{l, t-1}\\;\\; (x)} = y_{il} - p_{l, t-1}(x_i) \\] 观察上式可以看出，其实这里的误差就是样本𝑖对应类别\\(l\\)的真实概率和\\(t-1\\)轮预测概率的差值。 对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为 \\[ c_{tjl} = \\underbrace{arg\\; min}_{c_{jl}}\\sum\\limits_{i=0}^{m}\\sum\\limits_{k=1}^{K} L(y_k, f_{t-1, l}(x) + \\sum\\limits_{j=0}^{J}c_{jl} I(x_i \\in R_{tjl})) \\] 由于上式比较难优化，我们一般使用近似值代替 \\[ c_{tjl} = \\frac{K-1}{K} \\; \\frac{\\sum\\limits_{x_i \\in R_{tjl}}r_{til}}{\\sum\\limits_{x_i \\in R_{til}}|r_{til}|(1-|r_{til}|)} \\] 除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。 GBDT常用损失函数 这里我们再对常用的GBDT损失函数做一个总结。 对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种: 如果是指数损失函数，则损失函数表达式为 \\[ L(y, f(x)) = exp(-yf(x)) \\] 其负梯度计算和叶子节点的最佳负梯度拟合参见Adaboost原理篇。 如果是对数损失函数，分为二元分类和多元分类两种，参见4.1节和4.2节。 对于回归算法，常用损失函数有如下4种: a)均方差，这个是最常见的回归损失函数了 \\[ L(y, f(x)) =(y-f(x))^2 \\] b)绝对损失，这个损失函数也很常见 \\[ L(y, f(x)) =|y-f(x)| \\] 对应负梯度误差为： \\[ sign(y_i-f(x_i)) \\] c)Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下： \\[ L(y, f(x))= \\begin{cases} \\frac{1}{2}(y-f(x))^2&amp; {|y-f(x)| \\leq \\delta}\\\\ \\delta(|y-f(x)| - \\frac{\\delta}{2})&amp; {|y-f(x)| &gt; \\delta} \\end{cases} \\] 对应的负梯度误差为： \\[ r(y_i, f(x_i))= \\begin{cases} y_i-f(x_i)&amp; {|y_i-f(x_i)| \\leq \\delta}\\\\ \\delta sign(y_i-f(x_i))&amp; {|y_i-f(x_i)| &gt; \\delta} \\end{cases} \\] 分位数损失。它对应的是分位数回归的损失函数，表达式为 \\[ L(y, f(x)) =\\sum\\limits_{y \\geq f(x)}\\theta|y - f(x)| + \\sum\\limits_{y &lt; f(x)}(1-\\theta)|y - f(x)| \\] 其中\\(\\theta\\)为分位数，需要我们在回归前指定。对应的负梯度误差为： \\[ r(y_i, f(x_i))= \\begin{cases} \\theta&amp; { y_i \\geq f(x_i)}\\\\ \\theta - 1 &amp; {y_i &lt; f(x_i) } \\end{cases} \\] 对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。 GBDT的正则化 和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。 第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为\\(v\\),对于前面的弱学习器的迭代 \\[ f_{k}(x) = f_{k-1}(x) + h_k(x) \\] 如果我们加上了正则化项，则有 \\[ f_{k}(x) = f_{k-1}(x) + \\nu h_k(x) \\] \\(\\nu\\)的取值范围为\\(0 &lt; \\nu \\leq 1\\)。对于同样的训练集学习效果，较小的\\(\\nu\\)意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。 使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。 第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了。 GBDT小结 GBDT终于讲完了，GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。 最后总结下GBDT的优缺点。 GBDT主要的优点有： 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。 3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 GBDT的主要缺点有： 1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 以上就是GBDT的原理总结，后面会讲GBDT的scikit-learn调参，敬请期待。","link":"/posts/1772150868.html"},{"title":"用函数还是用复杂的表达式","text":"要不要使用复杂表达式 Perl语言的原作者Larry Wall曾经说过，伟大的程序员都有三个优点：懒惰、暴躁和自负。乍一看这三个词语没有一个是褒义词，但在程序员的世界里，这三个词有不同的意义。首先，懒惰会促使程序员去写一些省事儿的程序来辅助自己或别人更好的完成工作，这样我们就无需做那些重复和繁琐的劳动；同理能够用3行代码解决的事情，我们也绝不会写出10行代码来。其次，暴躁会让程序员主动的去完成一些你还没有提出的工作，去优化自己的代码让它更有效率，能够3秒钟完成的任务，我们绝不能容忍1分钟的等待。最后，自负会促使程序员写出可靠无误的代码，我们写代码不是为了接受批评和指责，而是为了让其他人来膜拜。 那么接下来就有一个很有意思的问题值得探讨一下，我们需要一个程序从输入的三个数中找出最大的那个数。这个程序对任何会编程的人来说都是小菜一碟，甚至不会编程的人经过10分钟的学习也能搞定。下面是用来解决这个问题的Python代码。 12345678910a = int(input('a = '))b = int(input('b = '))c = int(input('c = '))if a &gt; b: the_max = aelse: the_max = bif c &gt; the_max: the_max = cprint('The max is:', the_max) 但是我们刚才说了，程序员都是懒惰的，很多程序员都会使用三元条件运算符来改写上面的代码。 123456a = int(input('a = '))b = int(input('b = '))c = int(input('c = '))the_max = a if a &gt; b else bthe_max = c if c &gt; the_max else the_maxprint('The max is:', the_max) 需要说明的是，Python在2.5版本以前是没有上面代码第4行和第5行中使用的三元条件运算符的，究其原因是Guido van Rossum（Python之父）认为三元条件运算符并不能帮助 Python变得更加简洁，于是那些习惯了在C/C++或Java中使用三元条件运算符（在这些语言中，三元条件运算符也称为“Elvis运算符”，因为?:放在一起很像著名摇滚歌手猫王Elvis的大背头）的程序员试着用and和or运算符的短路特性来模拟出三元操作符，于是在那个年代，上面的代码是这样写的。 123456a = int(input('a = '))b = int(input('b = '))c = int(input('c = '))the_max = a &gt; b and a or bthe_max = c &gt; the_max and c or the_maxprint('The max is:', the_max) 但是这种做法在某些场景下是不能成立的，且看下面的代码。 123456a = 0b = -100# 下面的代码本来预期输出a的值，结果却得到了b的值# 因为a的值0在进行逻辑运算时会被视为False来处理print(True and a or b)# print(a if True else b) 所以在Python 2.5以后引入了三元条件运算符来避免上面的风险（上面代码被注释掉的最后一句话）。那么，问题又来了，上面的代码还可以写得更简短吗？答案是肯定的。 1234a = int(input('a = '))b = int(input('b = '))c = int(input('c = '))print('The max is:', (a if a &gt; b else b) if (a if a &gt; b else b) &gt; c else c) 但是，这样做真的好吗？如此复杂的表达式是不是让代码变得晦涩了很多呢？我们发现，在实际开发中很多开发者都喜欢过度的使用某种语言的特性或语法糖，于是简单的多行代码变成了复杂的单行表达式，这样做真的好吗？这个问题我也不止一次的问过自己，现在我能给出的答案是下面的代码，使用辅助函数。 12345678def the_max(x, y): return x if x &gt; y else ya = int(input('a = '))b = int(input('b = '))c = int(input('c = '))print('The max is:', the_max(the_max(a, b), c)) 上面的代码中，我定义了一个辅助函数the_max用来找出参数传入的两个值中较大的那一个，于是下面的输出语句可以通过两次调用the_max函数来找出三个数中的最大值，现在代码的可读性是不是好了很多。用辅助函数来替代复杂的表达式真的是一个不错的选择，关键是比较大小的逻辑转移到这个辅助函数后不仅可以反复调用它，而且还可以进行级联操作。 当然，很多语言中比较大小的函数根本没有必要自己来实现（通常都是内置函数），Python也是如此。Python内置的max函数利用了Python对可变参数的支持，允许一次性传入多个值或者一个迭代器并找出那个最大值，所以上面讨论的问题在Python中也就是一句话的事，但是从复杂表达式到使用辅助函数简化复杂表达式这个思想是非常值得玩味的，所以分享出来跟大家做一个交流。 1234a = int(input('a = '))b = int(input('b = '))c = int(input('c = '))print('The max is:', max(a, b, c))","link":"/posts/962273426.html"},{"title":"第一篇 监督学习","text":"统计学习或机器学习一般包括监督学习、无监督学习、强化学习，有时还包括半监督学习、主动学习 监督学习 监督学习指从标注数据中学习预测模型的机器学习问题，其本质是学习输入到输出的映射的统计规律。 输入变量\\(X\\)和输出变量\\(Y\\)有不同的类型，可以是连续或是离散的。根据输入输出变量的不同类型，对预测任务给予不同的名称：输入与输出均为连续变量的预测问题称为回归问题；输出变量为有限个离散变量的预测问题称为分类问题；输入与输出变量均为变量序列的预测问题称为标注问题。 无监督学习 无监督学习指从无标注数据中学习预测模型的机器学习问题，其本质是学习数据中的统计规律或内在结构。 无监督学习旨在从假设空间中选出在给定评价标准下的最优模型，模型可以实现对数据的聚类、降维或是概率估计。 强化学习 强化学习指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题，其本质是学习最优的序贯决策。 智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，系统不断地试错，以达到学习最优策略地目的。 半监督学习与主动学习 半监督学习指利用标注数据和未标注数据学习预测模型地机器学习问题。其旨在利用未标注数据中的信息，辅助标注数据，进行监督学习，以较低的成本达到较好的学习效果。 主动学习是指机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。主动学习的目标是找出对学习最有帮助的实例让教师标注，以较小的标注代价达到较好的学习效果。 这两种学习更接近监督学习。 实现统计学习方法的步骤 得到一个有限的训练数据集合 确定包含所有可能的模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，即学习的算法 通过学习方法选择最优的模型 利用学习的最优模型对新数据进行预测或分析 在上述步骤中涵盖了统计学习方法三要素：模型，策略，算法 在监督学习过程中，模型就是所要学习的条件概率分布或者决策函数。 注意书中的这部分描述，整理了一下到表格里： 假设空间\\(\\mathcal F\\) 输入空间\\(\\mathcal X\\) 输出空间\\(\\mathcal Y\\) 参数空间 决策函数 \\(\\mathcal F =\\{f\\)|\\(Y=f_{\\theta}(x), \\theta \\in \\bf R \\it ^n\\}\\) 变量 变量 \\(\\bf R\\it ^n\\) 条件概率分布 \\(\\mathcal F =\\{P\\)|\\(P_\\theta(Y\\)|\\(X), \\theta \\in \\bf R \\it ^n\\}\\) 随机变量 随机变量 \\(\\bf R\\it ^n\\) 损失函数与风险函数 损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。 损失函数(loss function)或代价函数(cost function)定义为给定输入\\(X\\)的预测值\\(f(X)\\)和真实值\\(Y\\)之间的非负实值函数，记作\\(L(Y,f(X))\\)。 风险函数(risk function)或期望损失(expected loss)和模型的泛化误差的形式是一样的 \\(R_{exp}(f)=E_p[L(Y, f(X))]=\\int_{\\mathcal X\\times\\mathcal Y}L(y,f(x))P(x,y)\\, {\\rm d}x{\\rm d}y\\) 上式是模型\\(f(X)\\)关于联合分布\\(P(X,Y)\\)的平均意义下的损失(期望损失)，但是因为\\(P(X,Y)\\)是未知的，所以前面的用词是期望，以及平均意义下的。这个表示其实就是损失的均值，反映了对整个数据的预测效果的好坏。 经验风险(empirical risk)或经验损失(empirical loss) \\(R_{emp}(f)=\\frac{1}{N}\\sum^{N}_{i=1}L(y_i,f(x_i))\\) 上式是模型\\(f\\)关于训练样本集的平均损失。根据大数定律，当样本容量N趋于无穷大时，经验风险趋于期望风险。 结构风险(structural risk) \\(R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,f(x_i))+\\lambda J(f)\\) 其中\\(J(f)\\)为模型复杂度, \\(\\lambda \\geqslant 0\\)是系数，用以权衡经验风险和模型复杂度。 常用损失函数 损失函数数值越小，模型就越好 0-1损失 \\(L(Y,f(X))=\\begin{cases}1, Y \\neq f(X) \\\\0, Y=f(X) \\end{cases}\\) 平方损失 \\(L(Y,f(X))=(Y-f(X))^2\\) 绝对损失 \\(L(Y,f(X))=|Y-f(X)|\\) 对数损失 \\(L(Y,P(Y|X))=−logP(Y|X)\\) ERM与SRM 经验风险最小化(ERM)与结构风险最小化(SRM) 极大似然估计是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化等价于极大似然估计，下面习题1.2中给出了证明。 结构风险最小化等价于正则化 贝叶斯估计中的最大后验概率估计是结构风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化等价于最大后验概率估计。 算法 算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么杨的计算方法来求解最优模型。 模型评估与选择 训练误差和测试误差是模型关于数据集的平均损失。 注意：统计学习方法具体采用的损失函数未必是评估时使用的损失函数。 过拟合是指学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。可以说模型选择旨在避免过拟合并提高模型的预测能力。 正则化与交叉验证 模型选择的典型方法是正则化.。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化就越大。比如，正则化项可以是模型参数向量的范数。 \\(L(w)=\\frac{1}{N}\\sum_{i=1}^{N}(f(x_i;w)-y_i)^2+\\frac{\\lambda}{2}\\|w\\|^2\\) \\(\\|w\\|\\)表示向量\\(w\\)的\\(L_2\\)范数 正则化符合奥卡姆剃刀原理：如无必要，勿增实体。在应用于模型选择中时，可以理解为：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单的才是最好的模型，也是应该选择的模型。 交叉验证的基本想法时重复地利用数据；把给定地数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择。 主要有简单交叉验证，S折交叉验证，留一交叉验证三种。 在算法学习的过程中，测试集可能是固定的，但是验证集和训练集可能是变化的。比如S折交叉验证的情况下，分成S折之后，其中的S-1折作为训练集，1折作为验证集，计算这S个模型每个模型的平均测试误差，最后选择平均测试误差最小的模型。这个过程中用来验证模型效果的那一折数据就是验证集。 生成模型与判别模型 监督学习方法可分为生成方法(generative approach)与判别方法(discriminative approach) 生成方法(generative approach) 可以还原出联合概率分布\\(P(X,Y)\\) 收敛速度快, 当样本容量增加时, 学到的模型可以更快收敛到真实模型 当存在隐变量时仍可以用 判别方法(discriminative approach) 直接学习条件概率\\(P(Y|X)\\)或者决策函数\\(f(X)\\) 直接面对预测, 往往学习准确率更高 可以对数据进行各种程度的抽象, 定义特征并使用特征, 可以简化学习问题 习题解答 1.1 说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学方法三要素 伯努利模型是定义在取值为0与1的随机变量上的概率分布。统计学分为两派：经典统计学派和贝叶斯统计学派。两者的不同主要是，经典统计学派认为模型已定，参数未知，参数是固定的，只是还不知道；贝叶斯统计学派是通过观察到的现象对概率分布中的主观认定不断进行修正。 极大似然估计用的是经典统计学派的策略，贝叶斯估计用的是贝叶斯统计学派的策略；为了得到使经验风险最小的参数值，使用的算法都是对经验风险求导，使导数为0。 定义随机变量\\(A\\)为一次伯努利试验的结果，\\(A\\)的取值为\\(\\{0,1\\}\\)，概率分布为\\(P(A)\\)： \\[P(A=1)=θ，P(A=0)=1-θ\\] * 极大似然估计 \\[L(θ)=\\prod_{i=1}^nP(A_i)=θ^k(1-θ)^{n-k}\\] \\[θ=\\arg\\max_{θ}L(θ)=\\frac{k}{n}\\] 上述估计通过取对数求导得到，\\(A_i\\)为第\\(i\\)次随机试验 贝叶斯估计 \\[P(θ|A_1,A_2,…，A_n)=\\frac{P(A_1,A_2,…，A_n|θ)P(θ)}{P(A_1,A_2,…，A_n)}\\] 根据观察到的结果修正\\(θ\\)，也就是假设\\(θ\\)是随机变量，\\(θ\\)服从β分布，有很多个可能的取值，我们要取的值是在已知观察结果的条件下使\\(θ\\)出现概率最大的值。上式分母是不变的，求分子最大就可以。 \\[ \\begin{aligned} \\theta &amp;=arg\\max \\limits_\\theta {P(A_1,A_2,...,A_n|\\theta)P(\\theta)} \\\\ &amp;= arg\\max \\limits_\\theta {\\prod_{i=1}^{n}P(A_i|\\theta)P(\\theta)} \\\\ &amp;=arg \\max \\limits_\\theta {\\theta^k(1-\\theta)^{n-k}\\theta^{a-1}(1-\\theta)^{b-1}} \\\\ &amp;=\\frac{k+(a-1)}{n+(a-1)+(b-1)} \\end{aligned}\\] β分布是一个作为伯努利分布和二项式分布的共轭先验分布的密度函数，是指一组定义在\\((0,1)\\)区间的连续概率分布，有两个参数α，β&gt;0。选定参数后就可以确定\\(\\theta\\)。 统计学习方法的三要素为模型，策略，算法。 在这里插入图片描述 1.2 通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。 模型是条件概率分布：\\(P_θ(Y|X)\\)， 损失函数是对数损失函数：\\(L(Y,P_θ(Y|X))=−logP_θ(Y|X)\\) 经验风险为：\\[ \\begin{aligned} R_{emp}(f)&amp;=\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,f(x_i)) \\\\ &amp;=\\frac{1}{N}\\sum_{i=1}^{N}-logP(y_i|x_i) \\\\ &amp;=-\\frac{1}{N}\\sum_{i=1}^{N}logP(y_i|x_i) \\end{aligned}\\] 极大似然估计的似然函数为： \\[L(\\theta)=\\prod_DP_{\\theta}(Y|X)\\] 取对数 \\[log(L(\\theta))=\\sum_DlogP_{\\theta}(Y|X)\\] \\[arg\\max_\\theta\\sum_DlogP_{\\theta}(Y|X)=arg\\min_{\\theta}\\sum_D-logP_{\\theta}(Y|X)\\] 因此，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。","link":"/posts/605.html"},{"title":"自动群发邮件","text":"利用Python实现自动发送邮件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import smtplibimport timeimport xlrdfrom email import encodersfrom email.header import Headerfrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextfrom email.utils import parseaddr,formataddr from email.mime.application import MIMEApplicationfrom datetime import date,datetimesender = '###@qq.com'receiver = '2956150544@qq.com'def read_excel():#打开Excel表格 #读取群发邮箱，将需要群发的邮箱复制到test的excel列表清单中 ExceFileUrl=xlrd.open_workbook(r'../data/email_list.xlsx') # print(ExceFileUrl.sheet_names()) #获取第一个sheet页面 sheet_date = ExceFileUrl.sheet_by_index(0) # print(sheet_date.name,sheet_date.nrows,sheet_date.ncols) #获取第2列的邮箱列表 email_data = sheet_date.col_values(1) # print(email_data) return email_datadef sender_mail(): #创建对象 smt_p = smtplib.SMTP() #设置smtp服务器 smt_p.connect(host='smtp.qq.com', port=25) #在qq邮箱设置开启SMTP服务并复制授权码 password=&quot;###&quot; #进行邮箱登录一次，填写你本人的邮箱 smt_p.login(sender,password) count_num = 1 #使用for循环来进行发邮件 for i in read_excel(): #表格中邮箱格式不正确，如有空字符，在发邮件的时候会出现异常报错，捕获到这些异常就跳过 try: #邮件设置 msg = MIMEMultipart() msg['From'] = &quot;brook&quot; #收件人 msg['To'] = i msg['Cc'] = 'CQ3754@126.com' #主题名称 msg['subject'] = Header('通知', 'utf-8') #附件 —附加发送excel、word、图片格式，新建文件夹，将以下路径及文件名称替换即可。 msg.attach(MIMEText('您好,' 'XXX2.0全新升级，XXX1.0版本到2018年10月31号停止所有服务。','plain', 'utf-8')) xlsxpart=MIMEApplication(open(r'../data/email_test.xlsx','rb').read()) xlsxpart.add_header('Content-Disposition','attachment',filename='1.xlsx') msg.attach(xlsxpart) message_docx1 = MIMEText(open(r'../data/email_test.docx', 'rb').read(), 'base64', 'utf8') message_docx1.add_header('crontent-disposition', 'attachment', filename='测试.docx') msg.attach(message_docx1) message_image = MIMEText(open(r'../data/email_test.jpg', 'rb').read(), 'base64', 'utf8') message_image.add_header('content-disposition', 'attachment', filename='plot2.jpg') msg.attach(message_image) #发送邮件 smt_p.sendmail(sender,i,msg.as_string()) #sleep10秒避免发送频率过快，可能被判定垃圾邮件。 time.sleep(10) print('第%d次发送给%s' % (count_num,i)) count_num = count_num + 1 except (UnicodeEncodeError,smtplib.SMTPRecipientsRefused,smtplib.SMTPSenderRefused,AttributeError) as e: #打印出来发送第几次的时候，邮箱出问题，一个邮箱最多不超过300个发送对象 print('第%d次给%s发送邮件异常' % (count_num,i)) continue smt_p.quit()sender_mail()","link":"/posts/3316626072.html"},{"title":"递归","text":"Recursion is a process for solving problems by subdividing a larger problem into smaller cases of the problem itself and then solving the smaller, more trivial parts. 递归是计算机科学里出现非常多的一个概念，有时候用递归解决问题看起来非常简单优雅。 之前讲过的数据结构中我们并没有使用递归，因为递归涉及到调用栈，可能会让初学者搞晕。这一章我们开始介绍递归， 后边讲到树和一些排序算法的时候我们还会碰到它。我非常推荐你先看看《算法图解》第三章 递归， 举的例子比较浅显易懂。 什么是递归？ 递归用一种通俗的话来说就是自己调用自己，但是需要分解它的参数，让它解决一个更小一点的问题，当问题小到一定规模的时候，需要一个递归出口返回。 这里举一个和其他很多老套的教科书一样喜欢举的例子，阶乘函数，我觉得用来它演示再直观不过。它的定义是这样的： 我们很容易根据它的定义写出这样一个递归函数，因为它本身就是递归定义的。 12345def fact(n): if n == 0: return 1 else: return n * fact(n-1) 看吧，几乎完全是按照定义来写的。我们来看下递归函数的几个特点: 递归必须包含一个基本的出口(base case)，否则就会无限递归，最终导致栈溢出。比如这里就是 n == 0 返回 1 递归必须包含一个可以分解的问题(recursive case)。 要想求得 fact(n)，就需要用 n * fact(n-1) 递归必须必须要向着递归出口靠近(toward the base case)。 这里每次递归调用都会 n-1，向着递归出口 n == 0 靠近 调用栈 看了上一个例子你可能觉得递归好简单，先别着急，我们再举个简单的例子，上边我们并没有讲递归如何工作的。 假如让你输出从 1 到 10 这十个数字，如果你是个正常人的话，我想你的第一反应都是这么写： 1234567def print_num(n): for i in range(1, n + 1): # 注意很多编程语言使用的都是 从 0 开始的左闭右开区间, python 也不例外 print(i)if __name__ == '__main__': print_num(10) 我们尝试写一个递归版本，不就是自己调用自己嘛： 1234def print_num_recursive(n): if n &gt; 0: print_num_recursive(n-1) print(n) 你猜下它的输出？然后我们调换下 print 顺序，你再猜下它的输出 1234def print_num_recursive_revserve(n): if n &gt; 0: print(n) print_num_recursive_revserve(n-1) 你能明白是为什么吗？我建议你运行下这几个小例子，它们很简单但是却能说明问题。 计算机内部使用调用栈来实现递归，这里的栈一方面指的是内存中的栈区，一方面栈又是之前讲到的后进先出这种数据结构。 每当进入递归函数的时候，系统都会为当前函数开辟内存保存当前变量值等信息，每个调用栈之间的数据互不影响，新调用的函数 入栈的时候会放在栈顶。视频里我们会画图来演示这个过程。 递归只用大脑不用纸笔模拟的话很容易晕，因为明明是同一个变量名字，但是在不同的调用栈里它是不同的值，所以我建议 你最好手动画画这个过程。 用栈模拟递归 刚才说到了调用栈，我们就用栈来模拟一把。之前栈这一章我们讲了如何自己实现栈，不过这里为了不拷贝太多代码，我们直接用 collections.deque 就可以 快速实现一个简单的栈。 12345678910111213141516171819202122232425from collections import dequeclass Stack(object): def __init__(self): self._deque = deque() def push(self, value): return self._deque.append(value) def pop(self): return self._deque.pop() def is_empty(self): return len(self._deque) == 0def print_num_use_stack(n): s = Stack() while n &gt; 0: # 不断将参数入栈 s.push(n) n -= 1 while not s.is_empty(): # 参数弹出 print(s.pop()) 这里结果也是输出 1 到 10，只不过我们是手动模拟了入栈和出栈的过程，帮助你理解计算机是如何实现递归的，是不是挺简单！现在你能明白为什么上边 print_num_recursive print_num_recursive_revserve 两个函数输出的区别了吗？ 尾递归 上边的代码示例(麻雀虽小五脏俱全)中实际上包含了两种形式的递归，一种是普通的递归，还有一种叫做尾递归： 12345678910def print_num_recursive(n): if n &gt; 0: print_num_recursive(n-1) print(n)def print_num_recursive_revserve(n): if n &gt; 0: print(n) print_num_recursive_revserve(n-1) # 尾递归 概念上它很简单，就是递归调用放在了函数的最后。有什么用呢？ 普通的递归, 每一级递归都产生了新的局部变量, 必须创建新的调用栈, 随着递归深度的增加, 创建的栈越来越多, 造成爆栈。虽然尾递归调用也会创建新的栈, 但是我们可以优化使得尾递归的每一级调用共用一个栈!, 如此便可解决爆栈和递归深度限制的问题! 不幸的是 python 默认不支持尾递归优化（见延伸阅读），不过一般尾递归我们可以用一个迭代来优化它。 汉诺塔问题 有三根杆子A，B，C。A杆上有N个(N&gt;1)穿孔圆盘，盘的尺寸由下到上依次变小。要求按下列规则将所有圆盘移至C杆： 但是有两个条件： 每次只能移动一个圆盘； 大盘不能叠在小盘上面。 最早发明这个问题的人是法国数学家爱德华·卢卡斯。 传说越南河内某间寺院有三根银棒，上串64个金盘。寺院里的僧侣依照一个古老的预言，以上述规则移动这些盘子；预言说当这些盘子移动完毕，世界就会灭亡。 这个传说叫做梵天寺之塔问题（Tower of Brahma puzzle）。但不知道是卢卡斯自创的这个传说，还是他受他人启发。 五个盘子的汉诺塔问题 理解这个问题需要我们一些思维上的转换，因为我们正常的思维可能都是从上边最小的盘子开始移动，但是这里我们从移动最底下的盘子开始思考。 假设我们已经知道了如何移动上边的四个盘子到 B(pole2)，现在把最大的盘子从 A -&gt; C 就很简单了。当把最大的盘子移动到 C 之后，只需要把 B 上的 4 个盘子从 B -&gt; C 就行。（这里的 pole1, 2, 3 分别就是 A, B, C 杆） 问题是仍要想办法如何移动上边的 4 个盘子，我们可以同样的方式来移动上边的 4 个盘子，这就是一种递归的解法。 给定 n 个盘子和三个杆分别是 源杆(Source), 目标杆(Destination)，和中介杆(Intermediate)，我们可以定义如下递归操作： 把上边的 n-1 个盘子从 S 移动到 I，借助 D 杆 把最底下的盘子从 S 移动到 D 把 n-1 个盘子从 I 移动到 D，借助 S 我们把它转换成代码： 1234567891011121314151617def hanoi_move(n, source, dest, intermediate): if n &gt;= 1: # 递归出口，只剩一个盘子 hanoi_move(n-1, source, intermediate, dest) print(&quot;Move %s -&gt; %s&quot; % (source, dest)) hanoi_move(n-1, intermediate, dest, source)hanoi_move(3, 'A', 'C', 'B')# 输出，建议你手动模拟下。三个盘子 A(Source), B(intermediate), C(Destination)&quot;&quot;&quot;Move A -&gt; CMove A -&gt; BMove C -&gt; BMove A -&gt; CMove B -&gt; AMove B -&gt; CMove A -&gt; C&quot;&quot;&quot; 是不是很神奇，但是老实说这个过程仅凭大脑空想是比较难以想象出来的。人的大脑『栈』深度很有限，因为你甚至都没法同时记住超过 8 个以上的 无意义数字，所以用大脑模拟不如用纸笔来模拟下。（不排除有些聪明的同学能迅速在脑瓜里完成这个过程） 汉诺塔代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# -*- coding: utf-8 -*-def fact(n): if n == 0: return 1 else: return n * fact(n - 1)def print_num(n): for i in range(1, n + 1): # 注意很多编程语言使用的都是 从 0 开始的左闭右开区间, python 也不例外 print(i)def print_num_recursive(n): if n &gt; 0: print_num_recursive(n - 1) print(n)def print_num_recursive_revserve(n): if n &gt; 0: print(n) print_num_recursive_revserve(n - 1)from collections import dequeclass Stack(object): def __init__(self): self._deque = deque() def push(self, value): return self._deque.append(value) def pop(self): return self._deque.pop() def is_empty(self): return len(self._deque) == 0def print_num_use_stack(n): s = Stack() while n &gt; 0: # 不断将参数入栈 s.push(n) n -= 1 while not s.is_empty(): # 参数弹出 print(s.pop())def hanoi_move(n, source, dest, intermediate): if n &gt;= 1: # 递归出口，只剩一个盘子 hanoi_move(n - 1, source, intermediate, dest) print(&quot;Move %s -&gt; %s&quot; % (source, dest)) hanoi_move(n - 1, intermediate, dest, source)def flatten(rec_list): for i in rec_list: if isinstance(i, list): for i in flatten(i): yield i else: yield idef test_flatten(): assert list(flatten([[[1], 2, 3], [1, 2, 3]])) == [1, 2, 3, 1, 2, 3] 延伸阅读 递归是个非常重要的概念，我们后边的数据结构和算法中还会多次碰到它，我建议你多阅读一些资料加深理解： 《算法图解》第三章 递归 《Data Structures and Algorithms in Python》 第 10 章 Recursion 《Python开启尾递归优化!》 尾调用优化 汉诺塔 思考题 你能举出其他一些使用到递归的例子吗？ 实现一个 flatten 函数，把嵌套的列表扁平化，你需要用递归函数来实现。比如 [[1,2], [1,2,3] -&gt; [1,2,1,2,3] 使用递归和循环各有什么优缺点，你能想到吗？怎么把一个尾递归用迭代替换？ 递归有时候虽然很优雅直观，但是时间复杂度却不理想，比如斐波那契数列，它的表达式是 F(n) = F(n-1) + F(n-2)，你能计算它的时间复杂度吗？请你画个树来表示它的计算过程，为什么这个时间复杂度很不理想？我们怎样去优化它。 python 内置的 dict 只能用 dict['key'] 的形式访问比较麻烦，我们想用 dict.key 的形式访问。tornado web 框架中提供了一个 ObjectDict，请你实现一个递归函数接收一个字典，并返回一个可以嵌套访问的 ObjectDict","link":"/posts/3328524931.html"},{"title":"逻辑斯谛回归与最大熵模型","text":"logistic regression是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，推广到分类问题得到最大熵模型(maxium entropy model) 这两种模型都属于对数线性模型 逻辑斯谛回归模型 二项逻辑斯谛回归模型是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的逻辑斯谛分布。 分类问题，可以表示成one-hot的形式，而one-hot可以认为是一种确定概率的表达。而最大熵模型，是一种不确定的概率表达，其中这个概率，是一个条件概率，是构建的特征函数生成的概率。 逻辑斯谛分布 \\(X\\)是连续随机变量，\\(X\\)服从逻辑斯谛分布 \\[ F(x)=P(X\\leqslant x)=\\frac{1}{1+\\exp(-(x-\\mu)/\\gamma)} \\] 关于逻辑斯谛， 更常见的一种表达是Logistic function \\[ \\sigma(z)=\\frac{1}{1+\\exp(-z)} \\] 这个函数把实数域映射到(0, 1)区间，这个范围正好是概率的范围， 而且可导，对于0输入， 得到的是0.5，可以用来表示等可能性。 二项逻辑斯谛回归模型 二项逻辑斯谛回归模型是如下的条件概率分布：(这里的\\(w\\)是对扩充的权值向量，包含参数\\(b\\)) \\[ \\begin{aligned} P(Y=1|x)&amp;=\\frac{\\exp(w\\cdot x)}{1+\\exp(w\\cdot x)}\\\\ &amp;=\\frac{\\exp(w\\cdot x)/\\exp(w\\cdot x)}{(1+\\exp(w\\cdot x))/(\\exp(w\\cdot x))}\\\\ &amp;=\\frac{1}{e^{-(w\\cdot x)}+1}\\\\ P(Y=0|x)&amp;=\\frac{1}{1+\\exp(w\\cdot x)}\\\\ &amp;=1-\\frac{1}{1+e^{-(w\\cdot x)}}\\\\ &amp;=\\frac{e^{-(w\\cdot x)}}{1+e^{-(w\\cdot x)}} \\end{aligned} \\] ### 模型参数估计 应用极大似然估计法估计模型参数，从而得到回归模型，具体步骤为求对数似然函数，并对\\(L(w)\\)求极大值，得到\\(w\\)的估计值 \\[ \\begin{aligned} L(w)&amp;=\\sum_{i=1}^N[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\ &amp;=\\sum_{i=1}^N[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot x_i))]\\\\ \\end{aligned} \\] * 上述过程将\\(P(Y=1|x)=\\pi(x)\\)代入\\(L(w)\\)中,从而对\\(L(w)\\)求极大值，得到\\(w\\)的估计值，这样问题就变成了以对数似然函数为目标函数的最优化问题。通常采用的方法是梯度下降法以及拟牛顿法。 多项逻辑斯谛回归 假设离散型随机变量\\(Y\\)的取值集合是\\({1,2,\\dots,K}\\), 多项逻辑斯谛回归模型是 \\[ \\begin{aligned} P(Y=k|x)&amp;=\\frac{\\exp(w_k\\cdot x)}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}, k=1,2,\\dots,K-1\\\\ P(Y=K|x)&amp;=\\frac{1}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}\\\\ \\end{aligned} \\] 上述两式和为1 最大熵模型 最大熵模型是由最大熵原理推导实现的，而最大熵原理是概率模型的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型就是最好的模型。通常用约束条件来确定概率模型的集合。 ### 联合熵 \\(H(X, Y) = H(X) + H(Y|X) = H(Y)+H(X|Y) = H(X|Y)+H(Y|X)+I(X;Y)\\) 如果\\(X\\)和\\(Y\\)独立同分布，联合概率分布\\(P(X,Y)=P(X)P(Y)\\) 条件熵 条件熵是最大熵原理提出的基础，最大的是条件熵，书中(定义6.3) 条件熵衡量了条件概率分布的均匀性 \\[\\begin{aligned} p^*&amp;=\\arg\\max\\limits_{p\\in \\mathcal C}H(p)\\\\ &amp;=\\arg \\max\\limits_{p\\in \\mathcal C}(-\\sum\\limits_{x,y} {\\tilde p(x)p(y|x)\\log p(y|x) }) \\end{aligned} \\] 互信息 互信息(mutual information)，对应熵里面的交集，常用来描述差异性 一般的，熵\\(H(Y)\\)与条件熵\\(H(Y|X)\\)之差称为互信息 相关性主要刻画线性，互信息刻画非线性 互信息和条件熵之间的关系 \\[ I(x,y)=H(x)-H(x|y)=H(y)-H(y|x) \\] 信息增益 这个对应的是Chapter5的内容，决策树学习应用信息增益准则选择特征 \\[ g(D,A)=H(D)-H(D|A) \\] 信息增益表示得知\\(X\\)的信息而使类\\(Y\\)的信息的不确定性减少的程度。 在决策树学习中，信息增益等价于训练数据集中类与特征的互信息。 相对熵 (KL 散度) 相对熵(Relative Entropy)描述差异性，从分布的角度描述差异性，可用于度量两个概率分布之间的差异 KL散度不是一个度量，度量要满足交换性 KL散度满足非负性 最大熵模型的学习 最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优的问题。自然而然想到了拉格朗日，这里用到了拉格朗日的对偶性，将原始问题转化为对偶问题，通过解对偶问题而得到原始问题的解。 简单来说，约束最优化问题包含\\(\\leqslant0\\)，和\\(=0\\)两种约束条件 \\[ \\begin{aligned} \\min_{x \\in R^n}\\quad &amp;f(x) \\\\ s.t.\\quad&amp;c_i(x) \\leqslant 0 , i=1,2,\\ldots,k\\\\ &amp;h_j(x) = 0 , j=1,2,\\ldots,l \\end{aligned} \\] 引入广义拉格朗日函数 \\[ L(x,\\alpha,\\beta) = f(x) + \\sum_{i=0}^k \\alpha_ic_i(x) + \\sum_{j=1}^l \\beta_jh_j(x) \\] 在KKT的条件下，原始问题和对偶问题的最优值相等 \\[ ∇_xL(x^∗,α^∗,β^∗)=0\\\\ ∇_αL(x^∗,α^∗,β^∗)=0\\\\ ∇_βL(x^∗,α^∗,β^∗)=0\\\\ α_i^∗c_i(x^*)=0,i=1,2,…,k\\\\ c_i(x^*)≤0,i=1,2,…,k\\\\ α^∗_i≥0,i=1,2,…,k\\\\ h_j(x^∗)=0,j=1,2,…,l \\] 前面三个条件是由解析函数的知识，对于各个变量的偏导数为0，后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束,第四个条件是KKT的对偶互补条件 回到最大熵模型的学习，书中详细介绍了约束最优化问题 在\\(L(P, w)\\)对\\(P\\)求偏导并令其为零解得 \\[ P(y|x)=\\exp{\\left(\\sum_{i=1}^{n}w_if_i(x,y)+w_0-1\\right)}=\\frac{\\exp{\\left(\\sum\\limits_{i=1}^{n}w_if_i(x,y)\\right)}}{\\exp{\\left(1-w_0\\right)}} \\] 因为\\(\\sum\\limits_{y}P(y|x)=1\\)，然后得到模型 \\[ P_w(y|x)=\\frac{1}{Z_w(x)}\\exp{\\sum\\limits_{i=1}^{n}w_if_i(x,y)}\\\\ \\] \\[ 其中，Z_w(x)=\\sum_{y}\\exp({\\sum_{i=1}^{n}w_if_i(x,y))} \\] * 这里\\(Z_w(x)\\)先用来代替\\(\\exp(1-w_0)\\),\\(Z_w\\)是归一化因子 并不是因为概率为1推导出了\\(Z_w\\)的表达式，这样一个表达式是凑出来的，意思就是遍历\\(y\\)的所有取值，求分子表达式的占比 对偶函数的极大化等价于最大熵模型的极大似然估计 已知训练数据的经验分布\\(\\widetilde {P}(X,Y)\\),条件概率分布\\(P(Y|X)\\)的对数似然函数表示为 \\[L_{\\widetilde {P}}(P_w)=\\log\\prod_{x,y}P(y|x)^{\\widetilde {P}(x,y)}=\\sum \\limits_{x,y}\\widetilde {P}(x,y)\\log{P}(y|x) \\] 当条件分布概率\\(P(y|x)\\)是最大熵模型时 \\[ \\begin{aligned} L_{\\widetilde {P}}(P_w)&amp;=\\sum \\limits_{x,y}\\widetilde {P}(x,y)\\log{P}(y|x)\\\\ &amp;=\\sum \\limits_{x,y}\\widetilde {P}(x,y)\\sum \\limits_{i=1}^{n}w_if_i(x,y) -\\sum \\limits_{x,y}\\widetilde{P}(x,y)\\log{(Z_w(x))}\\\\ &amp;=\\sum \\limits_{x,y}\\widetilde {P}(x,y)\\sum \\limits_{i=1}^{n}w_if_i(x,y) -\\sum \\limits_{x,y}\\widetilde{P}(x)P(y|x)\\log{(Z_w(x))}\\\\ &amp;=\\sum \\limits_{x,y}\\widetilde {P}(x,y)\\sum \\limits_{i=1}^{n}w_if_i(x,y) -\\sum \\limits_{x}\\widetilde{P}(x)\\log{(Z_w(x))}\\sum_{y}P(y|x)\\\\ &amp;=\\sum \\limits_{x,y}\\widetilde {P}(x,y)\\sum \\limits_{i=1}^{n}w_if_i(x,y) -\\sum \\limits_{x}\\widetilde{P}(x)\\log{(Z_w(x))} \\end{aligned} \\] 推导过程用到了\\(\\sum\\limits_yP(y|x)=1\\)","link":"/posts/12171.html"},{"title":"Python基础","text":"数据类型和变量 数据类型 Python可以直接表达的数据类型包括：整数，浮点数，复数，字符串，布尔值和空值。 整数 int 任意大小，正负皆可。并且可以用0b、0o、0x分别表示二进制、八进制和十六进制数： 12345678910111213141516171819202122&gt;&gt;&gt; 0b10 # 也可以是 0B102&gt;&gt;&gt; 0o10 # 也可以是 0O108&gt;&gt;&gt; 0x10 # 也可以是 0X1016# 十进制转其他进制&gt;&gt;&gt; bin(2)'0b10'&gt;&gt;&gt; oct(8)'0o10'&gt;&gt;&gt; hex(16)'0x10'# 其他进制转十进制（也可将字符串转换为十进制）&gt;&gt;&gt; int(0b10) # 也可以是 int('10', 2)2&gt;&gt;&gt; int(0o10) # 也可以是 int('10', 8)8&gt;&gt;&gt; int(0x10) # 也可以是 int('10', 16)16 浮点数 除了普通的写法之外，也可使用科学记数法，即1.23x10^9就是1.23e9 ，0.000012可以写成1.2e-5，等等。 整数运算永远是精确的(结果也是整数)，浮点数运算则有四舍五入的误差。 Python提供两种除法+求余运算： 符号 用途 / 结果为浮点数，即使两数整除，结果也是浮点数 // 结果为整数，也称地板除，对结果向下取整 % 结果为整数，用于计算余数 复数 Python中复数用j表示虚数部分，比如说 x=2.4 + 5.6j， 并且可以通过复数的类属性 real 取得实部，类属性 imag 取得虚部，通过类方法 conjugate() 获得共轭复数。 1234567&gt;&gt;&gt; x = 2.4 + 5.6j&gt;&gt;&gt; x.real2.4&gt;&gt;&gt; x.imag5.6&gt;&gt;&gt; x.conjugate()(2.4-5.6j) 字符串 使用单引号或双引号括起。而引号本身需要使用转义符\\来表达。 用\\'来表示'。 转义符还可以用来转义很多字符，如\\n表示换行。\\t表示制表符。 \\本身也需要转义用 双斜杠 来代替。 如果一个字符串中很多字符都要转义就会很麻烦，所以Python又提供一种简便的写法，r''表示两个引号之间的内容是不需要转义的。 对于多行的字符串，为了避免加入\\n的不方便，可以使用'''something'''的格式，即用三个引号来括起字符串，换行会被保留。 12345678910111213# 没使用三引号&gt;&gt;&gt; a = '123 File &quot;&lt;stdin&gt;&quot;, line 1a = '123 ^SyntaxError: EOL while scanning string literal# 使用了三引号&gt;&gt;&gt; a = '''123... 456... 789'''&gt;&gt;&gt; a'123\\n456\\n789' 布尔值 只有True和False两种，首字母大写。如果输入一个判断式，则Python会给出布尔值的结果。比方说输入3&gt;2，运行后会得到True。 对于布尔值有三个逻辑运算符，分别是and，or和not。 本质上True用1存储，False用0存储。 12345678&gt;&gt;&gt; 1 == 1True&gt;&gt;&gt; 1 == TrueTrue&gt;&gt;&gt; 0 != 0False&gt;&gt;&gt; 0 == FalseTrue 空值 Python中用None表示，不同于数值0。数值0是有意义的，而None是一个特殊的空值。可以将None赋值给变量，但无法创建None以外的其他NoneType对象。 1234567891011&gt;&gt;&gt; type(None)&lt;class 'NoneType'&gt;&gt;&gt;&gt; a = None&gt;&gt;&gt; type(a)&lt;class 'NoneType'&gt;&gt;&gt;&gt; None == aTrue&gt;&gt;&gt; None == ''False&gt;&gt;&gt; None == 0False 变量 Python是一门动态类型语言，这意味着在Python中，可以反复把任意数据类型的对象赋值给同一个变量，相比起静态语言更加地灵活。 在Python中变量名不可以以数字开头，构成可以包括大小写英文，数字及下划线。 当我们写：a=’ABC’ 时，Python解释器干了两件事情： 在内存中创建了一个值为'ABC'的字符串对象； 在内存中创建了一个名为a的变量，并把它指向对象'ABC'。 常量 Python中没有常量，因为变量都可以重复赋值。但是一般约定用全部字母大写的单词来表示一个常量，如：PI=3.14。 使用list和tuple list list是一种Python内置的数据类型，表示有序集合，可动态删除和插入。通过索引可以访问列表元素，索引从0开始，即访问第一个列表元素。并且列表是循环的，可以通过索引－1访问最尾的元素，索引－2访问倒数第二个元素。例如： 123456789&gt;&gt;&gt; classmates = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates[0]'Michael'&gt;&gt;&gt; classmates[-1]'Tracy'&gt;&gt;&gt; classmates[-2]'Bob' 另外，还可以用 len() 函数获取列表的元素个数。 list 是一个可变的有序表，所以，可以往 list 中追加元素到末尾： 123&gt;&gt;&gt; classmates.append('Adam')&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy', 'Adam'] 也可以把元素插入到指定的位置，比如索引号为 1 的位置： 123&gt;&gt;&gt; classmates.insert(1, 'Jack')&gt;&gt;&gt; classmates['Michael', 'Jack', 'Bob', 'Tracy', 'Adam'] 要删除 list 末尾的元素，用 pop() 方法： 1234&gt;&gt;&gt; classmates.pop()'Adam'&gt;&gt;&gt; classmates['Michael', 'Jack', 'Bob', 'Tracy'] 要删除指定位置的元素，用 pop(i) 方法，其中 i 是索引位置： 1234&gt;&gt;&gt; classmates.pop(1)'Jack'&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy'] 要把某个元素替换成别的元素，可以直接赋值给对应的索引位置： 123&gt;&gt;&gt; classmates[1] = 'Sarah'&gt;&gt;&gt; classmates['Michael', 'Sarah', 'Tracy'] list 里面的元素的数据类型可以不同，比如： 1&gt;&gt;&gt; L = ['Apple', 123, True] list 里面的元素也可以是另一个 list，比如： 123456789101112&gt;&gt;&gt; s = ['python', 'java', ['asp', 'php'], 'scheme'] # 四个元素，其中第三个元素是一个列表&gt;&gt;&gt; len(s)4&gt;&gt;&gt; s[0]'python'&gt;&gt;&gt; s[2]['asp', 'php']&gt;&gt;&gt; s[2][0]'asp'&gt;&gt;&gt; s[2][1]'php' 要注意s只有4个元素，s[2]作为一个list类型的元素。要拿到'php'可以用 s[2][1]，即把s看作一个二维数组，这样的嵌套可以有很多层。 如果一个 list 中一个元素也没有，就是一个空的 list，它的长度为 0： 123&gt;&gt;&gt; L = []&gt;&gt;&gt; len(L)0 tuple tuple也是一种有序列表，但tuple一旦初始化就无法再修改，也因为这个特性，所以代码更安全。和list不同，tuple用小括号来括起。例如： 1&gt;&gt;&gt; classmates = ('Michael', 'Bob', 'Tracy') 定义空的tuple如下如下： 123&gt;&gt;&gt; t = ()&gt;&gt;&gt; t() 但是，要定义一个只有1个元素的 tuple，就要注意一下，如果使用t = (1)来定义，则得到的不是一个tuple，而是整数1，因为括号既可以表示tuple又可以表示数学公式中的小括号。这种情况下默认为后者。要定义1个元素的tuple格式如下，使用一个逗号进行区分： 1234567&gt;&gt;&gt; t = (1)&gt;&gt;&gt; t1&gt;&gt;&gt; t = (1,)&gt;&gt;&gt; t(1,) tuple也有 \"可变\" 的例子，如果tuple的其中一个元素是list，则这个list元素的内容是可以修改的，如下： 12345&gt;&gt;&gt; t = ('a', 'b', ['A', 'B'])&gt;&gt;&gt; t[2][0] = 'X'&gt;&gt;&gt; t[2][1] = 'Y'&gt;&gt;&gt; t('a', 'b', ['X', 'Y']) 这个例子实际修改的是list而不是tuple，tuple指向的位置不会变，而list指向的位置可变，上面的例子实际上是创建了字符串X和Y，然后让tuple的第三个元素也即list元素指向这两个新的字符串。 使用dict和set dict dict即字典，用于存储键值对，查找速度极快。如果使用list来存键值对就需要两个list，要先从key list找出key，再从value list找到对应项的值，因此list越长，耗时越长。用dict实现则可以直接根据key来找value。格式如下： 123&gt;&gt;&gt; d = {'Michael': 95, 'Bob': 75, 'Tracy': 85}&gt;&gt;&gt; d['Michael']95 dict速度快是因为Python内部像字典一样建立了索引，字典有部首表，Python内部也会根据不同key算出一个存放的「页码」(哈希算法)，所以速度非常快。除了初始化赋值还可以对同一个key进行多次赋值，会覆盖原来的value，如果key不存在就会对dict插入一个新的键值对： 1234567891011&gt;&gt;&gt; d = {'Michael': 95, 'Bob': 75, 'Tracy': 85}&gt;&gt;&gt; d{'Tracy': 85, 'Michael': 95, 'Bob': 75}&gt;&gt;&gt; d['Michael'] = 20&gt;&gt;&gt; d{'Tracy': 85, 'Michael': 20, 'Bob': 75}&gt;&gt;&gt; d['Lincoln'] = 100&gt;&gt;&gt; d{'Tracy': 85, 'Michael': 20, 'Bob': 75, 'Lincoln': 100} 要判断key是否在dict里面有两种方法： 1.使用in关键字，有则返回True，无则返回False 12&gt;&gt;&gt; 'Thomas' in dFalse 2.使用dict提供的get方法，有则返回key对应的value，无则返回空值None或者自己指定的值。 1234&gt;&gt;&gt; d.get('Thomas')None&gt;&gt;&gt; d.get('Thomas', -1)-1 删除一个key则对应的value也会从dict中删除，使用pop方法来实现： 1234&gt;&gt;&gt; d.pop('Bob')75&gt;&gt;&gt; d{'Tracy': 85, 'Michael': 20, 'Lincoln': 100} dict的插入和查找速度极快，不会随着key的增加而增加，但需要占用大量的内存，内存浪费多。list则相反，插入和查找时间随元素增加而增加，但占用空间少。所以dict是一种用空间换时间的方法，注意dict的key必须是不可变对象，无法修改key，不然dict就混乱了。字符串和整数等都可以作为key，list无法作为key。 ###set set和dict的原理是一样的，同样不可以放入可变对象做key，唯一的区别是set只有key没有value。显然set里面是没有重复的元素的，不然哈希时会出错。set是有序的，需要使用列表/元组做初始化，定义方式如下： 1234567&gt;&gt;&gt; s = set([1, 2, 3])&gt;&gt;&gt; s{1, 2, 3}&gt;&gt;&gt; s = set((1,2,3))&gt;&gt;&gt; s{1, 2, 3} 列表中有重复元素时set会自动被过滤，添加可以使用add方法，如s.add(4)。删除则用remove方法，如s.remove(4)。 集合可以看成数学意义上无序和无重复的集合，可以做交集、并集、差集等操作。 12345678&gt;&gt;&gt; s1 = set([1, 2, 3])&gt;&gt;&gt; s2 = set([2, 3, 4])&gt;&gt;&gt; s1 &amp; s2{2, 3}&gt;&gt;&gt; s1 | s2{1, 2, 3, 4}&gt;&gt;&gt; s1-s2{1} 再议不可变对象 Python中一切皆对象，而对象又分可变对象和不可变两类对象两类，具体来说，它们的差别就是对象的内容是否可变。不可变对象包括int, float, string, tuple，空值等，可变对象包括list, dict, set等。要注意对象和变量的关系，在Python里，变量都是对对象的引用。举个例子： 12345678910111213&gt;&gt;&gt; a = 'abc'&gt;&gt;&gt; a[0]'a'&gt;&gt;&gt; a[0]='b' # 字符串对象本身是不可变的Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: 'str' object does not support item assignment# 变量可以指向另一个字符串对象# 但字符串对象'abc'并没有改变，它依然存在于内存中&gt;&gt;&gt; a = 'bbc'&gt;&gt;&gt; a'bbc' 关于参数传递，可以简单总结为以下两点： 当一个可变对象作为函数参数时，函数内对参数修改会生效。 当一个不可变对象作为函数参数时，函数内对参数修改无效，因为实际上函数是创建一个新的对象并返回而已，没有修改传入的对象。例如： 1234&gt;&gt;&gt; a = ['c', 'b', 'a']&gt;&gt;&gt; a.sort()&gt;&gt; a['a', 'b', 'c'] list是可变对象，使用函数操作，内容会变化，注意区分一下变量和对象，这里a是一个变量，它指向一个列表对象，这个列表对象的内容才是['c', 'b', 'a']。例如： 123456&gt;&gt;&gt; a = 'abc'&gt;&gt;&gt; b = a.replace('a', 'A')&gt;&gt;&gt; b'Abc'&gt;&gt;&gt; a'abc' 字符串是不可变对象，所以replace函数并不会修改变量a指向的对象，实际上调用不可变对象的任意方法都不改变该对象自身的内容，只会创建新对象并返回，如果使用一个新的变量来接收返回的对象，就能得到操作结果。不接收则直接打印，原变量指向的对象不会变化。 注意，虽然tuple是不可变对象，但是如果使用tuple作为dict或者set的key，还是有可能产生错误，因为tuple允许元素中包含列表，列表内容可变。如果使用了带有列表元素的tuple作为key就会报 TypeError: unhashable type: 'list' 的错误。 条件判断 条件判断 Python中代码块是以缩进作区分的，if－else条件判断注意要在判定条件后写冒号，并且代码块都正确对齐。多个判断条件可以使用多个elif来实现。例如： 1234567age = 20if age &gt;= 6:print('teenager')elif age &gt;= 18:print('adult')else:print('kid') 判断条件并不一定要是一个判断式，可以简写为一个变量，当变量为非零数值，非空字符串，非空list等时，判断为True，否则为False。 再议input 有时会采取 input('提示语句') 的方式读取用户输入，作为判定条件。要注意用户输入属于字符串类型，要进行数值比较必须先转换为对应的数据类型，否则会报错。 Python提供int(), float(), str()等方法进行数据类型的转换。 1234567891011121314151617# 输入1表示Yes，输入0表示No&gt;&gt;&gt; choose = input('If you choose yes, please input 1. Otherwise, input 0: ')If you choose yes, please input 1. Otherwise, input 0: 0&gt;&gt;&gt; if choose: # 没有进行转换... print('Yes')... else:... print('No')...Yes&gt;&gt;&gt; if int(choose): # 进行了转换... print('Yes')... else:... print('No')...No 循环 Python提供两种循环写法，一种是 for...in... 循环，一种是 while... 循环。for循环依次把list或tuple中的每个元素赋值给目标标识符，格式如下： 1234567&gt;&gt;&gt; names = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; for name in names:... print(name)...MichaelBobTracy 当列表为连续数值时，可以用range方法生成，格式如下： 12345678910&gt;&gt;&gt; sum = 0&gt;&gt;&gt; for x in range(5):... sum = sum + x... print(sum)...013610 while循环的写法和if－else很相似，也是在判定条件后面加一个冒号。例如： 12345678910111213&gt;&gt;&gt; sum = 0&gt;&gt;&gt; n = 11&gt;&gt;&gt; while n &gt; 0:... sum = sum + n... n = n - 2... print(sum)...112027323536 另外，对于死循环的程序，可以通过Ctrl＋C 强制终止。 字符串和编码 字符编码 ASCII &gt; 最早的编码，包含大小写英文，数字及一些符号。大写A编码为65，小写a编码为97。字符0编码为48。使用8位二进制组合(1个字节)，可表示128个不同字符(最高位用于校验)。 GB2312 &gt; 处理中文时一个字节显然不足，至少需要用两个字节，并且不与ASCII冲突，因此有了中国自己制定的GB2312编码，可用于编码中文。 Shift_JIS和Euc-kr &gt; 日本编码为Shift_JIS，韩文编码为Euc-kr。这样各个国家都用自己不同的标准和编码就很容易在多语言的环境产生冲突，变成乱码。因此又有了通用的一套编码。 Unicode &gt; Unicode将所有语言统一到一套编码中，一般使用2个字节，部分非常偏僻的字符会用到4个字节。但是使用Unicode编码，如果在大量英文的环境又非常浪费空间，因为存储和传输时大小是使用ASCII编码的两倍，不划算，于是又有了新的方式。 UTF-8 &gt; UTF-8是可变长编码，对Unicode字符不同的数字大小编码成1-6个字节，英文字母为1个字节，汉字一般是3个字节。需要用到4-6字节的字符非常少，这样比较节省空间。 Example： 字符 ASCII Unicode UTF-8 A 01000001 00000000 01000001 01000001 中 (超出范围) 01001110 00101101 11100100 10111000 10101101 插播一段课外知识 摘录整理自知乎提问为什么不少网站使用 UTF-8 编码？。 为什么要分Unicode和UTF-8，它们两者有什么区别和关联呢？ 其实最早的时候，由于各地区使用的编码方式不同，使用不同语言的人交流就成了一个大问题。国际标准组织ISO制订了统一的通用字符集（Universal Character Set，UCS），也简称为Unicode。早期的Unicode版本对应于UCS-2编码方式，使用16位的编码空间。也就是每个字符占用2个字节。这样理论上一共最多可以表示 2^16 也即 65536 个字符，基本满足各种语言的使用。根据维基的说法，当前最新版本的Unicode标准收录了128,000个字符，已经远远超出两字节所能表示的65536了。所以把Unicode视作UCS-2是一种过时的说法，Unicode标准存在多于两字节的字符。 Unicode有多种字符编码实现方式。一个Unicode字符的code point是确定的。但是在实际传输过程中，由于不同系统平台的设计不一定一致，以及出于节省空间的目的，会使用不同的字符编码来实现Unicode。这些字符编码统称为Unicode转换格式（Unicode Transformation Format，简称为UTF）。 例如，对于一个仅包含ASCII字符(只需要7位)的Unicode文件，如果每个字符都使用2字节的原Unicode编码传输，其第一字节的首位始终为0。这就造成了比较大的浪费。对于这种情况，可以使用UTF-8编码，这是一种变长编码，它将ASCII字符仍用7位编码表示，占用一个字节（首位补0）。而遇到与其他Unicode字符混合的情况，将按一定算法转换，每个字符使用1-3个字节编码，并利用首位为0或1进行识别。这样对以ASCII字符为主的西文文档就大大节省了编码长度。 但是，现在所说的Unicode一般都指的是字符集而非编码方式，廖雪峰老师在教程里提到的Unicode编码实际上指的是UCS-2这种编码方式。又因为以前UCS-2和UTF-16是等价的，所以微软的文档习惯把UTF16称为Unicode，这也导致很多开发者容易对此产生误会。 以上内容虽然有部分针对廖雪峰老师教程中描述不准确的地方进行了修改，但可能还是存在一些让人迷惑的说法，更详细的讲解可以看我的博文字符集与编码的恩怨情仇，以及Python3中的编码问题。 其他内容 在计算机存储中，内存统一使用Unicode编码，而硬盘保存或者传输数据就用UTF-8编码。比方说打开记事本时，数据会从硬盘中UTF-8编码格式转换为Unicode格式，保存时则相反。 浏览网页时，服务器端动态生成的内容是Unicode编码，而传输时则会变为传输UTF-8编码格式的网页。所以常常在网页源码中看到 &lt;meta charset=\"UTF-8\" /&gt;的语句，表示该网页正是用的UTF-8编码。 按我的理解，可以这样总结：在传输/储存时，为了节省带宽/空间，会使用UTF-8这样的Unicode字符集的实现方式；而在计算机要处理的时候，应当使用原Unicode编码方式，也即每个字符都是固定两字节，这样字符长度统一更便于操作。 特别注意，在涉及到字节流和字符串转换的程序里，我们应始终坚持使用UTF-8编码进行转换，从而避免乱码问题。 另外，代码本身也是文本，所以也有编码相关的问题。在编写Python脚本时，一般会在文件头部加入以下两句注释： 12#!/usr/bin/env python3# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序（这里说得不准确，具体看我第5章05模块的笔记），Windows系统则会忽略这个注释； 第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，如果源代码中有中文的话，就会出现乱码了。 注意，这里仅仅是声明了读取时采用UTF-8编码，但文件本身的编码不一定是UTF-8，要确定把代码文件编码为UTF-8格式，我们需要对写代码的IDE/文本编辑器进行相应的设置。廖老师提到，要确保使用UTF-8 without BOM编码，那么BOM和without BOM有什么区别呢？暂不在此进行讨论。 关于Python3中编码的信息，可以再看看官方文档。 字符串 Python中，字符串类型中的每个字符采用的是两字节的Unicode编码，支持多语言。 ord函数 ord函数可以获取字符的十进制整数表示： 1234&gt;&gt;&gt; ord('中')20013&gt;&gt;&gt; ord('文')25991 注意只能传入单个字符。 chr函数 chr函数可以将十进制整数转换为Unicode编码下对应的字符。 1234&gt;&gt;&gt; chr(20013)'中'&gt;&gt;&gt; chr(25991)'文' 此外,如果知道了字符的十进制编码，可以将其转换为十六进制，然后使用\\u转义得到对应的Unicode字符： 12345678910&gt;&gt;&gt; hex(20013)'0x4e2d'&gt;&gt;&gt; '\\u4e2d''中'&gt;&gt;&gt; hex(25991)'0x6587'&gt;&gt;&gt; '\\u6587''文'&gt;&gt;&gt; '\\u4e2d\\u6587' # 多个字符'中文' 补充一下，Unicode编码是多语言的，不仅限于中文和英文，还可以使用多种语言的字符及符号： 12345678910111213141516171819202122232425262728293031323334In [1]: ord('한') # 韩语Out[1]: 54620In [2]: hex(54620)Out[2]: '0xd55c'In [3]: '\\ud55c'Out[3]: '한'In [4]: ord('に') # 日语Out[4]: 12395In [5]: hex(12395)Out[5]: '0x306b'In [6]: '\\u306b'Out[6]: 'に'In [7]: ord('a') # 英语Out[7]: 97In [8]: hex(97)Out[8]: '0x61'In [9]: '\\u0061'Out[9]: 'a'In [10]: ord('1') # 数字Out[10]: 49In [11]: hex(49)Out[11]: '0x31'In [12]: '\\u0031'Out[12]: '1'In [13]: ord(',') # 符号Out[13]: 44In [14]: hex(44)Out[14]: '0x2c'In [15]: '\\u002c'Out[15]: ',' 从以上代码可以看到，这里的Unicode编码中**每个字符都是固定用两字节表示（一个字符用4位16进制数表示，每个16进制数需要4bit，所以一共是4*4 = 16bits = 2bytes），和廖老师教程中所说的若干个字节不一致，是否真的有多于两字节的Unicode字符呢？有待考究。** bytes类型 因为Python程序执行时，字符串类型是用Unicode编码的，一个字符对应若干字节（若干字节还是固定两个字节呢？），要在网络中传输或存储到硬盘中就要把字符串变为bytes类型（因为传输和存储都是以字节为单位的，所以需要进行转换。又因为要节省资源，所以会使用别的编码方式如utf-8）。 Python显示bytes类型的数据会用b作前缀，要注意 b'ABC' 和 'ABC' 的差别，尽管内容一样，但前者的每个字符都只占1个字节，而后者在Python中以Unicode进行编码，每个字符占两个字节。也即： 1234&gt;&gt;&gt; b'\\x41\\x42\\x43' # bytes类型，每个英文字符占1个字节b'ABC'&gt;&gt;&gt; '\\u0041\\u0042\\u0043' # Unicode类型，每个英文字符占2个字节，前8bit用0填充'ABC' Unicode字符串可以通过 encode() 方法编码为指定的bytes，如ASCII编码，utf-8编码etc，bytes类型的数据如果字符不属于ASCII码的范围，就用 '\\x## 的格式表示。 相应地，如果读取字节流的数据，就要用 decode() 方法解码。例如： 123456789&gt;&gt;&gt; 'ABC'.encode('ascii')b'ABC'&gt;&gt;&gt; '中文'.encode('utf-8')b'\\xe4\\xb8\\xad\\xe6\\x96\\x87'&gt;&gt;&gt; b'ABC'.decode('ascii')'ABC'&gt;&gt;&gt; b'\\xe4\\xb8\\xad\\xe6\\x96\\x87'.decode('utf-8')'中文' len函数 对字符串使用len函数会得到字符的数目，而对字节流使用len函数则会得到有多少字节。例如： 1234567891011121314151617&gt;&gt;&gt; len('中文') # 字符串'中文'包含两个字符2&gt;&gt;&gt; len('\\u4e2d\\u6587') # 用4个字节表示字符串'中文'2&gt;&gt;&gt; len('中文'.encode('utf-8')) # 字符串'中文'编码为utf-u格式后，占6个字节6&gt;&gt;&gt; len(b'\\xe4\\xb8\\xad\\xe6\\x96\\x87') # 用6个字节表示字节流'中文'6&gt;&gt;&gt; len('ABC') # 字符串'ABC'包含三个字符3&gt;&gt;&gt; len('\\u0041\\u0042\\u0043') # 用6个字节表示字符串'ABC'3&gt;&gt;&gt; len('ABC'.encode('ascii')) # 字符串'ABC'编码为ASCII格式后，占3个字符3&gt;&gt;&gt; len(b'\\x41\\x42\\x43') # 用3个字节表示字节流'ABC'3 格式化 和C语言类似，Python中使用百分号 ％ 占位符实现格式化的功能。％s表示用字符串替换，％d，％f，％x分别表示用整数，浮点数和十六进制数替换。其中％s可以将任意数据类型转换为字符串。而％d和％f则可以指定整数和小数部分的位数。例如： 12345678910111213141516171819# 普通的格式化&gt;&gt;&gt; '%s %d %f %x' % ('a',1,1.2,0x2)'a 1 1.200000 2'# %s可以转换任意数据类型&gt;&gt;&gt; '%s %s %s %s' % ('a',1,1.2,0x2)'a 1 1.2 2'# %d可以控制整数部分的位数，不足的位数默认以空格填充&gt;&gt;&gt; '%4d' % 5' 5'# 除了用空格填充之外，也可以使用0填充不足的位数&gt;&gt;&gt; '%04d' % 5'0005'# %f也可以控制整数和小数部分的位数&gt;&gt;&gt; '%f - %.2f - %2.2f - %02.2f' % (1.2, 1.2, 1.2, 1.2)'1.200000 - 1.20 - 1.20 - 1.20' 注意： 只有一个占位符时，后面变量不需要用括号括起。 如果字符串本身包含％，则需要转义，用％％来表示百分号。例如： 1234567&gt;&gt;&gt; 'growth rate: %d %' % 7 # 没有转义百分号Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;ValueError: incomplete format&gt;&gt;&gt; 'growth rate: %d %%' % 7 # 转义了百分号'growth rate: 7 %'","link":"/posts/14702.html"},{"title":"函数","text":"调用函数 Python支持函数，不仅可以灵活地自定义函数，而且本身也内置了很多有用的函数。 除了可以使用help(函数名)查看内置函数（built-in function, BIF）的用法和用途，也可以直接查看官方文档。 函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个别名。 123&gt;&gt;&gt; a = abs # 变量a指向abs函数&gt;&gt;&gt; a(-1) # 所以也可以通过a调用abs函数1 定义函数 定义函数要使用 def 语句，依次写出 函数名、括号、括号中的参数 和冒号，然后，在缩进块中编写函数体，函数的返回值用 return 语句返回。例如： 12345def my_abs(x): if x &gt;= 0: return x else: return -x 注意，如果没有return语句，则函数执行完毕也会返回 None，如果想要函数返回 None，除了写 return None 之外还可以直接写 return。 我们既可以直接在命令行定义函数，也可以把函数放在 .py 文件中定义。若采用后者,则使用函数时要先把工作目录跳转到文件保存的目录，再启动Python，然后用 from 文件名 import 函数名 即可导入函数。(这里文件名不需要包含文件扩展名 .py) 比方说把上面的 my_abs 函数写入到 my_abs.py 文件中，保存在桌面，使用该函数需要先 cd 到桌面目录，然后再导入和使用： 123456789101112C:\\Users\\Administrator&gt;cd DesktopC:\\Users\\Administrator\\Desktop&gt;pythonPython 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; from my_abs import my_abs&gt;&gt;&gt; my_abs(5)5&gt;&gt;&gt; my_abs(0)0&gt;&gt;&gt; my_abs(-5)5 空函数 如果还没想好怎么写一个函数，可以用 pass 语句来实现一个空函数，如： 12def nop(): pass pass 语句什么都不做，但可以用来做占位符。用在其他语句中也可以，如： 12if age &gt;= 18: pass 参数检查 当参数个数不对时，Python解释器会抛出 TypeError 错误，但是当参数类型错误时，如果函数里面没有给出对应的方法，Python解释器就无法抛出正确的错误提示信息。 上面实现的 my_abs 函数还不够完善，使用Python的内置函数 isinstance() 和 raise 语句来实现类型检查并报错的功能，如下： 1234567def my_abs(x): if not isinstance(x, (int, float)): raise TypeError('bad operand type') if x &gt;= 0: return x else: return -x 效果： 123456789&gt;&gt;&gt; my_abs('a') # 参数类型错误Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 3, in my_absTypeError: bad operand type&gt;&gt;&gt; my_abs(1,2) # 参数个数错误Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: my_abs() takes 1 positional argument but 2 were given 返回多个值 举一个返回坐标点的例子： 123456import mathdef move(x, y, step, angle=0): nx = x + step * math.cos(angle) ny = y - step * math.sin(angle) return nx, ny 这里用到math包的函数 cos 和 sin，返回坐标点的两个维度的值。接收时： 12345&gt;&gt;&gt; x, y = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(x)151.96152422706632&gt;&gt;&gt; print(y)70.0 或者： 123&gt;&gt;&gt; r = move(100, 100, 60, math.pi / 6)&gt;&gt;&gt; print(r)(151.96152422706632, 70.0) 实际上，在Python中，函数返回的仍然是一个变量，但在返回多个值时，Python会将它们合并为一个tuple返回，又因为语法上返回一个tuple可以省略括号，所以可以直接写成返回多个值的形式。 特别地，我们可以使用多个变量来接收一个返回的tuple，Python会按位置顺序来赋对应的值。 函数的参数 定义函数的时候，我们把参数的名字和位置确定下来，函数的接口定义就完成了。 Python的函数定义非常简单，但灵活度却非常大。除了正常定义的必选参数外，还可以使用默认参数、可变参数和关键字参数。 位置参数 传入值按位置顺序依次赋给参数。 123456def power(x, n): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 如这个幂函数，调用时使用 power(5,2) 这样的格式即可，5和2会按位置顺序分别被赋给变量x和n。 默认参数 有时候我们希望函数带有默认设置，比方说令幂函数默认计算平方，这样就不需要每次都传入参数n了。 可以使用默认参数来实现这样的功能： 123456def power(x, n=2):s = 1while n &gt; 0: n = n - 1 s = s * xreturn s 此时使用 power(5) 也能调用幂函数，计算的是5的平方。 在编写函数的参数列表时，应当注意： 必选参数在前，默认参数在后，否则Python的解释器会报错。 有多个参数时，把变化大的参数放前面，变化小的参数放后面。这样我们可以把变化小的参数设为默认参数，调用的时候就不需要每次都填写这个参数了。 例子： 12345def enroll(name, gender, age=6, city='Beijing'): print('name:', name) print('gender:', gender) print('age:', age) print('city:', city) age和city是默认参数，调用时可以不提供。并且提供默认参数时既可以按顺序也可以不按顺序： 12enroll('Bob', 'M', 7)enroll('Adam', 'M', city='Tianjin') 按顺序不需指定参数名，不按顺序时则必须提供参数名，这样其他未提供的参数依然使用默认参数的值。 注意默认参数必须指向不可变对象！举个例子： 123def add_end(L=[]): L.append('END') return L 多次使用默认参数时： 123456&gt;&gt;&gt; add_end()['END']&gt;&gt;&gt; add_end()['END', 'END']&gt;&gt;&gt; add_end()['END', 'END', 'END'] 可以看到这里默认参数的内容改变了，因为L是可变对象，每次调用add_end，函数会修改默认参数的内容。 所以切记默认参数要指向不可变对象，要实现同样的功能，使用None就可以了。 12345def add_end(L=None): if L is None: L = [] L.append('END') return L 使用不可变对象做参数，在多任务环境下读取对象不需要加锁，同时读没有问题。因此能使用不可变对象就尽量用不可变对象。 可变参数 可变参数即传入的参数个数可变，传入任意个参数都可以 。先看一个例子： 12345def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 这个求和函数只有一个参数，必须传入一个list或者tuple才行，即 calc([1, 2, 3，7]) 或者 calc((1, 3, 5, 7))。如果使用可变参数，则： 12345def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 只是在参数前面加了一个 * 号，函数内容不需要改变。这样定义的函数可以使用任意个数的参数，包括0个。 1234&gt;&gt;&gt; calc(1, 2)5&gt;&gt;&gt; calc()0 传入参数时不需要构建list或者tuple，函数接收参数时会自动构建为一个tuple。 如果已经有一个list或者tuple要调用可变参数也很方便，将它变成可变参数就可以了。 123&gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums) # 在列表前面加上一个星号即可完成转换14 同样只需要加一个 * 号即可完成转换。 args是一个tuple类型的对象，没有传入时就是一个空的tuple。 关键字参数 可变参数允许传入0个或任一个参数，这些可变参数会自动组装为一个tuple。 而关键字参数允许传入0个或任意个含参数名的参数，这些关键字参数会自动组装为一个dict。 12def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw) 调用时： 123456&gt;&gt;&gt; person('Michael', 30)name: Michael age: 30 other: {}&gt;&gt;&gt; person('Bob', 35, city='Beijing')name: Bob age: 35 other: {'city': 'Beijing'}&gt;&gt;&gt; person('Adam', 45, gender='M', job='Engineer')name: Adam age: 45 other: {'gender': 'M', 'job': 'Engineer'} kw是一个dict类型的对象，没有传入时就是一个空的dict。 和可变参数类似，先组装一个dict然后再传入也是可以的。 123&gt;&gt;&gt; extra = {'city': 'Beijing', 'job': 'Engineer'}&gt;&gt;&gt; person('Jack', 24, city=extra['city'], job=extra['job'])name: Jack age: 24 other: {'city': 'Beijing', 'job': 'Engineer'} 或者进行转换： 123&gt;&gt;&gt; extra = {'city': 'Beijing', 'job': 'Engineer'}&gt;&gt;&gt; person('Jack', 24, **extra) # 在字典前面加上两个星号即可完成转换name: Jack age: 24 other: {'city': 'Beijing', 'job': 'Engineer'} 这里使用 ** 转换的实质是把extra拷贝一份，然后令kw指向这个拷贝，所以函数内的操作不会对函数外的extra有任何影响。 命名关键字参数 关键字参数的自由度很大，但有时我们需要限制用户可以传入哪些参数，这时就需要用到命名关键字参数。 12def person(name, age, *, city, job): print(name, age, city, job) 和关键字参数不同，这里采用一个 * 号作为分隔符，* 号后面的参数被视为关键字参数。 调用如下： 12&gt;&gt;&gt; person('Jack', 24, city='Beijing', job='Engineer')Jack 24 Beijing Engineer 错误举例 1.没有给参数名 1234&gt;&gt;&gt; person('Jack', 24, 'Beijing', 'Engineer')Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: person() takes 2 positional arguments but 4 were given 命名关键字参数必须传入参数名，如果没有参数名，Python解释器会视其为位置参数，从而报参数个数超出的错误。 2.没有传入参数 12345&gt;&gt;&gt; person('Jack', 24)Traceback (most recent call last): File &quot;&lt;pyshell#83&gt;&quot;, line 1, in &lt;module&gt;person('Jack', 24)TypeError: person() missing 2 required keyword-only arguments: 'city' and 'job' 命名关键字参数若没有定义默认值则被视为必选参数。 可以为命名关键字参数设置默认值， 比如 def person(name, age, *, city='Beijing', job):，这样即使不传入也不会报错了。 3.传入没有定义的参数 12345&gt;&gt;&gt; person('Jack', 24, city='Beijing', joc='Engineer')Traceback (most recent call last): File &quot;&lt;pyshell#84&gt;&quot;, line 1, in &lt;module&gt;person('Jack', 24, city='Beijing', joc='Engineer')TypeError: person() got an unexpected keyword argument 'joc' 命名关键字参数限制了可以传入怎样的参数，如果传入参数的参数名不在其中也会报错。 ###参数组合 在Python中定义函数除了可变参数和命名关键字参数无法混合，可以任意组合必选参数、默认参数、可变参数、关键字参数和命名关键字参数。 注意！参数定义的顺序必须是： 必选参数 -&gt; 默认参数 -&gt; 可变参数/命名关键字参数 -&gt; 关键字参数。 例子： 12345def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw) 调用： 12345678910&gt;&gt;&gt; f1(1, 2)a = 1 b = 2 c = 0 args = () kw = {}&gt;&gt;&gt; f1(1, 2, c=3)a = 1 b = 2 c = 3 args = () kw = {}&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b')a = 1 b = 2 c = 3 args = ('a', 'b') kw = {}&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b', x=99)a = 1 b = 2 c = 3 args = ('a', 'b') kw = {'x': 99}&gt;&gt;&gt; f2(1, 2, d=99, ext=None)a = 1 b = 2 c = 0 d = 99 kw = {'ext': None} 除了这种普通的调用方式，通过tuple和dict也可以很神奇地调用！ 12345678&gt;&gt;&gt; args = (1, 2, 3, 4)&gt;&gt;&gt; kw = {'d': 99, 'x': '#'}&gt;&gt;&gt; f1(*args, **kw)a = 1 b = 2 c = 3 args = (4,) kw = {'d': 99, 'x': '#'}&gt;&gt;&gt; args = (1, 2, 3)&gt;&gt;&gt; kw = {'d': 88, 'x': '#'}&gt;&gt;&gt; f2(*args, **kw)a = 1 b = 2 c = 3 d = 88 kw = {'x': '#'} 赋值是按照上面的固定顺序来进行的！对于任意函数，都可以通过类似 func(*args, **kw) 的形式调用它，无论它的参数是如何定义的。 ###小结 Python的函数具有非常灵活的参数形态，既可以实现简单的调用，又可以传入非常复杂的参数。 默认参数一定要用不可变对象，如果是可变对象，程序运行时会有逻辑错误！ 要注意定义可变参数和关键字参数的语法： *args 是可变参数，*args 接收的是一个tuple； **kw 是关键字参数，**kw 接收的是一个dict。 以及调用函数时如何传入可变参数和关键字参数的语法： 可变参数既可以直接传入：func(1, 2, 3)，又可以先组装list或tuple，再通过 *args 传入： func(*(1, 2, 3))； 关键字参数既可以直接传入：func(a=1, b=2)，又可以先组装dict，再通过 **kw 传入： func(**{'a': 1, 'b': 2})。 使用 *args 和 **kw 这两个名字是Python的习惯写法，当然也可以用其他参数名，但最好使用习惯用法。 命名关键字参数是为了限制调用者可以传入的参数名，并且我们可以为其提供默认值。 **定义命名关键字参数不要忘了写分隔符 * ，否则定义的将是位置参数。 递归函数 若一个函数在函数内部调用自身，则该函数是一个递归函数。如： 1234def fact(n): if n==1: return 1 return n * fact(n - 1) 阶乘函数就是一个递归函数，使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的。 每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。由于栈的大小不是无限的，所以，递归调用的次数过多，会导致栈溢出。 解决递归调用栈溢出的方法是通过尾递归优化，尾递归和循环效果一样，实际上可以把循环看作特殊的尾递归函数。 尾递归要求函数返回时调用自身本身而不能包含表达式。这样编译器或解释器就可以把尾递归进行优化，无论递归了多少次都只占用一个栈帧。 1234567def fact(n): return fact_iter(n, 1)def fact_iter(num, product): if num == 1: return product return fact_iter(num - 1, num * product) 之前的函数定义有乘法表达式，所以不是尾递归。 计算过程如下： 12345678910===&gt; fact(5)===&gt; 5 * fact(4)===&gt; 5 * (4 * fact(3))===&gt; 5 * (4 * (3 * fact(2)))===&gt; 5 * (4 * (3 * (2 * fact(1))))===&gt; 5 * (4 * (3 * (2 * 1)))===&gt; 5 * (4 * (3 * 2))===&gt; 5 * (4 * 6)===&gt; 5 * 24===&gt; 120 这里改为在函数调用前先计算product，每次递归仅调用函数本身就可以了。 计算过程如下： 123456===&gt; fact_iter(5, 1)===&gt; fact_iter(4, 5)===&gt; fact_iter(3, 20)===&gt; fact_iter(2, 60)===&gt; fact_iter(1, 120)===&gt; 120 可惜Python标准的解释器没有对尾递归做优化，所以即使改为尾递归的写法还是有可能产生栈溢出。 汉诺塔 a有n个盘子（从上到下由轻到重），要求只借助a，b，c三个支架，把所有盘子移动到c。并且重的盘子不可以在轻的盘子上。 1234567def move(n, a, b, c): if n == 1: print(a,' --&gt; ',c) else: move(n-1,a,c,b) #将前n-1个盘子从A移动到B上 print(a,' --&gt; ',c) #将最底下的最后一个盘子从A移动到C上 move(n-1,b,a,c) #将B上的n-1个盘子移动到C上 代码很短，思路很清晰，基于规则，每次只能把余下盘子中最重的移到c上。 这里通过改变传入参数的顺序可以灵活使用三个支架。 a在一次移动中可能充当b的角色，b，c也可能充当a的角色。 但总的来说，我们都是希望把充当a的支架上n-1个盘子先移到充当b的支架上，再把a的剩下的最重的一个盘子移动到充当c的支架上，然后递归，这时充当b的支架就变成a，充当a的支架就变成b，直到最后完成所有移动。","link":"/posts/21513.html"},{"title":"模块","text":"在开发过程中，一个文件里代码越长就越不容易维护。 为了编写易于维护的代码，我们把很多函数分组，分别放到不同的文件里，这样，每个文件包含的代码就相对较少，很多编程语言都采用这种组织代码的方式。 在Python中，一个.py文件就称之为一个模块（Module）。 使用模块的几个好处 大大提高了代码的可维护性。 编写代码不必从零开始。当一个模块编写完毕，就可以被其他模块引用。 避免函数名和变量名冲突。相同名字的函数和变量完全可以分别存在不同的模块中。 但还是要注意变量名尽量不要与BIF（built-in functions，内建函数）名字冲突。 模块名和包名 由于不同的人编写的模块名可能会相同，为了避免模块名冲突，Python又引入了按目录来组织模块的方法，称为包（Package）。 图片 假设图中abc和xyz两个模块的名字和外面其他模块名字冲突了，我们可以通过包来组织模块，避免冲突。 只要顶层包名(也即这里的mycompany文件夹)不同即可。 此时abc的模块名变为 mycompany.abc, xyz的模块名变为 mycompany.xyz。 Notice： 注意区分模块和模块名！两者不一定相同！（比如上面的模块 abc 和它的模块名 mycompany.abc） 每个包目录下都必须有一个 __init__.py 文件，否则Python就不会把这个文件夹当作一个包。 __init__.py可以是空文件，也可以有Python代码，它本身就是一个模块，并且它的模块名就是包名(这里是 mycompany)。 图片 包结构可以是多级的。比方说这里www.py的模块名就是 mycompany.web.www 。 两个utils.py的模块名分别是 mycompany.utils 和 mycompany.web.utils，它们不会冲突 。 mycompany.web 这个模块名对应的就是web目录下的 __init__.py 模块。 Notice: 自己创建的模块的模块名不要和Python自带的模块的模块名冲突！ 比如系统自带sys模块，自己的模块就不要命名sys.py，否则无法会无法正确import自带的sys模块。 使用模块 Python本身就内置了很多模块可以直接import使用。 下面我们自己编写一个Hello模块作为例子： 1234567891011121314151617181920#!/usr/bin/env python3# -*- coding: utf-8 -*-'This is the docstring(document comment) of this module '__author__ = 'Lincoln Deng'import sysdef test(): args = sys.argv if len(args)==1: print('No argument is passed.') elif len(args)==2: print('Hello, %s!' % args[1]) else: print('Too many arguments!')if __name__=='__main__': test() 例子解析 Python模块的标准文件模板 文件的第1行和第2行是标准注释： 第1行注释是用于声明使用什么程序来执行这个脚本，它可以使得这个脚本能在Unix/Linux/Mac上直接运行（也即可以使用 ./Hello.py 的形式执行而不是 python Hello.py 的形式）。如果系统装了多个版本的python，#!/usr/bin/env python3 会保证调用环境变量 $PATH 中的第一个叫python3的程序来执行脚本。又因为在一些系统中python能被重定向到python3，也即默认使用python3，所以直接用 #!/usr/bin/env python 也是可以的。还有一种写法是 #!/usr/bin/python，这样写就是指定一个路径，兼容性不如使用env的写法好。注意，如果我们使用 python Hello.py 或者 python3 Hello.py 的方式直接指定解释器来执行的话，这句注释就没用了。另外，在Windows下这句注释也是被忽略的，但为了代码的兼容性最好还是写上。 第2行注释表示（解释器和编辑器）应使用UTF-8编码来读取这个.py脚本文件中的代码文本。在Python2中，默认源代码编码方式为ASCII，要用到中文就得采用很别扭的escape写法，直接写中文会出错。后来有了PEP 0263标准，规定了显式声明代码文本编码的方法。一般有三种格式，包括：能被大部分编辑器识别的 # -*- coding: utf-8 -*-，最简单的 # coding=utf-8 以及vim的 # vim: set fileencoding=utf-8 （其实还可以写成别的方式，主要看编辑器怎样正则匹配这一行）。当然这里的utf-8可以换为其他编码。注意编码声明必须放在代码文件的第一行或第二行。另外，这里只是声明读取时采用的编码方式，保存代码时用什么方式要自己设置编辑器。实测由于Python3默认用utf-8编码，所以只要我们正确使用utf-8编码保存代码文件，那么读取时不声明也没关系。当然，为了代码的兼容性最好还是写上。 文件的第4行是一个字符串，表示模块的文档注释，任何模块的第一个字符串都被视为模块的文档注释； 文件的第6行使用 __author__ 变量记录模块作者的名字。 以上就是Python模块的标准文件模板，在Windows下使用Python3时，其实不写也没关系，但养成良好的书写习惯更好。 正式代码部分 导入sys模块 如果想使用Python内置的sys模块，就要先导入该模块： import sys。 导入之后，相当于创建了一个变量sys，该变量指向sys模块，通过这个变量可以访问sys模块的全部功能。 使用argv变量获取参数列表 sys模块有一个 argv 变量，这个变量属于list类型，存储着命令行的所有参数（即我们在命令行执行该脚本时使用的参数）。argv 列表中至少包含一个元素，因为第一个参数永远是该脚本文件的名称，例如： 在命令行执行 python3 Hello.py 获得的 sys.argv 就是 ['Hello.py']； 在命令行执行 python3 Hello.py Michael 获得的 sys.argv 就是 ['Hello.py', 'Michael]。 要获取字符串 'Michael' 只需要调用 sys.argv[1]。 条件判断 12if __name__=='__main__': test() 在Hello模块中我们定义了一个test函数，这个条件判断的意思就是，如果程序执行到这里，__name__ 变量的值是 '__main__' 的话就执行 test 函数。 其中 __name__ 变量是个特殊变量，当我们在命令行执行Hello.py时，Python解释器就会把 __name__ 变量赋值为 __main__。 但如果在别的文件中导入模块，__name__ 的值就是模块的名字而非 __main__，此时if判断结果为 False。 借助这个特性，我们可以在if判断中编写一些额外的代码，用于测试模块的功能，而在使用（导入）模块时，if判断里的代码不会被执行。 在命令行下执行 保存代码文件后，打开命令行，先把路径切换到保存 Hello.py 的目录，然后执行： 12345C:\\Users\\Administrator\\Desktop&gt;python Hello.pyNo argument is passed.C:\\Users\\Administrator\\Desktop&gt;python Hello.py LincolnHello, Lincoln! 在交互环境下执行 比方说使用IDLE或者在命令行中输入python进入： 12345678C:\\Users\\Administrator\\Desktop&gt;pythonPython 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import Hello&gt;&gt;&gt; Hello.__name__'Hello'&gt;&gt;&gt; Hello.test()No argument is passed. 导入Hello模块时，它的 __name__ 变量会被赋值为模块名 Hello，而不是 __main__，所以if判断为 False，if判断里的代码不会被执行。如果要使用 test 函数，就要通过 模块名.函数名 的方式进行调用，使用模块内的其他变量/函数同理。 命名规范和作用域 在模块中我们会定义很多函数和变量，有些是希望给别人用的，有些则希望仅仅在模块内部使用。 公开(public)的变量和函数 命名格式如： abc，x123，PI， 如果是有特殊用途则在名称前后各加上两个下划线，如：__author__，__name__。 12345678&gt;&gt;&gt; Hello.__doc__'This is the docstring(document comment) of this module '&gt;&gt;&gt; Hello.__name__'Hello'&gt;&gt;&gt; Hello.__author__'Lincoln Deng'&gt;&gt;&gt; Hello.__file__'C:\\\\Users\\\\Administrator\\\\Desktop\\\\Hello.py' 可以看到 __doc__ 变量返回了我们前面模块例子的代码中第一个字符串，也即文档注释(DocString)。什么是文档注释呢？其实就是一个模块/类/函数/方法的定义中第一个声明的字符串，使用这些对象的 .doc 属性即可访问。关于文档注释的书写标准可以查看PEP 0257。 私有(private)的变量和函数 命名格式是在名称前加一个或两个下划线，如 _xxx 和 __xxx。这样的函数或变量不应该被外部直接引用(即通过 模块名.变量名 的方式调用)。比方说下面定义的 _private_1 函数和 _private_2 函数，我们不希望使用这个模块的人调用它们： 1234567891011def _private_1(name): return 'Hello, %s' % namedef _private_2(name): return 'Hi, %s' % namedef greeting(name): if len(name) &gt; 3: return _private_1(name) else: return _private_2(name) 虽然不希望用户调用私有函数，但我们可以暴露给用户一个接口（公开的函数），也即这里的 greeting 函数，把调用两个私有函数的代码和逻辑封装在里面，用户直接调用 greeting 函数，然后让 greeting 函数决定怎样调用私有函数。 当用户使用模块时不需要关心私有的变量和函数，直接使用公开的变量和函数就可以了。 这是一种常用的代码封装和抽象的方法。 注意： 这里说私有函数和变量 不应该被直接引用，而不是 不能被直接引用。 因为Python没有方法可以限制用户调用私有函数和变量（没有），所以这样命名只是一种约定的编程习惯，使用者怎么做就要看他自己怎么决定了。 良好的习惯是外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。 安装第三方库 在Python中，安装第三方库（从而使用第三方库中提供的第三方模块），可以通过包管理工具pip（也有其他的包管理工具）完成。 安装Python时选择了安装pip的话，就可以直接在命令行中使用pip工具了。 1pip install Pillow 在命令行键入 pip install 第三方库的库名 后， pip就会自动帮用户下载并安装第三方库。 安装1 安装2 这里安装的是Python Imaging Library这个第三方库，是一个Python下非常强大的图像处理工具库。 因为PIL只支持到Python2.7，所以这里用的是基于PIL开发的支持Python3的Pillow。 安装完成后打开 F:\\Python35\\Lib\\site-packages 文件夹（具体路径看安装Python的位置而定）就会发现多了两个文件夹，一个是 Pillow-3.1.1.dist-info, 另一个是 PIL。 前者包含该库的一些基本信息，后者就是我们需要用到的包了，里面是有 __init__.py 文件的。 安装好的包我们可以在文件中直接用 from 包名 import 模块名 来导入要使用的模块，而不需要先转到模块所在的目录下再导入，也不需要把模块复制到我们的工程文件夹中。 举一个使用Pillow包中利用Image模块生成图片缩略图的例子： 1234567&gt;&gt;&gt; from PIL import Image&gt;&gt;&gt; im = Image.open('C:/Users/Administrator/Desktop/test.png') # 打开指定路径下的一张照片&gt;&gt;&gt; print(im.format, im.size, im.mode) # 打印照片的文件格式&amp;尺寸&amp;颜色模式PNG (400, 300) RGB&gt;&gt;&gt; im.thumbnail((200, 100)) # 创建缩略图&gt;&gt;&gt; im.save('thumb.jpg', 'JPEG') # 保存缩略图&gt;&gt;&gt; im.show() # 查看图片 其他常用的第三方库还有MySQL的驱动：mysql-connector-python，用于科学计算的NumPy库：numpy，用于生成文本的模板工具Jinja2，等等。 模块搜索路径 在Python中导入模块时，Python解释器会从指定好的路径中进行搜索。我们可以使用sys模块的变量 path 来查看模块的搜索路径，导入模块时会从这些路径中查找.py文件： 123456789C:\\Users\\Administrator&gt;pythonPython 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path['', 'F:\\\\Anaconda3\\\\python35.zip', 'F:\\\\Anaconda3\\\\DLLs', 'F:\\\\Anaconda3\\\\lib', 'F:\\\\Anaconda3', 'F:\\\\Anaconda3\\\\lib\\\\site-packages','F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Sphinx-1.3.5-py3.5.egg','F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib','F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-20.3-py3.5.egg'] sys模块的 path 变量是一个列表，它会在启动Python时被初始化，初始赋值（按顺序）由三个部分组成，一是当前目录（即列表中的空字符串），二是环境变量 PYTHONPATH 中的路径，三是一些默认的路径（包含内置模块和一些通过pip安装的模块）。由于我没有设置环境变量 PYTHONPATH，所以上面只有一和三两部分。尝试新建一个名为 PYTHONPATH 的环境变量，添加一条路径指向F盘，重新启动Python程序，此时就会发现 path 变量的初始赋值中多了F盘的路径了： 123456789C:\\Users\\Administrator&gt;pythonPython 3.5.1 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:49:46) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path['', 'F:\\\\', 'F:\\\\Anaconda3\\\\python35.zip', 'F:\\\\Anaconda3\\\\DLLs', 'F:\\\\Anaconda3\\\\lib', 'F:\\\\Anaconda3', 'F:\\\\Anaconda3\\\\lib\\\\site-packages','F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Sphinx-1.3.5-py3.5.egg','F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib','F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'F:\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-20.3-py3.5.egg'] 如果需要使用自己编写的模块，可以把它们放到这些目录中。 也可以自己增加搜索路径。具体来说分为两种方法： 方法一：直接使用 apeend 往 sys.path 列表添加搜索路径，这种方法只在该次运行时有效，重启Python交互环境后会恢复原来的路径： 12&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path.append('C:/Users/Administrator/Desktop') 方法二：配置环境变量PYTHONPATH，只需要增加自己的搜索路径，默认的路径是不会被覆盖掉的，使用这种方法就不需要每次都修改 sys.path 了。 关于 sys.path 的详情可以查看官方文档。 文件搜索路径 这一节与模块无关，但是觉得有必要区分好文件搜索路径和模块搜索路径！ 文件搜索路径是当前工作目录，如果我们不指定路径，直接使用文件名访问文件的时候，Python会从当前路径中进行查找；模块搜索路径则是像上一节提及的那样，指 sys.path 列表中包含的路径。 要获取当前工作路径可以使用os模块的 getcwd 函数，也即get current work directory： 123&gt;&gt;&gt; import os&gt;&gt;&gt; os.getcwd()'C:\\\\Users\\\\Administrator\\\\Desktop' 要调用的文件如果放在当前工作路径上就可以直接用 文件名.文件格式 指定，比方说我在桌面放了一张 test.jpg，那么访问时直接用 im = Image.open('test.jpg') 就可以打开了，无须使用完整的路径，也即 im = Image.open('C:/Users/Administrator/Desktop/test.jpg')。 注意Python中路径字符串里斜杠的使用， 如果使用反斜杠划分就必须转义，也即写作双反斜杠 \\\\；如果使用左斜杠 / 则不需要进行转义。Python默认路径字符串都使用双反斜杠。 如果文件不在当前工作路径，那么我们写路径时可以有两种写法： 使用绝对路径 1im = Image.open('C:/Users/Administrator/Desktop/test.jpg') 这里使用了左斜杠，所以不需要转义。 使用相对路径 1im = Image.open('./test.jpg') 相对路径即相对当前工作路径而言的路径，这是为了避免路径过长而设计的，可以用 . 符来代替当前工作路径。","link":"/posts/60268.html"},{"title":"面向对象编程","text":"基本概念 面向对象编程 —— Object Oriented Programming，简称OOP，是一种编程思想。OOP把对象作为程序的基本单元，一个对象不仅包含数据还包含操作数据的函数。 ### 对比面向过程与面向对象 面向过程编程（Procedural programming）：把计算机程序视为一系列子程序的集合。为了简化程序设计，面向过程把子程序继续切分为更小的子程序，也即把大的功能分为若干小的功能进行实现，从而降低系统的复杂度，这种做法也称为模块化（Modularity）。 面向对象编程（Object-oriented programming）：把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 下面以保存和打印学生成绩表为例，分别展示面向过程编程和面向对象编程的不同： 面向过程编程： 1234567891011def save_score(name, score): return {'name':name, 'score':score}def print_score(std): print('%s: %s' % (std['name'], std['score']))bart = save_score('Michael', 98)lisa = save_score('Bob', 81)print_score(bart)print_score(lisa) 面向过程编程其实就是细分功能并逐步实现，这里分出了保存成绩和打印成绩两个细的功能，并分别封装成子程序（或者说函数），然后通过调用各个子程序来实现程序的最终目标（保存并打印成绩）。 面向对象编程： 1234567891011class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score))bart = Student('Bart Simpson', 59)lisa = Student('Lisa Simpson', 87)bart.print_score()lisa.print_score() 面向对象编程强调程序的主体是对象，这里我们把学生抽象成一个类（class），这个类的对象拥有 name 和 score 这两个属性（Property），那么保存成绩就是把学生类实例化为对象（object），打印成绩就是给每个学生对象发送一个 print_score 的消息（message），让对象自己打印自己的属性。这个发送消息的过程又称为调用对象的方法（method），注意区分函数和方法。 五种编程范式的区分 这一小节是额外加上的，因为之前第4章函数式编程也讲了一种编程范式，这章又引入了面向过程编程和面向对象编程的概念，所以就在这里整理一下。也可以直接查看维基词条：Wikipedia - Procedural programming 以及引用的文章，讲得比较细致和清晰。更详细的可以查找专门讲编程范式的书籍来浏览。 面向过程编程（Procedural programming） 面向过程就是拆分和逐步实现，前一小节已经说过了，这里不再累述。 命令式编程（Imperative programming） 有时候面向过程编程又称为命令式编程，两者经常混用，但它们之间还是有一点差别的。面向过程编程依赖于块和域，比方说有 while，for 等保留字；而命令式编程则没有这样的特征，一般采用 goto 或者分支来实现。 面向对象编程（Object-oriented programming） 面向对象也在上一小节简单介绍过了，它比面向过程编程抽象程度更高，面向过程将一个编程任务划分为若干变量、数据结构和子程序的组合，而面向对象则是划分为对象，对象的行为（方法）、使用对象的数据（成员/属性）的接口。面向过程编程使用子程序去操作数据，而面向对象则把这两者结合为对象，每一个对象的方法作用在自身上。 函数式编程（Functional programming） 在模块化和代码复用上，函数式编程和面向过程编程是很像的。但函数式编程中不再强调指令（赋值语句）。面向过程编写出的程序是一组指令的集合，这些指令可能会隐式地修改了一些公用的状态，而函数式编程则规避了这一点，每一个语句都是一个表达式，只依赖于自己而不依赖外部状态，因为我们现在所用的计算机都是基于指令运作的，所以函数式编程的效率会稍低一些，但是函数式编程一个很大的好处就是，既然每个语句都是独立的，那么就很容易实现并行化了。 逻辑编程（Logic programming） 逻辑编程是一种我们现在比较少接触的变成范式，它关注于表达问题是什么是不是怎样解决问题。感兴趣的话可以了解一下Prolog语言。 面向对象编程的三大特点 数据封装 继承 多态 类和实例 在Python中定义类是通过 class 关键字完成的，像前面例子一样： 12class Student(object): pass 定义类的格式是 class 类名(继承的类名) 。 类名一般用大写字母开头，关于继承的知识会留在后续的章节里详述，object类是所有类的父类，在Python3中不写也行（既可以写作 class Student: 或 class Student():），会自动继承，详情可以看python class inherits object。 定义类以后，即使没有定义构造函数和属性，我们也可以实例化对象： 12345&gt;&gt;&gt; bart = Student()&gt;&gt;&gt; bart&lt;__main__.Student object at 0x10a67a590&gt;&gt;&gt;&gt; Student&lt;class '__main__.Student'&gt; 可以看到变量bart指向的就是一个Student类的实例，每个实例的地址是不一样的。 与静态语言不同，Python作为动态语言，我们可以自由地给一个实例变量绑定属性： 123&gt;&gt;&gt; bart.name = 'Bart Simpson'&gt;&gt;&gt; bart.name'Bart Simpson' 绑定的属性在定义类时无须给出，随时都可以给一个实例绑定新属性。 但是！这样绑定的新属性仅仅绑定在这个实例上，别的实例是没有的！ 也就是说，同一个类的多个实例例拥有的属性可能不同！ 对于我们认为必须绑定的属性，可以通过定义特殊的 __init__ 方法进行初始化，在创建实例时就进行绑定： 12345class Student(object):def __init__(self, name, score): self.name = name self.score = score 使用 __init__ 方法要注意： 第一个参数永远是 self，指向创建出的实例本身。 有了 __init__ 方法，创建实例时就不能传入空的参数，必须传入与 __init__ 方法匹配的参数，参数self不用传，Python解释器会自动传入。 12345&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.name'Bart Simpson'&gt;&gt;&gt; bart.score59 数据封装 数据封装是面向对象编程特点之一，对于类的每个实例而言，属性访问可以通过函数来实现。 既然实例本身拥有属性数据，那么访问实例的数据就不需要通过外面的函数实现，可以直接在类的内部定义访问数据的函数。 利用内部定义的函数，就把数据封装起来了，这些函数和类本身是关联的，称为类的方法。 12345678class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 依然是前面的例子，可以看到在类中定义方法和外面定义的函数唯一区别就是方法的第一个参数永远是实例变量 self。其他一致，仍然可以用默认参数，可变参数，关键字参数和命名关键字参数等参数形式。和创建实例一样，调用方法时不需传入self。 对于外部，类的方法实现细节不用了解，只需要知道怎样调用，能返回什么就可以了。 访问限制 尽管前一节中我们把数据用方法进行了封装，但事实上，实例化对象后，我们依然可以直接通过属性名来访问一个实例的属性值，并且自由地修改属性值。要让实例的内部属性不被外部访问，只需要在属性的名称前加上两个下划线 __ 就可以了，此时属性就转换成了私有属性。 123456789101112131415161718&gt;&gt;&gt; class Student():... def __init__(self, name, score):... self.__name = name # 绑定私有属性__name... self.__score = score # 绑定私有属性__score... def print_score(self): # 通过方法访问私有属性... print(self.__name, self.__score)...&gt;&gt;&gt; bart = Student('Bart', 98) # 创建一个Student类的实例bart&gt;&gt;&gt; bart.__name # 由于访问限制的保护，对外部而言，实例bart是没有__name这个属性的Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute '__name'&gt;&gt;&gt; bart.__score # 由于访问限制的保护，对外部而言，实例bart是没有__score这个属性的Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute '__score'&gt;&gt;&gt; bart.print_score() # 但是通过print_score方法可以访问该实例的__name属性和__score属性Bart 98 这是怎么实现的呢？其实呀，不能访问的实质是Python解释器对外给私有变量添加了前缀 _类名，比如把 __name 会被改成 _Student__name 。所以我们在外部（即不是通过类的方法）访问时，__name 属性是不存在的，但访问 _Student__name 属性就可以了： 12&gt;&gt;&gt; bart._Student__name'Bart' 所以说，即使有访问限制，外部依然可以访问和修改内部属性，Python没有任何机制预防这一点，只有靠使用者自己注意了。 还有一点必须明白。使用访问限制跟绑定属性是不冲突的，所以虽然对内而言存在 __name 属性，但对外而言这个属性不存在，我们依然可以给实例绑定一个 __name 属性： 12345&gt;&gt;&gt; bart.__name = 'Alice' # 外部绑定__name属性&gt;&gt;&gt; bart.__name # 现在外部可以访问__name属性了'Alice'&gt;&gt;&gt; bart.print_score() # 但对内部方法来说，__name属性依然是原来的Bart 98 Python同样没有任何机制防止我们给实例绑定一个和私有属性同名的属性，从外部是可以访问这样绑定的属性的，但对内部方法而言，这种绑定的赋值不会覆盖私有属性原来的值，所以极容易出错，只有靠使用者自己注意了。 通过 dir(bart) 可以查看实例包含的所有变量（属性和方法）： 12345&gt;&gt;&gt; dir(bart)['_Student__name', '_Student__score', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__','__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__','__name', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__','__subclasshook__', '__weakref__', 'print_score'] 在Python中，变量名类似 __xxx__ 的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量允许直接访问，注意私有变量不要这样取名。有时会看到以一个下划线开头的变量名，比如 _name，这样的变量外部是可以访问的，不属于访问限制，但是按照约定，这样的变量我们应视为私有变量，不应在外部直接访问。 获取和修改限制访问的属性 对于限制访问的属性，外部代码还是需要进行访问和修改的，我们可以在类中定义对应的get方法和set方法： 123456789101112131415&gt;&gt;&gt; class Student():... def __init__(self, name, score):... self.__name = name # 绑定私有属性__name... self.__score = score # 绑定私有属性__score... def print_score(self): # 通过方法访问私有属性... print(self.__name, self.__score)... def get_name(self):... return self.__name... def get_score(self):... return self.__score... def set_score(self, score):... if 0 &lt;= score &lt;= 100:... self.__score = score... else:... raise ValueError('bad score') 通过类的set方法修改属性值，而不直接在外部修改有一个明显的好处，我们可以在类的方法中对参数做检查，避免传入无效的参数。 比如这里可以限制修改成绩时成绩的范围必须是0~100，超出就报错。 继承和多态 基本概念 在OOP程序设计中，当我们定义一个类的时候，可以从某个现有的类继承，新的类称为子类（Subclass），被继承的类则称为基类/父类/超类（Base class/Super class）。比方说我们创建一个Animal类，该类有一个run方法： 123&gt;&gt;&gt; class Animal(object): def run(self): print('Animal is running...') 定义一个Dog类继承Animal类，尽管我们没有为它编写任何方法，但它却可以获得父类的全部功能： 123456&gt;&gt;&gt; class Dog(Animal): pass&gt;&gt;&gt; dog=Dog()&gt;&gt;&gt; dog.run()Animal is running... 我们也可以为子类增加新的方法： 12345678910&gt;&gt;&gt; class Dog(Animal):... def run(self):... print('Dog is running...')... def eat(self):... print('Eating meat...')...&gt;&gt;&gt; dog.run()Dog is running&gt;&gt;&gt; dog.eat()Eating meat 这里我们除了定义一个新的 eat 方法之外，还定义了一个和父类方法同名的 run 方法，它会覆盖父类的方法，当调用子类实例的 run 方法时调用的就会是子类定义的 run 方法而不是父类的 run 方法了，这个特点称作多态。总结一下，如果子类也定义一个和父类相同的方法，则执行时总是调用子类的方法。 实例的数据类型 我们定义一个类，实际就是定义了一种数据类型，和 list，str 等没有什么区别，要判断一个变量是否属于某种数据类型可以使用 isinstance() 方法： 1234&gt;&gt;&gt; isinstance(dog,Animal)True&gt;&gt;&gt; isinstance(dog,Dog)True 实例化子类的对象既属于子类数据类型也属于父类数据类型！但是反过来就不可以，实例化父类的对象不属于子类数据类型。 多态的好处 比方说在外部编写一个函数，接收含有 run 方法的变量作参数： 123def run_twice(animal): animal.run() animal.run() 当传入Animal类的实例时就执行Animal类的 run 方法，当传入Dog类的实例时也能执行Dog类的 run 方法，非常方便： 123456&gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running...&gt;&gt;&gt; run_twice(Dog())Dog is running...Dog is running... 有了多态的特性： 实现同样的功能时，不需要为每个子类都在外部重写一个函数 只要一个外部函数能接收父类实例作参数，则任何子类实例都能直接使用这个外部函数 传入的任意子类实例在调用方法时，调用的都是子类中定义的方法（如果子类没有定义的话就调用父类的） 开闭原则 所谓开闭原则，指的是对扩展开放：允许新增 Animal 类的子类；对修改封闭：父类 Animal 可以调用的外部函数其子类也能直接调用，不需要对外部函数进行修改。 多态真正的威力：调用方只需要关注调用函数的对象，不需要关注所调用的函数内部的细节。当我们新增一种Animal的子类时，只要确保子类的 run() 方法编写正确即可，无须修改要调用的函数。对任意一个对象，我们只需要知道它属于父类类型，无需确切知道它的子类类型具体是什么，就可以放心地调用外部函数，而函数内部调用的方法是属于Animal类、Dog类、还是Cat类，由运行时该对象的确切类型决定。 静态语言 VS 动态语言 对于静态语言(如：Java)，如果函数需要传入 Animal 类型，则传入的参数必须是 Animal 类型或者它的子类类型，否则无法调用 run() 方法。 对于动态语言而言，则不一定要传入Animal类型。只要保证传入的对象有 run() 方法就可以了。 这个特性又称\"鸭子类型\"，即一个对象只需要 \"看起来像鸭子，能像鸭子那样走\" 就可以了。 123class Timer(object): def run(self): print('Start...') 比方说这里的Timer类既不属于Animal类型也不继承自Animal类，但它的实例依然可以传入 run_twice() 函数并且执行自己的 run() 方法。 Python的 file-like object 就是一种鸭子类型。真正的文件对象有一个 read() 方法，能返回其内容。 但是只要一个类中定义有 read() 方法，它的实例就可以被视为 file-like object。许多函数接收的参数都是 file-like object，不一定要传入真正的文件对象，传入任何实现了 read() 方法的对象都可以。 小结 继承可以把父类的所有功能都赋予子类，这样编写子类就不必从零做起，子类只需要新增自己特有的方法，把父类不合适的方法覆盖重写就可以了。 动态语言的鸭子类型特点决定了继承不像静态语言那样是必须的。 获取对象信息 这一小节主要介绍给定一个对象，如何了解对象的类型以及有哪些方法。 type函数 使用type函数获得各种变量的类型： 123456789101112&gt;&gt;&gt; type('str')&lt;class 'str'&gt;&gt;&gt;&gt; type(None)&lt;type(None) 'NoneType'&gt;&gt;&gt;&gt; type(abs)&lt;class 'builtin_function_or_method'&gt;&gt;&gt;&gt; animal = Animal()&gt;&gt;&gt; type(animal)&lt;class '__main__.Animal'&gt;&gt;&gt;&gt; dog = Dog()&gt;&gt;&gt; type(dog)&lt;class '__main__.Dog'&gt; type() 函数返回值是参数所属的类型，而这个返回值本身则属于type类型： 12&gt;&gt;&gt; type(type('123'))&lt;class 'type'&gt; 可以用来进行类型判断： 1234&gt;&gt;&gt; type('123')==strTrue&gt;&gt;&gt; type('123') == type(123)False 导入内建的 types 模块还可以做更多更强大的类型判断： 123456789101112&gt;&gt;&gt; import types&gt;&gt;&gt; def fn():... pass...&gt;&gt;&gt; type(fn)==types.FunctionType # 判断变量是否函数True&gt;&gt;&gt; type(abs)==types.BuiltinFunctionType # 判断变量是否内建函数True&gt;&gt;&gt; type(lambda x: x)==types.LambdaType # 判断变量是否匿名函数True&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorType # 判断变量是否生成器True isinstance函数 判断类型除了使用 type() 函数之外，使用 isinstance() 函数也能达到一样的效果： 12345678&gt;&gt;&gt; isinstance('a', str)True&gt;&gt;&gt; isinstance(123, int)True&gt;&gt;&gt; isinstance(b'a', bytes)True&gt;&gt;&gt; isinstance([1, 2, 3], (list, tuple))True 并且 isinstance() 函数的参数二还可以是一个tuple，此时 isinstance() 函数将判断参数一是否属于参数二tuple中所有类型中的其中一种，只要符合其中一种则返回 True。 对于类的继承关系来说，type() 函数不太合适，因为我们没办法知道一个子类是否属于继承自某个类，使用 isinstance() 函数就可以解决这个问题了。子类的实例也是父类的实例。 12345678910&gt;&gt;&gt; animal = Animal()&gt;&gt;&gt; dog = Dog()&gt;&gt;&gt; isinstance(animal, Animal)True&gt;&gt;&gt; isinstance(animal, Dog) # 父类实例不是子类实例False&gt;&gt;&gt; isinstance(dog, Animal) # 子类实例同时也是父类的实例True&gt;&gt;&gt; isinstance(dog, Dog)True dir函数 dir() 函数返回一个对象的所有属性和方法： 1234567891011&gt;&gt;&gt; dir('ABC')['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__','__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__','__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__','__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__','__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs','find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier','islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower','lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit','rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper','zfill'] 形如 __xxx__ 的属性和方法都是有特殊用途的，比如 __len__ 方法会返回对象长度，不过我们一般直接调用内建函数 len() 获取一个对象的长度。而事实上，len() 函数内部就是通过调用对象的 __len__() 方法来获取长度的。两种写法都可以： 1234&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; 'ABC'.__len__()3 如果自己写的类希望能用 len() 函数获取对象的长度，可以在定义类时实现一个 __len__() 方法，例如： 1234567&gt;&gt;&gt; class MyDog(object):... def __len__(self):... return 100...&gt;&gt;&gt; dog = MyDog()&gt;&gt;&gt; len(dog)100 hasattr函数、setattr函数、getattr函数 首先定义一个类，并创建一个该类的实例 obj： 1234567&gt;&gt;&gt; class MyObject(object):... def __init__(self):... self.x = 9... def power(self):... return self.x * self.x...&gt;&gt;&gt; obj = MyObject() 利用 hasattr() 函数、setattr() 函数、getattr() 函数可以分别实现判断属性/方法是否存在，绑定/赋值属性/方法，以及获取属性值/方法的功能： 12345678910111213141516&gt;&gt;&gt; hasattr(obj, 'x') # 有属性x吗？True&gt;&gt;&gt; obj.x # 直接访问属性x9&gt;&gt;&gt; setattr(obj, 'x', 10) # 为属性x设置新的值&gt;&gt;&gt; obj.x # 直接访问属性x10&gt;&gt;&gt; hasattr(obj, 'y') # 有属性y吗？False&gt;&gt;&gt; setattr(obj, 'y', 19) # 绑定一个新属性y&gt;&gt;&gt; hasattr(obj, 'y') # 有属性y吗？True&gt;&gt;&gt; obj.y # 直接访问属性y19&gt;&gt;&gt; getattr(obj, 'y') # 获取属性y19 如果试图获取不存在的属性，会抛出 AttributeError 的错误： 12345678&gt;&gt;&gt; obj.z # 直接访问属性zTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'MyObject' object has no attribute 'z'&gt;&gt;&gt; getattr(obj, 'z') # 获取属性zTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'MyObject' object has no attribute 'z' 特别地，我们可以为 getattr() 函数传入一个额外参数表示默认值，这样当属性不存在时就会返回默认值，而不是抛出错误了，但要注意，getattr()` 并不会把这个默认值绑定到对象： 123456&gt;&gt;&gt; getattr(obj, 'z', 404) # 获取属性z，如果不存在，返回默认值404404&gt;&gt;&gt; obj.z # 没有进行绑定，所以obj仍然没有属性zTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'MyObject' object has no attribute 'z' getattr() 函数除了可以获取属性之外，也可以用来获取方法并且赋值到变量，然后再通过变量使用： 1234567&gt;&gt;&gt; fn = getattr(obj, 'power') # 获取方法 power() 并赋值给变量fn&gt;&gt;&gt; fn # fn指向obj.power&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn() # 调用fn100&gt;&gt;&gt; obj.power() # 结果和调用obj.power是一样的100 看到这里也许有一些疑问，为什么我们明明可以通过 obj.x = 10 的方式设置属性值，还要用 setattr(obj, 'x', 10) 呢？ 为什么我们明明可以通过 obj.y 直接访问属性值，还要用 getattr(obj, 'x') 呢？显然后者的写法要繁琐得多。确实，前面举得例子中，我们都没有任何必要这样写，也不应该这样写。setattr() 函数和 getattr() 函数是为了一些更特别的情况而创造的，例如： 1234567&gt;&gt;&gt; attrname = 'x'&gt;&gt;&gt; getattr(obj, attrname)10&gt;&gt;&gt; obj.attrnameTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'MyObject' object has no attribute 'attrname' 当属性名绑定在一个变量上时，显然直接访问就没有办法使用了，但 getattr() 函数则不存在这方面的问题。并且前面也提到了 getattr() 函数允许我们设置默认返回值，这时直接访问无法做到的。又例如： 123456789101112&gt;&gt;&gt; attr = {'x':9, 'y':19, 'z':29}&gt;&gt;&gt; attr.items()dict_items([('y', 19), ('z', 29), ('x', 9)])&gt;&gt;&gt; for k,v in attr.items():... setattr(obj, k, v)...&gt;&gt;&gt; for k in attr.keys():... getattr(obj, k)...19299 我们可以非常方便地把属性名和属性值存储在一个dict里面，然后利用循环进行赋值，而无需显式地写出 obj.x = 9，obj.y = 19，和 obj.z = 29，当我们需要批量赋值大量属性时，好处就体现出来了。同样地，我们也可以利用循环来读取需要的每个属性的值，而无需显式地逐个属性进行访问。 当然，如果可以直接写 sum = obj.x + obj.y 就不要写： sum = getattr(obj, 'x') + getattr(obj, 'y')，属性少的时候完全没有必要给自己添麻烦，对编写和阅读代码都不友好。 最后举个使用 hasattr() 函数的例子，比方说读取对象fp，我们可以首先判断fp是否有 read() 方法，有则进行读取，无则直接返回空值： 1234def readSomething(fp): if hasattr(fp, 'read'): return readData(fp) return None 实例属性和类属性 由于Python是动态语言，根据类创建的实例可以任意绑定属性。 要给实例绑定属性除了通过实例变量之外，也可以通过self变量来完成绑定： 123456class Student(object): def __init__(self, name): self.name = name # 通过self变量绑定属性s = Student('Bob')s.score = 90 # 通过实例变量绑定属性 但是，如果Student类本身需要绑定一个属性呢？可以直接在类中定义属性，这种属性是类属性，归Student类所有： 12class Student(object): name = 'Student' 当我们定义了一个类属性后，这个类属性虽然归类所有，但类的所有实例都可以访问到： 12345678910111213141516&gt;&gt;&gt; class Student(object):... name = 'Student'...&gt;&gt;&gt; s = Student() # 创建实例s&gt;&gt;&gt; print(s.name) # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性Student&gt;&gt;&gt; print(Student.name) # 打印类的name属性Student&gt;&gt;&gt; s.name = 'Michael' # 给实例绑定name属性&gt;&gt;&gt; print(s.name) # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性Michael&gt;&gt;&gt; print(Student.name) # 但是类属性并未消失，用Student.name仍然可以访问Student&gt;&gt;&gt; del s.name # 删除实例的name属性&gt;&gt;&gt; print(s.name) # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了Student 从上面的例子可以看出，在编写程序的时候，千万不要给实例属性和类属性设置相同的名字，因为相同名称的实例属性将屏蔽掉类属性。但是删除实例属性后，再使用相同的名称进行访问，返回的就会变回类属性了。","link":"/posts/31432.html"},{"title":"常用内建模块（上）","text":"Python之所以自称 “batteries included”，就是因为内置了许多非常有用的模块，无需额外安装和配置，即可直接使用。 本章将介绍一些常用的内建模块。 datetime datetime 是Python处理日期和时间的标准库。 获取当前日期和时间 我们先看如何获取当前日期和时间： 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now() # 获取当前datetime&gt;&gt;&gt; print(now)2015-05-18 16:28:07.198690&gt;&gt;&gt; print(type(now))&lt;class 'datetime.datetime'&gt; 注意到，datetime 模块包含一个 datetime类，我们可以通过 from datetime import datetime 来导入 datetime 类。如果仅使用 import datetime 导入，则必须引用全名 datetime.datetime 才能使用这个类。datetime.now() 可以返回当前日期和时间，其类型是 datetime.datetime。 获取指定日期和时间 要指定某个日期和时间，我们直接用参数构造一个 datetime 类实例： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; print(dt)2015-04-19 12:20:00 datetime转换为timestamp 在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00时区的时刻称为新纪元时间（epoch time），记为数字0（1970年以前的时间timestamp为负数），当前时间就是相对于epoch time的秒数，称为timestamp。 你可以认为： 1timestamp = 0 # 等价于1970-1-1 00:00:00 UTC+0:00 对应的北京时间是： 1timestamp = 0 # 等价于1970-1-1 08:00:00 UTC+8:00 也即epoch time是北京时间1970年1月1日的早上8点钟。 可见timestamp的值与时区毫无关系，无论在哪个时区，同一时刻timestamp的值都相同，实际时间只需要再按时区推算就可以了。这就是为什么计算机存储的当前时间是以timestamp表示的，因为全球各地的计算机在任意时刻的timestamp都是完全相同的（假定时间已校准）。 把一个 datetime 类型转换为timestamp只需要简单调用 timestamp() 方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime&gt;&gt;&gt; dt.timestamp() # 把datetime转换为timestamp1429417200.0 注意Python的timestamp是一个浮点数。如果有小数位，小数位表示毫秒数。 某些编程语言（如Java和JavaScript）的timestamp使用整数表示毫秒数，这种情况下只需要把timestamp除以1000就得到Python的浮点表示方法。 timestamp转换为datetime 要把timestamp转换为 datetime，使用 datetime 提供的 fromtimestamp() 方法： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t))2015-04-19 12:20:00 注意到timestamp是一个浮点数，它没有时区的概念，而datetime是有时区的。上述转换是在timestamp和本地时间做转换（本地时间是指当前操作系统设定的时区）。例如北京时区是东8区，则本地时间： 12015-04-19 12:20:00 实际上就是UTC+8:00时区的时间，也即： 12015-04-19 12:20:00 UTC+8:00 而此刻的格林威治标准时间与北京时间差了8小时，也就是UTC+0:00时区的时间应该是： 12015-04-19 04:20:00 UTC+0:00 timestamp也可以直接被转换到UTC标准时区的时间： 123456&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; t = 1429417200.0&gt;&gt;&gt; print(datetime.fromtimestamp(t)) # 本地时间2015-04-19 12:20:00&gt;&gt;&gt; print(datetime.utcfromtimestamp(t)) # UTC时间2015-04-19 04:20:00 str转换为datetime 很多时候，用户输入的日期和时间是字符串，要处理日期和时间，首先必须把 str 转换为 datetime。转换方法是通过 datetime.strptime() 实现的： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; cday = datetime.strptime('2015-6-1 18:19:59', '%Y-%m-%d %H:%M:%S')&gt;&gt;&gt; print(cday)2015-06-01 18:19:59 字符串 '%Y-%m-%d %H:%M:%S' 规定了日期和时间部分的格式。当然也可以根据实际需要进行修改，详细的说明请参考Python文档。注意，转换后的 datetime 是不带时区信息的。 datetime转换为str 如果已经有了 datetime 对象，要把它格式化为字符串显示给用户，就需要转换为 str，转换方法是通过 strftime() 实现的： 1234&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; print(now.strftime('%a, %b %d %H:%M'))Mon, May 05 16:28 datetime加减 对日期和时间进行加减实际上就是把 datetime 往后或往前计算，得到新的 datetime。加减可以直接用 + 和 - 运算符，不过需要导入 timedelta 这个类： 12345678910&gt;&gt;&gt; from datetime import datetime, timedelta&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(hours=10)datetime.datetime(2015, 5, 19, 2, 57, 3, 540997)&gt;&gt;&gt; now - timedelta(days=1)datetime.datetime(2015, 5, 17, 16, 57, 3, 540997)&gt;&gt;&gt; now + timedelta(days=2, hours=12)datetime.datetime(2015, 5, 21, 4, 57, 3, 540997) 可见，使用 timedelta 你可以很容易地算出前几天和后几天的时刻。 我们也可以使用 timedelta 很方便地计算出两个日期之间的差： 1234&gt;&gt;&gt; f = datetime(2016,8,19)&gt;&gt;&gt; p = datetime(2017,1,16)&gt;&gt;&gt; print(p-f)150 days, 0:00:00 本地时间转换为UTC时间 本地时间是指系统设定时区的时间，例如北京时间是UTC+8:00时区的时间，而UTC时间指UTC+0:00时区的时间。datetime 类型有一个时区属性 tzinfo，但是默认为 None，所以无法区分这个 datetime 到底是哪个时区，除非强行给 datetime 设置一个时区： 12345678910&gt;&gt;&gt; from datetime import datetime, timedelta, timezone&gt;&gt;&gt; tz_utc_8 = timezone(timedelta(hours=8)) # 创建时区UTC+8:00&gt;&gt;&gt; now = datetime.now()&gt;&gt;&gt; nowdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012)&gt;&gt;&gt; dt = now.replace(tzinfo=tz_utc_8) # 强制设置为UTC+8:00&gt;&gt;&gt; dtdatetime.datetime(2015, 5, 18, 17, 2, 10, 871012, tzinfo=datetime.timezone(datetime.timedelta(0, 28800)))&gt;&gt;&gt; print(dt)2015-05-18 17:02:10.871012+08:00 如果系统时区恰好是UTC+8:00，那么上述代码就是正确的，否则，不应该强制设置为时区信息。 时区转换 我们可以先通过 utcnow() 拿到当前的UTC时间，再转换为任意时区的时间： 12345678910111213141516# 拿到UTC时间，并强制设置时区为UTC+0:00:&gt;&gt;&gt; utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)&gt;&gt;&gt; print(utc_dt)2015-05-18 09:05:12.377316+00:00# astimezone()将转换时区为北京时间:&gt;&gt;&gt; bj_dt = utc_dt.astimezone(timezone(timedelta(hours=8)))&gt;&gt;&gt; print(bj_dt)2015-05-18 17:05:12.377316+08:00# astimezone()将utc_dt转换时区为东京时间:&gt;&gt;&gt; tokyo_dt = utc_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt)2015-05-18 18:05:12.377316+09:00# astimezone()将bj_dt转换时区为东京时间:&gt;&gt;&gt; tokyo_dt2 = bj_dt.astimezone(timezone(timedelta(hours=9)))&gt;&gt;&gt; print(tokyo_dt2)2015-05-18 18:05:12.377316+09:00 注意，上述代码中首先通过 utcnow() 方法获取到一个处于UTC时间的 datetime 实例（按照系统时间推算出的，所以如果系统时间错则得到的实例也是错的），这个实例本身没有时区信息，所以要通过 replace() 方法强制设置时区信息为UTC时区。然后利用这个带时区的 datetime，通过 astimezone() 方法就可以转换到任意时区了。注意，任何带时区（不必是UTC时区）的 datetime 都可以被正确转换到别的时区，例如上述 bj_dt 到 tokyo_dt 的转换。 小结 datetime 表示的时间需要时区信息才能确定一个特定的时间，否则只能视为本地时间。 如果要存储 datetime，最佳方法是将其转换为 timestamp 再存储，因为 timestamp 的值与时区完全无关。 练习 假设你获取了用户输入的日期和时间如 2015-1-21 9:01:30，以及一个时区信息如 UTC+5:00，均是 str，请编写一个函数将其转换为 timestamp： 代码： 12345678910111213141516171819202122232425# -*- coding:utf-8 -*-import refrom datetime import datetime, timezone, timedeltadef to_timestamp(dt_str, tz_str): tz_match = re.match(r'UTC([\\+\\-]\\d+?):00', tz_str) idate = datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S') idate = idate.replace(tzinfo=timezone(timedelta(hours=int(tz_match.group(1))))) return idate.timestamp()# 测试:t1 = to_timestamp('2015-6-1 08:10:30', 'UTC+7:00')if t1 == 1433121030.0: print('Pass.')else: print('Fail.')t2 = to_timestamp('2015-5-31 16:10:30', 'UTC-09:00')if t2 == 1433121030.0: print('Pass.')else: print('Fail.') collections 简介 collections 是Python内建的一个集合模块，提供了许多有用的集合类。 namedtuple 我们知道 tuple 可以表示不变集合，例如，一个点的二维坐标就可以表示成： 1&gt;&gt;&gt; p = (1, 2) 但是，代码中使用 (1, 2) 的方式来写很难看出这个 tuple 是用来表示一个点的。如果我们特地为定义一个“点”类而编写代码，又有点小题大作。怎么办呢？collections 模块中的 namedtuple 可以帮到我们： 1234567&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; p.x1&gt;&gt;&gt; p.y2 namedtuple() 是一个函数，它可以用来方便地创建一个按需定制的 tuple 类的子类，我们可以把这样得到的子类看作一种特殊的元组，可以采用访问属性的方式来引用该元组的元素，使用十分方便。 可以验证创建的 Point 是 tuple 的一种子类： 1234&gt;&gt;&gt; isinstance(p, Point)True&gt;&gt;&gt; isinstance(p, tuple)True 类似的，如果要用坐标和半径表示一个圆，也可以用 namedtuple 来创建一个 Circle 类： 12# namedtuple('名称', [属性list]):Circle = namedtuple('Circle', ['x', 'y', 'r']) deque 使用 list 存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为 list 是线性存储，数据量大的时候，插入和删除效率很低。deque 是为了更高效实现插入和删除操作而创造的双向列表，适合用于队列和栈： 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x']) deque 除了实现 list 的 append() 和 pop() 外，还支持 appendleft() 和 popleft()，这样就可以非常高效地往头部添加或删除元素。 defaultdict 使用 dict 时，如果引用的Key不存在，就会抛出 KeyError。如果希望key不存在时也能返回一个默认值，可以使用 defaultdict： 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A' 注意默认值是调用函数返回的，而函数在创建 defaultdict 对象时传入。除了在Key不存在时返回默认值，defaultdict 的其他行为跟 dict 是完全一样的。 OrderedDict 使用 dict 时，Key是无序的。在对 dict 做迭代时，我们无法确定Key的顺序。 如果要保持Key的顺序，可以用 OrderedDict： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的{'a': 1, 'c': 3, 'b': 2}&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)]) 注意，OrderedDict 的Key是按照插入的顺序排列的，不是Key本身排序： 123456&gt;&gt;&gt; od = OrderedDict()&gt;&gt;&gt; od['z'] = 1&gt;&gt;&gt; od['y'] = 2&gt;&gt;&gt; od['x'] = 3&gt;&gt;&gt; list(od.keys()) # 按照插入的Key的顺序返回['z', 'y', 'x'] OrderedDict 可以实现一个FIFO（先进先出）的 dict，当容量超出限制时，优先删除最早添加的Key： 12345678910111213141516171819from collections import OrderedDictclass LastUpdatedOrderedDict(OrderedDict): def __init__(self, capacity): super(LastUpdatedOrderedDict, self).__init__() self._capacity = capacity def __setitem__(self, key, value): containsKey = 1 if key in self else 0 if len(self) - containsKey &gt;= self._capacity: last = self.popitem(last=False) print('remove:', last) if containsKey: del self[key] print('set:', (key, value)) else: print('add:', (key, value)) OrderedDict.__setitem__(self, key, value) 解析一下上面的例子，我们实现了一个容量有限的先进先出的 dict —— LastUpdatedOrderedDict，在实例化时，__init__() 方法会被调用来初始化要创建的实例。self 参数指的便是要创建的实例本身，此外我们传入一个 capacity 参数用来表示这个 dict 的容量。需要注意，在这个 __init__() 方法中，我们使用 super(LastUpdatedOrderedDict, self).__init__() 这个语句来进行初始化，它会自动找到 LastUpdatedOrderedDict 的父类，把实例 self 转换为一个特殊的父类对象，从而可以调用父类的 __init__() 方法。self._capacity = capacity 这一句则是给实例绑定容量属性，使用 _capacity 这个变量名来和传入参数区别开来，前置单下划线表示这个属性应被视作私有属性来使用。 再看看 __setitem__() 方法，首先判断一下设置的key在 dict 里是否存在，如果存在则 containsKey 为1否则为0。接下来判断容量是否超出，如果超出则pop掉最前面的一个key-value对。然后再判断key是否存在，是则先删除掉原来的key-value对。最后调用父类的 __setitem__() 方法来完成赋值。 Counter Counter 是一个简单的计数器，例如，统计字符出现的个数： 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter({'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1}) Counter 实际上也是 dict 的一个子类，上面的结果可以看出，字符 'g'、'm'、'r' 各出现了两次，其他字符各出现了一次。 小结 collections 模块提供了一些有用的集合类，我们可以根据需要选用。 base64 简介 Base64是一种用64个字符来表示任意二进制数据的方法。 用记事本打开exe、jpg、pdf这些文件时，我们都会看到一大堆乱码，因为二进制文件包含很多无法显示和打印的字符，所以，如果要让记事本这样的文本处理软件能处理二进制数据，就需要一个二进制到字符串的转换方法。Base64是一种最常见的二进制编码方法。 原理 Base64的原理很简单，首先，它使用总共64个字符进行编码（26个大小写字母+10个数字+加号+左斜杠）： 1['A', 'B', 'C', ... 'a', 'b', 'c', ... '0', '1', ... '+', '/'] 然后，对二进制数据进行处理时，每3个字节一组，就得到 3x8=24 bit，重新划分为4组，每组正好6个bit，有 2^6=64 种取值，对应64个字符的一个，： base64-encode 这样我们就可以把二进制数据的3个字节使用4个字符来表示，这就是Base64编码。采用Base64编码会把3字节的二进制数据编码为4字节（4个字符所以是4字节）的文本数据，长度会增加33%。但好处是编码后的文本数据可以在邮件正文、网页等直接显示，而不会出现乱码的情况。 如果要编码的二进制数据不是3的倍数，最后多出1个或2个字节怎么办呢？ Base64编码采用 \\x00 字节在末尾补充到3个字节，在编码的末尾会使用 = 号表示补了多少字节。解码的时候，会自动去掉用于补足的 \\x00 字节。 Python中的实现方式 Python内置的 base64 模块可以直接进行base64的编解码： 12345&gt;&gt;&gt; import base64&gt;&gt;&gt; base64.b64encode(b'binary\\x00string')b'YmluYXJ5AHN0cmluZw=='&gt;&gt;&gt; base64.b64decode(b'YmluYXJ5AHN0cmluZw==')b'binary\\x00string' 上面的代码中我们使用了 b 把字符串转为二进制，然后传入 b64encode() 函数进行编码，字符串长度为13个字节（注意 \\x00 是一个字符，占1字节）。可以看到编码后所得字符串20个字节，并且使用两个 = 号表明编码过程中，由于13无法整除3，所以末尾补充了两个 \\x00。注意，= 号是算在20个字节（15÷3×4=20 bit）里面的，而不是额外放在编码后的字符串后面。 由于标准的Base64编码后可能出现字符 + 和 /，在URL中就不能直接作为参数，所以又有一种 \"url safe\" 的base64编码，把字符 + 和 / 分别替换成 - 和 _： 123456&gt;&gt;&gt; base64.b64encode(b'i\\xb7\\x1d\\xfb\\xef\\xff')b'abcd++//'&gt;&gt;&gt; base64.urlsafe_b64encode(b'i\\xb7\\x1d\\xfb\\xef\\xff')b'abcd--__'&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')b'i\\xb7\\x1d\\xfb\\xef\\xff' 我们也可以自定义64个字符的排列顺序，也即自定义Base64编码，不过，通常情况下没有必要这样做。注意，Base64仅仅是一种通过查表进行编码的方法，不能用于加密，即使使用自定义的编码表也不行（依然能很容易被破解）。Base64适用于小段内容的编码，比如数字证书签名、Cookie的内容等。由于 = 字符也可能出现在Base64编码中，但 = 用在URL、Cookie里面会造成歧义，所以，很多人会在Base64编码后把 = 去掉： 1234# 标准Base64:'abcd' -&gt; 'YWJjZA=='# 自动去掉=:'abcd' -&gt; 'YWJjZA' 那么去掉 = 后解码要怎样完成呢？不用担心，因为Base64是把3个字节变为4个字节，所以，Base64编码的长度一定是4的倍数，解码时我们只需要在末尾加上足够的 = 把Base64字符串的长度变回4的倍数，就可以正常解码了。 小结 Base64是一种把任意二进制转换为文本字符串的编码方法，常用于在URL、Cookie、网页中传输少量二进制数据。 练习 请写一个能兼容去掉 = 的base64编码字符串的解码函数： 代码： 12345678910111213141516# -*- coding: utf-8 -*-import base64def safe_base64_decode(s): remainder = len(s) % 4 if remainder == 0: return base64.b64decode(s) else: return base64.b64decode(s+remainder*b'=')# 测试:assert b'abcd' == safe_base64_decode(b'YWJjZA=='), safe_base64_decode('YWJjZA==')assert b'abcd' == safe_base64_decode(b'YWJjZA'), safe_base64_decode('YWJjZA')print('Pass') struct 为何需要struct 准确地讲，Python没有专门处理字节的数据类型。但由于 b'str' 可以用来表示字节，所以在Python中可以认为 字节数组＝二进制str。而在C语言中，我们可以很方便地用 struct、union 等库来处理字节以及字节和int，float的转换。 假设我们要把一个32位无符号整数转换为字节（4个bytes）。在Python中，得这么写： 12345678&gt;&gt;&gt; n = 10240099&gt;&gt;&gt; b1 = (n &amp; 0xff000000) &gt;&gt; 24&gt;&gt;&gt; b2 = (n &amp; 0xff0000) &gt;&gt; 16&gt;&gt;&gt; b3 = (n &amp; 0xff00) &gt;&gt; 8&gt;&gt;&gt; b4 = n &amp; 0xff&gt;&gt;&gt; bs = bytes([b1, b2, b3, b4])&gt;&gt;&gt; bsb'\\x00\\x9c@c' 稍微解析一下，十进制数 10240099 转换为十六进制是 0x009c4063，这里使用与运算和右移来拆分出这个32位无符号整数的每一个byte（注意得到的不是字节数组而是一个整数），其中 b1=0, b2=156, b3=64, b4=99，将这四个整数放入一个列表中传入 bytes() 函数，就能得到字节数组了。注意 bs 中是有4个字节的，len(bs) 的值为4。由于对应整数64的十六进制数 0x40 属于ASCII码范围，所以用字符 @ 表示，而对应整数99的十六进制数 0x63 则对应字符 c。 可以看到，这样要逐个byte来拆分，再进行转换实在是非常麻烦。如果要把浮点数转换为字节就无能为力了。幸好，Python提供了一个 struct 模块来解决 bytes 和其他二进制数据类型的转换问题。 struct的用法 struct 模块的 pack 函数把任意数据类型变成 bytes： 123&gt;&gt;&gt; import struct&gt;&gt;&gt; struct.pack('&gt;I', 10240099)b'\\x00\\x9c@c' pack() 的第一个参数是处理指令，这里 '&gt;I' 的意思分为两部分： 字节顺序：&gt; 表示的是使用大端序作为字节顺序。 数据类型：I 表示的是要转换一个32位无符号整数。可以有多个数据类型从而转换出多个数。 第二个参数要注意和处理指令一致。 unpack() 函数与 pack() 相反，它是把 bytes 转换为相应的数据类型： 12&gt;&gt;&gt; struct.unpack('&gt;IH', b'\\xf0\\xf0\\xf0\\xf0\\x80\\x80')(4042322160, 32896) 这里的 &gt;IH 表示这个字节数组用的是大端序，并且要依次转换出一个32位无符号整数和16位无符号整数。 struct 模块定义的数据类型可以参考Python官方文档。关于大端序（big-endian，BE）和小端序（little-endian，LE）的区别可以看看这篇博文，讲解得很清晰。 使用struct分析bmp文件 Windows的位图文件（.bmp）是一种非常简单的文件格式，我们可以使用 struct 来分析一下。 首先找到一个bmp文件，没有的话用Windows自带的【画图】画一个即可。读入它的前30个字节来分析： 1&gt;&gt;&gt; s = b'\\x42\\x4d\\x38\\x8c\\x0a\\x00\\x00\\x00\\x00\\x00\\x36\\x00\\x00\\x00\\x28\\x00\\x00\\x00\\x80\\x02\\x00\\x00\\x68\\x01\\x00\\x00\\x01\\x00\\x18\\x00' BMP格式采用小端序的方式存储数据，文件头的结构按顺序如下： 两个字节：'BM'表示Windows位图，'BA'表示OS/2位图； 一个32位无符号整数：表示位图大小； 一个32位无符号整数：保留位，始终为0； 一个32位无符号整数：实际图像的偏移量； 一个32位无符号整数：Header的字节数； 一个32位无符号整数：图像宽度(单位为像素)； 一个32位无符号整数：图像高度(单位为像素)； 一个16位无符号整数：始终为1； 一个16位无符号整数：颜色数。 所以，组合起来用unpack读取： 12&gt;&gt;&gt; struct.unpack('&lt;ccIIIIIIHH', s)(b'B', b'M', 691256, 0, 54, 40, 640, 360, 1, 24) 这里的 c 表示对应的数据类型是一个字符。结果显示，b'B'、b'M' 说明是一张Windows位图，位图大小为640x360，颜色数为24。 小结 尽管Python不适合编写底层操作字节流的代码，但在对性能要求不高的地方，利用 struct 操作能够更加方便。 练习 请编写一个bmpinfo.py，可以检查任意文件是否是Windows位图文件，如果是，打印出图片大小和颜色数。 代码： 1234567891011121314151617181920212223242526272829#-*- coding: utf-8 -*-import sysimport structdef readBmpFile(file): f = open(file, 'rb') bs = f.read() f.close() return bs[0:30]def checkBmp(info): ts = struct.unpack('&lt;ccIIIIIIHH', info) if ts[0] == b'B' and ts[1] == b'M': print('图片大小：%d * %d' % (ts[6], ts[7])) print('颜色数：%d' % ts[9]) else: print('非位图文件')if __name__=='__main__': if len(sys.argv) == 2: info = readBmpFile(sys.argv[1]) checkBmp(info) else: info = input('Please input the file name: ') checkBmp(info) hashlib 摘要算法简介 Python的 hashlib 模块提供了常见的摘要算法，如MD5，SHA1等等。 什么是摘要算法呢？摘要算法又称哈希算法、散列算法。它通过一个摘要函数（也称哈希函数），把任意长度的数据转换为一个固定长度的数据串（称为摘要（digest），通常表示为由16进制数字组成的字符串）。 摘要函数应当是一个单向函数，也即计算摘要容易，但通过摘要反推原始数据却非常困难。并且即使仅对原始数据做一个bit的修改也会导致计算出的摘要完全不同。 Python实现 以常见的摘要算法MD5为例，计算一个字符串的MD5值： 12345import hashlibmd5 = hashlib.md5()md5.update('how to use md5 in python hashlib?'.encode('utf-8'))print(md5.hexdigest()) 计算结果如下： 1d26a53750bc40b38b65a520292f69306 如果数据量很大，我们可以多次调用 update() 来传入新数据。只要保证输入一致，那么最后计算的结果一定都是一样的： 123456import hashlibmd5 = hashlib.md5()md5.update('how to use md5 in '.encode('utf-8'))md5.update('python hashlib?'.encode('utf-8'))print(md5.hexdigest()) 但只要改动一个字母，计算的结果就会完全不同。 MD5是最常见的摘要算法，速度很快，所得摘要长度为128 bit，通常用一个32位的16进制字符串表示。 另一种常见的摘要算法是SHA1，调用SHA1和调用MD5完全类似： 123456import hashlibsha1 = hashlib.sha1()sha1.update('how to use sha1 in '.encode('utf-8'))sha1.update('python hashlib?'.encode('utf-8'))print(sha1.hexdigest()) SHA1所得摘要长度为160 bit，通常用一个40位的16进制字符串表示。 比SHA1更安全的算法是SHA256和SHA512，不过越安全的算法不仅越慢，而且摘要的长度也更长。 有没有可能两个不同的数据通过某个摘要算法得到了相同的摘要？完全有可能。因为任何摘要算法都是把无限多的数据集合映射到一个有限的集合中。这种情况称为碰撞（collasion）。 摘要算法应用 摘要算法能应用到什么地方？举个常用例子： 任何允许用户登录的网站都会存储用户登录的用户名和口令。如何存储用户名和口令呢？方法是存到数据库表中： name password michael 123456 bob abc999 alice alice2008 如果以明文保存用户口令，一旦数据库泄露，所有用户的口令就会落入黑客的手里。此外，网站运维人员是可以访问数据库的，如果他们心有不轨，那么也会很容易地获取到所有用户的口令。 正确的保存口令的方式是不存储用户的明文口令，而是存储用户口令的摘要，比如MD5： username password michael e10adc3949ba59abbe56e057f20f883e bob 878ef96e86145580c38c87f0410ad153 alice 99b1c2188db85afee403b1536010c2c9 当用户登录时，首先计算用户输入的明文口令的MD5，然后和数据库存储的MD5对比，如果一致，说明口令输入正确，如果不一致，口令肯定错误。 练习一 根据用户输入的口令计算出对应的MD5值，并验证是否与数据库中保存的一致 代码： 123456789101112131415161718192021222324252627282930313233#-*- coding: utf-8 -*-import sysimport hashlibdb = { 'michael': 'e10adc3949ba59abbe56e057f20f883e', 'bob': '878ef96e86145580c38c87f0410ad153', 'alice': '99b1c2188db85afee403b1536010c2c9'}def calc_md5(password): md5 = hashlib.md5() md5.update(password.encode('utf-8')) return md5.hexdigest()def login(user, password): digest = calc_md5(password) if db[user] == digest: return True else: return Falseif __name__=='__main__': user = input('请输入用户名：') password = input('请输入密码：') if login(user, password): print('密码正确，登录成功。') else: print('密码错误，登录失败。') 采用MD5存储口令是否就一定安全呢？也不一定。假设你是一个黑客，已经拿到了存储MD5口令的数据库，如何通过MD5反推用户的明文口令呢？暴力破解费事费力，真正的黑客不会这么干。 考虑这么个情况，很多用户喜欢用123456，888888，password这些简单的口令。于是，黑客可以事先计算出这些常用口令的MD5值，得到一个反推表： 123'e10adc3949ba59abbe56e057f20f883e': '123456''21218cca77804d2ba1922c33e0151105': '888888''5f4dcc3b5aa765d61d8327deb882cf99': 'password' 这样，无需破解，只需要对比数据库的MD5，黑客就获得了使用常用口令的用户账号。 加盐 对于用户来讲，当然不应该使用过于简单的口令。但是，我们能否在程序设计上对简单口令加强保护呢？ 由于常用口令的MD5值很容易被计算出来，为了加强保护，我们可以对原始口令添加一个复杂字符串（俗称“盐”），这样再计算MD5值就不容易被黑客蒙中了，这种方法俗称“加盐”： 123456def calc_md5(password): md5 = hashlib.md5() md5.update(password.encode('utf-8')) md5.update('the-Salt'.encode('utf-8')) return md5.hexdigest() 经过加盐处理的MD5口令，只要盐不被黑客知道，即使用户使用简单口令，也很难通过MD5被反推出来。 但是如果有两个用户都使用了相同的简单口令比如123456，数据库中就会存储两条相同的MD5值，这说明这两个用户的口令是一样的。有没有办法让使用相同口令的用户存储不同的MD5呢？如果假定用户无法修改登录名，就可以通过把登录名作为“盐”的一部分来计算MD5，从而实现相同口令的用户有不同的MD5值。 练习二 根据用户输入的登录名和口令模拟用户注册，计算更安全的MD5： 1234567891011db = {}def register(username, password): db[username] = calc_md5(password + username + 'the-Salt')def calc_md5(password): md5 = hashlib.md5() md5.update(password.encode('utf-8')) return md5.hexdigest() 根据修改后的MD5算法实现用户登录的验证： 1234567def login(user, password): digest = calc_md5(password + username + 'the-Salt') if db[user] == digest: return True else: return False 小结 摘要算法在很多地方都有广泛的应用。要注意摘要算法不是加密算法，不能用于加密（因为无法通过摘要反推明文），只能用于防篡改，但是它的单向计算特性决定了可以在不存储明文口令的情况下验证用户口令。","link":"/posts/33557.html"},{"title":"BiLSTM和CRF算法的序列标注原理","text":"CRF原理 条件随机场(conditional_random_field)，是一类判别型算法，特别适合于预测任务，任务中的 上下文信息或临近状态会影响当前状态 。如序列标注任务： &gt;判别式模型(discriminative model)计算条件概率，而生成式模型(generative model)计算联合概率分布 词性标注(Part_Of_Speech Tagging,POS Tagging)：确定句子中的单词的词性，如名称、形容词、副词等 命名实体识别(Named Entity Recognize)：确定句子中单词属于那种实体，如组织、机构、人名等 HMM生成模型 给定句子 \\(S\\)，对应的输出词性序列 \\(T\\)，HMM模型的联合概率： \\[ \\begin{align} P(T|S) &amp;= \\frac{P(S|T)\\cdot P(T)}{P(S)}\\\\ P(S,T) &amp;= P(S|T)\\cdot P(T)\\\\ &amp;= \\prod_{i=1}^{n}P(w_i|T)\\cdot P(T)\\\\ &amp;= \\prod_{i=1}^{n}P(w_i|t_i)\\cdot P(T)\\\\ &amp;= \\prod_{i=1}^{n}P(w_i|t_i)\\cdot P(t_i|t_{i-1})\\\\ \\end{align} \\] &gt; 首先贝叶斯公式展开，然后利用 以下假设 简化： - 由词之间相互独立假设，得到 \\(\\prod_{i=1}^{n}P(w_i|T)\\) - 由单词概率仅依赖于其自身的标签，得到发射(emission)概率 \\(\\prod_{i=1}^{n}P(w_i|t_i)\\) - 由马尔可夫假设，使用 bi-gram 得到转移(transition)概率 \\(P(t_i|t_{i-1})\\) 目标函数： \\[ (\\hat{t_1},\\hat{t_2}...\\hat{t_n})=arg max\\prod_{i=1}^{n}P(w_i|t_i)\\cdot P(t_i|t_{i-1}) \\] 综上，HMM假设了两类特征：当前词性与上一词性的关系，当前词与当前词性的关系 HMM的学习过程就是在训练集中学习这两个概率矩阵，大小分别为(t,t),(w,t)，w为单词的个数，t为词性的个数 CRF判别模型 CRF并没有做出上述的假设，而是使用特征方程feature function来更抽象地表达特征，而不再局限于HMM的两类特征 特征方程 条件随机场中，特征方程 \\(f_j\\) 的输入为： - 句子 \\(S\\) - 一个单词在句子中的位置 \\(i\\) - 当前单词的标签 \\(l_i\\) - 前一个单词的标签 \\(l_{i-1}\\) 输出实值 0 或 1 &gt; 上述示例为 线性链 条件随机场，特征方程只依赖于当前与 前一个 标签，而不是序列中的任意标签； 如给定之前的单词 “很” ，特征方程判断当前单词 “简单” 的词性 给每个特征方程 \\(f_j\\) 一个权重 \\(\\lambda_j\\)，可以计算一个句子 \\(s\\) 对应一组标签 \\(l\\) 的 \"分数\" \\[score(l|s)=\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\lambda_jf_j(s,i,l_i,l_{i-1})\\] &gt; 其中 \\(i\\) 表示句子中的位置，\\(j\\) 表示特定的特征方程 “分数”然后转化成概率分布 \\[p(s|l)=\\frac{exp\\big[\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\lambda_jf_j(s,i,l_i,l_{i-1})\\big]}{\\sum_{l^{’}}exp\\big[\\sum_{j=1}^{m}\\sum_{i=1}^{n}\\lambda_jf_j(s,i,l^{’}_i,l^{’}_{i-1})\\big]}\\] &gt; \\(l^{’}\\) 表示所有可能的序列标签组合 特征方程示例： \\(f_1(s,i,l_i,l_{i-1})\\) 表示 \\(l_i\\) 是否为 副词；若第 \\(i\\) 个单词以 -ly 结尾，该值为 1，否则为 0。即若该特征对应的权重 \\(\\lambda_i\\) 较大，说明偏向于将该特征的单词标注为 “副词” \\(f_2(s,i,l_i,l_{i-1})\\) 表示 \\(l_i\\) 是否为 动词；若第 \\(i=1\\) 且句子以?结尾，该值为 1，否则为 0。 \\(f_3(s,i,l_i,l_{i-1})\\) 表示 \\(l_i\\) 是否为 形容词；若第 \\(l_{i-1}\\) 为名词，则该值为 1，否则为 0。 \\(f_4(s,i,l_i,l_{i-1})\\) 表示 \\(l_i\\) 是否为 介词；若第 \\(l_{i-1}\\) 为介词，则该值为 0。 介词不能跟着介词 因此：要创建条件随机场，需要定义一系列的特征，然后给每个特征分配权重，然后遍历整个序列，再将其转换成概率 损失函数 综上，给定训练样本 \\(D=\\big[(x^1,y^1),(x^2,y^2)...(x^m,y^m)\\big]\\)，其中\\(m\\)表示\\(m\\)个句子 利用最大似然估计计算参数 \\(\\lambda\\)， \\[ \\begin{align} L(\\lambda,D) &amp;= log\\Big(\\prod_{k=1}^{n}P(y^k|x^k,\\lambda)\\Big)\\\\ &amp;= \\sum_{k=1}^{m}\\Big[log\\frac{1}{Z(x_k)}+\\sum_{j=1}^{n}\\sum_{i=1}^{l}\\lambda_jf_j(x_i^k,i,y^k_i,y^k_{i-1})\\Big] \\end{align} \\] &gt; 其中\\(k\\)表示第\\(k\\)个句子，共\\(m\\)个句子；\\(i\\)表示句子的第\\(i\\)个单词，共\\(l\\)个单词，\\(j\\)表示第\\(j\\)个特征，共\\(n\\)个特征，\\(\\frac{1}{Z(x^k)}\\)为正则项 然后利用梯度下降算法即可求解出 \\(\\lambda\\) 参数 与HMM的关系 将特征方程定义\\(f_1(x,i,y_i,y_{i-1})=1\\)定义为\\(p(y_i|y_{i-1})\\)，将特征对应的权重\\(\\lambda\\)定义为 \\(\\lambda=log p(x_i|y_i)\\)，即可从CRF中推导出HMM，HMM为CRF的特例 与逻辑回归类比： 逻辑回归用于分类的线性(log-linear)模型，CRFs则用于序列分类的线性(log-linear)模型 关键是：CRF模型中的特征方程？ 特征方程约束了输出标签序列，即确定标签与标签之间的关系，可以作为形状为(tag_size,tag_size)模型参数学习得到，(i,j)表示从标签i&lt;-j的关系 BiLSTM+CRF实现命名实体识别 参考连接: CRF Layer on the Top of BiLSTM - 输入单词序列的词表征经过BiLSTM处理，生成每个单词所属实体类别的权重 - 再将权重分布组成的序列，输入到CRF层，获得最终的实体类别分布 - BiLSTM层已经可以获得了单词的实体类别了 - 但CRF层给上一层的输出添加了一些规则限制，即的CRF特征方程 转移矩阵和发射矩阵 BiLSTM层的输出为emission score \\(E\\)，形状为(seq_len,tag_size)，\\(E_{i,j}\\) 表示第 \\(i\\) 个单词属于第 \\(j\\) 个类别的权重，上图中 \\(E_{w_0,B-person}=1.5\\) 使用 \\(t_{i,j}\\) 表示 transition score，例如 \\(t_{B-Person,I-Person}=0.9\\) ，表示B-Person --&gt; I-Person的转移权重为 0.9 .所有标签之间都有权重分数； 额外添加了表征开始和结束的两个标签START+END， START B-Person I-Person B-Organization I-Organization O END START 0 0.8 0.007 0.7 0.0008 0.9 0.08 B-Person 0 0.6 0.9 0.2 0.0006 0.6 0.009 I-Person -1 0.5 0.53 0.55 0.0003 0.85 0.008 B-Organization 0.9 0.5 0.0003 0.25 0.8 0.77 0.006 I-Organization -0.9 0.45 0.007 0.7 0.65 0.76 0.2 O 0 0.65 0.0007 0.7 0.0008 0.9 0.08 END 0 0 0 0 0 0 0 如上表所示，转移矩阵学习了一些有用信息： 句子应该以 B-Person 和 B-Organization，而不应该以I-Person开始，等等 转移矩阵为模型的参数，在训练之前随机初始化，随着训练进行逐渐进行更新；而不用手动设置 损失函数 损失函数 有5个单词组成的句子，可能的实体类别序列： 1234561) START B-Person B-Person B-Person B-Person B-Person END2) START B-Person I-Person B-Person B-Person B-Person END ......10) START B-Person I-Person O B-Organization O END ......N) O O O O O O O 假设每种可能的序列有一个分数 \\(P_i\\)，总共 \\(N\\) 种路径，则所有路径分数和为 \\(P_{total}=P_1+P_2+...+P_{N}=e^{S_1}+e^{S_2}+...+e^{S_N}\\)； 假设第10种路径为真实标签路径，则分数 \\(P_{10}\\)应该为最大的，则损失函数为\\(Loss=\\frac{P_{RealPath}}{P_1+P_2+...+P_{N}}\\) 将其转化为 负log函数，便于梯度下降法计算最小值 \\[ \\begin{align} \\text{Loss} &amp;= -log \\frac{P_{RealPath}}{P_1+P_2+...+P_{N}}\\\\ &amp;= -log \\frac{e^{S_{RealPath}}}{e^{S_1}+e^{S_2}+...+e^{S_N}}\\\\ &amp;= -\\big(s_{RealPath}-log(e^{S_1}+e^{S_2}+...+e^{S_N})\\big)\\\\ \\end{align} \\] \\(S_{RealPath}\\)为真实路径的分数，\\(log(e^{S_1}+e^{S_2}+...+e^{S_N})\\)为所有路径分数 真实路径分数 真实路径START B-Person I-Person O B-Organization O END，如何计算真实路径的分数 \\(P_i=e^{S_i}\\)，需要先计算 \\(S_i\\)， - 对于上述 5 单词的句子 \\(w_1,w_2,w_3,w_4,w_5\\) - 加上开始和结束标签，START,END \\(S_i=\\text{EmissionScore}+\\text{TransitionScore}\\) - \\(EmissionScore=x_{0,START}+x_{1,B-Person}+x_{2,I-Person}+x_{3,O}+x_{4,B-Organization}+x_{5,O}+x_{6,END}\\) - \\(x_{i,label}\\)，表示第i个单词标签为label的分数，直接从BiLSTM层的输出为emission score \\(E\\)中获得 - \\(x_{0,START}\\) 和 \\(x_{6,END}\\) 直接设置为 0 \\(TransitionScore=t_{I-Person-&gt;O} + t_{0-&gt;B-Organization} + t_{B-Organization-&gt;O} + t_{O-&gt;END}\\) 这些分数来源于CRF层 12345678910111213141516def _score_sentence(self, feats, tags): &quot;&quot;&quot; feats: 发射矩阵，lstm 层的输出，(seq_len,num_tags) tags: 真实的标签序列 示例代码不包含数据批的维度，一次只能处理一个序列 &quot;&quot;&quot; score = torch.zeros(1) tags = torch.cat([ torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags ]) for i, feat in enumerate(feats): score = score + \\ self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]] score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]] return score 1 如何计算所有可能路径的分数？ 已知长 \\(n\\) 的序列{w0,w1,w2}，\\(m\\) 个标签{l1,l2}，发射矩阵\\(x_{ij}\\)，转移矩阵\\(t_{ij}\\)， 连续两个标签 \\((w_t,w_{t+1})\\) 对应标签组合 \\((l_a, l_b)\\) 的分数表示为：\\(x_{t,a}+x_{t+1,b}+t_{a,b}\\) ，三项分别表示第 t 个单词属于标签 a 的分数、第 t+1 个单词属于标签 b 的分数、以及标签 a-&gt;b 的转移分数 所有的路径即所有的标签排列组合：(l1,l1,l1),(l1,l1,l2),(l1,l2,l1)...等 \\(m^n\\) 种，如上图中的8种。最终的分数即为所有路径的log_sum_exp之和 123456789101112131415import torchdef log_sum_exp(vec): &quot;&quot;&quot; [3,4,5] --&gt; log( e^3+e^4+e^5 ) --&gt; log( e^5*(e^(3-5)+e^(4-5)+e^(5-5)) ) --&gt; 5 + log( e^(3-5)+e^(4-5)+e^(5-5) ) &quot;&quot;&quot; max_score, idx = torch.max(vec, 1) max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1]) return max_score + torch.log( torch.sum(torch.exp(vec - max_score_broadcast)))vec = torch.tensor([[3., 4., 5.]])log_sum_exp(vec) tensor([5.4076]) 动态规划求解过程： 利用向量 \\(D\\) 表示当前单词选择各个标签时的分数；如上图中所示；当前单词选择某个标签的分数，可由上一步的 \\(D\\) 向量推导出： 单词 w0，没有前继单词，所以没有转移分数：\\(D=[log(e^{x_{01}}), log(e^{x_{02}})]\\) 单词 w1 选择标签 l1 时的分数可以表示为：\\(d_1 = log(e^{d_1+x_{11}+t_{11}}+e^{d_2 + x_{11}+t_{21}})\\) 选择标签 l2 时的分数可以表示为：\\(d_2 = log(e^{d_1+x_{12}+t_{12}}+e^{d_2+x_{12}+t_{22}})\\) 同理单词w2 选择标签 l1 时的分数可以表示为：\\(d_1 = log(e^{d_1+x_{21}+t_{11}}+e^{d_2+x_{21}+t_{21}})\\) 选择标签 l2 时的分数可以表示为：\\(d_2 = log(e^{d_1+x_{22}+t_{12}}+e^{d_2+x_{22}+t_{22}})\\) 从最后一个单词得到的 \\(D\\)，得到所有路径的分数：\\(log(e^{d_1} + e^{d_2})\\) 1234567891011121314151617181920212223242526272829def _forward_alg(self, feats): &quot;&quot;&quot; feats: (seq_len, tag_size) &quot;&quot;&quot; init_alphas = torch.full((1, self.tag_size), -10000.) # 前一个单词选择各个标签时的分数 init_alphas[0][self.tag2idx[START_TAG]] = 0. # 开始标签 forward_var = init_alphas for feat in feats: alphas_t = [] # 动态规划遍历到当前单词时，当前单词选择各个标签时的分数 for next_tag in range(self.tag_size): # 单词选择当前标签的分数 emit_score = feat[next_tag].view(1, -1).expand(1, self.tag_size) # 上一单词所有标签指向当前标签的转移分数 trans_score = self.transitions[next_tag].view(1, -1) # 再加上一单词选择各个标签的分数，然后求 log-sum-exp，即为当前单词选择当前标签的分数 next_tag_var = forward_var + trans_score + emit_score alphas_t.append(log_sum_exp(next_tag_var).view(1)) forward_var = torch.cat(alphas_t).view(1, -1) # 更新当前层的值，作为下一层的参数 terminalL_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]] alpha = log_sum_exp(terminal_var) return alpha 模型损失即为： \\[ \\begin{align} \\text{Loss} = log(e^{S_1}+e^{S_2}+...+e^{S_N}) -S_{RealPath} \\end{align} \\] \\(log(e^{S_1}+e^{S_2}+...+e^{S_N})\\)为所有路径分数，\\(S_{RealPath}\\)为真实路径的分数 12345def neg_log_likelihood(self, sentence, tags): feats = self._get_lstm_features(sentence) # emission matrix forward_score = self._forward_alg(feats) # all possibile pathes score gold_score = self._score_sentence(feats, tags) # real path score return forward_score - gold_score 如何进行预测 模型训练好后，如何进行预测？ &gt; 通常模型 forward 方法进行预测，然后预测结果与 target 求交叉熵或MSE就可以计算损失函数，此过程没有增加其它参数；而 crf 模型预测结果与 target 计算损失函数时还引入了转移矩阵作为参数，所以需要额外定义损失函数 维特比算法求解： 输入经过 lstm 层获得 发射矩阵，及模型训练得到的特征方程 转移矩阵，然后从所有可能路径中选择最优的路径。 1234567891011121314151617181920212223242526272829303132333435363738394041424344def _veterbi_decode(self, feats): # [i,j] 记录第 i 个单词选择第 j 个标签时的最佳路径中，上一步选择的哪个标签 backpointers = [] # Initialize the viterbi variables in log space init_vvars = torch.full((1, self.tagset_size), -10000.) init_vvars[0][self.tag_to_ix[START_TAG]] = 0 # 保存上一步各个标签对应的最佳分数 forward_var = init_vvars for feat in feats: bptrs_t = [] # holds the backpointers for this step viterbivars_t = [] # holds the viterbi variables for this step for next_tag in range(self.tagset_size): # 各个标签对应的分数 next_tag_var = forward_var + self.transitions[next_tag] # 到当前标签的最佳路径中 上一个标签的索引 best_tag_id = argmax(next_tag_var) bptrs_t.append(best_tag_id) # 最佳路径的分数 viterbivars_t.append(next_tag_var[0][best_tag_id].view(1)) # 分数还要加上发射分数 forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1) backpointers.append(bptrs_t) # Transition to STOP_TAG terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] best_tag_id = argmax(terminal_var) path_score = terminal_var[0][best_tag_id] # 最佳路径分数 best_path = [best_tag_id] # 最佳路径 for bptrs_t in reversed(backpointers): best_tag_id = bptrs_t[best_tag_id] best_path.append(best_tag_id) # Pop off the start tag (we dont want to return that to the caller) start = best_path.pop() assert start == self.tag_to_ix[START_TAG] # Sanity check best_path.reverse() return path_score, best_path BiLSTM+CRF完整代码 123456789import torchimport torch.autograd as autogradimport torch.nn as nnimport torch.optim as optimtorch.manual_seed(1)device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)device device(type='cuda') 123456789101112131415161718192021222324252627def argmax(vec): &quot;&quot;&quot; vec: (1,n) &quot;&quot;&quot; _, idx = torch.max(vec, 1) return idx.item()def prepare_sequence(seq, to_ix): &quot;&quot;&quot; word seq --&gt; idx seq &quot;&quot;&quot; idxs = [to_ix[w] for w in seq] return torch.tensor(idxs, dtype=torch.long) # dtype must be float &lt;- long/int not implemented for torch.expdef log_sum_exp(vec): &quot;&quot;&quot; [3,4,5] --&gt; log( e^3+e^4+e^5 ) --&gt; log( e^5*(e^(3-5)+e^(4-5)+e^(5-5)) ) --&gt; 5 + log( e^(3-5)+e^(4-5)+e^(5-5) ) &quot;&quot;&quot; max_score = vec[0, argmax(vec)] max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1]) return max_score + torch.log( torch.sum(torch.exp(vec - max_score_broadcast))) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142class BiLSTM_CRF(nn.Module): def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim): super(BiLSTM_CRF, self).__init__() self.embedding_dim = embedding_dim self.hidden_dim = hidden_dim self.vocab_size = vocab_size self.tag_to_ix = tag_to_ix self.tagset_size = len(tag_to_ix) self.word_embeds = nn.Embedding(vocab_size, embedding_dim) self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True) # Maps the output of the LSTM into tag space. self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size) # Matrix of transition parameters. Entry i,j is the score of # transitioning *to* i *from* j. self.transitions = nn.Parameter( torch.randn(self.tagset_size, self.tagset_size)) # These two statements enforce the constraint that we never transfer # to the start tag and we never transfer from the stop tag self.transitions.data[tag_to_ix[START_TAG], :] = -10000 self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000 self.hidden = self.init_hidden() def init_hidden(self): return (torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2)) def _forward_alg(self, feats): # Do the forward algorithm to compute the partition function init_alphas = torch.full((1, self.tagset_size), -10000.) # START_TAG has all of the score. init_alphas[0][self.tag_to_ix[START_TAG]] = 0. # Wrap in a variable so that we will get automatic backprop forward_var = init_alphas # Iterate through the sentence for feat in feats: alphas_t = [] # The forward tensors at this timestep for next_tag in range(self.tagset_size): # broadcast the emission score: it is the same regardless of # the previous tag emit_score = feat[next_tag].view(1, -1).expand( 1, self.tagset_size) # the ith entry of trans_score is the score of transitioning to # next_tag from i trans_score = self.transitions[next_tag].view(1, -1) # The ith entry of next_tag_var is the value for the # edge (i -&gt; next_tag) before we do log-sum-exp next_tag_var = forward_var + trans_score + emit_score # The forward variable for this tag is log-sum-exp of all the # scores. alphas_t.append(log_sum_exp(next_tag_var).view(1)) forward_var = torch.cat(alphas_t).view(1, -1) terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] alpha = log_sum_exp(terminal_var) return alpha def _get_lstm_features(self, sentence): self.hidden = self.init_hidden() embeds = self.word_embeds(sentence).view(len(sentence), 1, -1) lstm_out, self.hidden = self.lstm(embeds, self.hidden) lstm_out = lstm_out.view(len(sentence), self.hidden_dim) lstm_feats = self.hidden2tag(lstm_out) return lstm_feats def _score_sentence(self, feats, tags): # Gives the score of a provided tag sequence score = torch.zeros(1) tags = torch.cat([ torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags ]) for i, feat in enumerate(feats): score = score + \\ self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]] score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]] return score def _viterbi_decode(self, feats): backpointers = [] # Initialize the viterbi variables in log space init_vvars = torch.full((1, self.tagset_size), -10000.) init_vvars[0][self.tag_to_ix[START_TAG]] = 0 # forward_var at step i holds the viterbi variables for step i-1 forward_var = init_vvars for feat in feats: bptrs_t = [] # holds the backpointers for this step viterbivars_t = [] # holds the viterbi variables for this step for next_tag in range(self.tagset_size): # next_tag_var[i] holds the viterbi variable for tag i at the # previous step, plus the score of transitioning # from tag i to next_tag. # We don't include the emission scores here because the max # does not depend on them (we add them in below) next_tag_var = forward_var + self.transitions[next_tag] best_tag_id = argmax(next_tag_var) bptrs_t.append(best_tag_id) viterbivars_t.append(next_tag_var[0][best_tag_id].view(1)) # Now add in the emission scores, and assign forward_var to the set # of viterbi variables we just computed forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1) backpointers.append(bptrs_t) # Transition to STOP_TAG terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]] best_tag_id = argmax(terminal_var) path_score = terminal_var[0][best_tag_id] # Follow the back pointers to decode the best path. best_path = [best_tag_id] for bptrs_t in reversed(backpointers): best_tag_id = bptrs_t[best_tag_id] best_path.append(best_tag_id) # Pop off the start tag (we dont want to return that to the caller) start = best_path.pop() assert start == self.tag_to_ix[START_TAG] # Sanity check best_path.reverse() return path_score, best_path def neg_log_likelihood(self, sentence, tags): feats = self._get_lstm_features(sentence) forward_score = self._forward_alg(feats) gold_score = self._score_sentence(feats, tags) return forward_score - gold_score def forward(self, sentence): # dont confuse this with _forward_alg above. # Get the emission scores from the BiLSTM lstm_feats = self._get_lstm_features(sentence) # Find the best path, given the features. score, tag_seq = self._viterbi_decode(lstm_feats) return score, tag_seq 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354START_TAG = &quot;&lt;START&gt;&quot;STOP_TAG = &quot;&lt;STOP&gt;&quot;EMBEDDING_DIM = 5HIDDEN_DIM = 4# Make up some training datatraining_data = [ (&quot;the wall street journal reported today that apple corporation made money&quot; .split(), &quot;B I I I O O O B I O O&quot;.split()), (&quot;georgia tech is a university in georgia&quot;.split(), &quot;B I O O O O B&quot;.split())]word_to_ix = {}for sentence, tags in training_data: for word in sentence: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix)tag_to_ix = {&quot;B&quot;: 0, &quot;I&quot;: 1, &quot;O&quot;: 2, START_TAG: 3, STOP_TAG: 4}model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)# Check predictions before trainingwith torch.no_grad(): precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long) print(model(precheck_sent))for epoch in range(300): for sentence, tags in training_data: # Step 1. Remember that Pytorch accumulates gradients. # We need to clear them out before each instance model.zero_grad() # Step 2. Get our inputs ready for the network, that is, # turn them into Tensors of word indices. sentence_in = prepare_sequence(sentence, word_to_ix) targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long) # Step 3. Run our forward pass. loss = model.neg_log_likelihood(sentence_in, targets) # Step 4. Compute the loss, gradients, and update the parameters by # calling optimizer.step() loss.backward() optimizer.step()# Check predictions after trainingwith torch.no_grad(): precheck_sent = prepare_sequence(training_data[0][0], word_to_ix) print(model(precheck_sent)) (tensor(2.6907), [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]) (tensor(20.4906), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])","link":"/posts/51094.html"},{"title":"Numpy 学习","text":"Python模块中的numpy，这是一个处理数组的强大模块，而该模块也是其他数据分析模块（如pandas和scipy）的核心。下面将从这5个方面来介绍numpu模块的内容： 1. 数组的创建 2. 有关数组的属性和函数 3. 数组元素的获取--普通索引、切片、布尔索引和花式索引 4. 统计函数与线性代数运算 5. 随机数的生成 数组的创建 一维数组的创建 可以使用numpy中的arange()函数创建一维有序数组，它是内置函数range的扩展版。 123456789101112In [1]: import numpy as npIn [2]: ls1 = range(10)In [3]: list(ls1)Out[3]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]In [4]: type(ls1)Out[4]: rangeIn [5]: ls2 = np.arange(10)In [6]: list(ls2)Out[6]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]In [7]: type(ls2)Out[7]: numpy.ndarray 通过arange生成的序列就不是简简单单的列表类型了，而是一个一维数组。 如果一维数组不是一个规律的有序元素，而是人为的输入，就需要array()函数创建了。numpy中使用array()函数创建数组,array的首个参数一定是一个序列，可以是元组也可以是列表。 12345In [8]: arr1 = np.array((1,20,13,28,22))In [9]: arr1Out[9]: array([ 1, 20, 13, 28, 22])In [10]: type(arr1)Out[10]: numpy.ndarray 上面是由元组序列构成的一维数组。 12345In [11]: arr2 = np.array([1,1,2,3,5,8,13,21])In [12]: arr2Out[12]: array([ 1, 1, 2, 3, 5, 8, 13, 21])In [13]: type(arr2)Out[13]: numpy.ndarray 上面是由列表序列构成的一维数组。 二维数组的创建 二维数组的创建，其实在就是列表套列表或元组套元组 123456In [14]: arr3 = np.array(((1,1,2,3),(5,8,13,21),(34,55,89,144)))In [15]: arr3Out[15]:array([[ 1, 1, 2, 3],[ 5, 8, 13, 21],[ 34, 55, 89, 144]]) 上面是使用元组套元组的方式。 123456In [16]: arr4 = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])In [17]: arr4Out[17]:array([[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 9, 10, 11, 12]]) 上面是使用列表套列表的方式。 对于高维数组在将来的数据分析中用的比较少，这里关于高维数组的创建就不赘述了，构建方法仍然是使用嵌套的方式。 上面所介绍的都是人为设定的一维、二维或高维数组，numpy中也提供了几种特殊的数组，它们是： 1234567891011121314151617181920212223242526In [18]: np.ones(3) #返回一维元素全为1的数组Out[18]: array([ 1., 1., 1.])In [19]: np.ones([3,4]) #返回元素全为1的3×4二维数组Out[19]:array([[ 1., 1., 1., 1.],[ 1., 1., 1., 1.],[ 1., 1., 1., 1.]])In [20]: np.zeros(3) #返回一维元素全为0的数组Out[20]: array([ 0., 0., 0.])In [21]: np.zeros([3,4]) #返回元素全为0的3×4二维数组Out[21]:array([[ 0., 0., 0., 0.],[ 0., 0., 0., 0.],[ 0., 0., 0., 0.]])In [22]: np.empty(3) #返回一维空数组Out[22]: array([ 1.13224202e+277, 6.00769955e-307, 2.55195390e-303])In [23]: np.empty([3,4]) #返回3×4二维空数组Out[23]:array([[ 1.09494983e-311, 8.19904309e-314, 1.09678092e-311, 1.09494983e-311], [ 2.09290548e-313, 1.09678288e-311, 1.09494983e-311, 3.36629926e-313], [ 1.09678538e-311, 4.03179200e-313, 1.09678287e-311, 1.09494983e-311]]) 原文中对于np.empty的举例不够严谨，这个函数返回的是指定规模未经初始化的数组，速度要比 ones 和 zeros 快，但里面的值都是随机的，要谨慎使用。 有关数组的属性和函数 当一个数组构建好后，我们看看关于数组本身的操作又有哪些属性和函数： 12345678910111213141516171819In [24]: arr3Out[24]:array([[ 1, 1, 2, 3],[ 5, 8, 13, 21],[ 34, 55, 89, 144]])In [25]: arr3.shape #shape方法返回数组的行数和列数Out[25]: (3, 4)In [26]: arr3.dtype #dtype方法返回数组的数据类型Out[26]: dtype('int32')In [27]: a = arr3.ravel() #通过ravel的方法将数组拉直（多维数组降为一维数组）In [28]: aOut[28]: array([ 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144])In [29]: b = arr3.flatten() #通过flatten的方法将数组拉直In [30]: bOut[30]: array([ 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144]) 两者的区别在于ravel方法生成的是原数组的视图，无需占有内存空间，但视图的改变会影响到原数组的变化。而flatten方法返回的是真实值，其值的改变并不会影响原数组的更改。 通过下面的例子也许就能明白了： 123456In [31]: b[:3] = 0In [32]: arr3Out[32]:array([[ 1, 1, 2, 3],[ 5, 8, 13, 21],[ 34, 55, 89, 144]]) 通过更改b的值，原数组没有变化。 123456In [33]: a[:3] = 0In [34]: arr3Out[34]:array([[ 0, 0, 0, 3],[ 5, 8, 13, 21],[ 34, 55, 89, 144]]) a的值变化后，会导致原数组跟着变化。 123456789101112131415161718In [35]: arr4Out[35]:array([[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 9, 10, 11, 12]])In [36]: arr4.ndim #返回数组的维数(嵌套的层数)Out[36]: 2In [37]: arr4.size #返回数组元素的个数Out[37]: 12In [38]: arr4.T #返回数组的转置结果Out[38]:array([[ 1, 5, 9],[ 2, 6, 10],[ 3, 7, 11],[ 4, 8, 12]]) 如果数组的数据类型为复数的话，real方法可以返回复数的实部，imag方法返回复数的虚部。 介绍完数组的一些方法后，接下来我们看看数组自身有哪些函数可操作： 1234567891011121314151617181920In [39]: len(arr4) #返回数组有多少行Out[39]: 3In [40]: arr3Out[40]:array([[ 0, 0, 0, 3],[ 5, 8, 13, 21],[ 34, 55, 89, 144]])In [41]: arr4Out[41]:array([[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 9, 10, 11, 12]])In [42]: np.hstack((arr3,arr4))Out[42]:array([[ 0, 0, 0, 3, 1, 2, 3, 4],[ 5, 8, 13, 21, 5, 6, 7, 8],[ 34, 55, 89, 144, 9, 10, 11, 12]]) 横向拼接arr3和arr4两个数组，但必须满足两个数组的行数相同。 12345678In [43]: np.vstack((arr3,arr4))Out[43]:array([[ 0, 0, 0, 3],[ 5, 8, 13, 21],[ 34, 55, 89, 144],[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 9, 10, 11, 12]]) 纵向拼接arr3和arr4两个数组，但必须满足两个数组的列数相同。 1234567891011121314In [44]: np.column_stack((arr3,arr4)) #与hstack函数具有一样的效果Out[44]:array([[ 0, 0, 0, 3, 1, 2, 3, 4],[ 5, 8, 13, 21, 5, 6, 7, 8],[ 34, 55, 89, 144, 9, 10, 11, 12]])In [45]: np.row_stack((arr3,arr4)) #与vstack函数具有一样的效果Out[45]:array([[ 0, 0, 0, 3],[ 5, 8, 13, 21],[ 34, 55, 89, 144],[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 9, 10, 11, 12]]) reshape()函数和resize()函数可以重新设置数组的行数和列数： 123456789101112In [46]: arr5 = np.arange(24)In [47]: arr5 #此为一维数组Out[47]:array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])In [48]: a = arr5.reshape(4,6)In [49]: aOut[49]:array([[ 0, 1, 2, 3, 4, 5],[ 6, 7, 8, 9, 10, 11],[12, 13, 14, 15, 16, 17],[18, 19, 20, 21, 22, 23]]) 通过reshape函数将一维数组设置为二维数组，且为4行6列的数组。reshape函数不会改变原数组。 123456789In [50]: a.resize(6,4)In [51]: aOut[51]:array([[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 11],[12, 13, 14, 15],[16, 17, 18, 19],[20, 21, 22, 23]]) 通过resize函数会直接改变原数组的形状。 数组转换：tolist将数组转换为列表，astype()强制转换数组的数据类型，下面是两个函数的例子： 1234567891011121314151617181920212223242526In [53]: b = a.tolist()In [54]: bOut[54]:[[0, 1, 2, 3],[4, 5, 6, 7],[8, 9, 10, 11],[12, 13, 14, 15],[16, 17, 18, 19],[20, 21, 22, 23]]In [55]: type(b)Out[55]: listIn [56]: c = a.astype(float)In [57]: cOut[57]:array([[ 0., 1., 2., 3.],[ 4., 5., 6., 7.],[ 8., 9., 10., 11.],[ 12., 13., 14., 15.],[ 16., 17., 18., 19.],[ 20., 21., 22., 23.]])In [58]: a.dtypeOut[58]: dtype('int32')In [59]: c.dtypeOut[59]: dtype('float64') 数组元素的获取 通过索引和切片的方式获取数组元素，一维数组元素的获取与列表、元组的获取方式一样： 1234567891011121314151617In [60]: arr7 = np.arange(10)In [61]: arr7Out[61]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [62]: arr7[3] #获取第4个元素Out[62]: 3In [63]: arr7[:3] #获取前3个元素Out[63]: array([0, 1, 2])In [64]: arr7[3:] #获取第4个元素即之后的所有元素Out[64]: array([3, 4, 5, 6, 7, 8, 9])In [65]: arr7[-2:] #获取末尾的2个元素Out[65]: array([8, 9])In [66]: arr7[::2] #从第1个元素开始，获取步长为2的所有元素Out[66]: array([0, 2, 4, 6, 8]) 补充一下，如果要获取多个元素，且它们的索引没有固定步长这样的规则的话，可以传入一个list作为索引： 12345&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; aarray([0, 1, 2, 3, 4])&gt;&gt;&gt; a[[0,3,4]] #返回数组的第1，4，5个元素array([0, 3, 4]) 二维数组元素的获取： 12345678910111213141516171819202122232425262728293031323334353637In [67]: arr8 = np.arange(12).reshape(3,4)In [68]: arr8Out[68]:array([[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 11]])In [69]: arr8[1] #返回数组的第2行Out[69]: array([4, 5, 6, 7])In [70]: arr8[:2] #返回数组的前2行Out[70]:array([[0, 1, 2, 3],[4, 5, 6, 7]])In [71]: arr8[[0,2]] #返回指定的第1行和第3行Out[71]:array([[ 0, 1, 2, 3],[ 8, 9, 10, 11]])In [72]: arr8[:,0] #返回数组的第1列Out[72]: array([0, 4, 8])In [73]: arr8[:,-2:] #返回数组的后2列Out[73]:array([[ 2, 3],[ 6, 7],[10, 11]])In [74]: arr8[:,[0,2]] #返回数组的第1列和第3列Out[74]:array([[ 0, 2],[ 4, 6],[ 8, 10]])In [75]: arr8[1,2] #返回数组中第2行第3列对应的元素Out[75]: 6 布尔索引，即索引值为True和False，需要注意的是布尔索引必须是数组对象。 12345678910111213141516171819202122In [76]: log = np.array([True,False,False,True,True,False])In [77]: arr9 = np.arange(24).reshape(6,4)In [78]: arr9Out[78]:array([[ 0, 1, 2, 3],[ 4, 5, 6, 7],[ 8, 9, 10, 11],[12, 13, 14, 15],[16, 17, 18, 19],[20, 21, 22, 23]])In [79]: arr9[log] #返回所有为True的对应行Out[79]:array([[ 0, 1, 2, 3],[12, 13, 14, 15],[16, 17, 18, 19]])In [80]: arr9[-log] #通过负号筛选出所有为False的对应行Out[80]:array([[ 4, 5, 6, 7],[ 8, 9, 10, 11],[20, 21, 22, 23]]) 举一个场景，一维数组表示区域，二维数组表示观测值，如何选取目标区域的观测？ 12345678910111213141516171819202122In [81]: area = np.array(['A','B','A','C','A','B','D'])In [82]: areaOut[82]:array(['A', 'B', 'A', 'C', 'A', 'B', 'D'],dtype='&lt;U1')In [83]: observes = np.arange(21).reshape(7,3)In [84]: observesOut[84]:array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11],[12, 13, 14],[15, 16, 17],[18, 19, 20]])In [85]: observes[area == 'A'] #这里[]内的&quot;area == 'A'&quot;语句返回的就是一个全是布尔值的数组对象Out[85]:array([[ 0, 1, 2],[ 6, 7, 8],[12, 13, 14]]) 返回所有A区域的观测。 123456In [86]: observes[(area == 'A') | (area == 'D')] #条件值需要在&amp;(and),|(or)两端用圆括号括起来Out[86]:array([[ 0, 1, 2],[ 6, 7, 8],[12, 13, 14],[18, 19, 20]]) 返回所有A区域和D区域的观测。 当然，布尔索引也可以与普通索引或切片混合使用： 12345In [87]: observes[area == 'A'][:,[0,2]]Out[87]:array([[ 0, 2],[ 6, 8],[12, 14]]) 返回A区域的所有行，且只获取第1列与第3列数据。 花式索引：实际上就是将数组作为索引将原数组的元素提取出来 1234567891011121314151617181920212223242526In [88]: arr10 = np.arange(1,29).reshape(7,4)In [89]: arr10Out[89]:array([[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 9, 10, 11, 12],[13, 14, 15, 16],[17, 18, 19, 20],[21, 22, 23, 24],[25, 26, 27, 28]])In [90]: arr10[[4,1,3,5]] #按照指定顺序返回指定行Out[90]:array([[17, 18, 19, 20],[ 5, 6, 7, 8],[13, 14, 15, 16],[21, 22, 23, 24]])In [91]: arr10[[4,1,5]][:,[0,2,3]] #返回指定的行与列，可以分解为两条命令来看Out[91]:array([[17, 19, 20],[ 5, 7, 8],[21, 23, 24]])In [92]: arr10[[4,1,5],[0,2,3]] # 返回指定位置的数字Out[92]: array([17, 7, 24]) 请注意！这与上面的返回结果是截然不同的，上面的命令返回的是二维数组，而这条命令返回的是一维数组。P.S.原文这里没有解释的很清楚，简单来说，如果在方括号内使用两个甚至多个数组/列表作为索引，中间以逗号分割，那么数组的顺序对应的就是从低维到高维，比如 In [92] 中第一个列表的第一个元素是4，第二个列表的第一个元素是0，这就表示要从arr10这个二维数组中取出的第一个元素在第一维第5个对象和第二维第1个对象交叠的地方，也即二维数组的第5行第1个元素17。最后会将所有取出的元素放入一个一维数组中返回。但如果某一维没有用数组/列表指定，而是使用了冒号表示取所有的话，返回的就是二维对象了。再举一个例子吧： 1234567891011&gt;&gt;&gt; a = np.arange(10).reshape(2,5)&gt;&gt;&gt; aarray([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])&gt;&gt;&gt; a[:,[0,4]]array([[0, 4], [5, 9]])&gt;&gt;&gt; a[[0,0,1,1],[0,4,0,4]]array([0, 4, 5, 9]) 如果想使用比较简单的方式返回指定行与列的二维数组的话，可以使用ix_()函数 12345In [93]: arr10[np.ix_([4,1,5],[0,2,3])] # 允许我们用指定位置的方式，并且保持返回的数组形状不变Out[93]:array([[17, 19, 20],[ 5, 7, 8],[21, 23, 24]]) 这与 arr10[[4,1,5]][:,[0,2,3]] 返回的结果是一致的。 统计函数与线性代数运算 统计函数 统计运算中常见的聚合函数有：最小值、最大值、中位数、均值、方差、标准差等。首先来看看数组元素级别的计算： 123456789101112131415161718192021222324252627282930In [94]: arr11 = 5-np.arange(1,13).reshape(4,3)# randint返回指定区间的随机整数，区间左闭右开，size不指定时为None，返回1个随机整数In [95]: arr12 = np.random.randint(1,10,size = 12).reshape(4,3)In [96]: arr11Out[96]:array([[ 4, 3, 2],[ 1, 0, -1],[-2, -3, -4],[-5, -6, -7]])In [97]: arr12Out[97]:array([[1, 3, 7],[7, 3, 7],[3, 7, 4],[6, 1, 2]])In [98]: arr11 ** 2 #计算每个元素的平方Out[98]:array([[16, 9, 4],[ 1, 0, 1],[ 4, 9, 16],[25, 36, 49]])In [99]: np.sqrt(arr11) #计算每个元素的平方根Out[99]:array([[ 2. , 1.73205081, 1.41421356],[ 1. , 0. , nan],[ nan, nan, nan],[ nan, nan, nan]]) 由于负值的平方根没有意义，故返回nan。 1234567891011121314151617181920In [100]: np.exp(arr11) #计算每个元素的指数值，exp(a) 即 e^aOut[100]:array([[ 5.45981500e+01, 2.00855369e+01, 7.38905610e+00],[ 2.71828183e+00, 1.00000000e+00, 3.67879441e-01],[ 1.35335283e-01, 4.97870684e-02, 1.83156389e-02],[ 6.73794700e-03, 2.47875218e-03, 9.11881966e-04]])In [101]: np.log(arr12) #计算每个元素的自然对数值，log(a) 即 log_e(a)，还有log2和log10两个函数Out[101]:array([[ 0. , 1.09861229, 1.94591015],[ 1.94591015, 1.09861229, 1.94591015],[ 1.09861229, 1.94591015, 1.38629436],[ 1.79175947, 0. , 0.69314718]])In [102]: np.abs(arr11) #计算每个元素的绝对值Out[102]:array([[4, 3, 2],[1, 0, 1],[2, 3, 4],[5, 6, 7]]) 相同形状数组间元素的操作： 1234567891011121314151617181920212223242526272829303132333435363738394041In [103]: arr11 + arr12 #加Out[103]:array([[ 5, 6, 9],[ 8, 3, 6],[ 1, 4, 0],[ 1, -5, -5]])In [104]: arr11 - arr12 #减Out[104]:array([[ 3, 0, -5],[ -6, -3, -8],[ -5, -10, -8],[-11, -7, -9]])In [105]: arr11 * arr12 #乘Out[105]:array([[ 4, 9, 14],[ 7, 0, -7],[ -6, -21, -16],[-30, -6, -14]])In [106]: arr11 / arr12 #除Out[106]:array([[ 4. , 1. , 0.28571429],[ 0.14285714, 0. , -0.14285714],[-0.66666667, -0.42857143, -1. ],[-0.83333333, -6. , -3.5 ]])In [107]: arr11 // arr12 #整除Out[107]:array([[ 4, 1, 0],[ 0, 0, -1],[-1, -1, -1],[-1, -6, -4]], dtype=int32)In [108]: arr11 % arr12 #取余Out[108]:array([[0, 0, 2],[1, 0, 6],[1, 4, 0],[1, 0, 1]], dtype=int32) 统计运算函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849In [109]: np.sum(arr11) #计算所有元素的和Out[109]: -18In [110]: np.sum(arr11,axis = 0) #对每一列求和，注意axis是0Out[110]: array([ -2, -6, -10])In [111]: np.sum(arr11, axis = 1) #对每一行求和，注意axis是1Out[111]: array([ 9, 0, -9, -18])In [112]: np.cumsum(arr11) #对每一个元素求累积和（从上到下，从左到右的元素顺序），即每移动一次就把当前数字加到和值Out[112]: array([ 4, 7, 9, 10, 10, 9, 7, 4, 0, -5, -11, -18], dtype=int32)In [113]: np.cumsum(arr11, axis = 0) #计算每一列的累积和，并返回二维数组Out[113]:array([[ 4, 3, 2],[ 5, 3, 1],[ 3, 0, -3],[ -2, -6, -10]], dtype=int32)In [114]: np.cumprod(arr11, axis = 1) #计算每一行的累计积，并返回二维数组Out[114]:array([[ 4, 12, 24],[ 1, 0, 0],[ -2, 6, -24],[ -5, 30, -210]], dtype=int32)In [115]: np.min(arr11) #计算所有元素的最小值Out[115]: -7In [116]: np.max(arr11, axis = 0) #计算每一列的最大值Out[116]: array([4, 3, 2])In [117]: np.mean(arr11) #计算所有元素的均值Out[117]: -1.5In [118]: np.mean(arr11, axis = 1) #计算每一行的均值Out[118]: array([ 3., 0., -3., -6.])In [119]: np.median(arr11) #计算所有元素的中位数Out[119]: -1.5In [120]: np.median(arr11, axis = 0) #计算每一列的中位数Out[120]: array([-0.5, -1.5, -2.5])In [121]: np.var(arr12) #计算所有元素的方差Out[121]: 5.354166666666667In [122]: np.std(arr12, axis = 1) #计算每一行的标准差Out[122]: array([ 2.49443826, 1.88561808, 1.69967317, 2.1602469 ]) numpy中的统计函数运算是非常灵活的，既可以计算所有元素的统计值，也可以计算指定行或列的统计指标。还有其他常用的函数，如符号函数sign(将正数、负数、零分别映射为1、-1、0)，ceil(取&gt;=x的最小整数)，floor(取&lt;=x的最大整数)，modf(将浮点数的整数部分与小数部分分别存入两个独立的数组)，cos，arccos，sin，arcsin，tan，arctan等。 让我很兴奋的一个函数是where()，它类似于Excel中的if函数，可以进行灵活的变换： 1234567891011121314In [123]: arr11Out[123]:array([[ 4, 3, 2],[ 1, 0, -1],[-2, -3, -4],[-5, -6, -7]])In [124]: np.where(arr11 &lt; 0, 'negtive','positive')Out[124]:array([['positive', 'positive', 'positive'],['positive', 'positive', 'negtive'],['negtive', 'negtive', 'negtive'],['negtive', 'negtive', 'negtive']],dtype='&lt;U8') 当然，np.where 还可以嵌套使用，完成复杂的运算。 其它函数 unique(x): 计算x的唯一元素，并返回有序结果 intersect(x,y)： 计算x和y的公共元素，即交集 union1d(x,y): 计算x和y的并集 setdiff1d(x,y): 计算x和y的差集，即元素在x中，不在y中 setxor1d(x,y): 计算集合的对称差，即存在于一个数组中，但不同时存在于两个数组中 in1d(x,y): 判断x的元素是否包含于y中 线性代数运算 同样numpu也跟R语言一样，可以非常方便的进行线性代数方面的计算，如行列式、逆、迹、特征根、特征向量等。但需要注意的是，有关线性代数的函数并不在numpy中，而是numpy的子例linalg中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263In [125]: arr13 = np.array([[1,2,3,5],[2,4,1,6],[1,1,4,3],[2,5,4,1]])In [126]: arr13Out[126]:array([[1, 2, 3, 5],[2, 4, 1, 6],[1, 1, 4, 3],[2, 5, 4, 1]])In [127]: np.linalg.det(arr13) #返回方阵的行列式Out[127]: 51.000000000000021In [128]: np.linalg.inv(arr13) #返回方阵的逆Out[128]:array([[-2.23529412, 1.05882353, 1.70588235, -0.29411765],[ 0.68627451, -0.25490196, -0.7254902 , 0.2745098 ],[ 0.19607843, -0.21568627, 0.07843137, 0.07843137],[ 0.25490196, 0.01960784, -0.09803922, -0.09803922]])In [129]: np.trace(arr13) #返回方阵的迹（对角线元素之和），注意迹的求解不在linalg子例程中Out[129]: 10In [130]: np.linalg.eig(arr13) #返回由特征根和特征向量组成的元组Out[130]:(array([ 11.35035004, -3.99231852, -0.3732631 , 3.01523159]),array([[-0.4754174 , -0.48095078, -0.95004728, 0.19967185],[-0.60676806, -0.42159999, 0.28426325, -0.67482638],[-0.36135292, -0.16859677, 0.08708826, 0.70663129],[-0.52462832, 0.75000995, 0.09497472, -0.07357122]]))In [131]: np.linalg.qr(arr13) #返回方阵的QR分解Out[131]:(array([[-0.31622777, -0.07254763, -0.35574573, -0.87645982],[-0.63245553, -0.14509525, 0.75789308, -0.06741999],[-0.31622777, -0.79802388, -0.38668014, 0.33709993],[-0.63245553, 0.580381 , -0.38668014, 0.33709993]]),array([[-3.16227766, -6.64078309, -5.37587202, -6.95701085],[ 0. , 1.37840488, -1.23330963, -3.04700025],[ 0. , 0. , -3.40278524, 1.22190924],[ 0. , 0. , 0. , -3.4384193 ]]))In [132]:np.linalg.svd(arr13) #返回方阵的奇异值分解Out[132]:(array([[-0.50908395, 0.27580803, 0.35260559, -0.73514132],[-0.59475561, 0.4936665 , -0.53555663, 0.34020325],[-0.39377551, -0.10084917, 0.70979004, 0.57529852],[-0.48170545, -0.81856751, -0.29162732, -0.11340459]]),array([ 11.82715609, 4.35052602, 3.17710166, 0.31197297]),array([[-0.25836994, -0.52417446, -0.47551003, -0.65755329],[-0.10914615, -0.38326507, -0.54167613, 0.74012294],[-0.18632462, -0.68784764, 0.69085326, 0.12194478],[ 0.94160248, -0.32436807, -0.05655931, -0.07050652]]))In [133]: np.dot(arr13,arr13) #方阵的正真乘积运算Out[133]:array([[18, 38, 37, 31],[23, 51, 38, 43],[13, 25, 32, 26],[18, 33, 31, 53]])In [134]:arr14 = np.array([[1,-2,1],[0,2,-8],[-4,5,9]])In [135]: vector = np.array([0,8,-9])In [136]: np.linalg.solve(arr14,vector) # 求解线性方程组Out[136]: array([ 29., 16., 3.]) #1*29 - 2*16 + 1*3 = 0，以此类推 随机数生成 统计学中经常会讲到数据的分布特征，如正态分布、指数分布、卡方分布、二项分布、泊松分布等，下面就讲讲有关分布的随机数生成。 正态分布直方图 12345In [137]: import matplotlib #用于绘图的模块In [138]: np.random.seed(1234) #设置随机种子In [139]: N = 10000 #随机产生的样本量In [140]: randnorm = np.random.normal(size = N) #生成正态随机数In [141]: counts, bins, path = matplotlib.pylab.hist(randnorm, bins = np.sqrt(N), normed = True, color = 'blue') #绘制直方图 以上将直方图的频数和组距存放在counts和bins内。 123In [142]: sigma = 1; mu = 0In [143]: norm_dist = (1/np.sqrt(2*sigma*np.pi))*np.exp(-((bins-mu)**2)/2) #正态分布密度函数In [144]: matplotlib.pylab.plot(bins,norm_dist,color = 'red') #绘制正态分布密度函数图 graph1 补充一下，这个例子想说的是 random.normal 生成指定数目的随机数（可以进一步指定分布的均值、标准差和中心），然后把这些随机数用直方图画了出来。并且画了一条同一区间内的正态分布曲线来对比。注意种子的设置和生成随机数的大小是无关的，只是说使用不同的种子会生成不同的随机数，详细可以查找相关资料。 使用二项分布进行赌博 同时抛弃9枚硬币，如果正面朝上少于5枚，则输掉8元，否则就赢8元。如果手中有1000元作为赌资，请问赌博10000次后可能会是什么情况呢？ 123456789101112In [146]: np.random.seed(1234)In [147]: binomial = np.random.binomial(9,0.5,10000) #生成二项分布随机数In [148]: money = np.zeros(10000) #生成10000次赌资的列表In [149]: money[0] = 1000 #首次赌资为1000元In [150]: for i in range(1,10000): ...: if binomial[i] &lt; 5: ...: money[i] = money[i-1] - 8#如果少于5枚正面，则在上一次赌资的基础上输掉8元 ...: else: ...: money[i] = money[i-1] + 8#如果至少5枚正面，则在上一次赌资的基础上赢取8元In [151]: matplotlib.pylab.plot(np.arange(10000), money) graph2 使用随机整数实现随机游走 一个醉汉在原始位置上行走10000步后将会在什么地方呢？如果他每走一步是随机的，即下一步可能是1也可能是-1。 123456789In [152]: np.random.seed(1234) #设定随机种子In [153]: position = 0 #设置初始位置In [154]: walk = [] #创建空列表In [155]: steps = 10000 #假设接下来行走10000步In [156]: for i in np.arange(steps): ...: step = 1 if np.random.randint(0,2) else -1 #每一步都是随机的（如果随机到1就走1步，随机到0就走-1步） ...: position = position + step #对每一步进行累计求和 ...: walk.append(position) #确定每一步所在的位置In [157]: matplotlib.pylab.plot(np.arange(10000), walk) #绘制随机游走图 graph3 上面的代码还可以写成（结合前面所讲的where函数，cumsum函数）： 1234In [158]: np.random.seed(1234)In [159]: step = np.where(np.random.randint(0,2,10000)&gt;0,1,-1)In [160]: position = np.cumsum(step)In [161]: matplotlib.pylab.plot(np.arange(10000), position) graph4 避免for循环，可以达到同样的效果。","link":"/posts/21992.html"},{"title":"k-means、k-means++以及k-means算法分析","text":"本文会介绍一般的k-means算法、k-means++算法以及基于k-means++算法的k-means||算法。在spark ml，已经实现了k-means算法以及k-means||算法。 本文首先会介绍这三个算法的原理，然后在了解原理的基础上分析spark中的实现代码。 ## k-means算法原理分析 k-means算法是聚类分析中使用最广泛的算法之一。它把n个对象根据它们的属性分为k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。 k-means算法的基本过程如下所示： （1）任意选择k个初始中心\\(c_{1},c_{2},...,c_{k}\\) 。 （2）计算X中的每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分； （3）重新计算每个中心对象\\(C_{i}\\)的值 （4）计算标准测度函数，当满足一定条件，如函数收敛时，则算法终止；如果条件不满足则重复步骤（2），（3）。 k-means算法的缺点 k-means算法虽然简单快速，但是存在下面的缺点： 聚类中心的个数K需要事先给定，但在实际中K值的选定是非常困难的，很多时候我们并不知道给定的数据集应该分成多少个类别才最合适。 k-means算法需要随机地确定初始聚类中心，不同的初始聚类中心可能导致完全不同的聚类结果。 第一个缺陷我们很难在k-means算法以及其改进算法中解决，但是我们可以通过k-means++算法来解决第二个缺陷。 k-means++算法原理分析 k-means++算法选择初始聚类中心的基本原则是：初始的聚类中心之间的相互距离要尽可能的远。它选择初始聚类中心的步骤是： （1）从输入的数据点集合中随机选择一个点作为第一个聚类中心\\(c_{1}\\) ； （2）对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)，并根据概率选择新的聚类中心\\(c_{i}\\) 。 （3）重复过程（2）直到找到k个聚类中心。 第(2)步中，依次计算每个数据点与最近的种子点（聚类中心）的距离，依次得到D(1)、D(2)、...、D(n)构成的集合D，其中n表示数据集的大小。在D中，为了避免噪声，不能直接选取值最大的元素，应该选择值较大的元素，然后将其对应的数据点作为种子点。 如何选择值较大的元素呢，下面是spark中实现的思路。 求所有的距离和Sum(D(x)) 取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先用Sum(D(x))乘以随机值Random得到值r，然后用currSum += D(x)，直到其currSum &gt; r，此时的点就是下一个“种子点”。 为什么用这样的方式呢？我们换一种比较好理解的方式来说明。把集合D中的每个元素D(x)想象为一根线L(x)，线的长度就是元素的值。将这些线依次按照L(1)、L(2)、...、L(n)的顺序连接起来，组成长线L。L(1)、L(2)、…、L(n)称为L的子线。 根据概率的相关知识，如果我们在L上随机选择一个点，那么这个点所在的子线很有可能是比较长的子线，而这个子线对应的数据点就可以作为种子点。 k-means++算法的缺点 虽然k-means++算法可以确定地初始化聚类中心，但是从可扩展性来看，它存在一个缺点，那就是它内在的有序性特性：下一个中心点的选择依赖于已经选择的中心点。 针对这种缺陷，k-means||算法提供了解决方法。 k-means||算法原理分析 k-means||算法是在k-means++算法的基础上做的改进，和k-means++算法不同的是，它采用了一个采样因子l，并且l=A(k)，在spark的实现中l=2k，。这个算法首先如k-means++算法一样，随机选择一个初始中心， 然后计算选定初始中心确定之后的初始花费\\(\\psi\\)(指与最近中心点的距离)。之后处理\\(log(\\psi )\\)次迭代，在每次迭代中，给定当前中心集，通过概率\\(ld^{2}(x,C)/\\phi_{X}(C)\\)来 抽样x，将选定的x添加到初始化中心集中，并且更新\\(\\phi_{X}(C)\\)。该算法的步骤如下图所示： 第1步随机初始化一个中心点，第2-6步计算出满足概率条件的多个候选中心点C，候选中心点的个数可能大于k个，所以通过第7-8步来处理。第7步给C中所有点赋予一个权重值\\(w_{x}\\) ，这个权重值表示距离x点最近的点的个数。 第8步使用本地k-means++算法聚类出这些候选点的k个聚类中心。在spark的源码中，迭代次数是人为设定的，默认是5。 该算法与k-means++算法不同的地方是它每次迭代都会抽样出多个中心点而不是一个中心点，且每次迭代不互相依赖，这样我们可以并行的处理这个迭代过程。由于该过程产生出来的中心点的数量远远小于输入数据点的数量， 所以第8步可以通过本地k-means++算法很快的找出k个初始化中心点。何为本地k-means++算法？就是运行在单个机器节点上的k-means++。 下面我们详细分析上述三个算法的代码实现。 源代码分析 在spark中，org.apache.spark.mllib.clustering.KMeans文件实现了k-means算法以及k-means||算法，org.apache.spark.mllib.clustering.LocalKMeans文件实现了k-means++算法。 在分步骤分析spark中的源码之前我们先来了解KMeans类中参数的含义。 12345678class KMeans private ( private var k: Int,//聚类个数 private var maxIterations: Int,//迭代次数 private var runs: Int,//运行kmeans算法的次数 private var initializationMode: String,//初始化模式 private var initializationSteps: Int,//初始化步数 private var epsilon: Double,//判断kmeans算法是否收敛的阈值 private var seed: Long) 在上面的定义中，k表示聚类的个数，maxIterations表示最大的迭代次数，runs表示运行KMeans算法的次数，在spark 2.0。0开始，该参数已经不起作用了。为了更清楚的理解算法我们可以认为它为1。 initializationMode表示初始化模式，有两种选择：随机初始化和通过k-means||初始化，默认是通过k-means||初始化。initializationSteps表示通过k-means||初始化时的迭代步骤，默认是5，这是spark实现与第三章的算法步骤不一样的地方，这里迭代次数人为指定， 而第三章的算法是根据距离得到的迭代次数，为log(phi)。epsilon是判断算法是否已经收敛的阈值。 下面将分步骤分析k-means算法、k-means||算法的实现过程。 处理数据，转换为VectorWithNorm集。 123456//求向量的二范式，返回double值val norms = data.map(Vectors.norm(_, 2.0))norms.persist()val zippedData = data.zip(norms).map { case (v, norm) =&gt; new VectorWithNorm(v, norm)} 初始化中心点。 初始化中心点根据initializationMode的值来判断，如果initializationMode等于KMeans.RANDOM，那么随机初始化k个中心点，否则使用k-means||初始化k个中心点。 123456789101112val centers = initialModel match { case Some(kMeansCenters) =&gt; { Array(kMeansCenters.clusterCenters.map(s =&gt; new VectorWithNorm(s))) } case None =&gt; { if (initializationMode == KMeans.RANDOM) { initRandom(data) } else { initKMeansParallel(data) } } } （1）随机初始化中心点。 随机初始化k个中心点很简单，具体代码如下： 12345678910private def initRandom(data: RDD[VectorWithNorm]) : Array[Array[VectorWithNorm]] = { //采样固定大小为k的子集 //这里run表示我们运行的KMeans算法的次数，默认为1，以后将停止提供该参数 val sample = data.takeSample(true, runs * k, new XORShiftRandom(this.seed).nextInt()).toSeq //选取k个初始化中心点 Array.tabulate(runs)(r =&gt; sample.slice(r * k, (r + 1) * k).map { v =&gt; new VectorWithNorm(Vectors.dense(v.vector.toArray), v.norm) }.toArray) } （2）通过k-means||初始化中心点。 相比于随机初始化中心点，通过k-means||初始化k个中心点会麻烦很多，它需要依赖第三章的原理来实现。它的实现方法是initKMeansParallel。 下面按照第三章的实现步骤来分析。 第一步，我们要随机初始化第一个中心点。 1234//初始化第一个中心点val seed = new XORShiftRandom(this.seed).nextInt()val sample = data.takeSample(true, runs, seed).toSeqval newCenters = Array.tabulate(runs)(r =&gt; ArrayBuffer(sample(r).toDense)) 第二步，通过已知的中心点，循环迭代求得其它的中心点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152var step = 0while (step &lt; initializationSteps) { val bcNewCenters = data.context.broadcast(newCenters) val preCosts = costs //每个点距离最近中心的代价 costs = data.zip(preCosts).map { case (point, cost) =&gt; Array.tabulate(runs) { r =&gt; //pointCost获得与最近中心点的距离 //并与前一次迭代的距离对比取更小的那个 math.min(KMeans.pointCost(bcNewCenters.value(r), point), cost(r)) } }.persist(StorageLevel.MEMORY_AND_DISK) //所有点的代价和 val sumCosts = costs.aggregate(new Array[Double](runs))( //分区内迭代 seqOp = (s, v) =&gt; { // s += v var r = 0 while (r &lt; runs) { s(r) += v(r) r += 1 } s }, //分区间合并 combOp = (s0, s1) =&gt; { // s0 += s1 var r = 0 while (r &lt; runs) { s0(r) += s1(r) r += 1 } s0 } ) //选择满足概率条件的点 val chosen = data.zip(costs).mapPartitionsWithIndex { (index, pointsWithCosts) =&gt; val rand = new XORShiftRandom(seed ^ (step &lt;&lt; 16) ^ index) pointsWithCosts.flatMap { case (p, c) =&gt; val rs = (0 until runs).filter { r =&gt; //此处设置l=2k rand.nextDouble() &lt; 2.0 * c(r) * k / sumCosts(r) } if (rs.length &gt; 0) Some(p, rs) else None } }.collect() mergeNewCenters() chosen.foreach { case (p, rs) =&gt; rs.foreach(newCenters(_) += p.toDense) } step += 1} 在这段代码中，我们并没有选择使用log(pha)的大小作为迭代的次数，而是直接使用了人为确定的initializationSteps，这是与论文中不一致的地方。 在迭代内部我们使用概率公式 来计算满足要求的点，其中，l=2k。公式的实现如代码rand.nextDouble() &lt; 2.0 * c(r) * k / sumCosts(r)。sumCosts表示所有点距离它所属类别的中心点的欧式距离之和。 上述代码通过aggregate方法并行计算获得该值。 第三步，求最终的k个点。 通过以上步骤求得的候选中心点的个数可能会多于k个，这样怎么办呢？我们给每个中心点赋一个权重，权重值是数据集中属于该中心点所在类别的数据点的个数。 然后我们使用本地k-means++来得到这k个初始化点。具体的实现代码如下： 12345678910111213val bcCenters = data.context.broadcast(centers) //计算权重值，即各中心点所在类别的个数 val weightMap = data.flatMap { p =&gt; Iterator.tabulate(runs) { r =&gt; ((r, KMeans.findClosest(bcCenters.value(r), p)._1), 1.0) } }.reduceByKey(_ + _).collectAsMap() //最终的初始化中心 val finalCenters = (0 until runs).par.map { r =&gt; val myCenters = centers(r).toArray val myWeights = (0 until myCenters.length).map(i =&gt; weightMap.getOrElse((r, i), 0.0)).toArray LocalKMeans.kMeansPlusPlus(r, myCenters, myWeights, k, 30) } 上述代码的关键点时通过本地k-means++算法求最终的初始化点。它是通过LocalKMeans.kMeansPlusPlus来实现的。它使用k-means++来处理。 12345678910111213141516171819202122232425// 初始化一个中心点centers(0) = pickWeighted(rand, points, weights).toDense//for (i &lt;- 1 until k) { // 根据概率比例选择下一个中心点 val curCenters = centers.view.take(i) //每个点的权重与距离的乘积和 val sum = points.view.zip(weights).map { case (p, w) =&gt; w * KMeans.pointCost(curCenters, p) }.sum //取随机值 val r = rand.nextDouble() * sum var cumulativeScore = 0.0 var j = 0 //寻找概率最大的点 while (j &lt; points.length &amp;&amp; cumulativeScore &lt; r) { cumulativeScore += weights(j) * KMeans.pointCost(curCenters, points(j)) j += 1 } if (j == 0) { centers(i) = points(0).toDense } else { centers(i) = points(j - 1).toDense }} 上述代码中，points指的是候选的中心点，weights指这些点相应地权重。寻找概率最大的点的方式就是第二章提到的方式。初始化k个中心点后， 就可以通过一般的k-means流程来求最终的k个中心点了。具体的过程4.3会讲到。 4.3 确定数据点所属类别 找到中心点后，我们就需要根据距离确定数据点的聚类，即数据点和哪个中心点最近。具体代码如下： 1234567891011121314151617181920212223242526272829// 找到每个聚类中包含的点距离中心点的距离和以及这些点的个数val totalContribs = data.mapPartitions { points =&gt; val thisActiveCenters = bcActiveCenters.value val runs = thisActiveCenters.length val k = thisActiveCenters(0).length val dims = thisActiveCenters(0)(0).vector.size val sums = Array.fill(runs, k)(Vectors.zeros(dims)) val counts = Array.fill(runs, k)(0L) points.foreach { point =&gt; (0 until runs).foreach { i =&gt; //找到离给定点最近的中心以及相应的欧几里得距离 val (bestCenter, cost) = KMeans.findClosest(thisActiveCenters(i), point) costAccums(i) += cost //距离和 val sum = sums(i)(bestCenter) //y += a * x axpy(1.0, point.vector, sum) //点数量 counts(i)(bestCenter) += 1 } } val contribs = for (i &lt;- 0 until runs; j &lt;- 0 until k) yield { ((i, j), (sums(i)(j), counts(i)(j))) } contribs.iterator}.reduceByKey(mergeContribs).collectAsMap() 4.4 重新确定中心点 找到类别中包含的数据点以及它们距离中心点的距离，我们可以重新计算中心点。代码如下： 1234567891011121314151617181920212223242526//更新中心点for ((run, i) &lt;- activeRuns.zipWithIndex) { var changed = false var j = 0 while (j &lt; k) { val (sum, count) = totalContribs((i, j)) if (count != 0) { //x = a * x，求平均距离即sum/count scal(1.0 / count, sum) val newCenter = new VectorWithNorm(sum) //如果新旧两个中心点的欧式距离大于阈值 if (KMeans.fastSquaredDistance(newCenter, centers(run)(j)) &gt; epsilon * epsilon) { changed = true } centers(run)(j) = newCenter } j += 1 } if (!changed) { active(run) = false logInfo(&quot;Run &quot; + run + &quot; finished in &quot; + (iteration + 1) + &quot; iterations&quot;) } costs(run) = costAccums(i).value} 参考文献 【1】Bahman Bahmani,Benjamin Moseley,Andrea Vattani.Scalable K-Means++ 【2】David Arthur and Sergei Vassilvitskii.k-means++: The Advantages of Careful Seeding","link":"/posts/3118261352.html"},{"title":"L-BFGS","text":"牛顿法 设f(x)是二次可微实函数，又设\\(x^{(k)}\\)是f(x)一个极小点的估计，我们把f(x)在\\(x^{(k)}\\)处展开成Taylor级数， 并取二阶近似。 上式中最后一项的中间部分表示f(x)在\\(x^{(k)}\\)处的Hesse矩阵。对上式求导并令其等于0，可以的到下式： 设Hesse矩阵可逆，由上式可以得到牛顿法的迭代公式如下 (1.1) 值得注意 ， 当初始点远离极小点时，牛顿法可能不收敛。原因之一是牛顿方向不一定是下降方向，经迭代，目标函数可能上升。此外，即使目标函数下降，得到的点也不一定是沿牛顿方向最好的点或极小点。 因此，我们在牛顿方向上增加一维搜索，提出阻尼牛顿法。其迭代公式是 (1.2)： 其中，lambda是由一维搜索（参考文献【1】了解一维搜索）得到的步长，即满足 拟牛顿法 拟牛顿条件 前面介绍了牛顿法，它的突出优点是收敛很快，但是运用牛顿法需要计算二阶偏导数，而且目标函数的Hesse矩阵可能非正定。为了克服牛顿法的缺点，人们提出了拟牛顿法，它的基本思想是用不包含二阶导数的矩阵近似牛顿法中的Hesse矩阵的逆矩阵。 由于构造近似矩阵的方法不同，因而出现不同的拟牛顿法。 下面分析怎样构造近似矩阵并用它取代牛顿法中的Hesse矩阵的逆。上文 (1.2) 已经给出了牛顿法的迭代公式，为了构造Hesse矩阵逆矩阵的近似矩阵\\(H_{(k)}\\) ，需要先分析该逆矩阵与一阶导数的关系。 设在第k次迭代之后，得到\\(x^{(k+1)}\\) ，我们将目标函数f(x)在点\\(x^{(k+1)}\\)展开成Taylor级数， 并取二阶近似，得到 由此可知，在\\(x^{(k+1)}\\)附近有， 记 则有 又设Hesse矩阵可逆，那么上式可以写为如下形式。 这样，计算出p和q之后，就可以通过上面的式子估计Hesse矩阵的逆矩阵。因此，为了用不包含二阶导数的矩阵\\(H_{(k+1)}\\)取代牛顿法中Hesse矩阵的逆矩阵，有理由令\\(H_{(k+1)}\\)满足公式 (2.1) ： 公式(2.1)称为拟牛顿条件。 秩1校正 当Hesse矩阵的逆矩阵是对称正定矩阵时，满足拟牛顿条件的矩阵\\(H_{(k)}\\)也应该是对称正定矩阵。构造这样近似矩阵的一般策略是，\\(H_{(1)}\\)取为任意一个n阶对称正定矩阵，通常选择n阶单位矩阵I，然后通过修正\\(H_{(k)}\\)给定\\(H_{(k+1)}\\)。 令， 秩1校正公式写为如下公式(2.2)形式。 DFP算法 著名的DFP方法是Davidon首先提出，后来又被Feltcher和Powell改进的算法，又称为变尺度法。在这种方法中，定义校正矩阵为公式 (2.3) 那么得到的满足拟牛顿条件的DFP公式如下 (2.4) 查看文献【1】，了解DFP算法的计算步骤。 BFGS算法 前面利用拟牛顿条件 (2.1) 推导出了DFP公式 (2.4) 。下面我们用不含二阶导数的矩阵\\(B_{(k+1)}\\)近似Hesse矩阵，从而给出另一种形式的拟牛顿条件 (2.5) : 将公式 (2.1) 的H换为B，p和q互换正好可以得到公式 (2.5) 。所以我们可以得到B的修正公式 (2.6) : 这个公式称关于矩阵B的BFGS修正公式，也称为DFP公式的对偶公式。设\\(B_{(k+1)}\\)可逆，由公式 (2.1) 以及 (2.5) 可以推出： 这样可以得到关于H的BFGS公式为下面的公式 (2.7): 这个重要公式是由Broyden,Fletcher,Goldfard和Shanno于1970年提出的，所以简称为BFGS。数值计算经验表明，它比DFP公式还好，因此目前得到广泛应用。 L-BFGS（限制内存BFGS）算法 在BFGS算法中，仍然有缺陷，比如当优化问题规模很大时，矩阵的存储和计算将变得不可行。为了解决这个问题，就有了L-BFGS算法。L-BFGS即Limited-memory BFGS。 L-BFGS的基本思想是只保存最近的m次迭代信息，从而大大减少数据的存储空间。对照BFGS，重新整理一下公式： 之前的BFGS算法有如下公式(2.8) 那么同样有 将该式子带入到公式(2.8)中，可以推导出如下公式 假设当前迭代为k，只保存最近的m次迭代信息，按照上面的方式迭代m次，可以得到如下的公式(2.9) 上面迭代的最终目的就是找到k次迭代的可行方向，即 为了求可行方向r，可以使用two-loop recursion算法来求。该算法的计算过程如下，算法中出现的y即上文中提到的t： 算法L-BFGS的步骤如下所示。 OWL-QN算法 L1 正则化 在机器学习算法中，使用损失函数作为最小化误差，而最小化误差是为了让我们的模型拟合我们的训练数据，此时， 若参数过分拟合我们的训练数据就会有过拟合的问题。正则化参数的目的就是为了防止我们的模型过分拟合训练数据。此时，我们会在损失项之后加上正则化项以约束模型中的参数： \\[J(x) = l(x) + r(x)\\] 公式右边的第一项是损失函数，用来衡量当训练出现偏差时的损失，可以是任意可微凸函数（如果是非凸函数该算法只保证找到局部最优解）。 第二项是正则化项。用来对模型空间进行限制，从而得到一个更“简单”的模型。 根据对模型参数所服从的概率分布的假设的不同，常用的正则化一般有L2正则化（模型参数服从Gaussian分布）、L1正则化（模型参数服从Laplace分布）以及它们的组合形式。 L1正则化的形式如下 \\[J(x) = l(x) + C ||x||_{1}\\] L2正则化的形式如下 \\[J(x) = l(x) + C ||x||_{2}\\] L1正则化和L2正则化之间的一个最大区别在于前者可以产生稀疏解，这使它同时具有了特征选择的能力，此外，稀疏的特征权重更具有解释意义。如下图： 图左侧是L2正则，右侧为L1正则。当模型中只有两个参数，即\\(w_1\\)和\\(w_2\\)时，L2正则的约束空间是一个圆，而L1正则的约束空间为一个正方形，这样，基于L1正则的约束会产生稀疏解，即图中某一维(\\(w_2\\))为0。 而L2正则只是将参数约束在接近0的很小的区间里，而不会正好为0(不排除有0的情况)。对于L1正则产生的稀疏解有很多的好处，如可以起到特征选择的作用，因为有些维的系数为0，说明这些维对于模型的作用很小。 这里有一个问题是，L1正则化项不可微，所以无法像求L-BFGS那样去求。微软提出了OWL-QN(Orthant-Wise Limited-Memory Quasi-Newton)算法，该算法是基于L-BFGS算法的可用于求解L1正则的算法。 简单来讲，OWL-QN算法是指假定变量的象限确定的条件下使用L-BFGS算法来更新，同时，使得更新前后变量在同一个象限中(使用映射来满足条件)。 OWL-QN算法的具体过程 1 次微分 设\\(f:I\\rightarrow R\\)是一个实变量凸函数，定义在实数轴上的开区间内。这种函数不一定是处处可导的，例如绝对值函数\\(f(x)=|x|\\)。但是，从下面的图中可以看出（也可以严格地证明），对于定义域中的任何\\(x_0\\)，我们总可以作出一条直线，它通过点(\\(x_0\\), \\(f(x_0)\\))，并且要么接触f的图像，要么在它的下方。 这条直线的斜率称为函数的次导数。推广到多元函数就叫做次梯度。 凸函数\\(f:I\\rightarrow R\\)在点\\(x_0\\)的次导数，是实数c使得： 对于所有I内的x。我们可以证明，在点\\(x_0\\)的次导数的集合是一个非空闭区间\\([a, b]\\)，其中a和b是单侧极限。 它们一定存在，且满足\\(a \\leqslant b\\)。所有次导数的集合\\([a, b]\\)称为函数f在\\(x_0\\)的次微分。 2 伪梯度 利用次梯度的概念推广了梯度，定义了一个符合上述原则的伪梯度，求一维搜索的可行方向时用伪梯度来代替L-BFGS中的梯度。 其中 我们要如何理解这个伪梯度呢？对于不是处处可导的凸函数，可以分为下图所示的三种情况。 左侧极限小于0： 右侧极限大于0： 其它情况： 结合上面的三幅图表示的三种情况以及伪梯度函数公式，我们可以知道，伪梯度函数保证了在\\(x_0\\)处取得的方向导数是最小的。 3 映射 有了函数的下降的方向，接下来必须对变量的所属象限进行限制，目的是使得更新前后变量在同一个象限中，定义函数：\\(\\pi: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) 上述函数\\(\\pi\\)直观的解释是若\\(x\\)和\\(y\\)在同一象限则取\\(x\\)，若两者不在同一象限中，则取0。 4 线搜索 上述的映射是防止更新后的变量的坐标超出象限，而对坐标进行的一个约束，具体的约束的形式如下： 其中\\(x^{k} + \\alpha p _{k}\\)是更新公式，\\(\\zeta\\)表示\\(x^k\\)所在的象限，\\(p^k\\)表示伪梯度下降的方向，它们具体的形式如下： 上面的公式中，\\(v^k\\)为负伪梯度方向，\\(d^k = H_{k}v^{k}\\)。 选择\\(\\alpha\\)的方式有很多种，在OWL-QN中，使用了backtracking line search的一种变种。选择常数\\(\\beta, \\gamma \\subset (0,1)\\)，对于\\(n=0,1,2,...\\)，使得 \\(\\alpha = \\beta^{n}\\)满足： 5 算法流程 与L-BFGS相比，第一步用伪梯度代替梯度，第二、三步要求一维搜索不跨象限，也就是迭代前的点与迭代后的点处于同一象限，第四步要求估计Hessian矩阵时依然使用损失函数的梯度。 源码解析 BreezeLBFGS spark Ml调用breeze中实现的BreezeLBFGS来解最优化问题。 123val optimizer = new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))val states = optimizer.iterations(new CachedDiffFunction(costFun), initialWeights.toBreeze.toDenseVector) 下面重点分析lbfgs.iterations的实现。 1234567891011121314151617181920212223242526272829303132333435363738394041def iterations(f: DF, init: T): Iterator[State] = { val adjustedFun = adjustFunction(f) infiniteIterations(f, initialState(adjustedFun, init)).takeUpToWhere(_.converged)}//调用infiniteIterations，其中State是一个样本类def infiniteIterations(f: DF, state: State): Iterator[State] = { var failedOnce = false val adjustedFun = adjustFunction(f) //无限迭代 Iterator.iterate(state) { state =&gt; try { //1 选择梯度下降方向 val dir = chooseDescentDirection(state, adjustedFun) //2 计算步长 val stepSize = determineStepSize(state, adjustedFun, dir) //3 更新权重 val x = takeStep(state,dir,stepSize) //4 利用CostFun.calculate计算损失值和梯度 val (value,grad) = calculateObjective(adjustedFun, x, state.history) val (adjValue,adjGrad) = adjust(x,grad,value) val oneOffImprovement = (state.adjustedValue - adjValue)/(state.adjustedValue.abs max adjValue.abs max 1E-6 * state.initialAdjVal.abs) //5 计算s和t val history = updateHistory(x,grad,value, adjustedFun, state) //6 只保存m个需要的s和t val newAverage = updateFValWindow(state, adjValue) failedOnce = false var s = State(x,value,grad,adjValue,adjGrad,state.iter + 1, state.initialAdjVal, history, newAverage, 0) val improvementFailure = (state.fVals.length &gt;= minImprovementWindow &amp;&amp; state.fVals.nonEmpty &amp;&amp; state.fVals.last &gt; state.fVals.head * (1-improvementTol)) if(improvementFailure) s = s.copy(fVals = IndexedSeq.empty, numImprovementFailures = state.numImprovementFailures + 1) s } catch { case x: FirstOrderException if !failedOnce =&gt; failedOnce = true logger.error(&quot;Failure! Resetting history: &quot; + x) state.copy(history = initialHistory(adjustedFun, state.x)) case x: FirstOrderException =&gt; logger.error(&quot;Failure again! Giving up and returning. Maybe the objective is just poorly behaved?&quot;) state.copy(searchFailed = true) } } } 看上面的代码注释，它的流程可以分五步来分析。 选择梯度下降方向 123protected def chooseDescentDirection(state: State, fn: DiffFunction[T]):T = { state.history * state.grad} 这里的*是重写的方法，它的实现如下： 123456789101112131415161718192021222324252627282930313233def *(grad: T) = { val diag = if(historyLength &gt; 0) { val prevStep = memStep.head val prevGradStep = memGradDelta.head val sy = prevStep dot prevGradStep val yy = prevGradStep dot prevGradStep if(sy &lt; 0 || sy.isNaN) throw new NaNHistory sy/yy } else { 1.0 } val dir = space.copy(grad) val as = new Array[Double](m) val rho = new Array[Double](m) //第一次递归 for(i &lt;- 0 until historyLength) { rho(i) = (memStep(i) dot memGradDelta(i)) as(i) = (memStep(i) dot dir)/rho(i) if(as(i).isNaN) { throw new NaNHistory } axpy(-as(i), memGradDelta(i), dir) } dir *= diag //第二次递归 for(i &lt;- (historyLength - 1) to 0 by (-1)) { val beta = (memGradDelta(i) dot dir)/rho(i) axpy(as(i) - beta, memStep(i), dir) } dir *= -1.0 dir } } 非常明显，该方法就是实现了上文提到的two-loop recursion算法。 计算步长 12345678910protected def determineStepSize(state: State, f: DiffFunction[T], dir: T) = { val x = state.x val grad = state.grad val ff = LineSearch.functionFromSearchDirection(f, x, dir) val search = new StrongWolfeLineSearch(maxZoomIter = 10, maxLineSearchIter = 10) // TODO: Need good default values here. val alpha = search.minimize(ff, if(state.iter == 0.0) 1.0/norm(dir) else 1.0) if(alpha * norm(grad) &lt; 1E-10) throw new StepSizeUnderflow alpha } 这一步对应L-BFGS的步骤的Step 5，通过一维搜索计算步长。 更新权重 1protected def takeStep(state: State, dir: T, stepSize: Double) = state.x + dir * stepSize 这一步对应L-BFGS的步骤的Step 5，更新权重。 计算损失值和梯度 123protected def calculateObjective(f: DF, x: T, history: History): (Double, T) = { f.calculate(x) } 这一步对应L-BFGS的步骤的Step 7，使用传人的CostFun.calculate方法计算梯度和损失值。并计算出s和t。 计算s和t，并更新history 12345678910//计算s和tprotected def updateHistory(newX: T, newGrad: T, newVal: Double, f: DiffFunction[T], oldState: State): History = { oldState.history.updated(newX - oldState.x, newGrad :- oldState.grad)}//添加新的s和t，并删除过期的s和tprotected def updateFValWindow(oldState: State, newAdjVal: Double):IndexedSeq[Double] = { val interm = oldState.fVals :+ newAdjVal if(interm.length &gt; minImprovementWindow) interm.drop(1) else interm } BreezeOWLQN BreezeOWLQN的实现与BreezeLBFGS的实现主要有下面一些不同点。 选择梯度下降方向 1234567891011override protected def chooseDescentDirection(state: State, fn: DiffFunction[T]) = { val descentDir = super.chooseDescentDirection(state.copy(grad = state.adjustedGradient), fn) // The original paper requires that the descent direction be corrected to be // in the same directional (within the same hypercube) as the adjusted gradient for proof. // Although this doesn't seem to affect the outcome that much in most of cases, there are some cases // where the algorithm won't converge (confirmed with the author, Galen Andrew). val correctedDir = space.zipMapValues.map(descentDir, state.adjustedGradient, { case (d, g) =&gt; if (d * g &lt; 0) d else 0.0 }) correctedDir } 此处调用了BreezeLBFGS的chooseDescentDirection方法选择梯度下降的方向，然后调整该下降方向为正确的方向（方向必须一致）。 计算步长\\(\\alpha\\) 1234567891011121314151617181920override protected def determineStepSize(state: State, f: DiffFunction[T], dir: T) = { val iter = state.iter val normGradInDir = { val possibleNorm = dir dot state.grad possibleNorm } val ff = new DiffFunction[Double] { def calculate(alpha: Double) = { val newX = takeStep(state, dir, alpha) val (v, newG) = f.calculate(newX) // 计算梯度 val (adjv, adjgrad) = adjust(newX, newG, v) // 调整梯度 adjv -&gt; (adjgrad dot dir) } } val search = new BacktrackingLineSearch(state.value, shrinkStep= if(iter &lt; 1) 0.1 else 0.5) val alpha = search.minimize(ff, if(iter &lt; 1) .5/norm(state.grad) else 1.0) alpha } takeStep方法用于更新参数。 12345678910// projects x to be on the same orthant as y// this basically requires that x'_i = x_i if sign(x_i) == sign(y_i), and 0 otherwise.override protected def takeStep(state: State, dir: T, stepSize: Double) = { val stepped = state.x + dir * stepSize val orthant = computeOrthant(state.x, state.adjustedGradient) space.zipMapValues.map(stepped, orthant, { case (v, ov) =&gt; v * I(math.signum(v) == math.signum(ov)) })} calculate方法用于计算梯度，adjust方法用于调整梯度。 1234567891011121314151617181920212223// Adds in the regularization stuff to the gradient override protected def adjust(newX: T, newGrad: T, newVal: Double): (Double, T) = { var adjValue = newVal val res = space.zipMapKeyValues.mapActive(newX, newGrad, {case (i, xv, v) =&gt; val l1regValue = l1reg(i) require(l1regValue &gt;= 0.0) if(l1regValue == 0.0) { v } else { adjValue += Math.abs(l1regValue * xv) xv match { case 0.0 =&gt; { val delta_+ = v + l1regValue //计算左导数 val delta_- = v - l1regValue //计算右导数 if (delta_- &gt; 0) delta_- else if (delta_+ &lt; 0) delta_+ else 0.0 } case _ =&gt; v + math.signum(xv) * l1regValue } } }) adjValue -&gt; res } 参考文献 【1】陈宝林，最优化理论和算法 【2】Updating Quasi-Newton Matrices with Limited Storage 【3】On the Limited Memory BFGS Method for Large Scale Optimization 【4】L-BFGS算法 【5】BFGS算法 【6】逻辑回归模型及LBFGS的Sherman Morrison(SM) 公式推导 【7】Scalable Training of L1-Regularized Log-Linear Models","link":"/posts/4166596681.html"},{"title":"朴素贝叶斯","text":"介绍 朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。 举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。 对于某些类型的概率模型，在有监督学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换言之，在不用贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。 尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够取得相当好的效果。尽管如此，有论文证明更新的方法（如提升树和随机森林）的性能超过了贝叶斯分类器。 朴素贝叶斯分类器的一个优势在于只需要根据少量的训练数据估计出必要的参数（变量的均值和方差）。由于变量独立假设，只需要估计各个变量，而不需要确定整个协方差矩阵。 朴素贝叶斯的优缺点 优点：学习和预测的效率高，且易于实现；在数据较少的情况下仍然有效，可以处理多分类问题。 缺点：分类效果不一定很高，特征独立性假设会是朴素贝叶斯变得简单，但是会牺牲一定的分类准确率。 朴素贝叶斯概率模型 理论上，概率模型分类器是一个条件概率模型。 独立的类别变量C有若干类别，条件依赖于若干特征变量F_1,F_2,...,F_n。但问题在于如果特征数量n较大或者每个特征能取大量值时，基于概率模型列出概率表变得不现实。所以我们修改这个模型使之变得可行。 贝叶斯定理有以下式子： 实际中，我们只关心分式中的分子部分，因为分母不依赖于C而且特征F_i的值是给定的，于是分母可以认为是一个常数。这样分子就等价于联合分布模型。 重复使用链式法则，可将该式写成条件概率的形式，如下所示： 现在“朴素”的条件独立假设开始发挥作用:假设每个特征F_i对于其他特征F_j是条件独立的。这就意味着 所以联合分布模型可以表达为 这意味着上述假设下，类变量C的条件分布可以表达为： 其中Z是一个只依赖与F_1,...,F_n等的缩放因子，当特征变量的值已知时是一个常数。 从概率模型中构造分类器 讨论至此为止我们导出了独立分布特征模型，也就是朴素贝叶斯概率模型。朴素贝叶斯分类器包括了这种模型和相应的决策规则。一个普通的规则就是选出最有可能的那个：这就是大家熟知的最大后验概率（MAP）决策准则。相应的分类器便是如下定义的公式： 参数估计 所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计。类的先验概率P(C)可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。 对于类条件概率P(X|c)来说，直接根据样本出现的频率来估计会很困难。在现实应用中样本空间的取值往往远远大于训练样本数，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计P(x|c)不可行，因为\"未被观察到\"和\"出现概率为零\"是不同的。 为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。 这种假设称为朴素贝叶斯分类器的事件模型（event model）。对于离散的特征数据（例如文本分类中使用的特征），多元分布和伯努利分布比较流行。 高斯朴素贝叶斯 如果要处理的是连续数据，一种通常的假设是这些连续数值服从高斯分布。例如，假设训练集中有一个连续属性x。我们首先对数据根据类别分类，然后计算每个类别中x的均值和方差。令mu_c表示为x在c类上的均值，令sigma^2_c为x在c类上的方差。在给定类中某个值的概率 P(x=v|c)，可以通过将v表示为均值为mu_c，方差为sigma^2_c的正态分布计算出来。 处理连续数值问题的另一种常用的技术是通过离散化连续数值的方法。通常，当训练样本数量较少或者是精确的分布已知时，通过概率分布的方法是一种更好的选择。 在大量样本的情形下离散化的方法表现更优，因为大量的样本可以学习到数据的分布。由于朴素贝叶斯是一种典型的用到大量样本的方法（越大计算量的模型可以产生越高的分类精确度），所以朴素贝叶斯方法都用到离散化方法，而不是概率分布估计的方法。 多元朴素贝叶斯 在多元事件模型中，样本（特征向量）表示特定事件发生的次数。用p_i表示事件i发生的概率。特征向量X=(x_1,x_2,...,x_n)是一个histogram，其中x_i表示事件i在特定的对象中被观察到的次数。事件模型通常用于文本分类。相应的x_i表示词i在单个文档中出现的次数。 X的似然函数如下所示： 当用对数空间表达时，多元朴素贝叶斯分类器变成了线性分类器。 如果一个给定的类和特征值在训练集中没有一起出现过，那么基于频率的估计下该概率将为0。这将是一个问题。因为与其他概率相乘时将会把其他概率的信息统统去除。所以常常要求要对每个小类样本的概率估计进行修正，以保证不会出现有为0的概率出现。常用到的平滑就是加1平滑（也称拉普拉斯平滑）。 根据参考文献【2】，我们以文本分类的训练和测试为例子来介绍多元朴素贝叶斯的训练和测试过程。如下图所示。 这里的CondProb[t][c]即上文中的P(x|C)。T_ct表示类别为c的文档中t出现的次数。+1就是平滑手段。 伯努利朴素贝叶斯 在多变量伯努利事件模型中，特征是独立的二值变量。和多元模型一样，这个模型在文本分类中也非常流行。它的似然函数如下所示。 其中p_ki表示类别C_k生成term w_i的概率。这个模型通常用于短文本分类。 根据参考文献【2】，我们以文本分类的训练和测试为例子来介绍多元朴素贝叶斯的训练和测试过程。如下图所示。 源码分析 MLlib中实现了多元朴素贝叶斯和伯努利朴素贝叶斯。下面先看看朴素贝叶斯的使用实例。 实例 123456789101112131415161718import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}import org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPoint//读取并处理数据val data = sc.textFile(&quot;data/mllib/sample_naive_bayes_data.txt&quot;)val parsedData = data.map { line =&gt; val parts = line.split(',') LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))}// 切分数据为训练数据和测试数据val splits = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)val training = splits(0)val test = splits(1)//训练模型val model = NaiveBayes.train(training, lambda = 1.0, modelType = &quot;multinomial&quot;)//测试数据val predictionAndLabel = test.map(p =&gt; (model.predict(p.features), p.label))val accuracy = 1.0 * predictionAndLabel.filter(x =&gt; x._1 == x._2).count() / test.count() 训练模型 从上文的原理分析我们可以知道，朴素贝叶斯模型的训练过程就是获取概率p(C)和p(F|C)的过程。根据MLlib的源码，我们可以将训练过程分为两步。 第一步是聚合计算每个标签对应的term的频率，第二步是迭代计算p(C)和p(F|C)。 1 计算每个标签对应的term的频率 123456789101112131415161718192021val aggregated = data.map(p =&gt; (p.label, p.features)).combineByKey[(Long, DenseVector)]( createCombiner = (v: Vector) =&gt; { if (modelType == Bernoulli) { requireZeroOneBernoulliValues(v) } else { requireNonnegativeValues(v) } (1L, v.copy.toDense) }, mergeValue = (c: (Long, DenseVector), v: Vector) =&gt; { requireNonnegativeValues(v) //c._2 = v*1 + c._2 BLAS.axpy(1.0, v, c._2) (c._1 + 1L, c._2) }, mergeCombiners = (c1: (Long, DenseVector), c2: (Long, DenseVector)) =&gt; { BLAS.axpy(1.0, c2._2, c1._2) (c1._1 + c2._1, c1._2) } //根据标签进行排序 ).collect().sortBy(_._1) 这里我们需要先了解createCombiner函数的作用。createCombiner的作用是将原RDD中的Vector类型转换为(long,Vector)类型。 如果modelType为Bernoulli，那么v中包含的值只能为0或者1。如果modelType为multinomial，那么v中包含的值必须大于0。 123456789101112131415161718192021//值非负val requireNonnegativeValues: Vector =&gt; Unit = (v: Vector) =&gt; { val values = v match { case sv: SparseVector =&gt; sv.values case dv: DenseVector =&gt; dv.values } if (!values.forall(_ &gt;= 0.0)) { throw new SparkException(s&quot;Naive Bayes requires nonnegative feature values but found $v.&quot;) }}//值为0或者1val requireZeroOneBernoulliValues: Vector =&gt; Unit = (v: Vector) =&gt; { val values = v match { case sv: SparseVector =&gt; sv.values case dv: DenseVector =&gt; dv.values } if (!values.forall(v =&gt; v == 0.0 || v == 1.0)) { throw new SparkException( s&quot;Bernoulli naive Bayes requires 0 or 1 feature values but found $v.&quot;) }} mergeValue函数的作用是将新来的Vector累加到已有向量中，并更新词率。mergeCombiners则是合并不同分区的(long,Vector)数据。 通过这个函数，我们就找到了每个标签对应的词频率，并得到了标签对应的所有文档的累加向量。 2 迭代计算p(C)和p(F|C) 1234567891011121314151617181920212223242526272829303132333435//标签数val numLabels = aggregated.length//文档数var numDocuments = 0Laggregated.foreach { case (_, (n, _)) =&gt; numDocuments += n}//特征维数val numFeatures = aggregated.head match { case (_, (_, v)) =&gt; v.size }val labels = new Array[Double](numLabels)//表示logP(C)val pi = new Array[Double](numLabels)//表示logP(F|C)val theta = Array.fill(numLabels)(new Array[Double](numFeatures))val piLogDenom = math.log(numDocuments + numLabels * lambda)var i = 0aggregated.foreach { case (label, (n, sumTermFreqs)) =&gt; labels(i) = label //训练步骤的第5步 pi(i) = math.log(n + lambda) - piLogDenom val thetaLogDenom = modelType match { case Multinomial =&gt; math.log(sumTermFreqs.values.sum + numFeatures * lambda) case Bernoulli =&gt; math.log(n + 2.0 * lambda) case _ =&gt; // This should never happen. throw new UnknownError(s&quot;Invalid modelType: $modelType.&quot;) } //训练步骤的第6步 var j = 0 while (j &lt; numFeatures) { theta(i)(j) = math.log(sumTermFreqs(j) + lambda) - thetaLogDenom j += 1 } i += 1 } 这段代码计算上文提到的p(C)和p(F|C)。这里的lambda表示平滑因子，一般情况下，我们将它设置为1。代码中，p(c_i)=log (n+lambda)/(numDocs+numLabels*lambda)，这对应上文训练过程的第5步prior(c)=N_c/N。 根据modelType类型的不同，p(F|C)的实现则不同。当modelType为Multinomial时，P(F|C)=T_ct/sum(T_ct)，这里sum(T_ct)=sumTermFreqs.values.sum + numFeatures * lambda。这对应多元朴素贝叶斯训练过程的第10步。 当modelType为Bernoulli时，P(F|C)=(N_ct+lambda)/(N_c+2*lambda)。这对应伯努利贝叶斯训练算法的第8行。 需要注意的是，代码中的所有计算都是取对数计算的。 预测数据 12345678override def predict(testData: Vector): Double = { modelType match { case Multinomial =&gt; labels(multinomialCalculation(testData).argmax) case Bernoulli =&gt; labels(bernoulliCalculation(testData).argmax) }} 预测也是根据modelType的不同作不同的处理。当modelType为Multinomial时，调用multinomialCalculation函数。 12345private def multinomialCalculation(testData: Vector) = { val prob = thetaMatrix.multiply(testData) BLAS.axpy(1.0, piVector, prob) prob } 这里的thetaMatrix和piVector即上文中训练得到的P(F|C)和P(C)，根据P(C|F)=P(F|C)*P(C)即可以得到预测数据归属于某类别的概率。 注意，这些概率都是基于对数结果计算的。 当modelType为Bernoulli时，实现代码略有不同。 123456789101112private def bernoulliCalculation(testData: Vector) = { testData.foreachActive((_, value) =&gt; if (value != 0.0 &amp;&amp; value != 1.0) { throw new SparkException( s&quot;Bernoulli naive Bayes requires 0 or 1 feature values but found $testData.&quot;) } ) val prob = thetaMinusNegTheta.get.multiply(testData) BLAS.axpy(1.0, piVector, prob) BLAS.axpy(1.0, negThetaSum.get, prob) prob } 当词在训练数据中出现与否处理的过程不同。伯努利模型测试过程中，如果词存在，需要计算log(condprob)，否在需要计算log(1-condprob)，condprob为P(f|c)=exp(theta)。所以预先计算log(1-exp(theta))以及它的和可以应用到预测过程。这里thetaMatrix表示logP(F|C)，negTheta代表log(1-exp(theta))=log(1-condprob)，thetaMinusNegTheta代表log(theta - log(1-exp(theta)))。 12345678910111213private val (thetaMinusNegTheta, negThetaSum) = modelType match { case Multinomial =&gt; (None, None) case Bernoulli =&gt; val negTheta = thetaMatrix.map(value =&gt; math.log(1.0 - math.exp(value))) val ones = new DenseVector(Array.fill(thetaMatrix.numCols){1.0}) val thetaMinusNegTheta = thetaMatrix.map { value =&gt; value - math.log(1.0 - math.exp(value)) } (Option(thetaMinusNegTheta), Option(negTheta.multiply(ones))) case _ =&gt; // This should never happen. throw new UnknownError(s&quot;Invalid modelType: $modelType.&quot;) } 这里math.exp(value)将对数概率恢复成真实的概率。 参考文献 【1】朴素贝叶斯分类器 【2】Naive Bayes text classification 【3】The Bernoulli model","link":"/posts/4152295526.html"},{"title":"十大经典排序算法动画与解析，看我就够了！（配代码完全版）","text":"排序算法是《数据结构与算法》中最基本的算法之一。 排序算法可以分为内部排序和外部排序。 内部排序是数据记录在内存中进行排序。 而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。 常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。 用一张图概括： image 关于时间复杂度： 平方阶\\((O(n^2))\\)排序 各类简单排序：直接插入、直接选择和冒泡排序。 线性对数阶 \\((O(nlog2n))\\) 排序 快速排序、堆排序和归并排序； \\(O(n^{1+§})\\) 排序，§ 是介于 0 和 1 之间的常数。 希尔排序 线性阶 \\((O(n))\\) 排序 基数排序，此外还有桶、箱排序。 关于稳定性： 稳定的排序算法：冒泡排序、插入排序、归并排序和基数排序。 不是稳定的排序算法：选择排序、快速排序、希尔排序、堆排序。 冒泡排序 算法步骤 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 动画演示 #### 参考代码 1234567891011121314151617181920212223242526272829// Java 代码实现public class BubbleSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); for (int i = 1; i &lt; arr.length; i++) { // 设定一个标记，若为true，则表示此次循环没有进行交换，也就是待排序列已经有序，排序已经完成。 boolean flag = true; for (int j = 0; j &lt; arr.length - i; j++) { if (arr[j] &gt; arr[j + 1]) { int tmp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = tmp; flag = false; } } if (flag) { break; } } return arr; }} 选择排序 算法步骤 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 重复第二步，直到所有元素均排序完毕。 动画演示 #### 参考代码 123456789101112131415161718192021222324252627282930//Java 代码实现public class SelectionSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); // 总共要经过 N-1 轮比较 for (int i = 0; i &lt; arr.length - 1; i++) { int min = i; // 每轮需要比较的次数 N-i for (int j = i + 1; j &lt; arr.length; j++) { if (arr[j] &lt; arr[min]) { // 记录目前能找到的最小值元素的下标 min = j; } } // 将找到的最小值和i位置所在的值进行交换 if (i != min) { int tmp = arr[i]; arr[i] = arr[min]; arr[min] = tmp; } } return arr; }} 插入排序 算法步骤 将第一待排序序列第一个元素看做一个有序序列，把第二个元素到最后一个元素当成是未排序序列。 从头到尾依次扫描未排序序列，将扫描到的每个元素插入有序序列的适当位置。（如果待插入的元素与有序序列中的某个元素相等，则将待插入元素插入到相等元素的后面。） 动画演示 #### 参考代码 123456789101112131415161718192021222324252627282930//Java 代码实现public class InsertSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); // 从下标为1的元素开始选择合适的位置插入，因为下标为0的只有一个元素，默认是有序的 for (int i = 1; i &lt; arr.length; i++) { // 记录要插入的数据 int tmp = arr[i]; // 从已经排序的序列最右边的开始比较，找到比其小的数 int j = i; while (j &gt; 0 &amp;&amp; tmp &lt; arr[j - 1]) { arr[j] = arr[j - 1]; j--; } // 存在比其小的数，插入 if (j != i) { arr[j] = tmp; } } return arr; }} 希尔排序 算法步骤 选择一个增量序列 t1，t2，……，tk，其中 ti &gt; tj, tk = 1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 动画演示 #### 参考代码 1234567891011121314151617181920212223242526272829//Java 代码实现public class ShellSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int gap = 1; while (gap &lt; arr.length) { gap = gap * 3 + 1; } while (gap &gt; 0) { for (int i = gap; i &lt; arr.length; i++) { int tmp = arr[i]; int j = i - gap; while (j &gt;= 0 &amp;&amp; arr[j] &gt; tmp) { arr[j + gap] = arr[j]; j -= gap; } arr[j + gap] = tmp; } gap = (int) Math.floor(gap / 3); } return arr; }} ### 归并排序 算法步骤 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列； 设定两个指针，最初位置分别为两个已经排序序列的起始位置； 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置； 重复步骤 3 直到某一指针达到序列尾； 将另一序列剩下的所有元素直接复制到合并序列尾。 动画演示 #### 参考代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class MergeSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); if (arr.length &lt; 2) { return arr; } int middle = (int) Math.floor(arr.length / 2); int[] left = Arrays.copyOfRange(arr, 0, middle); int[] right = Arrays.copyOfRange(arr, middle, arr.length); return merge(sort(left), sort(right)); } protected int[] merge(int[] left, int[] right) { int[] result = new int[left.length + right.length]; int i = 0; while (left.length &gt; 0 &amp;&amp; right.length &gt; 0) { if (left[0] &lt;= right[0]) { result[i++] = left[0]; left = Arrays.copyOfRange(left, 1, left.length); } else { result[i++] = right[0]; right = Arrays.copyOfRange(right, 1, right.length); } } while (left.length &gt; 0) { result[i++] = left[0]; left = Arrays.copyOfRange(left, 1, left.length); } while (right.length &gt; 0) { result[i++] = right[0]; right = Arrays.copyOfRange(right, 1, right.length); } return result; }} 快速排序 算法步骤 从数列中挑出一个元素，称为 “基准”（pivot）; 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序； 动画演示 #### 参考代码 1234567891011121314151617181920212223242526272829303132333435363738394041//Java 代码实现public class QuickSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); return quickSort(arr, 0, arr.length - 1); } private int[] quickSort(int[] arr, int left, int right) { if (left &lt; right) { int partitionIndex = partition(arr, left, right); quickSort(arr, left, partitionIndex - 1); quickSort(arr, partitionIndex + 1, right); } return arr; } private int partition(int[] arr, int left, int right) { // 设定基准值（pivot） int pivot = left; int index = pivot + 1; for (int i = index; i &lt;= right; i++) { if (arr[i] &lt; arr[pivot]) { swap(arr, i, index); index++; } } swap(arr, pivot, index - 1); return index - 1; } private void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; }} 堆排序 算法步骤 创建一个堆 H[0……n-1]； 把堆首（最大值）和堆尾互换； 把堆的尺寸缩小 1，并调用 shift_down(0)，目的是把新的数组顶端数据调整到相应位置； 重复步骤 2，直到堆的尺寸为 1。 动画演示 #### 参考代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//Java 代码实现public class HeapSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int len = arr.length; buildMaxHeap(arr, len); for (int i = len - 1; i &gt; 0; i--) { swap(arr, 0, i); len--; heapify(arr, 0, len); } return arr; } private void buildMaxHeap(int[] arr, int len) { for (int i = (int) Math.floor(len / 2); i &gt;= 0; i--) { heapify(arr, i, len); } } private void heapify(int[] arr, int i, int len) { int left = 2 * i + 1; int right = 2 * i + 2; int largest = i; if (left &lt; len &amp;&amp; arr[left] &gt; arr[largest]) { largest = left; } if (right &lt; len &amp;&amp; arr[right] &gt; arr[largest]) { largest = right; } if (largest != i) { swap(arr, i, largest); heapify(arr, largest, len); } } private void swap(int[] arr, int i, int j) { int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; }} 计数排序 算法步骤 花O(n)的时间扫描一下整个序列 A，获取最小值 min 和最大值 max 开辟一块新的空间创建新的数组 B，长度为 ( max - min + 1) 数组 B 中 index 的元素记录的值是 A 中某元素出现的次数 最后输出目标整数序列，具体的逻辑是遍历数组 B，输出相应元素以及对应的个数 动画演示 #### 参考代码 123456789101112131415161718192021222324252627282930313233343536373839404142//Java 代码实现public class CountingSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int maxValue = getMaxValue(arr); return countingSort(arr, maxValue); } private int[] countingSort(int[] arr, int maxValue) { int bucketLen = maxValue + 1; int[] bucket = new int[bucketLen]; for (int value : arr) { bucket[value]++; } int sortedIndex = 0; for (int j = 0; j &lt; bucketLen; j++) { while (bucket[j] &gt; 0) { arr[sortedIndex++] = j; bucket[j]--; } } return arr; } private int getMaxValue(int[] arr) { int maxValue = arr[0]; for (int value : arr) { if (maxValue &lt; value) { maxValue = value; } } return maxValue; }} 桶排序 算法步骤 设置固定数量的空桶。 把数据放到对应的桶中。 对每个不为空的桶中数据进行排序。 拼接不为空的桶中数据，得到结果 动画演示 #### 参考代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465//Java 代码实现public class BucketSort implements IArraySort { private static final InsertSort insertSort = new InsertSort(); @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); return bucketSort(arr, 5); } private int[] bucketSort(int[] arr, int bucketSize) throws Exception { if (arr.length == 0) { return arr; } int minValue = arr[0]; int maxValue = arr[0]; for (int value : arr) { if (value &lt; minValue) { minValue = value; } else if (value &gt; maxValue) { maxValue = value; } } int bucketCount = (int) Math.floor((maxValue - minValue) / bucketSize) + 1; int[][] buckets = new int[bucketCount][0]; // 利用映射函数将数据分配到各个桶中 for (int i = 0; i &lt; arr.length; i++) { int index = (int) Math.floor((arr[i] - minValue) / bucketSize); buckets[index] = arrAppend(buckets[index], arr[i]); } int arrIndex = 0; for (int[] bucket : buckets) { if (bucket.length &lt;= 0) { continue; } // 对每个桶进行排序，这里使用了插入排序 bucket = insertSort.sort(bucket); for (int value : bucket) { arr[arrIndex++] = value; } } return arr; } /** * 自动扩容，并保存数据 * * @param arr * @param value */ private int[] arrAppend(int[] arr, int value) { arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; }} 基数排序 算法步骤 将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零 从最低位开始，依次进行一次排序 从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列 动画演示 #### 参考代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//Java 代码实现public class RadixSort implements IArraySort { @Override public int[] sort(int[] sourceArray) throws Exception { // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int maxDigit = getMaxDigit(arr); return radixSort(arr, maxDigit); } /** * 获取最高位数 */ private int getMaxDigit(int[] arr) { int maxValue = getMaxValue(arr); return getNumLenght(maxValue); } private int getMaxValue(int[] arr) { int maxValue = arr[0]; for (int value : arr) { if (maxValue &lt; value) { maxValue = value; } } return maxValue; } protected int getNumLenght(long num) { if (num == 0) { return 1; } int lenght = 0; for (long temp = num; temp != 0; temp /= 10) { lenght++; } return lenght; } private int[] radixSort(int[] arr, int maxDigit) { int mod = 10; int dev = 1; for (int i = 0; i &lt; maxDigit; i++, dev *= 10, mod *= 10) { // 考虑负数的情况，这里扩展一倍队列数，其中 [0-9]对应负数，[10-19]对应正数 (bucket + 10) int[][] counter = new int[mod * 2][0]; for (int j = 0; j &lt; arr.length; j++) { int bucket = ((arr[j] % mod) / dev) + mod; counter[bucket] = arrayAppend(counter[bucket], arr[j]); } int pos = 0; for (int[] bucket : counter) { for (int value : bucket) { arr[pos++] = value; } } } return arr; } private int[] arrayAppend(int[] arr, int value) { arr = Arrays.copyOf(arr, arr.length + 1); arr[arr.length - 1] = value; return arr; }} 说明：本文思路来源于：https://github.com/hustcc/JS-Sorting-Algorithm，整理人 hustcc。","link":"/posts/2698666258.html"},{"title":"基于HMM和Viterbi算法的序列标注","text":"HMM生成模型 给定句子 \\(S\\)，对应的输出词性序列 \\(T\\)，HMM模型的联合概率： \\[ \\begin{align} P(T|S) &amp;= \\frac{P(S|T)\\cdot P(T)}{P(S)}\\\\ P(S,T) &amp;= P(S|T)\\cdot P(T)\\\\ &amp;= \\prod_{i=1}^{n}P(w_i|T)\\cdot P(T)\\\\ &amp;= \\prod_{i=1}^{n}P(w_i|t_i)\\cdot P(T)\\\\ &amp;= \\prod_{i=1}^{n}P(w_i|t_i)\\cdot P(t_i|t_{i-1})\\\\ \\end{align} \\] 首先贝叶斯公式展开，然后利用 以下假设 简化： - 由词之间相互独立假设，得到 \\(\\prod_{i=1}^{n}P(w_i|T)\\) - 由单词概率仅依赖于其自身的标签，得到发射(emission)概率 \\(\\prod_{i=1}^{n}P(w_i|t_i)\\) - 由马尔可夫假设，使用 bi-gram 得到转移(transition)概率 \\(P(t_i|t_{i-1})\\) 目标函数： \\[ (\\hat{t_1},\\hat{t_2}...\\hat{t_n})=arg max\\prod_{i=1}^{n}P(w_i|t_i)\\cdot P(t_i|t_{i-1}) \\] 综上，HMM假设了两类特征：当前词性与上一词性的关系，当前词与当前词性的关系 HMM的学习过程就是在训练集中学习这两个概率矩阵，大小分别为(t,t),(w,t)，w为单词的个数，t为词性的个数 1 实战：词性标注 词性标注(Part-Of-Speech tagging, POS tagging)，判断句子中单词的词性：谓词、虚词、代词、感叹词等 本质上属于分类问题，将句子中的单词按词性分类 因此需要词性标注好的语料库，其中给定句子\\(s=w_1w_2...w_n\\)及对应的词性 \\(t=z_1z_2...z_n\\) &gt; 语料格式：每一行为词+词性，特殊符号 , 等表示句子结尾 1234567'Newsweek/NNP\\n',',/,\\n','trying/VBG\\n','to/TO\\n','keep/VB\\n','pace/NN\\n','with/IN\\n', 12# 语料样本open('../datasets/pos_tagging_data.txt','r').readlines(50) ['Newsweek/NNP\\n', ',/,\\n', 'trying/VBG\\n', 'to/TO\\n', 'keep/VB\\n', 'pace/NN\\n', 'with/IN\\n'] 创建词汇表 1234567891011121314151617181920# 创建字典，便于将文本数值化tag2id, id2tag = {}, {}word2id, id2word = {}, {}for line in open('../datasets/pos_tagging_data.txt', 'r'): items = line.split('/') word, tag = items[0], items[1].rstrip() if word not in word2id: word2id[word] = len(word2id) id2word[len(id2word)] = word if tag not in tag2id: tag2id[tag] = len(tag2id) id2tag[len(tag2id)] = tagM = len(word2id) # 词典的大小N = len(tag2id) # 词性的种类print(M,N) 发射矩阵和转移矩阵 基于语料库，计算发射矩阵和转移矩阵 12345678910111213141516171819202122232425262728import numpy as nppi = np.zeros(N) # 每个 tag 出现在句首的概率A = np.zeros((N, M)) # A[i][j],给定 tag i,出现单词 j 的概率B = np.zeros((N, N)) # B[i][j],词性为 tag i 时，其后单词的词性为 tag j 的概率prev_tag = &quot;&quot;for line in open('../datasets/pos_tagging_data.txt', 'r'): items = line.split('/') wordId, tagId = word2id[items[0]], tag2id[items[1].rstrip()] if prev_tag == &quot;&quot;: # 判断句子的开始 pi[tagId] += 1 A[tagId][wordId] += 1 else: A[tagId][wordId] += 1 B[tag2id[prev_tag]][tagId] += 1 if items[0] == &quot;.&quot;: prev_tag = &quot;&quot; else: prev_tag = items[1].rstrip()# 转化成概率pi = pi / sum(pi)for i in range(N): A[i] /= sum(A[i]) B[i] /= sum(B[i]) 1pi 维特比算法求解最优标注 1234def log_(v): if v==0: return np.log(v+0.000001) return np.log(v) 1234567891011121314151617181920212223242526272829303132from math import logdef viterbi(x, pi, A, B): # x 为输入句子，&quot;I like playing soccer&quot; x = [word2id[word] for word in x.split(&quot; &quot;)] T = len(x) dp = np.zeros((T, N)) # 默认浮点数 ptr = np.array([[0 for x in range(N)] for y in range(T)]) # 整数 for j in range(N): dp[0][j] = log_(pi[j]) + log_(A[j][x[0]]) # 需要添加平滑项 for i in range(1, T): for j in range(N): dp[i][j] = float('-inf') for k in range(N): score = dp[i - 1][k] + log_(B[k][j]) + log_(A[j][x[i]]) if score &gt; dp[i][j]: dp[i][j] = score ptr[i][j] = k # decoding：找出最好的 tag sequence best_seq = [0] * T # step1：找出最后一个单词的词性 best_seq[T - 1] = np.argmax(dp[T - 1]) # step2：从后向前循环依次找出每个单词的词性 for i in range(T - 2, -1, -1): best_seq[i] = ptr[i + 1][best_seq[i + 1]] return [id2tag[id] for id in best_seq] 12x = &quot;I like play soccer&quot;viterbi(x,pi,A,B) 实战：命名实体识别 基于人民日报语料，词汇表及标签已经处理完成 根据语料获取转移矩阵与发射矩阵 语料每一行为一个子句，每个子句中用空格隔开标注好的单字，单字与其词性用/连接 1234567891011with open(&quot;../datasets/ner/renmin/vocab.pkl&quot;, 'rb') as inp: token2idx = pickle.load(inp) idx2token = pickle.load(inp)with open(&quot;../datasets/ner/renmin/tags.pkl&quot;, &quot;rb&quot;) as inp: tag2idx = pickle.load(inp) idx2tag = pickle.load(inp) # 模型参数 N = len(tag2idx)M = len(token2idx) 123456789101112131415161718192021222324252627282930313233343536import codecsdef train_hmm(data_file): input_data = codecs.open(data_file, 'r', 'utf-8') pi = np.zeros(N) # 每个 tag 出现在句首的概率 A = np.zeros((N, M)) # A[i][j],给定 tag i,出现单词 j 的概率 B = np.zeros((N, N)) # B[i][j],词性为 tag i 时，其后单词的词性为 tag j 的概率 for line in input_data.readlines(): line = line.strip().split() tokens = [token2idx[string.split('/')[0].strip()] for string in line] tags = [tag2idx[string.split('/')[1].strip()] for string in line] for idx in range(len(tokens)): if idx == 0: pi[tags[idx]] += 1 A[tags[idx]][tokens[idx]] += 1 else: A[tags[idx]][tokens[idx]] += 1 B[tags[idx - 1]][tags[idx]] += 1 pi = pi / sum(pi) A = A / A.sum(axis=-1).reshape(-1, 1) B = B / B.sum(axis=-1).reshape(-1, 1) return pi, A, B# data_file = '../datasets/ner/renmin/renmin4.txt'# pi, A, B = train_hmm(data_file)# with open('../models/ner/hmm.pkl', 'wb') as output:# pickle.dump(pi, output)# pickle.dump(A, output) # pickle.dump(B, output) 1234with open('../models/ner/hmm.pkl', 'rb') as inp: pi = pickle.load(inp) A = pickle.load(inp) B = pickle.load(inp) 1234567891011import pandas as pd# 转移矩阵，给定前一个标记的标签，后一个标记的标签的概率分布index = [ 'B_nt', 'M_nt', 'E_nt', 'B_nr', 'M_nr', 'E_nr', 'B_ns', 'M_ns', 'E_ns', 'O']transitions = pd.DataFrame(B, index=idx2tag.values(), columns=idx2tag.values())transitions.reindex(index, axis=0).reindex(index, axis=1).round( 2).style.applymap(lambda v: 'background-color: %s' % '#B0C4DE' if v &gt; 0 else 'background-color: %s' % '#FFFFFF') 从上表中，通过语料训练得到的转移矩阵，可以看出，连续的两个标签之间的联合概率 123456# 句首标记的标签分布start_status = pd.DataFrame(pi, index=idx2tag.values())start_status.reindex(index, axis=0).round(2).style.applymap( lambda v: 'background-color: %s' % '#B0C4DE' if v &gt; 0 else 'background-color: %s' % '#FFFFFF') 句首单字的标签的概率分布 1 评估性能 1234567891011121314151617181920import picklepickle_path = '../datasets/ner/renmin/renmindata.pkl'with open(pickle_path, 'rb') as inp: word2id = pickle.load(inp) id2word = pickle.load(inp) tag2id = pickle.load(inp) id2tag = pickle.load(inp) x_train = pickle.load(inp) y_train = pickle.load(inp) # 测试数据集 x_test = pickle.load(inp) y_test = pickle.load(inp) x_valid = pickle.load(inp) y_valid = pickle.load(inp)print(&quot;train len:&quot;, len(x_train))print(&quot;test len:&quot;, len(x_test))print(&quot;valid len:&quot;, len(x_valid)) train len: 24271 test len: 7585 valid len: 6068 1x_test[0], y_test[0] (array([ 3, 33, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([3, 3, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])) 12345678910111213141516171819202122232425262728def log_(v): return np.log(v + 0.000001)# 解码def viterbi_decode(x, pi, A, B): T = len(x) N = len(tag2idx) dp = np.full((T, N), float('-inf')) ptr = np.zeros_like(dp, dtype=np.int32) dp[0] = log_(pi) + log_(A[:, x[0]]) for i in range(1, T): v = dp[i - 1].reshape(-1, 1) + log_(B) dp[i] = np.max(v, axis=0) + log_(A[:, x[i]]) ptr[i] = np.argmax(v, axis=0) best_seq = [0] * T best_seq[-1] = np.argmax(dp[-1]) for i in range(T - 2, -1, -1): best_seq[i] = ptr[i + 1][best_seq[i + 1]] return best_seq 12345678910111213141516171819from sklearn.metrics import precision_score, recall_score, f1_score, classification_reportdef test(x_test, y_test): preds, labels = [], [] for index, x in enumerate(x_test): x = x[:sum(x &gt; 0)] y_pred = viterbi_decode(x, pi, A, B) y_true = y_test[index][:sum(x &gt; 0)] preds.extend(y_pred) labels.extend(y_true) # 损失值与评测指标 precision = precision_score(labels, preds, average='macro') recall = recall_score(labels, preds, average='macro') f1 = f1_score(labels, preds, average='macro') report = classification_report(labels, preds) print(report) 1test(x_test, y_test) precision recall f1-score support 0 0.73 0.69 0.71 2151 1 0.74 0.75 0.74 8090 2 0.89 0.78 0.83 3965 3 0.94 0.96 0.95 77532 4 0.92 0.83 0.87 3964 5 0.83 0.80 0.81 4522 6 0.66 0.70 0.68 2691 7 0.80 0.76 0.78 4524 8 0.88 0.84 0.86 3654 9 0.75 0.74 0.74 2146 accuracy 0.90 113239 macro avg 0.81 0.78 0.80 113239 weighted avg 0.90 0.90 0.90 113239 进行预测 1 文本向量化 12345678910111213141516171819202122232425262728293031import re, pickle# 文本向量化class Tokenizer: def __init__(self, vocab_file): with open(vocab_file, 'rb') as inp: self.token2idx = pickle.load(inp) self.idx2token = pickle.load(inp) def encode(self, text, maxlen): seqs = re.split('[，。！？、‘’“”:]', text.strip()) # 将文本转换成索引 seq_ids = [] for seq in seqs: token_ids = [] if seq: for char in seq: if char not in self.token2idx: token_ids.append(self.token2idx['[unknown]']) else: token_ids.append(self.token2idx[char]) seq_ids.append(token_ids) # 等长化处理 num_samples = len(seq_ids) x = np.full((num_samples, maxlen), 0., dtype=np.int64) for idx, s in enumerate(seq_ids): trunc = np.array(s[:maxlen], dtype=np.int64) x[idx, :len(trunc)] = trunc return x 12vocab_file = &quot;../datasets/ner/renmin/vocab.pkl&quot;tokenizer = Tokenizer(vocab_file) 123456text = &quot;新冠肺炎疫情发生后，以习近平同志为核心的党中央将疫情防控作为头等大事来抓，习近平\\总书记亲自指挥、亲自部署，坚持把人民生命安全和身体健康放在第一位，领导全党全军全国各族人民打好疫情\\防控的人民战争、总体战、阻击战。经过艰苦卓绝的努力，武汉保卫战、湖北保卫战取得决定性成果，疫情防控\\阻击战取得重大战略成果，统筹推进疫情防控和经济社会发展工作取得积极成效。&quot;tokenizer.encode(text, maxlen=30); 进行预测 1234567def predict(input_ids): res = [] for idx, x in enumerate(input_ids): x = x[x &gt; 0] y_pred = viterbi_decode(x, pi, A, B) res.append(y_pred) return res 12input_ids = tokenizer.encode(text, maxlen=30)predict(input_ids) [[3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 9, 1, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3], [9, 1, 1, 1, 1], [5, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]] 预测向量转标签序列 1234567891011121314151617181920212223242526272829303132333435363738class Parser: def __init__(self, tags_file): with open(tags_file, &quot;rb&quot;) as inp: self.tag2idx = pickle.load(inp) self.idx2tag = pickle.load(inp) def decode(self, text, paths): seqs = re.split('[，。！？、‘’“”:]', text) labels = [[self.idx2tag[idx] for idx in seq] for seq in paths] res = [] for sent, tags in zip(seqs, labels): print(tags) tags = self._correct_tags(tags) print(tags) print('-'*100) res.append(list(zip(sent, tags))) return res def _correct_tags(self, tags): stack = [] for idx, tag in enumerate(tags): # 判断标签是否合理 if tag.startswith(&quot;B&quot;): stack.append(idx) elif tag.startswith(&quot;M&quot;) and stack and tags[ stack[-1]] == 'B_' + tag[2:]: continue elif tag.startswith(&quot;E&quot;) and stack and tags[ stack[-1]] == 'B_' + tag[2:]: stack.pop() else: stack.append(idx) for idx in stack: tags[idx] = 'O' return tags 123456tags_file = &quot;../datasets/ner/renmin/tags.pkl&quot;parser = Parser(tags_file)paths = predict(input_ids)parser.decode(text, paths) ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_nt', 'M_nt', 'E_nt', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B_nt', 'M_nt', 'E_nt', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O'] ['O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O'] ['O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['B_nt', 'M_nt', 'M_nt', 'M_nt', 'M_nt'] ['O', 'M_nt', 'M_nt', 'M_nt', 'M_nt'] ---------------------------------------------------------------------------------------------------- ['B_ns', 'E_ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['B_ns', 'E_ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] ---------------------------------------------------------------------------------------------------- [[('新', 'O'), ('冠', 'O'), ('肺', 'O'), ('炎', 'O'), ('疫', 'O'), ('情', 'O'), ('发', 'O'), ('生', 'O'), ('后', 'O')], [('以', 'O'), ('习', 'O'), ('近', 'O'), ('平', 'O'), ('同', 'O'), ('志', 'O'), ('为', 'O'), ('核', 'O'), ('心', 'O'), ('的', 'O'), ('党', 'B_nt'), ('中', 'M_nt'), ('央', 'E_nt'), ('将', 'O'), ('疫', 'O'), ('情', 'O'), ('防', 'O'), ('控', 'O'), ('作', 'O'), ('为', 'O'), ('头', 'O'), ('等', 'O'), ('大', 'O'), ('事', 'O'), ('来', 'O'), ('抓', 'O')], [('习', 'O'), ('近', 'O'), ('平', 'O'), ('总', 'O'), ('书', 'O'), ('记', 'O'), ('亲', 'O'), ('自', 'O'), ('指', 'O'), ('挥', 'O')], [('亲', 'O'), ('自', 'O'), ('部', 'O'), ('署', 'O')], [('坚', 'O'), ('持', 'O'), ('把', 'O'), ('人', 'O'), ('民', 'O'), ('生', 'O'), ('命', 'O'), ('安', 'O'), ('全', 'O'), ('和', 'O'), ('身', 'O'), ('体', 'O'), ('健', 'O'), ('康', 'O'), ('放', 'O'), ('在', 'O'), ('第', 'O'), ('一', 'O'), ('位', 'O')], [('领', 'O'), ('导', 'O'), ('全', 'O'), ('党', 'O'), ('全', 'O'), ('军', 'O'), ('全', 'O'), ('国', 'O'), ('各', 'O'), ('族', 'O'), ('人', 'O'), ('民', 'O'), ('打', 'O'), ('好', 'O'), ('疫', 'O'), ('情', 'O'), ('防', 'O'), ('控', 'O'), ('的', 'O'), ('人', 'O'), ('民', 'O'), ('战', 'O'), ('争', 'O')], [('总', 'O'), ('体', 'O'), ('战', 'O')], [('阻', 'O'), ('击', 'O'), ('战', 'O')], [('经', 'O'), ('过', 'O'), ('艰', 'O'), ('苦', 'O'), ('卓', 'O'), ('绝', 'O'), ('的', 'O'), ('努', 'O'), ('力', 'O')], [('武', 'O'), ('汉', 'M_nt'), ('保', 'M_nt'), ('卫', 'M_nt'), ('战', 'M_nt')], [('湖', 'B_ns'), ('北', 'E_ns'), ('保', 'O'), ('卫', 'O'), ('战', 'O'), ('取', 'O'), ('得', 'O'), ('决', 'O'), ('定', 'O'), ('性', 'O'), ('成', 'O'), ('果', 'O')], [('疫', 'O'), ('情', 'O'), ('防', 'O'), ('控', 'O'), ('阻', 'O'), ('击', 'O'), ('战', 'O'), ('取', 'O'), ('得', 'O'), ('重', 'O'), ('大', 'O'), ('战', 'O'), ('略', 'O'), ('成', 'O'), ('果', 'O')], [('统', 'O'), ('筹', 'O'), ('推', 'O'), ('进', 'O'), ('疫', 'O'), ('情', 'O'), ('防', 'O'), ('控', 'O'), ('和', 'O'), ('经', 'O'), ('济', 'O'), ('社', 'O'), ('会', 'O'), ('发', 'O'), ('展', 'O'), ('工', 'O'), ('作', 'O'), ('取', 'O'), ('得', 'O'), ('积', 'O'), ('极', 'O'), ('成', 'O'), ('效', 'O')]] 1ttt = [('武', 'O'), ('汉', 'M_nt'), ('保', 'M_nt'), ('卫', 'M_nt'), ('战', 'M_nt')] 1tttags = [tmp[1] for tmp in ttt] 1tttags ['O', 'M_nt', 'M_nt', 'M_nt', 'M_nt'] 1parser._correct_tags(tttags) ['O', 'O', 'O', 'O', 'O'] 12seqs = re.findall('[，。！？、‘’“”:]', text.strip()) 1seqs ['，', '，', '、', '，', '，', '、', '、', '。', '，', '、', '，', '，', '。'] 1text[9] '，' 1text[10] '以' 1import string 1string.punctuation '!&quot;#$%&amp;\\'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~' 12345678910111213141516171819202122232425262728293031323334353637383940414243class Tokenizer: def __init__(self, vocab_file): with open(vocab_file, 'rb') as inp: self.token2idx = pickle.load(inp) self.idx2token = pickle.load(inp) def encode(self, text): sep = set('，。！？、‘’“”:') res = {} start = 0 end = 0 for idx, char in enumerate(text): end = idx if char in sep: ids = [self.token2idx[char] for char in text[start:end]] res[(start,end)] = ids start seqs = re.split('[，。！？、‘’“”:]', text.strip()) # 将文本转换成索引 seq_ids = [] for seq in seqs: token_ids = [] if seq: for char in seq: if char not in self.token2idx: token_ids.append(self.token2idx['[unknown]']) else: token_ids.append(self.token2idx[char]) seq_ids.append(token_ids) # 等长化处理 num_samples = len(seq_ids) x = np.full((num_samples, maxlen), 0., dtype=np.int64) for idx, s in enumerate(seq_ids): trunc = np.array(s[:maxlen], dtype=np.int64) x[idx, :len(trunc)] = trunc return x 1 1234text = &quot;新冠肺炎疫情发生后，以习近平同志为核心的党中央将疫情防控作为头等大事来抓，习近平\\总书记亲自指挥、亲自部署，坚持把人民生命安全和身体健康放在第一位，领导全党全军全国各族人民打好疫情\\防控的人民战争、总体战、阻击战。经过艰苦卓绝的努力，武汉保卫战、湖北保卫战取得决定性成果，疫情防控\\阻击战取得重大战略成果，统筹推进疫情防控和经济社会发展工作取得积极成效。&quot; 12345678910res = []start = 0for m in re.finditer('[，。！？、‘’“”:]', text): end = m.span()[0] seg = text[start:end] seg_ids = [token2idx[token] for token in seg] seg_labels = viterbi_decode(seg_ids, pi, A, B) res.append(list(zip(list(range(start, end)), seg_labels))) start = m.span()[1]res [[(0, 3), (1, 3), (2, 3), (3, 3), (4, 3), (5, 3), (6, 3), (7, 3), (8, 3)], [(10, 3), (11, 3), (12, 3), (13, 3), (14, 3), (15, 3), (16, 3), (17, 3), (18, 3), (19, 3), (20, 9), (21, 1), (22, 0), (23, 3), (24, 3), (25, 3), (26, 3), (27, 3), (28, 3), (29, 3), (30, 3), (31, 3), (32, 3), (33, 3), (34, 3), (35, 3)], [(37, 3), (38, 3), (39, 3), (40, 3), (41, 3), (42, 3), (43, 3), (44, 3), (45, 3), (46, 3)], [(48, 3), (49, 3), (50, 3), (51, 3)], [(53, 3), (54, 3), (55, 3), (56, 3), (57, 3), (58, 3), (59, 3), (60, 3), (61, 3), (62, 3), (63, 3), (64, 3), (65, 3), (66, 3), (67, 3), (68, 3), (69, 3), (70, 3), (71, 3)], [(73, 3), (74, 3), (75, 3), (76, 3), (77, 3), (78, 3), (79, 3), (80, 3), (81, 3), (82, 3), (83, 3), (84, 3), (85, 3), (86, 3), (87, 3), (88, 3), (89, 3), (90, 3), (91, 3), (92, 3), (93, 3), (94, 3), (95, 3)], [(97, 3), (98, 3), (99, 3)], [(101, 3), (102, 3), (103, 3)], [(105, 3), (106, 3), (107, 3), (108, 3), (109, 3), (110, 3), (111, 3), (112, 3), (113, 3)], [(115, 9), (116, 1), (117, 1), (118, 1), (119, 1)], [(121, 5), (122, 7), (123, 3), (124, 3), (125, 3), (126, 3), (127, 3), (128, 3), (129, 3), (130, 3), (131, 3), (132, 3)], [(134, 3), (135, 3), (136, 3), (137, 3), (138, 3), (139, 3), (140, 3), (141, 3), (142, 3), (143, 3), (144, 3), (145, 3), (146, 3), (147, 3), (148, 3)], [(150, 3), (151, 3), (152, 3), (153, 3), (154, 3), (155, 3), (156, 3), (157, 3), (158, 3), (159, 3), (160, 3), (161, 3), (162, 3), (163, 3), (164, 3), (165, 3), (166, 3), (167, 3), (168, 3), (169, 3), (170, 3), (171, 3), (172, 3)]] 123for seg in res: for idx, tag_id in seg: print(text[idx], idx2tag[tag_id]) 新 O 冠 O 肺 O 炎 O 疫 O 情 O 发 O 生 O 后 O 以 O 习 O 近 O 平 O 同 O 志 O 为 O 核 O 心 O 的 O 党 B_nt 中 M_nt 央 E_nt 将 O 疫 O 情 O 防 O 控 O 作 O 为 O 头 O 等 O 大 O 事 O 来 O 抓 O 习 O 近 O 平 O 总 O 书 O 记 O 亲 O 自 O 指 O 挥 O 亲 O 自 O 部 O 署 O 坚 O 持 O 把 O 人 O 民 O 生 O 命 O 安 O 全 O 和 O 身 O 体 O 健 O 康 O 放 O 在 O 第 O 一 O 位 O 领 O 导 O 全 O 党 O 全 O 军 O 全 O 国 O 各 O 族 O 人 O 民 O 打 O 好 O 疫 O 情 O 防 O 控 O 的 O 人 O 民 O 战 O 争 O 总 O 体 O 战 O 阻 O 击 O 战 O 经 O 过 O 艰 O 苦 O 卓 O 绝 O 的 O 努 O 力 O 武 B_nt 汉 M_nt 保 M_nt 卫 M_nt 战 M_nt 湖 B_ns 北 E_ns 保 O 卫 O 战 O 取 O 得 O 决 O 定 O 性 O 成 O 果 O 疫 O 情 O 防 O 控 O 阻 O 击 O 战 O 取 O 得 O 重 O 大 O 战 O 略 O 成 O 果 O 统 O 筹 O 推 O 进 O 疫 O 情 O 防 O 控 O 和 O 经 O 济 O 社 O 会 O 发 O 展 O 工 O 作 O 取 O 得 O 积 O 极 O 成 O 效 O","link":"/posts/553.html"},{"title":"异构计算， GPU和框架选型指南","text":"深度学习训练和推理的过程中，会涉及到大量的向量(vector)，矩阵(matrix)和张量(tensor)操作，通常需要大量的浮点计算，包括高精度（在训练的时候）和低精度（在推理和部署的时候）。GPU， 作为一种通用可编程的加速器，最初设计是用来进行图形处理和渲染功能，但是从2007年开始，英伟达(NVIDIA)公司提出了第一个可编程通用计算平台（GPU），同时提出了CUDA框架，从此开启了GPU用于通用计算的新纪元。此后，不计其数的科研人员和开发者，对各种不同类型的算法用CUDA进行（部分）改写，从而达到几倍到数百倍的加速效果。尤其是在机器学习，特别是深度学习的浪潮来临后，GPU加速已经是各类工具实现的基本底层构架之一。本章里，会简单介绍GPU的基本架构，性能指标，框架选择等等和深度学习相关的内容。 什么是异构计算？ 异构计算是基于一个更加朴素的概念，”异构现象“，也就是不同计算平台之间，由于硬件结构（包括计算核心和内存），指令集和底层软件实现等方面的不同而有着不同的特性。异构计算就是使用结合了两个或者多个不同的计算平台，并进行协同运算。比如，比较常见的，在深度学习和机器学习中已经比较成熟的架构：CPU和GPU的异构计算;此外还有比较新的Google推出的协处理器（TPU），根据目的而定制的ASIC，可编程的FPGA等也都是现在在异构计算中使用比较多的协处理器。而，本章中会着重介绍和深度学习共同繁荣的图形加算器，也就是常说的GPU。 什么是GPU？ GPU,就如名字所包含的内容，原本开发的目的是为了进行计算机图形渲染，而减少对于CPU的负载。由于图像的原始特性，也就是像素间的独立性，所以GPU在设计的时候就遵从了“单指令流多数据流（SIMD）”架构，使得同一个指令（比如图像的某种变换），可以同时在多一个像素点上进行计算，从而得到比较大的吞吐量，才能使得计算机可以实时渲染比较复杂的2D/3D场景。在最初的应用场景里，GPU并不是作为一种通用计算平台出现的，直到2007年左右，一家伟大的公司将GPU带到通用计算的世界里，使得其可以在相对比较友好的编程环境（CUDA/OpenCL）里加速通用程序成了可能。从此之后，GPU通用计算，也就是GPU就成了学界和工业界都频繁使用的技术，在深度学习爆发的年代里，GPU成了推动这股浪潮非常重要的力量。 GPU架构简介 GPU，图形显示芯片作为不同于CPU的设计逻辑和应用场景，有着非常不同的架构，本部分将简单介绍GPU究竟是如何架构，其中的计算核心有哪些特性。 如何通俗理解GPU的架构？ 首先，下图简单地展示了几个GPU不同于CPU的特性： * 计算核心： 图中的CPU,i7-5960，Intel的第五代Broadwell架构，其中包括了8个CPU核心(支持16线程)，也就是理论上可以有16个不同的运算同时进行。除了8个核心计算单元，大部分的芯片面积是被3级缓存，内存和控制电路占据了。同样的，来自Nvidia的GTX980GPU，在差不多的芯片面积上，大部分是计算单元，16个SM，也就是流处理单元，每个流处理单元中包含着128个CUDA计算核心，所以总共来说，有2048个GPU运算单元，相应地这颗GPU理论上可以在一个时钟周期内可以进行2048次单精度运算。 计算核心频率：时钟频率，代表每一秒中内能进行同步脉冲次数，也是从一个侧面反映一个计算元件的工作速度。下图中对比了个别早期产品，比如Intel的x5650和几款Nvidia的GPU。可以看出核心频率而言，CPU要远高于GPU。对于CPU而言，在不考虑能源消耗和制程工艺限制的情况下，追求更高的主频。但，在GPU的设计中，采用了多核心设计，即使是提高一些频率，其实对于总体性能影像不会特别大。当然，其中还有能耗方面的考虑，避免发热过高，也进行了权衡。还有一个可能的原因是，在一个流处理器中的每个核心（CUDA核心）的运行共享非常有限的缓存和寄存器，由于共享内存也是有性能极限的，所以即使每个GPU核心频率提高，如果被缓存等拖累也是无法展现出高性能的。 内存架构：GPU的多层内存架构包括全局内存（也就是通常意义上大部分比较关注的内存，在若干到16GB之间，截至到当前最新），2级缓存，和芯片上的存储（包括寄存器，和1级缓存共用的共享内存，只读/纹理缓存和常量缓存）。通常来说，最高速的共享内存/缓存和寄存器都是非常有限的，比如在Tesla的K20中，只有48K的缓存可以作为共享内存或者1级缓存使用，所以在很多用GPU加速算法实现的过程中，有效地利用这些高速缓存是使得性能提升的非常重要的方面。 CUDA 核心是什么？ 上面提到在一个GPU芯片里，会很几千个CUDA核心，被分布在多个流处理单元（SM）中，比如上面提到早期的GTX980中的16个SM中各包含了128个CUDA核心。如下图所示，作为GPU架构中的最小单元，其实它的设计和CPU有着非常类似的结构，其中包括了一个浮点运算单元和整型运算单元，和控制单元。同一个流处理器中，所有的CUDA核心将同步执行同一个指令，但是作用于不同的数据点上。 一般来说，更加多的CUDA核心意味着有更多的并行执行单元，所以也就可以片面地认为是有更加高的性能。但是，其实这个也是取决于很多方面，最重要的是算法在并行实现的时候有没有高效地调度和内存的使用优化。在现在我们使用的大部分GPU加速的深度学习框架里，包括Tensorflow，PyTorch等都是依赖于底层的GPU的矩阵加速代码的实现。为此Nvidia公司也是制定和实现了统一的接口，比如cuDNN，方便上层框架更好的利用GPU的性能。 为什么要使用GPU？ 对于并行计算来说，可以非常粗略地分为： * 并行指令： 也就是多个指令可以同时分配到不同的计算核心上同时进行，而他们的操作是不同的，并且他们之间相互独立，不需要额外的同步和信息共享。 * 并行数据流： 如果数据本身存在的天然的独立性，比如图像中的每一个像素，那么在对这个图像做处理的过程中，同一个指令可以同时作用于每一个像素。在这种情况下，这个对于完整图像的操作可以并行化。理论上，如果内存不是问题，并且计算单元的数量大于整个图像中总像素点的话，这个操作可以在一个时钟周期内完成。 GPU整体的架构而言，某种意义上是同时支持以上两种并行模式。在同一个流处理器中，采用了“单一指令并行数据流的模式”，而在多个流处理器中，同一时间可以派发不同的指令。从这一点出发，GPU芯片算是一个非常灵活的架构。一个芯片中，流处理器的个数和其中包含的CUDA核心的数量也是一种面向应用设计时候找到的一个平衡点。 基于深度学习中大部分的操作的天然并行性（大量的矩阵操作），GPU在当下还是一种非常适合的计算平台。一个非常典型的例子就是常见的矩阵相乘（如下图），要计算Z = X×Y，通过并行计算，X和Y中的行向量和列向量的逐元素相乘就可以同时进行，只要得到结果后再进行累加，而且累加的过程中也是可以进行并行化，使得效率有非常大的提高。Nvidia也是制定和开发了一套底层类库，CUBlas方便开发者。我们熟悉的几大框架(e.g. Tensorflow, PyTorch等)也是遵循和使用了这些并行类库，所以才使得训练和部署性能有了非常多的提高。 深度学习中的GPU应用 深度学习在最近几年内出现的井喷现象背后也是GPU的存在和发展作为坚实的推动力量。 哪些场景使用GPU ImageNet的例子 15.3.5 新图灵架构里的tensor core对深度学习有什么作用？ 15.4 CUDA 框架 15.4.1 做CUDA编程难不难？ 15.4.2 cuDNN 15.5 GPU硬件环境配置推荐 15.5.1 GPU主要性能指标 GPU的性能主要由以下三个参数构成： 计算能力。通常我们关心的是32位浮点计算能力。16位浮点训练也开始流行，如果只做预测的话也可以用8位整数。 内存大小。当模型越大，或者训练时的批量越大时，所需要的GPU内存就越多。 内存带宽。只有当内存带宽足够时才能充分发挥计算能力。 对于大部分用户来说，只要考虑计算能力就可以了。GPU内存尽量不小于4GB。但如果GPU要同时显示图形界面，那么推荐的内存大小至少为6GB。内存带宽通常相对固定，选择空间较小。 下图描绘了GTX 900和1000系列里各个型号的32位浮点计算能力和价格的对比。其中价格为Wikipedia的建议价格。 我们可以从图中读出两点信息： 在同一个系列里面，价格和性能大体上成正比。但后发布的型号性价比更高，例如980 TI和1080 TI。 GTX 1000系列比900系列在性价比上高出2倍左右。 如果大家继续比较GTX较早的系列，也可以发现类似的规律。据此，我们推荐大家在能力范围内尽可能买较新的GPU。 对于RTX系列，新增了Tensor Cores单元及支持FP16，使得显卡的可选择范围更加多元。 购买建议 首先给出一些总体的建议： 性价比高但较贵：RTX 2070，GTX 1080 Ti 性价比高又便宜：RTX 2060，GTX 1060（6GB） 当使用数据集&gt; 250GB：GTX Titan X（Maxwell） ，NVIDIA Titan X Pascal或NVIDIA Titan Xp 没有足够的钱：GTX 1060（6GB） 几乎没有钱，入门级：GTX 1050 Ti（4GB） 做Kaggle比赛：RTX 2070、GTX 1060（6GB）适用于任何“正常”比赛，GTX 1080 Ti（预算足够可以选择RTX 2080 Ti）用于“深度学习竞赛” 计算机视觉研究员：RTX 2080 Ti（涡轮散热或水冷散热较好，方便后期增加新的显卡）如果网络很深可以选择Titan RTX 一名NLP研究人员：RTX 2080 Ti，并使用FP16来训练 搭建一个GPU集群：这个有点复杂，另做探讨。 刚开始进行深度学习研究：从RTX 2060或GTX 1060（6GB）开始，根据你下一步兴趣（入门，Kaggle比赛，研究，应用深度学习）等等，再进行选择。目前，RTX 2060和GTX 1060都比较合适入门的选择。 想尝试下深度学习，但没有过多要求：GTX 1050 ti（4或2GB） 目前独立GPU主要有AMD和Nvidia两家厂商。其中Nvidia在深度学习布局较早，对深度学习框架支持更好。因此，目前大家主要会选择Nvidia的GPU。 Nvidia有面向个人用户（例如GTX系列）和企业用户（例如Tesla系列）的两类GPU。这两类GPU的计算能力相当。然而，面向企业用户的GPU通常使用被动散热并增加了内存校验，从而更适合数据中心，并通常要比面向个人用户的GPU贵上10倍。 如果你是拥有100台机器以上的大公司用户，通常可以考虑针对企业用户的Nvidia Tesla系列。如果你是拥有10到100台机器的实验室和中小公司用户，预算充足的情况下可以考虑Nvidia DGX系列，否则可以考虑购买如Supermicro之类的性价比比较高的服务器，然后再购买安装GTX系列的GPU。 Nvidia一般每一两年发布一次新版本的GPU，例如2017年发布的是GTX 1000系列。每个系列中会有数个不同的型号，分别对应不同的性能。 软件环境搭建 深度学习其实就是指基于一套完整的软件系统来构建算法，训练模型。如何搭建一套完整的软件系统，比如操作系统的选择？安装环境中遇到的问题等等，本节做一个简单的总结。 操作系统选择？ 针对硬件厂商来说，比如NVIDIA，对各个操作系统的支持都是比较好的 ，比如Windows系列,Linux系列，但是由于Linux系统对专业技术人员比较友好，所以目前几乎所有的深度学习系统构建都是基于Linux的，比较常用的系统如Ubuntu系列，CentOS系列等等。 在构建系统的时候，如何选择合适的操作系是一个刚刚入门深度学习的工作者面临的问题，在这里给出几点建议： （1）刚刚入门，熟悉Windows系统，但是对Linux和深度学习都不太熟，这个时候可以基于windows系列系统来做入门学习 （2）简单了解Linux的使用，不太懂深度学习相关知识，可以直接基于Linux系统来搭建框架，跑一些开源的项目，慢慢深入研究学习 （3）熟悉Linux，不熟悉深度学习理论，毫无疑问，强烈推荐使用Linux系统，安装软件简单，工作效率高 总之一句话，如果不熟悉Linux，就先慢慢熟悉，最终还是要回归到Linux系统来构建深度学习系统 常用基础软件安装？ 目前有众多深度学习框架可供大家使用，但是所有框架基本都有一个共同的特点，目前几乎都是基于Nvidia的GPU来训练模型，要想更好的使用Nvidia的GPU，cuda和cudnn就是必备的软件安装。 1. 安装cuda 上文中有关于cuda的介绍，这里只是简单介绍基于Linux系统安装cuda的具体步骤，可以根据自己的需要安装cuda8.0或者cuda9.0，这两种版本的安装步骤基本一致，这里以最常用的ubuntu 16.04 lts版本为例： 1. 官网下载，地址 cuda8.0https://developer.nvidia.com/cuda-80-ga2-download-archive cuda9.0https://developer.nvidia.com/cuda-90-download-archive 进入网址之后选择对应的系统版本即可，如下图所示： ![](https://tva1.sinaimg.cn/large/0082zybply1gbqz6ymcqlj30on0l241n.jpg) 命令行中进入到cuda所在的位置，授予运行权限： cuda8.0: sudo chmod +x cuda_8.0.61_375.26_linux.run cuda9.0:sudo chmod +x cuda_9.0.176_384.81_linux.run 执行命令安装cuda： cuda8.0:sudo sh cuda_8.0.61_375.26_linux.run cuda9.0:sudo sh cuda_9.0.176_384.81_linux.run 之后命令之后下面就是安装步骤，cuda8.0和cuda9.0几乎一致： * 首先出现cuda软件的版权说明，可以直接按q键跳过阅读 * Do you accept the previously read EULA? ​accept/decline/quit: **accept** * Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81? ​(y)es/(n)o/(q)uit:**no** * Install the CUDA 9.0 Toolkit? ​(y)es/(n)o/(q)uit:**yes** * Enter Toolkit Location ​ [ default is /usr/local/cuda-9.0 ]:直接按enter键即可 * Do you want to install a symbolic link at /usr/local/cuda? ​(y)es/(n)o/(q)uit:**yes** * Install the CUDA 9.0 Samples? ​ (y)es/(n)o/(q)uit:**yes** 以上步骤基本就是cuda的安装步骤。 安装cudnn cudnn是Nvidia的专门针对深度学习的加速库。。。 本机安装还是使用docker？ GPU驱动问题 框架选择 主流框架比较 （一个大表格比较） 框架详细信息 Tensorflow Tensorflow是Google于2015年开源的基于数据流编程的深度学习框架，得益于Google强大的技术实力和品牌背书，目前Tensorflow发展迅猛，其用户量远远超过其它框架用户。 优点： 由谷歌开发、维护，因此可以保障支持、开发的持续性 巨大、活跃的社区 网络训练的低级、高级接口 「TensorBoard」是一款强大的可视化套件，旨在跟踪网络拓扑和性能，使调试更加简单 TensorFlow 不仅支持深度学习，还有支持强化学习和其他算法的工具 缺点： 计算图是纯 Python 的，因此速度较慢 图构造是静态的，意味着图必须先被「编译」再运行 PyTorch pytorch是Facebook于2017年才推出的深度学习框架，相对于其它框架，算是比较晚的了，但是这个同时也是优势，在设计的时候就会避免很多之前框架的问题，所以一经推出，就收到大家极大的欢迎 优点： 接口简洁且规范，文档齐全，和python无缝结合， 社区非常活跃，开源实现较多 提供动态计算图（意味着图是在运行时生成的），允许你处理可变长度的输入和输出，例如，在使用 RNN 时非常有用 易于编写自己的图层类型，易于在 GPU 上运行 「TensorBoard」缺少一些关键功能时，「Losswise」可以作为 Pytorch 的替代品 缺点: 1. 模型部署相对其它框架稍有劣势，不过后续的pytorch1.0版本应该会有很大改善，和caffe2合并后，caffe2的优秀的模型部署能力可以弥补这个不足 2. 3. 相关资源链接： 1. 官网教程：https://pytorch.org/tutorials/ 2. 基于pytorch的开源项目汇总：https://github.com/bharathgs/Awesome-pytorch-list 3. Keras Keras 是一个更高级、对用户最友好的 API，具有可配置的后端，由 Google Brain 团队成员 Francis Chollet 编写和维护 优点： 提供高级 API 来构建深度学习模型，使其易于阅读和使用 编写规范的文档 大型、活跃的社区 位于其他深度学习库（如 Theano 和 TensorFlow，可配置）之上 使用面向对象的设计，因此所有内容都被视为对象（如网络层、参数、优化器等）。所有模型参数都可以作为对象属性进行访问 缺点： 由于用途非常普遍，所以在性能方面比较欠缺 与 TensorFlow 后端配合使用时会出现性能问题（因为并未针对其进行优化），但与 Theano 后端配合使用时效果良好 不像 TensorFlow 或 PyTorch 那样灵活 Sonnet Caffe caffe是第一个主流产品级深度学习库，于 2014 年由 UC Berkeley 发布开源 优点： 简单网络结构无需编写代码，可快速实现 漂亮的 Matlab 和 Python 接口 完全由c++编程实现，部署方便 缺点： 1. 不灵活。在 Caffe 中，每个节点被当做一个层，因此如果你想要一种新的层类型，你需要定义完整的前向、后向和梯度更新过程。这些层是网络的构建模块，你需要在无穷无尽的列表中进行选择。（相反，在 TensorFlow 中，每个节点被当做一个张量运算例如矩阵相加、相乘或卷积。你可以轻易地定义一个层作为这些运算的组合。因此 TensorFlow 的构建模块更小巧，允许更灵活的模块化。） 2. 需要大量的非必要冗长代码。如果你希望同时支持 CPU 和 GPU，你需要为每一个实现额外的函数。你还需要使用普通的文本编辑器来定义你的模型。真令人头疼！几乎每个人都希望程序化地定义模型，因为这有利于不同组件之间的模块化。有趣的是，Caffe 的主要架构师现在在 TensorFlow 团队工作 3. 专一性。仅定位在计算机视觉（但做得很不错） 4. 不是以 Python 编写！如果你希望引入新的变动，你需要在 C++和 CUDA 上编程（对于更小的变动，你可以使用它的 Python 和 Matlab 接口） 5. 糟糕的文档 6. 安装比较困难！有大量的依赖包 Caffe2 MxNet MxNet是dmlc社区推出的深度学习框架，MXNet由学术界发起，包括数个顶尖大学的多个学科的研究人员的贡献，在2017年被亚马逊指定为官方框架。 mxnet的最知名的优点就是其对多GPU的支持和扩展性强，其优秀的性能使之在工业界占有一席之地，在amazon支持之后，其文档和开发进度明显好很多。除了高可扩展性，MXNet 还提供混合编程模型（命令式和声明式），同时兼容多种编程语言（包括 Python、C ++、R、Scala、Julia、Matlab 和 JavaScript）的代码，目前主要在推python高层接口gluon 优点： 1. 多GPU支持好，扩展性强，支持多种编程语言接口，主要是由华人团队开发，中文社区活跃，中文文档资源和课程丰富 2. 针对两大热门领域推出gluoncv和gluonNLP模块，复现经典论文，达到State-of-the-art，接口设计简单，文档齐全，拿来就可以用 缺点: 1. 现在mxnet官方社区主要在推gluon接口，接口稍有混乱，坑较多，入手门槛稍高 2. 偏小众，经典网络和项目的开源实现相对于tensorflow和pytorch还是比较少，很多还是需要自己手动实现 相关资源链接： 1. 官方教程：http://mxnet.incubator.apache.org 提供有快速入门教程和详细文档说明 2. 中文教程：http://zh.gluon.ai/ 官方的中文教程，此课程有对应的中文版视频，主要由李沐大神讲课 3. 中文论坛：https://discuss.gluon.ai/ 官方发中文论坛，mxnet的主要作者都在这里，论坛比较活跃，可及时得到作者的回答 4. 基于mxnet的开源项目实现：https://github.com/chinakook/Awesome-MXNet这里主要列举了mxnet在各个领域的项目的开源实现 CNTK PaddlePaddle 其他国内自主开发开源框架 哪些框架对于部署环境友好？ Tensorflow Serving ONNX 标准 TensorRT ONNPACK Clipper 移动平台的框架如何选择？ Tensorflow Lite Caffe2 其他 多GPU环境的配置 Tensorflow PyTorch 是不是可以分布式训练？ 可以在SPARK环境里训练或者部署模型吗？ 怎么进一步优化性能？ TVM nGraph TPU和GPU的区别？ 未来量子计算对于深度学习等AI技术的影响？ GPU购买指南 深度学习训练通常需要大量的计算资源。GPU目前是深度学习最常使用的计算加速硬件。相对于CPU来说，GPU更便宜且计算更加密集。一方面，相同计算能力的GPU的价格一般是CPU价格的十分之一。另一方面，一台服务器通常可以搭载8块或者16块GPU。因此，GPU数量可以看作是衡量一台服务器的深度学习计算能力的一个标准。 如何选择GPU GPU的主要性能指标 在选择GPU时，首先要考虑的第一个GPU性能问题是什么呢：是否为cuda核心？时钟速度多大？内存大小多少？ 这些都不是，对于深度学习性能而言，最重要的特征是内存带宽（memory bandwidth）。 简而言之：GPU针对内存带宽进行了优化，但同时牺牲了内存访问时间（延迟）。CPU的设计恰恰相反：如果涉及少量内存（例如几个数字相乘（3 * 6 * 9）），CPU可以快速计算，但是对于大量内存（如矩阵乘法（A * B * C）则很慢。由于内存带宽的限制，当涉及大量内存的问题时，GPU快速计算的优势往往会受到限制。当然，GPU和CPU之间还有更复杂的区别，关于为何GPU如此适用于处理深度学习问题，另做探讨。 所以如果你想购买一个快速的GPU，首先要关注的是GPU的带宽（bandwidth）。 整机配置 通常，我们主要用GPU做深度学习训练。因此，不需要购买高端的CPU。至于整机配置，尽量参考网上推荐的中高档的配置就好。不过，考虑到GPU的功耗、散热和体积，我们在整机配置上也需要考虑以下三个额外因素。 机箱体积。GPU尺寸较大，通常考虑较大且自带风扇的机箱。 电源。购买GPU时需要查一下GPU的功耗，例如50W到300W不等。购买电源要确保功率足够，且不会过载机房的供电。 主板的PCIe卡槽。推荐使用PCIe 3.0 16x来保证充足的GPU到主内存的带宽。如果搭载多块GPU，要仔细阅读主板说明，以确保多块GPU一起使用时仍然是16x带宽。注意，有些主板搭载4块GPU时会降到8x甚至4x带宽。 小结 在预算范围之内，尽可能买较新的GPU。 整机配置需要考虑到GPU的功耗、散热和体积。 框架选型 目前常用的框架有tensorflow,keras,pytorch,mxnet等等，各个框架的优缺点在此简单介绍： 常用框架简介 tensorflow： tensorflow由于有google的强大背书，加上其优秀的分布式设计，丰富的教程资源和论坛，工业部署方便，基本很多人都是从tensorflow入门的 优点：google的强大背书，分布式训练，教程资源丰富，常见问题基本都可以在互联网中找到解决办法，工业部署方便 缺点: 接口混乱，官方文档不够简洁，清晰， keras: keras是一种高层编程接口，其可以选择不同的后端，比如tensorflow，therao等等 优点：接口简洁，上手快，文档好，资源多 缺点: 封装的太好了导致不理解其技术细节 pytorch: PyTorch是一个开源的Python机器学习库，基于Torch，从官方1.0版本开始已经完美结合caffe2，主要应用于人工智能领域，如自然语言处理。它最初由Facebook的人工智能研究团队开发. 优点：文档清晰，兼容NumPy的张量计算，基于带基自动微分系统的深度神经网络，由Facebook开发维护，常见model都有pytorch复现版 缺点：工业部署稍弱（但是合并caffe2后支持全平台部署） mxnet mxnet是dmlc社区推出的深度学习框架，在2017年被亚马逊指定为官方框架 优点：支持多种语言，代码设计优秀，省显存，华人团队开发，中文社区活跃，官方复现经典论文推出gluoncv和gluonNLP模块，非常方便，拿来就可以用。 缺点:现在mxnet官方社区主要在推gluon接口，接口稍有混乱，坑较多，入手门槛稍高 caffe： 目前很多做深度学习比较早的大厂基本都是在用caffe，因为在2013-2015年基本就是caffe的天下，并且caffe的代码设计很优秀，基本所有代码都被翻了很多遍了，被各种分析，大厂基本都是魔改caffe，基于caffe来进行二次开发，所在目前在很多大厂还是在使用caffe 优点：资源丰富，代码容易理解，部署方便 缺点：入门门槛高，文档较少 框架选型总结: 1. 新手入门，首推pytorch，上手快，资源丰富,官方文档写的非常好(https://pytorch.org/tutorials/) 2. 目前工业部署，tensorflow是首选,资源丰富，并且在分布式训练这一块基本一家独大 3. mxnet的gluon接口有比较丰富的中文资源（教程：zh.gluon.ai，论坛：discuss.gluon.ai）,gluoncv模块（https://gluon-cv.mxnet.io）,gluonNLP模块（https://gluon-nlp.mxnet.io） 模型部署 我们一般都是通过python或者其他语言来编码训练模型，然后基于后端来进行部署 一般的框架都有自身的部署框架，比如tensorflow，pytorch，caffe2，mxnet等等 有一些框架是专门做推理部署使用的，比如 (1)tensorRT (2)TVM (3)ONNX 相关文献 [1] Aston Zhang, Mu Li, Zachary C. Lipton, and Alex J. Smola. 《动手学深度学习》附录 购买GPU, 2019. [2] Tim Dettmers. Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning, 2019.","link":"/posts/3990258621.html"},{"title":"数学基础","text":"深度学习通常又需要哪些数学基础？深度学习里的数学到底难在哪里？通常初学者都会有这些问题，在网络推荐及书本推荐里，经常看到会列出一系列数学科目，比如微积分、线性代数、概率论、复变函数、数值计算、优化理论、信息论等等。这些数学知识有相关性，但实际上按照这样的知识范围来学习，学习成本会很久，而且会很枯燥，本章我们通过选举一些数学基础里容易混淆的一些概念做以介绍，帮助大家更好的理清这些易混淆概念之间的关系。 ## 向量和矩阵 标量、向量、矩阵、张量之间的联系 标量（scalar） 一个标量表示一个单独的数，它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小写的变量名称。 向量（vector） ​一个向量表示一组有序排列的数。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比如xx。向量中的元素可以通过带脚标的斜体表示。向量\\(X\\)的第一个元素是\\(X_1\\)，第二个元素是\\(X_2\\)，以此类推。我们也会注明存储在向量中的元素的类型（实数、虚数等）。 矩阵（matrix） ​矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。其意义是一个对象表示为矩阵中的一行，一个特征表示为矩阵中的一列，每个特征都有数值型的取值。通常会赋予矩阵粗体的大写变量名称，比如\\(A\\)。 张量（tensor） ​在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用 \\(A\\) 来表示张量“A”。张量\\(A\\)中坐标为\\((i,j,k)\\)的元素记作\\(A_{(i,j,k)}\\)。 四者之间关系 标量是0阶张量，向量是一阶张量。举例： ​标量就是知道棍子的长度，但是你不会知道棍子指向哪儿。 ​向量就是不但知道棍子的长度，还知道棍子指向前面还是后面。 ​张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子又向上/下和左/右偏转了多少。 张量与矩阵的区别 从代数角度讲， 矩阵它是向量的推广。向量可以看成一维的“表格”（即分量按照顺序排成一排）， 矩阵是二维的“表格”（分量按照纵横位置排列）， 那么\\(n\\)阶张量就是所谓的\\(n\\)维的“表格”。 张量的严格定义是利用线性映射来描述。 从几何角度讲， 矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。 张量可以用3×3矩阵形式来表达。 表示标量的数和表示向量的三维数组也可分别看作1×1，1×3的矩阵。 矩阵和向量相乘结果 若使用爱因斯坦求和约定（Einstein summation convention），矩阵\\(A\\), \\(B\\)相乘得到矩阵\\(C\\)可以用下式表示： \\[ a_{ik}*b_{kj}=c_{ij} \\tag{1.3-1} \\] 其中，\\(a_{ik}\\), \\(b_{kj}\\), \\(c_{ij}\\)分别表示矩阵\\(A, B, C\\)的元素，\\(k\\)出现两次，是一个哑变量（Dummy Variables）表示对该参数进行遍历求和。 而矩阵和向量相乘可以看成是矩阵相乘的一个特殊情况，例如：矩阵\\(B\\)是一个\\(n \\times 1\\)的矩阵。 向量和矩阵的范数归纳 向量的范数(norm) ​ 定义一个向量为：\\(\\vec{a}=[-5, 6, 8, -10]\\)。任意一组向量设为\\(\\vec{x}=(x_1,x_2,...,x_N)\\)。其不同范数求解如下： 向量的1范数：向量的各个元素的绝对值之和，上述向量\\(\\vec{a}\\)的1范数结果就是：29。 \\[ \\Vert\\vec{x}\\Vert_1=\\sum_{i=1}^N\\vert{x_i}\\vert \\] 向量的2范数：向量的每个元素的平方和再开平方根，上述\\(\\vec{a}\\)的2范数结果就是：15。 \\[ \\Vert\\vec{x}\\Vert_2=\\sqrt{\\sum_{i=1}^N{\\vert{x_i}\\vert}^2} \\] 向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量\\(\\vec{a}\\)的负无穷范数结果就是：5。 \\[ \\Vert\\vec{x}\\Vert_{-\\infty}=\\min{|{x_i}|} \\] 向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量\\(\\vec{a}\\)的正无穷范数结果就是：10。 \\[ \\Vert\\vec{x}\\Vert_{+\\infty}=\\max{|{x_i}|} \\] 向量的p范数： \\[ L_p=\\Vert\\vec{x}\\Vert_p=\\sqrt[p]{\\sum_{i=1}^{N}|{x_i}|^p} \\] 矩阵的范数 定义一个矩阵\\(A=[-1, 2, -3; 4, -6, 6]\\)。 任意矩阵定义为：\\(A_{m\\times n}\\)，其元素为 \\(a_{ij}\\)。 矩阵的范数定义为 \\[ \\Vert{A}\\Vert_p :=\\sup_{x\\neq 0}\\frac{\\Vert{Ax}\\Vert_p}{\\Vert{x}\\Vert_p} \\] 当向量取不同范数时, 相应得到了不同的矩阵范数。 矩阵的1范数（列范数）：矩阵的每一列上的元 素绝对值先求和，再从中取个最大的,（列和最大），上述矩阵\\(A\\)的1范数先得到\\([5,8,9]\\)，再取最大的最终结果就是：9。 \\[ \\Vert A\\Vert_1=\\max_{1\\le j\\le n}\\sum_{i=1}^m|{a_{ij}}| \\] 矩阵的2范数：矩阵\\(A^TA\\)的最大特征值开平方根，上述矩阵\\(A\\)的2范数得到的最终结果是：10.0623。 \\[ \\Vert A\\Vert_2=\\sqrt{\\lambda_{max}(A^T A)} \\] 其中， \\(\\lambda_{max}(A^T A)\\) 为 \\(A^T A​\\) 的特征值绝对值的最大值。 - 矩阵的无穷范数（行范数）：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大），上述矩阵\\(A\\)的行范数先得到\\([6；16]\\)，再取最大的最终结果就是：16。 \\[ \\Vert A\\Vert_{\\infty}=\\max_{1\\le i \\le m}\\sum_{j=1}^n |{a_{ij}}| \\] 矩阵的核范数：矩阵的奇异值（将矩阵svd分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287。 矩阵的L0范数：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵\\(A\\)最终结果就是：6。 矩阵的L1范数：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵\\(A\\)最终结果就是：22。 矩阵的F范数：矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，它的优点在于它是一个凸函数，可以求导求解，易于计算，上述矩阵A最终结果就是：10.0995。 \\[ \\Vert A\\Vert_F=\\sqrt{(\\sum_{i=1}^m\\sum_{j=1}^n{| a_{ij}|}^2)} \\] 矩阵的L21范数：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵\\(A\\)最终结果就是：17.1559。 矩阵的 p范数 \\[ \\Vert A\\Vert_p=\\sqrt[p]{(\\sum_{i=1}^m\\sum_{j=1}^n{| a_{ij}|}^p)} \\] 如何判断一个矩阵为正定 判定一个矩阵是否为正定，通常有以下几个方面： 顺序主子式全大于0； 存在可逆矩阵\\(C\\)使\\(C^TC\\)等于该矩阵； 正惯性指数等于\\(n\\)； 合同于单位矩阵\\(E\\)（即：规范形为\\(E\\)） 标准形中主对角元素全为正； 特征值全为正； 是某基的度量矩阵。 导数和偏导数 导数偏导计算 导数定义: 导数(derivative)代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量的变化的比值。几何意义是这个点的切线。物理意义是该时刻的（瞬时）变化率。 ​ 注意：在一元函数中，只有一个自变量变动，也就是说只存在一个方向的变化率，这也就是为什么一元函数没有偏导数的原因。在物理学中有平均速度和瞬时速度之说。平均速度有 \\[ v=\\frac{s}{t} \\] 其中\\(v\\)表示平均速度，\\(s\\)表示路程，\\(t\\)表示时间。这个公式可以改写为 \\[ \\bar{v}=\\frac{\\Delta s}{\\Delta t}=\\frac{s(t_0+\\Delta t)-s(t_0)}{\\Delta t} \\] 其中\\(\\Delta s\\)表示两点之间的距离，而\\(\\Delta t\\)表示走过这段距离需要花费的时间。当\\(\\Delta t\\)趋向于0（\\(\\Delta t \\to 0\\)）时，也就是时间变得很短时，平均速度也就变成了在\\(t_0\\)时刻的瞬时速度，表示成如下形式： \\[ v(t_0)=\\lim_{\\Delta t \\to 0}{\\bar{v}}=\\lim_{\\Delta t \\to 0}{\\frac{\\Delta s}{\\Delta t}}=\\lim_{\\Delta t \\to 0}{\\frac{s(t_0+\\Delta t)-s(t_0)}{\\Delta t}} \\] 实际上，上式表示的是路程\\(s\\)关于时间\\(t\\)的函数在\\(t=t_0\\)处的导数。一般的，这样定义导数：如果平均变化率的极限存在，即有 \\[ \\lim_{\\Delta x \\to 0}{\\frac{\\Delta y}{\\Delta x}}=\\lim_{\\Delta x \\to 0}{\\frac{f(x_0+\\Delta x)-f(x_0)}{\\Delta x}} \\] 则称此极限为函数 \\(y=f(x)\\) 在点 \\(x_0\\) 处的导数。记作 \\(f'(x_0)\\) 或 \\(y'\\vert_{x=x_0}\\) 或 \\(\\frac{dy}{dx}\\vert_{x=x_0}\\) 或 \\(\\frac{df(x)}{dx}\\vert_{x=x_0}\\)。 通俗地说，导数就是曲线在某一点切线的斜率。 偏导数: 既然谈到偏导数(partial derivative)，那就至少涉及到两个自变量。以两个自变量为例，\\(z=f(x,y)​\\)，从导数到偏导数，也就是从曲线来到了曲面。曲线上的一点，其切线只有一条。但是曲面上的一点，切线有无数条。而偏导数就是指多元函数沿着坐标轴的变化率。 注意：直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。 设函数\\(z=f(x,y)​\\)在点\\((x_0,y_0)​\\)的领域内有定义，当\\(y=y_0​\\)时，\\(z​\\)可以看作关于\\(x​\\)的一元函数\\(f(x,y_0)​\\)，若该一元函数在\\(x=x_0​\\)处可导，即有 \\[ \\lim_{\\Delta x \\to 0}{\\frac{f(x_0+\\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}}=A \\] 函数的极限\\(A\\)存在。那么称\\(A\\)为函数\\(z=f(x,y)\\)在点\\((x_0,y_0)\\)处关于自变量\\(x\\)的偏导数，记作\\(f_x(x_0,y_0)\\)或\\(\\frac{\\partial z}{\\partial x}\\vert_{y=y_0}^{x=x_0}\\)或\\(\\frac{\\partial f}{\\partial x}\\vert_{y=y_0}^{x=x_0}\\)或\\(z_x\\vert_{y=y_0}^{x=x_0}\\)。 偏导数在求解时可以将另外一个变量看做常数，利用普通的求导方式求解，比如\\(z=3x^2+xy\\)关于\\(x\\)的偏导数就为\\(z_x=6x+y\\)，这个时候\\(y\\)相当于\\(x\\)的系数。 某点\\((x_0,y_0)\\)处的偏导数的几何意义为曲面\\(z=f(x,y)\\)与面\\(x=x_0\\)或面\\(y=y_0\\)交线在\\(y=y_0\\)或\\(x=x_0\\)处切线的斜率。 导数和偏导数有什么区别？ 导数和偏导没有本质区别，如果极限存在，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。 一元函数，一个\\(y\\)对应一个\\(x\\)，导数只有一个。 二元函数，一个\\(z\\)对应一个\\(x\\)和一个\\(y\\)，有两个导数：一个是\\(z\\)对\\(x\\)的导数，一个是\\(z\\)对\\(y\\)的导数，称之为偏导。 求偏导时要注意，对一个变量求导，则视另一个变量为常数，只对改变量求导，从而将偏导的求解转化成了一元函数的求导。 特征值和特征向量 特征值分解与特征向量 特征值分解可以得到特征值(eigenvalues)与特征向量(eigenvectors)； 特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。 如果说一个向量\\(\\vec{v}\\)是方阵\\(A\\)的特征向量，将一定可以表示成下面的形式： \\[ A\\nu = \\lambda \\nu \\] \\(\\lambda\\)为特征向量\\(\\vec{v}\\)对应的特征值。特征值分解是将一个矩阵分解为如下形式： \\[ A=Q\\sum Q^{-1} \\] 其中，\\(Q\\)是这个矩阵\\(A\\)的特征向量组成的矩阵，\\(\\sum\\)是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵\\(A\\)的信息可以由其特征值和特征向量表示。 奇异值与特征值有什么关系 那么奇异值和特征值是怎么对应起来的呢？我们将一个矩阵\\(A\\)的转置乘以\\(A\\)，并对\\(A^TA​\\)求特征值，则有下面的形式： \\[ (A^TA)V = \\lambda V \\] 这里\\(V​\\)就是上面的右奇异向量，另外还有： \\[ \\sigma_i = \\sqrt{\\lambda_i}, u_i=\\frac{1}{\\sigma_i}A\\mu_i \\] 这里的\\(\\sigma​\\)就是奇异值，\\(u​\\)就是上面说的左奇异向量。【证明那个哥们也没给】 ​奇异值\\(\\sigma​\\)跟特征值类似，在矩阵\\(\\sum​\\)中也是从大到小排列，而且\\(\\sigma​\\)的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前\\(r​\\)（\\(r​\\)远小于\\(m、n​\\)）个的奇异值来近似描述矩阵，即部分奇异值分解： \\[ A_{m\\times n}\\approx U_{m \\times r}\\sum_{r\\times r}V_{r \\times n}^T \\] 右边的三个矩阵相乘的结果将会是一个接近于\\(A\\)的矩阵，在这儿，\\(r\\)越接近于\\(n\\)，则相乘的结果越接近于\\(A\\)。 概率分布与随机变量 机器学习为什么要使用概率 事件的概率是衡量该事件发生的可能性的量度。虽然在一次随机试验中某个事件的发生是带有偶然性的，但那些可在相同条件下大量重复的随机试验却往往呈现出明显的数量规律。 ​机器学习除了处理不确定量，也需处理随机量。不确定性和随机性可能来自多个方面，使用概率论来量化不确定性。 ​概率论在机器学习中扮演着一个核心角色，因为机器学习算法的设计通常依赖于对数据的概率假设。 ​ 例如在机器学习（Andrew Ng）的课中，会有一个朴素贝叶斯假设就是条件独立的一个例子。该学习算法对内容做出假设，用来分辨电子邮件是否为垃圾邮件。假设无论邮件是否为垃圾邮件，单词x出现在邮件中的概率条件独立于单词y。很明显这个假设不是不失一般性的，因为某些单词几乎总是同时出现。然而，最终结果是，这个简单的假设对结果的影响并不大，且无论如何都可以让我们快速判别垃圾邮件。 变量与随机变量有什么区别 随机变量（random variable） 表示随机现象（在一定条件下，并不总是出现相同结果的现象称为随机现象）中各种结果的实值函数（一切可能的样本点）。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数等，都是随机变量的实例。 ​随机变量与模糊变量的不确定性的本质差别在于，后者的测定结果仍具有不确定性，即模糊性。 变量与随机变量的区别： ​当变量的取值的概率不是1时,变量就变成了随机变量；当随机变量取值的概率为1时,随机变量就变成了变量。 比如： ​ 当变量\\(x\\)值为100的概率为1的话,那么\\(x=100\\)就是确定了的,不会再有变化,除非有进一步运算. ​ 当变量\\(x\\)的值为100的概率不为1,比如为50的概率是0.5,为100的概率是0.5,那么这个变量就是会随不同条件而变化的,是随机变量,取到50或者100的概率都是0.5,即50%。 随机变量与概率分布的联系 一个随机变量仅仅表示一个可能取得的状态，还必须给定与之相伴的概率分布来制定每个状态的可能性。用来描述随机变量或一簇随机变量的每一个可能的状态的可能性大小的方法，就是 概率分布(probability distribution). 随机变量可以分为离散型随机变量和连续型随机变量。 相应的描述其概率分布的函数是 概率质量函数(Probability Mass Function, PMF):描述离散型随机变量的概率分布，通常用大写字母 \\(P\\)表示。 概率密度函数(Probability Density Function, PDF):描述连续型随机变量的概率分布，通常用小写字母\\(p\\)表示。 离散型随机变量和概率质量函数 PMF 将随机变量能够取得的每个状态映射到随机变量取得该状态的概率。 一般而言，\\(P(x)​\\) 表示时\\(X=x​\\)的概率. 有时候为了防止混淆，要明确写出随机变量的名称\\(P(​\\)x\\(=x)​\\) 有时候需要先定义一个随机变量，然后制定它遵循的概率分布x服从\\(P(​\\)x​\\()​\\) PMF 可以同时作用于多个随机变量，即联合概率分布(joint probability distribution) \\(P(X=x,Y=y)\\)*表示 \\(X=x\\)和\\(Y=y\\)同时发生的概率，也可以简写成 \\(P(x,y)\\). 如果一个函数\\(P​\\)是随机变量 \\(X​\\) 的 PMF， 那么它必须满足如下三个条件 \\(P​\\)的定义域必须是的所有可能状态的集合 \\(∀x∈​\\)x, \\(0 \\leq P(x) \\leq 1 ​\\). \\(∑_{x∈X} P(x)=1\\). 我们把这一条性质称之为 归一化的(normalized) 连续型随机变量和概率密度函数 如果一个函数\\(p​\\)是x的PDF，那么它必须满足如下几个条件 \\(p\\)的定义域必须是 xx 的所有可能状态的集合。 \\(∀x∈X,p(x)≥0\\). 注意，我们并不要求$ p(x)≤1$，因为此处 \\(p(x)\\)不是表示的对应此状态具体的概率，而是概率的一个相对大小(密度)。具体的概率，需要积分去求。 \\(∫p(x)dx=1\\), 积分下来，总和还是1，概率之和还是1. 注：PDF\\(p(x)\\)并没有直接对特定的状态给出概率，给出的是密度，相对的，它给出了落在面积为 \\(δx\\)的无线小的区域内的概率为$ p(x)δx$. 由此，我们无法求得具体某个状态的概率，我们可以求得的是 某个状态 \\(x\\) 落在 某个区间\\([a,b]\\)内的概率为$ _{a}^{b}p(x)dx$. 举例理解条件概率 条件概率公式如下： \\[ P(A|B) = P(A\\cap B) / P(B) \\] 说明：在同一个样本空间\\(\\Omega\\)中的事件或者子集\\(A\\)与\\(B\\)，如果随机从\\(\\Omega\\)中选出的一个元素属于\\(B\\)，那么下一个随机选择的元素属于\\(A\\) 的概率就定义为在\\(B\\)的前提下\\(A\\)的条件概率。条件概率文氏图示意如图1.1所示。 图1.1 条件概率文氏图示意 根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是\\(P(A\\bigcap B)\\)除以\\(P(B)\\)。 ​举例：一对夫妻有两个小孩，已知其中一个是女孩，则另一个是女孩子的概率是多少？（面试、笔试都碰到过） ​穷举法：已知其中一个是女孩，那么样本空间为男女，女女，女男，则另外一个仍然是女生的概率就是1/3。 ​条件概率法：\\(P(女|女)=P(女女)/P(女)\\),夫妻有两个小孩，那么它的样本空间为女女，男女，女男，男男，则\\(P(女女)\\)为1/4，\\(P（女）= 1-P(男男)=3/4\\),所以最后\\(1/3\\)。 这里大家可能会误解，男女和女男是同一种情况，但实际上类似姐弟和兄妹是不同情况。 联合概率与边缘概率联系区别 区别： ​联合概率：联合概率指类似于\\(P(X=a,Y=b)\\)这样，包含多个条件，且所有条件同时成立的概率。联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。 ​边缘概率：边缘概率是某个事件发生的概率，而与其它事件无关。边缘概率指类似于\\(P(X=a)\\)，\\(P(Y=b)\\)这样，仅与单个随机变量有关的概率。 联系： ​联合分布可求边缘分布，但若只知道边缘分布，无法求得联合分布。 条件概率的链式法则 由条件概率的定义，可直接得出下面的乘法公式： ​乘法公式 设\\(A, B\\)是两个事件，并且\\(P(A) &gt; 0\\), 则有 \\[ P(AB) = P(B|A)P(A) \\] 推广 \\[ P(ABC)=P(C|AB)P(B|A)P(A) \\] 一般地，用归纳法可证：若\\(P(A_1A_2...A_n)&gt;0\\)，则有 \\[ P(A_1A_2...A_n)=P(A_n|A_1A_2...A_{n-1})P(A_{n-1}|A_1A_2...A_{n-2})...P(A_2|A_1)P(A_1) =P(A_1)\\prod_{i=2}^{n}P(A_i|A_1A_2...A_{i-1}) \\] 任何多维随机变量联合概率分布，都可以分解成只有一个变量的条件概率相乘形式。 独立性和条件独立性 独立性 ​两个随机变量\\(x\\)和\\(y\\)，概率分布表示成两个因子乘积形式，一个因子只包含\\(x\\)，另一个因子只包含\\(y\\)，两个随机变量相互独立(independent)。 ​条件有时为不独立的事件之间带来独立，有时也会把本来独立的事件，因为此条件的存在，而失去独立性。 ​举例：\\(P(XY)=P(X)P(Y)\\), 事件\\(X\\)和事件\\(Y\\)独立。此时给定\\(Z\\)， \\[ P(X,Y|Z) \\not = P(X|Z)P(Y|Z) \\] 事件独立时，联合概率等于概率的乘积。这是一个非常好的数学性质，然而不幸的是，无条件的独立是十分稀少的，因为大部分情况下，事件之间都是互相影响的。 条件独立性 ​给定\\(Z\\)的情况下,\\(X\\)和\\(Y\\)条件独立，当且仅当 \\[ X\\bot Y|Z \\iff P(X,Y|Z) = P(X|Z)P(Y|Z) \\] \\(X\\)和\\(Y\\)的关系依赖于\\(Z\\)，而不是直接产生。 举例定义如下事件： \\(X\\)：明天下雨； \\(Y\\)：今天的地面是湿的； \\(Z\\)：今天是否下雨； \\(Z\\)事件的成立，对\\(X\\)和\\(Y\\)均有影响，然而，在\\(Z\\)事件成立的前提下，今天的地面情况对明天是否下雨没有影响。 常见概率分布 Bernoulli分布 Bernoulli分布是单个二值随机变量分布, 单参数\\(\\phi​\\)∈[0,1]控制,\\(\\phi​\\)给出随机变量等于1的概率. 主要性质有: \\[ \\begin{align*} P(x=1) &amp;= \\phi \\\\ P(x=0) &amp;= 1-\\phi \\\\ P(x=x) &amp;= \\phi^x(1-\\phi)^{1-x} \\\\ \\end{align*} \\] 其期望和方差为： \\[ \\begin{align*} E_x[x] &amp;= \\phi \\\\ Var_x(x) &amp;= \\phi{(1-\\phi)} \\end{align*} \\] Multinoulli分布也叫范畴分布, 是单个k值随机分布,经常用来表示对象分类的分布. 其中\\(k\\)是有限值.Multinoulli分布由向量\\(\\vec{p}\\in[0,1]^{k-1}\\)参数化,每个分量\\(p_i\\)表示第\\(i\\)个状态的概率, 且\\(p_k=1-1^Tp​\\). 适用范围: 伯努利分布适合对离散型随机变量建模. 高斯分布 高斯也叫正态分布(Normal Distribution), 概率度函数如下: \\[ N(x;\\mu,\\sigma^2) = \\sqrt{\\frac{1}{2\\pi\\sigma^2}}exp\\left ( -\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right ) \\] 其中, \\(\\mu​\\)和\\(\\sigma​\\)分别是均值和方差, 中心峰值x坐标由\\(\\mu​\\)给出, 峰的宽度受\\(\\sigma​\\)控制, 最大点在\\(x=\\mu​\\)处取得, 拐点为\\(x=\\mu\\pm\\sigma​\\) 正态分布中，±1\\(\\sigma\\)、±2\\(\\sigma\\)、±3\\(\\sigma\\)下的概率分别是68.3%、95.5%、99.73%，这3个数最好记住。 此外, 令\\(\\mu=0,\\sigma=1​\\)高斯分布即简化为标准正态分布: \\[ N(x;\\mu,\\sigma^2) = \\sqrt{\\frac{1}{2\\pi}}exp\\left ( -\\frac{1}{2}x^2 \\right ) \\] 对概率密度函数高效求值: \\[ N(x;\\mu,\\beta^{-1})=\\sqrt{\\frac{\\beta}{2\\pi}}exp\\left(-\\frac{1}{2}\\beta(x-\\mu)^2\\right) \\] 其中，\\(\\beta=\\frac{1}{\\sigma^2}\\)通过参数\\(\\beta∈（0，\\infty）​\\)来控制分布精度。 何时采用正态分布 问: 何时采用正态分布? 答: 缺乏实数上分布的先验知识, 不知选择何种形式时, 默认选择正态分布总是不会错的, 理由如下: 中心极限定理告诉我们, 很多独立随机变量均近似服从正态分布, 现实中很多复杂系统都可以被建模成正态分布的噪声, 即使该系统可以被结构化分解. 正态分布是具有相同方差的所有概率分布中, 不确定性最大的分布, 换句话说, 正态分布是对模型加入先验知识最少的分布. 正态分布的推广: 正态分布可以推广到\\(R^n\\)空间, 此时称为多位正态分布, 其参数是一个正定对称矩阵\\(\\Sigma​\\): \\[ N(x;\\vec\\mu,\\Sigma)=\\sqrt{\\frac{1}{(2\\pi)^ndet(\\Sigma)}}exp\\left(-\\frac{1}{2}(\\vec{x}-\\vec{\\mu})^T\\Sigma^{-1}(\\vec{x}-\\vec{\\mu})\\right) \\] 对多为正态分布概率密度高效求值: \\[ N(x;\\vec{\\mu},\\vec\\beta^{-1}) = \\sqrt{det(\\vec\\beta)}{(2\\pi)^n}exp\\left(-\\frac{1}{2}(\\vec{x}-\\vec\\mu)^T\\beta(\\vec{x}-\\vec\\mu)\\right) \\] 此处，\\(\\vec\\beta\\)是一个精度矩阵。 指数分布 深度学习中, 指数分布用来描述在\\(x=0​\\)点处取得边界点的分布, 指数分布定义如下: \\[ p(x;\\lambda)=\\lambda I_{x\\geq 0}exp(-\\lambda{x}) \\] 指数分布用指示函数\\(I_{x\\geq 0}​\\)来使\\(x​\\)取负值时的概率为零。 Laplace 分布 一个联系紧密的概率分布是 Laplace 分布（Laplace distribution），它允许我们在任意一点 \\(\\mu\\)处设置概率质量的峰值 \\[ Laplace(x;\\mu;\\gamma)=\\frac{1}{2\\gamma}exp\\left(-\\frac{|x-\\mu|}{\\gamma}\\right) \\] Dirac分布和经验分布 Dirac分布可保证概率分布中所有质量都集中在一个点上. Diract分布的狄拉克\\(\\delta​\\)函数(也称为单位脉冲函数)定义如下: \\[ p(x)=\\delta(x-\\mu), x\\neq \\mu \\] \\[ \\int_{a}^{b}\\delta(x-\\mu)dx = 1, a &lt; \\mu &lt; b \\] Dirac 分布经常作为 经验分布（empirical distribution）的一个组成部分出现 \\[ \\hat{p}(\\vec{x})=\\frac{1}{m}\\sum_{i=1}^{m}\\delta(\\vec{x}-{\\vec{x}}^{(i)}) \\] , 其中, m个点\\(x^{1},...,x^{m}\\)是给定的数据集, 经验分布将概率密度\\(\\frac{1}{m}​\\)赋给了这些点. 当我们在训练集上训练模型时, 可以认为从这个训练集上得到的经验分布指明了采样来源. 适用范围: 狄拉克δ函数适合对连续型随机变量的经验分布. 期望、方差、协方差、相关系数 期望 在概率论和统计学中，数学期望（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和。它反映随机变量平均取值的大小。 线性运算： \\(E(ax+by+c) = aE(x)+bE(y)+c\\) 推广形式： \\(E(\\sum_{k=1}^{n}{a_ix_i+c}) = \\sum_{k=1}^{n}{a_iE(x_i)+c}\\) 函数期望：设\\(f(x)\\)为\\(x\\)的函数，则\\(f(x)\\)的期望为 离散函数： \\(E(f(x))=\\sum_{k=1}^{n}{f(x_k)P(x_k)}\\) 连续函数： \\(E(f(x))=\\int_{-\\infty}^{+\\infty}{f(x)p(x)dx}\\) 注意： 函数的期望大于等于期望的函数（Jensen不等式），即\\(E(f(x))\\geqslant f(E(x))\\) 一般情况下，乘积的期望不等于期望的乘积。 如果\\(X\\)和\\(Y\\)相互独立，则\\(E(xy)=E(x)E(y)​\\)。 方差 概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。方差是一种特殊的期望。定义为： \\[ Var(x) = E((x-E(x))^2) \\] 方差性质： 1）\\(Var(x) = E(x^2) -E(x)^2\\) 2）常数的方差为0; 3）方差不满足线性性质; 4）如果\\(X\\)和\\(Y\\)相互独立, \\(Var(ax+by)=a^2Var(x)+b^2Var(y)\\) 协方差 协方差是衡量两个变量线性相关性强度及变量尺度。 两个随机变量的协方差定义为： \\[ Cov(x,y)=E((x-E(x))(y-E(y))) \\] 方差是一种特殊的协方差。当\\(X=Y\\)时，\\(Cov(x,y)=Var(x)=Var(y)\\)。 协方差性质： 1）独立变量的协方差为0。 2）协方差计算公式： \\[ Cov(\\sum_{i=1}^{m}{a_ix_i}, \\sum_{j=1}^{m}{b_jy_j}) = \\sum_{i=1}^{m} \\sum_{j=1}^{m}{a_ib_jCov(x_iy_i)} \\] 3）特殊情况： \\[ Cov(a+bx, c+dy) = bdCov(x, y) \\] 相关系数 相关系数是研究变量之间线性相关程度的量。两个随机变量的相关系数定义为： \\[ Corr(x,y) = \\frac{Cov(x,y)}{\\sqrt{Var(x)Var(y)}} \\] 相关系数的性质： 1）有界性。相关系数的取值范围是 [-1,1]，可以看成无量纲的协方差。 2）值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性。 参考文献 [1]Ian，Goodfellow，Yoshua，Bengio，Aaron...深度学习[M]，人民邮电出版，2017 [2]周志华.机器学习[M].清华大学出版社，2016. [3]同济大学数学系.高等数学（第七版）[M]，高等教育出版社，2014. [4]盛骤，试式千，潘承毅等编. 概率论与数理统计（第4版）[M]，高等教育出版社，2008","link":"/posts/239090248.html"},{"title":"经典网络解读","text":"LeNet-5 模型介绍 ​ LeNet-5是由\\(LeCun\\) 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络（Convolutional Neural Network，CNN）\\(^{[1]}\\)，其命名来源于作者\\(LeCun\\)的名字，5则是其研究成果的代号，在LeNet-5之前还有LeNet-4和LeNet-1鲜为人知。LeNet-5阐述了图像中像素特征之间的相关性能够由参数共享的卷积操作所提取，同时使用卷积、下采样（池化）和非线性映射这样的组合结构，是当前流行的大多数深度图像识别网络的基础。 ### 模型结构 ​ 图4.1 LeNet-5网络结构图 ​ 如图4.1所示，LeNet-5一共包含7层（输入层不作为网络结构），分别由2个卷积层、2个下采样层和3个连接层组成，网络的参数配置如表4.1所示，其中下采样层和全连接层的核尺寸分别代表采样范围和连接矩阵的尺寸（如卷积核尺寸中的\\(“5\\times5\\times1/1,6”\\)表示核大小为\\(5\\times5\\times1\\)、步长为\\(1​\\)且核个数为6的卷积核）。 ​ 表4.1 LeNet-5网络参数配置 网络层 输入尺寸 核尺寸 输出尺寸 可训练参数量 卷积层\\(C_1\\) \\(32\\times32\\times1\\) \\(5\\times5\\times1/1,6\\) \\(28\\times28\\times6\\) \\((5\\times5\\times1+1)\\times6\\) 下采样层\\(S_2\\) \\(28\\times28\\times6\\) \\(2\\times2/2\\) \\(14\\times14\\times6\\) \\((1+1)\\times6\\) \\(^*\\) 卷积层\\(C_3\\) \\(14\\times14\\times6\\) \\(5\\times5\\times6/1,16\\) \\(10\\times10\\times16\\) \\(1516^*\\) 下采样层\\(S_4\\) \\(10\\times10\\times16\\) \\(2\\times2/2\\) \\(5\\times5\\times16\\) \\((1+1)\\times16\\) 卷积层\\(C_5\\)\\(^*\\) \\(5\\times5\\times16\\) \\(5\\times5\\times16/1,120\\) \\(1\\times1\\times120\\) \\((5\\times5\\times16+1)\\times120\\) 全连接层\\(F_6\\) \\(1\\times1\\times120\\) \\(120\\times84\\) \\(1\\times1\\times84\\) \\((120+1)\\times84\\) 输出层 \\(1\\times1\\times84\\) \\(84\\times10\\) \\(1\\times1\\times10\\) \\((84+1)\\times10\\) ​ \\(^*\\) 在LeNet中，下采样操作和池化操作类似，但是在得到采样结果后会乘以一个系数和加上一个偏置项，所以下采样的参数个数是\\((1+1)\\times6​\\)而不是零。 ​ \\(^*\\) \\(C_3\\)卷积层可训练参数并未直接连接\\(S_2\\)中所有的特征图（Feature Map），而是采用如图4.2所示的采样特征方式进行连接（稀疏连接），生成的16个通道特征图中分别按照相邻3个特征图、相邻4个特征图、非相邻4个特征图和全部6个特征图进行映射，得到的参数个数计算公式为\\(6\\times(25\\times3+1)+6\\times(25\\times4+1)+3\\times(25\\times4+1)+1\\times(25\\times6+1)=1516\\)，在原论文中解释了使用这种采样方式原因包含两点：限制了连接数不至于过大（当年的计算能力比较弱）;强制限定不同特征图的组合可以使映射得到的特征图学习到不同的特征模式。 ​ 图4.2 \\(S_2\\)与\\(C_3\\)之间的特征图稀疏连接 ​ \\(^*\\) \\(C_5\\)卷积层在图4.1中显示为全连接层，原论文中解释这里实际采用的是卷积操作，只是刚好在\\(5\\times5\\)卷积后尺寸被压缩为\\(1\\times1​\\)，输出结果看起来和全连接很相似。 模型特性 卷积网络使用一个3层的序列组合：卷积、下采样（池化）、非线性映射（LeNet-5最重要的特性，奠定了目前深层卷积网络的基础） 使用卷积提取空间特征 使用映射的空间均值进行下采样 使用\\(tanh\\)或\\(sigmoid\\)进行非线性映射 多层神经网络（MLP）作为最终的分类器 层间的稀疏连接矩阵以避免巨大的计算开销 AlexNet 模型介绍 ​ AlexNet是由\\(Alex\\) \\(Krizhevsky\\)提出的首个应用于图像分类的深层卷积神经网络，该网络在2012年ILSVRC（ImageNet Large Scale Visual Recognition Competition）图像分类竞赛中以15.3%的top-5测试错误率赢得第一名\\(^{[2]}\\)。AlexNet使用GPU代替CPU进行运算，使得在可接受的时间范围内模型结构能够更加复杂，它的出现证明了深层卷积神经网络在复杂模型下的有效性，使CNN在计算机视觉中流行开来，直接或间接地引发了深度学习的热潮。 模型结构 ​ 图4.3 AlexNet网络结构图 ​ 如图4.3所示，除去下采样（池化层）和局部响应规范化操作（Local Responsible Normalization, LRN），AlexNet一共包含8层，前5层由卷积层组成，而剩下的3层为全连接层。网络结构分为上下两层，分别对应两个GPU的操作过程，除了中间某些层（\\(C_3\\)卷积层和\\(F_{6-8}\\)全连接层会有GPU间的交互），其他层两个GPU分别计算结 果。最后一层全连接层的输出作为\\(softmax\\)的输入，得到1000个图像分类标签对应的概率值。除去GPU并行结构的设计，AlexNet网络结构与LeNet十分相似，其网络的参数配置如表4.2所示。 ​ 表4.2 AlexNet网络参数配置 网络层 输入尺寸 核尺寸 输出尺寸 可训练参数量 卷积层\\(C_1\\) \\(^*\\) \\(224\\times224\\times3\\) \\(11\\times11\\times3/4,48(\\times2_{GPU})\\) \\(55\\times55\\times48(\\times2_{GPU})\\) \\((11\\times11\\times3+1)\\times48\\times2\\) 下采样层\\(S_{max}\\)\\(^*\\) \\(55\\times55\\times48(\\times2_{GPU})\\) \\(3\\times3/2(\\times2_{GPU})\\) \\(27\\times27\\times48(\\times2_{GPU})\\) 0 卷积层\\(C_2\\) \\(27\\times27\\times48(\\times2_{GPU})\\) \\(5\\times5\\times48/1,128(\\times2_{GPU})\\) \\(27\\times27\\times128(\\times2_{GPU})\\) \\((5\\times5\\times48+1)\\times128\\times2\\) 下采样层\\(S_{max}\\) \\(27\\times27\\times128(\\times2_{GPU})\\) \\(3\\times3/2(\\times2_{GPU})\\) \\(13\\times13\\times128(\\times2_{GPU})\\) 0 卷积层\\(C_3\\) \\(^*\\) \\(13\\times13\\times128\\times2_{GPU}\\) \\(3\\times3\\times256/1,192(\\times2_{GPU})\\) \\(13\\times13\\times192(\\times2_{GPU})\\) \\((3\\times3\\times256+1)\\times192\\times2\\) 卷积层\\(C_4\\) \\(13\\times13\\times192(\\times2_{GPU})\\) \\(3\\times3\\times192/1,192(\\times2_{GPU})\\) \\(13\\times13\\times192(\\times2_{GPU})\\) \\((3\\times3\\times192+1)\\times192\\times2\\) 卷积层\\(C_5\\) \\(13\\times13\\times192(\\times2_{GPU})\\) \\(3\\times3\\times192/1,128(\\times2_{GPU})\\) \\(13\\times13\\times128(\\times2_{GPU})\\) \\((3\\times3\\times192+1)\\times128\\times2\\) 下采样层\\(S_{max}\\) \\(13\\times13\\times128(\\times2_{GPU})\\) \\(3\\times3/2(\\times2_{GPU})\\) \\(6\\times6\\times128(\\times2_{GPU})\\) 0 全连接层\\(F_6\\) \\(^*\\) \\(6\\times6\\times128\\times2_{GPU}\\) \\(9216\\times2048(\\times2_{GPU})\\) \\(1\\times1\\times2048(\\times2_{GPU})\\) \\((9216+1)\\times2048\\times2\\) 全连接层\\(F_7\\) \\(1\\times1\\times2048\\times2_{GPU}\\) \\(4096\\times2048(\\times2_{GPU})\\) \\(1\\times1\\times2048(\\times2_{GPU})\\) \\((4096+1)\\times2048\\times2\\) 全连接层\\(F_8\\) \\(1\\times1\\times2048\\times2_{GPU}\\) \\(4096\\times1000\\) \\(1\\times1\\times1000\\) \\((4096+1)\\times1000\\times2\\) 卷积层\\(C_1\\)输入为\\(224\\times224\\times3\\)的图片数据，分别在两个GPU中经过核为\\(11\\times11\\times3\\)、步长（stride）为4的卷积卷积后，分别得到两条独立的\\(55\\times55\\times48\\)的输出数据。 下采样层\\(v\\)实际上是嵌套在卷积中的最大池化操作，但是为了区分没有采用最大池化的卷积层单独列出来。在\\(C_{1-2}\\)卷积层中的池化操作之后（ReLU激活操作之前），还有一个LRN操作，用作对相邻特征点的归一化处理。 卷积层\\(C_3\\)的输入与其他卷积层不同，\\(13\\times13\\times192\\times2_{GPU}\\)表示汇聚了上一层网络在两个GPU上的输出结果作为输入，所以在进行卷积操作时通道上的卷积核维度为384。 全连接层\\(F_{6-8}\\)中输入数据尺寸也和\\(C_3\\)类似，都是融合了两个GPU流向的输出结果作为输入。 模型特性 所有卷积层都使用ReLU作为非线性映射函数，使模型收敛速度更快 在多个GPU上进行模型的训练，不但可以提高模型的训练速度，还能提升数据的使用规模 使用LRN对局部的特征进行归一化，结果作为ReLU激活函数的输入能有效降低错误率 重叠最大池化（overlapping max pooling），即池化范围z与步长s存在关系\\(z&gt;s\\)（如\\(S_{max}\\)中核尺度为\\(3\\times3/2\\)），避免平均池化（average pooling）的平均效应 使用随机丢弃技术（dropout）选择性地忽略训练中的单个神经元，避免模型的过拟合 ZFNet 模型介绍 ​ ZFNet是由\\(Matthew\\) \\(D. Zeiler\\)和\\(Rob\\) \\(Fergus\\)在AlexNet基础上提出的大型卷积网络，在2013年ILSVRC图像分类竞赛中以11.19%的错误率获得冠军（实际上原ZFNet所在的队伍并不是真正的冠军，原ZFNet以13.51%错误率排在第8，真正的冠军是\\(Clarifai\\)这个队伍，而\\(Clarifai\\)这个队伍所对应的一家初创公司的CEO又是\\(Zeiler\\)，而且\\(Clarifai\\)对ZFNet的改动比较小，所以通常认为是ZFNet获得了冠军）\\(^{[3-4]}​\\)。ZFNet实际上是微调（fine-tuning）了的AlexNet，并通过反卷积（Deconvolution）的方式可视化各层的输出特征图，进一步解释了卷积操作在大型网络中效果显著的原因。 模型结构 ​ 图4.4 ZFNet网络结构图（原始结构图与AlexNet风格结构图） ​ 如图4.4所示，ZFNet与AlexNet类似，都是由8层网络组成的卷积神经网络，其中包含5层卷积层和3层全连接层。两个网络结构最大的不同在于，ZFNet第一层卷积采用了\\(7\\times7\\times3/2\\)的卷积核替代了AlexNet中第一层卷积核\\(11\\times11\\times3/4\\)的卷积核。图4.5中ZFNet相比于AlexNet在第一层输出的特征图中包含更多中间频率的信息，而AlexNet第一层输出的特征图大多是低频或高频的信息，对中间频率特征的缺失导致后续网络层次如图4.5（c）能够学习到的特征不够细致，而导致这个问题的根本原因在于AlexNet在第一层中采用的卷积核和步长过大。 ​ 图4.5 （a）ZFNet第一层输出的特征图（b）AlexNet第一层输出的特征图（c）AlexNet第二层输出的特征图（d）ZFNet第二层输出的特征图 ​ 表4.3 ZFNet网络参数配置 ​ | 网络层 | 输入尺寸 | 核尺寸 | 输出尺寸 | 可训练参数量 | | :-------------------: | :----------------------------------: | :--------------------------------------: | :----------------------------------: | :-------------------------------------: | | 卷积层\\(C_1\\) \\(^*\\) | \\(224\\times224\\times3\\) | \\(7\\times7\\times3/2,96\\) | \\(110\\times110\\times96\\) | \\((7\\times7\\times3+1)\\times96\\) | | 下采样层\\(S_{max}\\) | \\(110\\times110\\times96\\) | \\(3\\times3/2\\) | \\(55\\times55\\times96\\) | 0 | | 卷积层\\(C_2\\) \\(^*\\) | \\(55\\times55\\times96\\) | \\(5\\times5\\times96/2,256\\) | \\(26\\times26\\times256\\) | \\((5\\times5\\times96+1)\\times256\\) | | 下采样层\\(S_{max}\\) | \\(26\\times26\\times256\\) | \\(3\\times3/2\\) | \\(13\\times13\\times256\\) | 0 | | 卷积层\\(C_3\\) | \\(13\\times13\\times256\\) | \\(3\\times3\\times256/1,384\\) | \\(13\\times13\\times384\\) | \\((3\\times3\\times256+1)\\times384\\) | | 卷积层\\(C_4\\) | \\(13\\times13\\times384\\) | \\(3\\times3\\times384/1,384\\) | \\(13\\times13\\times384\\) | \\((3\\times3\\times384+1)\\times384\\) | | 卷积层\\(C_5\\) | \\(13\\times13\\times384\\) | \\(3\\times3\\times384/1,256\\) | \\(13\\times13\\times256\\) | \\((3\\times3\\times384+1)\\times256\\) | | 下采样层\\(S_{max}\\) | \\(13\\times13\\times256\\) | \\(3\\times3/2\\) | \\(6\\times6\\times256\\) | 0 | | 全连接层\\(F_6\\) | \\(6\\times6\\times256\\) | \\(9216\\times4096\\) | \\(1\\times1\\times4096\\) | \\((9216+1)\\times4096\\) | | 全连接层\\(F_7\\) | \\(1\\times1\\times4096\\) | \\(4096\\times4096\\) | \\(1\\times1\\times4096\\) | \\((4096+1)\\times4096\\) | | 全连接层\\(F_8\\) | \\(1\\times1\\times4096\\) | \\(4096\\times1000\\) | \\(1\\times1\\times1000\\) | \\((4096+1)\\times1000\\) | 卷积层\\(C_1\\)与AlexNet中的\\(C_1\\)有所不同，采用\\(7\\times7\\times3/2\\)的卷积核代替\\(11\\times11\\times3/4​\\)，使第一层卷积输出的结果可以包含更多的中频率特征，对后续网络层中多样化的特征组合提供更多选择，有利于捕捉更细致的特征。 卷积层\\(C_2\\)采用了步长2的卷积核，区别于AlexNet中\\(C_2\\)的卷积核步长，所以输出的维度有所差异。 模型特性 ​ ZFNet与AlexNet在结构上几乎相同，此部分虽属于模型特性，但准确地说应该是ZFNet原论文中可视化技术的贡献。 可视化技术揭露了激发模型中每层单独的特征图。 可视化技术允许观察在训练阶段特征的演变过程且诊断出模型的潜在问题。 可视化技术用到了多层解卷积网络，即由特征激活返回到输入像素空间。 可视化技术进行了分类器输出的敏感性分析，即通过阻止部分输入图像来揭示那部分对于分类是重要的。 可视化技术提供了一个非参数的不变性来展示来自训练集的哪一块激活哪个特征图，不仅需要裁剪输入图片，而且自上而下的投影来揭露来自每块的结构激活一个特征图。 可视化技术依赖于解卷积操作，即卷积操作的逆过程，将特征映射到像素上。 Network in Network 模型介绍 ​ Network In Network (NIN)是由\\(Min Lin\\)等人提出，在CIFAR-10和CIFAR-100分类任务中达到当时的最好水平，因其网络结构是由三个多层感知机堆叠而被成为NIN\\(^{[5]}\\)。NIN以一种全新的角度审视了卷积神经网络中的卷积核设计，通过引入子网络结构代替纯卷积中的线性映射部分，这种形式的网络结构激发了更复杂的卷积神经网络的结构设计，其中下一节中介绍的GoogLeNet的Inception结构就是来源于这个思想。 模型结构 ​ 图 4.6 NIN网络结构图 ​ NIN由三层的多层感知卷积层（MLPConv Layer）构成，每一层多层感知卷积层内部由若干层的局部全连接层和非线性激活函数组成，代替了传统卷积层中采用的线性卷积核。在网络推理（inference）时，这个多层感知器会对输入特征图的局部特征进行划窗计算，并且每个划窗的局部特征图对应的乘积的权重是共享的，这两点是和传统卷积操作完全一致的，最大的不同在于多层感知器对局部特征进行了非线性的映射，而传统卷积的方式是线性的。NIN的网络参数配置表4.4所示（原论文并未给出网络参数，表中参数为编者结合网络结构图和CIFAR-100数据集以\\(3\\times3\\)卷积为例给出）。 ​ 表4.4 NIN网络参数配置（结合原论文NIN结构和CIFAR-100数据给出） 网络层 输入尺寸 核尺寸 输出尺寸 参数个数 局部全连接层\\(L_{11}\\) \\(^*\\) \\(32\\times32\\times3\\) \\((3\\times3)\\times16/1\\) \\(30\\times30\\times16\\) \\((3\\times3\\times3+1)\\times16\\) 全连接层\\(L_{12}\\) \\(^*\\) \\(30\\times30\\times16\\) \\(16\\times16\\) \\(30\\times30\\times16\\) \\(((16+1)\\times16)\\) 局部全连接层\\(L_{21}\\) \\(30\\times30\\times16\\) \\((3\\times3)\\times64/1\\) \\(28\\times28\\times64\\) \\((3\\times3\\times16+1)\\times64\\) 全连接层\\(L_{22}\\) \\(28\\times28\\times64\\) \\(64\\times64\\) \\(28\\times28\\times64\\) \\(((64+1)\\times64)\\) 局部全连接层\\(L_{31}\\) \\(28\\times28\\times64\\) \\((3\\times3)\\times100/1\\) \\(26\\times26\\times100\\) \\((3\\times3\\times64+1)\\times100\\) 全连接层\\(L_{32}\\) \\(26\\times26\\times100\\) \\(100\\times100\\) \\(26\\times26\\times100\\) \\(((100+1)\\times100)\\) 全局平均采样\\(GAP\\) \\(^*\\) \\(26\\times26\\times100\\) \\(26\\times26\\times100/1\\) \\(1\\times1\\times100\\) \\(0\\) 局部全连接层\\(L_{11}\\)实际上是对原始输入图像进行划窗式的全连接操作，因此划窗得到的输出特征尺寸为\\(30\\times30\\)（\\(\\frac{32-3_k+1}{1_{stride}}=30\\)） 全连接层\\(L_{12}\\)是紧跟\\(L_{11}\\)后的全连接操作，输入的特征是划窗后经过激活的局部响应特征，因此仅需连接\\(L_{11}\\)和\\(L_{12}\\)的节点即可，而每个局部全连接层和紧接的全连接层构成代替卷积操作的多层感知卷积层（MLPConv）。 全局平均采样层或全局平均池化层\\(GAP\\)（Global Average Pooling）将\\(L_{32}\\)输出的每一个特征图进行全局的平均池化操作，直接得到最后的类别数，可以有效地减少参数量。 模型特点 使用多层感知机结构来代替卷积的滤波操作，不但有效减少卷积核数过多而导致的参数量暴涨问题，还能通过引入非线性的映射来提高模型对特征的抽象能力。 使用全局平均池化来代替最后一个全连接层，能够有效地减少参数量（没有可训练参数），同时池化用到了整个特征图的信息，对空间信息的转换更加鲁棒，最后得到的输出结果可直接作为对应类别的置信度。 VGGNet 模型介绍 ​ VGGNet是由牛津大学视觉几何小组（Visual Geometry Group, VGG）提出的一种深层卷积网络结构，他们以7.32%的错误率赢得了2014年ILSVRC分类任务的亚军（冠军由GoogLeNet以6.65%的错误率夺得）和25.32%的错误率夺得定位任务（Localization）的第一名（GoogLeNet错误率为26.44%）\\(^{[5]}\\)，网络名称VGGNet取自该小组名缩写。VGGNet是首批把图像分类的错误率降低到10%以内模型，同时该网络所采用的\\(3\\times3\\)卷积核的思想是后来许多模型的基础，该模型发表在2015年国际学习表征会议（International Conference On Learning Representations, ICLR）后至今被引用的次数已经超过1万4千余次。 模型结构 ​ 图 4.7 VGG16网络结构图 ​ 在原论文中的VGGNet包含了6个版本的演进，分别对应VGG11、VGG11-LRN、VGG13、VGG16-1、VGG16-3和VGG19，不同的后缀数值表示不同的网络层数（VGG11-LRN表示在第一层中采用了LRN的VGG11，VGG16-1表示后三组卷积块中最后一层卷积采用卷积核尺寸为\\(1\\times1\\)，相应的VGG16-3表示卷积核尺寸为\\(3\\times3\\)），本节介绍的VGG16为VGG16-3。图4.7中的VGG16体现了VGGNet的核心思路，使用\\(3\\times3\\)的卷积组合代替大尺寸的卷积（2个\\(3\\times3卷积即可与\\)\\(5\\times5\\)卷积拥有相同的感受视野），网络参数设置如表4.5所示。 ​ 表4.5 VGG16网络参数配置 网络层 输入尺寸 核尺寸 输出尺寸 参数个数 卷积层\\(C_{11}\\) \\(224\\times224\\times3\\) \\(3\\times3\\times64/1\\) \\(224\\times224\\times64\\) \\((3\\times3\\times3+1)\\times64\\) 卷积层\\(C_{12}\\) \\(224\\times224\\times64\\) \\(3\\times3\\times64/1\\) \\(224\\times224\\times64\\) \\((3\\times3\\times64+1)\\times64\\) 下采样层\\(S_{max1}\\) \\(224\\times224\\times64\\) \\(2\\times2/2\\) \\(112\\times112\\times64\\) \\(0\\) 卷积层\\(C_{21}\\) \\(112\\times112\\times64\\) \\(3\\times3\\times128/1\\) \\(112\\times112\\times128\\) \\((3\\times3\\times64+1)\\times128\\) 卷积层\\(C_{22}\\) \\(112\\times112\\times128\\) \\(3\\times3\\times128/1\\) \\(112\\times112\\times128\\) \\((3\\times3\\times128+1)\\times128\\) 下采样层\\(S_{max2}\\) \\(112\\times112\\times128\\) \\(2\\times2/2\\) \\(56\\times56\\times128\\) \\(0\\) 卷积层\\(C_{31}\\) \\(56\\times56\\times128\\) \\(3\\times3\\times256/1\\) \\(56\\times56\\times256\\) \\((3\\times3\\times128+1)\\times256\\) 卷积层\\(C_{32}\\) \\(56\\times56\\times256\\) \\(3\\times3\\times256/1\\) \\(56\\times56\\times256\\) \\((3\\times3\\times256+1)\\times256\\) 卷积层\\(C_{33}\\) \\(56\\times56\\times256\\) \\(3\\times3\\times256/1\\) \\(56\\times56\\times256\\) \\((3\\times3\\times256+1)\\times256\\) 下采样层\\(S_{max3}\\) \\(56\\times56\\times256\\) \\(2\\times2/2\\) \\(28\\times28\\times256\\) \\(0\\) 卷积层\\(C_{41}\\) \\(28\\times28\\times256\\) \\(3\\times3\\times512/1\\) \\(28\\times28\\times512\\) \\((3\\times3\\times256+1)\\times512\\) 卷积层\\(C_{42}\\) \\(28\\times28\\times512\\) \\(3\\times3\\times512/1\\) \\(28\\times28\\times512\\) \\((3\\times3\\times512+1)\\times512\\) 卷积层\\(C_{43}\\) \\(28\\times28\\times512\\) \\(3\\times3\\times512/1\\) \\(28\\times28\\times512\\) \\((3\\times3\\times512+1)\\times512\\) 下采样层\\(S_{max4}\\) \\(28\\times28\\times512\\) \\(2\\times2/2\\) \\(14\\times14\\times512\\) \\(0\\) 卷积层\\(C_{51}\\) \\(14\\times14\\times512\\) \\(3\\times3\\times512/1\\) \\(14\\times14\\times512\\) \\((3\\times3\\times512+1)\\times512\\) 卷积层\\(C_{52}\\) \\(14\\times14\\times512\\) \\(3\\times3\\times512/1\\) \\(14\\times14\\times512\\) \\((3\\times3\\times512+1)\\times512\\) 卷积层\\(C_{53}\\) \\(14\\times14\\times512\\) \\(3\\times3\\times512/1\\) \\(14\\times14\\times512\\) \\((3\\times3\\times512+1)\\times512\\) 下采样层\\(S_{max5}\\) \\(14\\times14\\times512\\) \\(2\\times2/2\\) \\(7\\times7\\times512\\) \\(0\\) 全连接层\\(FC_{1}\\) \\(7\\times7\\times512\\) \\((7\\times7\\times512)\\times4096\\) \\(1\\times4096\\) \\((7\\times7\\times512+1)\\times4096\\) 全连接层\\(FC_{2}\\) \\(1\\times4096\\) \\(4096\\times4096\\) \\(1\\times4096\\) \\((4096+1)\\times4096\\) 全连接层\\(FC_{3}\\) \\(1\\times4096\\) \\(4096\\times1000\\) \\(1\\times1000\\) \\((4096+1)\\times1000\\) 模型特性 整个网络都使用了同样大小的卷积核尺寸\\(3\\times3\\)和最大池化尺寸\\(2\\times2\\)。 \\(1\\times1\\)卷积的意义主要在于线性变换，而输入通道数和输出通道数不变，没有发生降维。 两个\\(3\\times3\\)的卷积层串联相当于1个\\(5\\times5\\)的卷积层，感受野大小为\\(5\\times5\\)。同样地，3个\\(3\\times3\\)的卷积层串联的效果则相当于1个\\(7\\times7\\)的卷积层。这样的连接方式使得网络参数量更小，而且多层的激活函数令网络对特征的学习能力更强。 VGGNet在训练时有一个小技巧，先训练浅层的的简单网络VGG11，再复用VGG11的权重来初始化VGG13，如此反复训练并初始化VGG19，能够使训练时收敛的速度更快。 在训练过程中使用多尺度的变换对原始数据做数据增强，使得模型不易过拟合。 GoogLeNet 模型介绍 ​ GoogLeNet作为2014年ILSVRC在分类任务上的冠军，以6.65%的错误率力压VGGNet等模型，在分类的准确率上面相比过去两届冠军ZFNet和AlexNet都有很大的提升。从名字GoogLeNet可以知道这是来自谷歌工程师所设计的网络结构，而名字中GoogLeNet更是致敬了LeNet\\(^{[0]}\\)。GoogLeNet中最核心的部分是其内部子网络结构Inception，该结构灵感来源于NIN，至今已经经历了四次版本迭代（Inception\\(_{v1-4}\\)）。 ​ 图 4.8 Inception性能比较图 模型结构 ​ 图 4.9 GoogLeNet网络结构图 ​ 如图4.9中所示，GoogLeNet相比于以前的卷积神经网络结构，除了在深度上进行了延伸，还对网络的宽度进行了扩展，整个网络由许多块状子网络的堆叠而成，这个子网络构成了Inception结构。图4.9为Inception的四个版本：\\(Inception_{v1}​\\)在同一层中采用不同的卷积核，并对卷积结果进行合并;\\(Inception_{v2}​\\)组合不同卷积核的堆叠形式，并对卷积结果进行合并;\\(Inception_{v3}​\\)则在\\(v_2​\\)基础上进行深度组合的尝试;\\(Inception_{v4}​\\)结构相比于前面的版本更加复杂，子网络中嵌套着子网络。 \\(Inception_{v1}\\) \\(Inception_{v2}\\) \\(Inception_{v3}\\) \\(Inception_{v4}\\) ​ 图 4.10 Inception\\(_{v1-4}\\)结构图 ​ 表 4.6 GoogLeNet中Inception\\(_{v1}\\)网络参数配置 网络层 输入尺寸 核尺寸 输出尺寸 参数个数 卷积层\\(C_{11}\\) \\(H\\times{W}\\times{C_1}\\) \\(1\\times1\\times{C_2}/2\\) \\(\\frac{H}{2}\\times\\frac{W}{2}\\times{C_2}\\) \\((1\\times1\\times{C_1}+1)\\times{C_2}\\) 卷积层\\(C_{21}\\) \\(H\\times{W}\\times{C_2}\\) \\(1\\times1\\times{C_2}/2\\) \\(\\frac{H}{2}\\times\\frac{W}{2}\\times{C_2}\\) \\((1\\times1\\times{C_2}+1)\\times{C_2}\\) 卷积层\\(C_{22}\\) \\(H\\times{W}\\times{C_2}\\) \\(3\\times3\\times{C_2}/1\\) \\(H\\times{W}\\times{C_2}/1\\) \\((3\\times3\\times{C_2}+1)\\times{C_2}\\) 卷积层\\(C_{31}\\) \\(H\\times{W}\\times{C_1}\\) \\(1\\times1\\times{C_2}/2\\) \\(\\frac{H}{2}\\times\\frac{W}{2}\\times{C_2}\\) \\((1\\times1\\times{C_1}+1)\\times{C_2}\\) 卷积层\\(C_{32}\\) \\(H\\times{W}\\times{C_2}\\) \\(5\\times5\\times{C_2}/1\\) \\(H\\times{W}\\times{C_2}/1\\) \\((5\\times5\\times{C_2}+1)\\times{C_2}\\) 下采样层\\(S_{41}\\) \\(H\\times{W}\\times{C_1}\\) \\(3\\times3/2\\) \\(\\frac{H}{2}\\times\\frac{W}{2}\\times{C_2}\\) \\(0\\) 卷积层\\(C_{42}\\) \\(\\frac{H}{2}\\times\\frac{W}{2}\\times{C_2}\\) \\(1\\times1\\times{C_2}/1\\) \\(\\frac{H}{2}\\times\\frac{W}{2}\\times{C_2}\\) \\((3\\times3\\times{C_2}+1)\\times{C_2}\\) 合并层\\(M\\) \\(\\frac{H}{2}\\times\\frac{W}{2}\\times{C_2}(\\times4)\\) 拼接 \\(\\frac{H}{2}\\times\\frac{W}{2}\\times({C_2}\\times4)\\) \\(0\\) 模型特性 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； 之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了； 网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN2，采用1x1卷积核来进行降维。 为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？ 评测对比：为了让自己的结果更有说服力，在发表自己成果的时候会同一个标准的baseline及在baseline上改进而进行比较，常见的比如各种检测分割的问题都会基于VGG或者Resnet101这样的基础网络。 时间和精力有限：在科研压力和工作压力中，时间和精力只允许大家在有限的范围探索。 模型创新难度大：进行基本模型的改进需要大量的实验和尝试，并且需要大量的实验积累和强大灵感，很有可能投入产出比比较小。 资源限制：创造一个新的模型需要大量的时间和计算资源，往往在学校和小型商业团队不可行。 在实际的应用场景中，其实是有大量的非标准模型的配置。 参考文献 [1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, november 1998. [2] A. Krizhevsky, I. Sutskever and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25. Curran Associates, Inc. 1097–1105. [3] LSVRC-2013. http://www.image-net.org/challenges/LSVRC/2013/results.php [4] M. D. Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks. European Conference on Computer Vision. [5] M. Lin, Q. Chen, and S. Yan. Network in network. Computing Research Repository, abs/1312.4400, 2013. [6] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Machine Learning, 2015. [7] Bharath Raj. a-simple-guide-to-the-versions-of-the-inception-network, 2018. [8] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 2016. [9] Sik-Ho Tsang. review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification, 2018. [10] Zbigniew Wojna, Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens. Rethinking the Inception Architecture for Computer Vision, 2015. [11] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. Going deeper with convolutions, 2014.","link":"/posts/541303645.html"},{"title":"词表征与词向量","text":"词表征(Word Representation) 文本数据经过预处理后，需要转化成数值特征，便于后续处理 one-hot 表征 (localist representation)： - 例如给定词典，包含\\(10^7\\)个单词，则每个单词表示为 \\(10^7\\times 1\\) 的 one-hot 向量；“我们”表示为 \\([0,0,...,0,0,1,0,0,0...0,0,0]_{(10^7,1)}\\)，1 的索引为“我们”在词典中的位置 - 句子或文档向量则可表示为 \\(10^7\\times 1\\) 的向量，词典中每个单词在句子或文档中是否出现；Bolean 向量。 - 也可以表示为，词典中每个单词在句子或文档中出现的次数；Count-based 句子向量 1234567891011121314151617# 单词级 one-hot 编码import numpy as npsamples = ['The cat sat on the mat.', 'The dog ate my homework.']token_index = {}for sample in samples: for word in sample.split(): if word not in token_index: token_index[word] = len(token_index) + 1max_length = 10results = np.zeros(shape=(len(samples), max_length, len(token_index) + 1))for i, sample in enumerate(samples): for j, word in list(enumerate(sample.split()))[:max_length]: index = token_index.get(word) results[i, j, index] = 1.token_index, results ({'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}, array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])) 123456789101112131415# 字符级 one-hot 编码import stringsamples = ['The cat sat on the mat.', 'The dog ate my homework.']characters = string.printabletoken_index = dict(zip(characters, range(1, len(characters) + 1)))max_length = 50results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))for i, sample in enumerate(samples): for j, character in enumerate(sample): index = token_index.get(character) results[i, j, index] = 1 token_index, results ({'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '&quot;': 64, '#': 65, '$': 66, '%': 67, '&amp;': 68, &quot;'&quot;: 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '&lt;': 80, '=': 81, '&gt;': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}, array([[[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], [[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]])) 12345678910111213# tf 实现 one-hot 编码，句子的 Bolean 向量from tensorflow.keras.preprocessing.text import Tokenizersamples = ['The cat sat on the mat.', 'The dog ate my homework.']tokenizer = Tokenizer(num_words=20) # 只考虑前20个最常见的单词tokenizer.fit_on_texts(samples)sequences = tokenizer.texts_to_sequences(samples) # 整数索引列表one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary') # one-hot 向量word_index = tokenizer.word_index # 单词-索引词典word_index, one_hot_results ({'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}, array([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])) 使用散列技巧的单词级 one-hot 编码 - 当词汇表非常大大时，散列技巧降低向量长度 - 可能出现散列冲突 1234567891011samples = ['The cat sat on the mat.', 'The dog ate my homework.']# 如词汇表有 10000 个单词时，将向量长度从 10000 降低到 1000dimensionality = 1000max_length = 10results = np.zeros(shape=(len(samples), max_length, dimensionality))for i, sample in enumerate(samples): for j, word in list(enumerate(sample.split()))[:max_length]: index = abs(hash(word)) % dimensionality results[i, j, index] = 1 \\(tf-idf\\) 表征 \\[tfidf(w)=tf(d,w)\\times idf(w)\\] \\(tf(d,w)\\) - 文档 d 中单词 w 的词频； \\(idf(w)=log\\frac{N}{N_{w}}\\)，\\(N\\) - 语料库的文档总数，\\(N_{w}\\) - 词语 w 出现在多少个文档中 12345678910from sklearn.feature_extraction.text import TfidfVectorizercorpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?',]vectorizer = TfidfVectorizer()X = vectorizer.fit_transform(corpus)X.todense() matrix([[0. , 0.46979139, 0.58028582, 0.38408524, 0. , 0. , 0.38408524, 0. , 0.38408524], [0. , 0.6876236 , 0. , 0.28108867, 0. , 0.53864762, 0.28108867, 0. , 0.28108867], [0.51184851, 0. , 0. , 0.26710379, 0.51184851, 0. , 0.26710379, 0.51184851, 0.26710379], [0. , 0.46979139, 0.58028582, 0.38408524, 0. , 0. , 0.38408524, 0. , 0.38408524]]) 1print(vectorizer.get_feature_names()) ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] localist representation 的缺点： 1. 稀疏性表征，向量长度为词典内单词总数 2. 无法表征单词的相似度，任意两个单词向量的点积为 0 3. 表达能力弱，如无法表达语义；同样 8 维向量，one-hot只能表示 8 个单词，而词向量每个元素取值连续，因此能表示无穷个单词 词向量 分布语义(distributional Semantic)，特定单词的含义由频繁出现在其前后的词决定 \\(\\Longrightarrow\\) 分布式表征，通过单词的上下文(context)表征单词。 不同的单词，其上下文越相近，该单词的含义越相似。例如单词 w，其上下文由其前 n 个单词及其后 n 个单词的集合组成，n 表示窗口尺寸(fixed window)。如下所示，单词 \"banking\" 的上下文，窗口尺寸 \\(n=5\\) : \\[ \\begin{align*} ...goverment\\ debt\\ problems\\ turning\\ into\\ &amp;\\boxed{banking}\\ crisises\\ as\\ happened\\ in\\ 2009 ...\\\\ ...saying\\ that\\ Europe\\ needs\\ unified\\ &amp;\\boxed{banking}\\ regulation\\ to\\ replace\\ the\\ hodgepodge...\\\\ ...Indian\\ has\\ just\\ given\\ its\\ &amp;\\boxed{banking}\\ system\\ a\\ shot\\ in\\ the\\ arm... \\end{align*} \\] 词向量，也称为 word embedding 或 word representation，将文本映射到特定维度的向量空间（vector space）中： 更致密的向量， 点积表示不同词间的相似性，表达语义 capacity，表达能力强，词向量每个元素取值连续，因此能表示无穷个单词 泛化能力强，即在测试集上也有很好的效果，可用于迁移学习进行后续操作 训练词向量原理： 大量语料库，从中得出固定的词汇表，每个单词通过一个向量表示，向量维度 50、200或300 按语料库中文本的顺序，遍历每一个位置，以该位置单词作为中心词 c，选定窗口尺寸 n，窗口内上下文单词为 o 利用词向量的相似性，计算给定 c 时，o 的概率，或相反；c 的词向量就可以用来计算 o 中的单词 不断调整词向量，最大化概率 Skip-Gram Model 将文本按顺序，将固定窗口尺寸 n=2 内的单词，转变成 中心词-上下文词 组成的词对；中心词将作为输入，上下文词将作为训练标签。词汇表，10000个单词 构建如下图神经网络，输入为中心词 ‘ants’ 的 one-hot 向量；隐藏层矩阵本质为词向量查找表 (\\(10000\\times 300\\))，通过 one-hot 向量查找到 ’ants‘ 的词向量 (\\(300,\\))；再经过输出层矩阵 (\\(300\\times 10000\\))的处理，得到 (\\(10000,\\)) 的向量，为词汇表中每个单词的概率分布。 例如，输入词对，（’ants‘，’car‘），训练目标：使得 ‘ants' 的上下文词 ’car‘ 的概率最大，而其它非上下文词的概率最小。最终得到隐藏层的权重矩阵即为所要得到的词向量，输出层矩阵则丢弃。 输出层有 (\\(300\\times 10000\\)) 个参数，且 softmax 函数还需要计算 \\(\\sum_i^{10000}{e^i}\\) ，模型最终的计算量会非常大 CBOW模型 与 skip-gram 相反，以 上下文词 作为输入，来训练模型，以最大化中心词的概率 重采样 在语料中频繁出现的一些单词，对中心词并没有给出多大信息，如 (fox, the) 词对中的 the。Word2Vec 实现了 重采样机制，以解决该问题 给定单词 \\(w_i\\) ，其在语料库(\\(10^6\\)个单词)中的词频为 \\(z(w_i)\\) sample 参数决定出现多少重采样，值越小，单词越不可能被选择，sample=0.001 重采样单词 \\(w_i\\) 的概率为： \\[P(w_i)=(\\sqrt{\\frac{z(w_i)}{0.001}}+1)\\cdot \\frac{0.001}{z(w_i)}\\] \\(P(w_i)=1.0\\)，当单词词频 \\(z(w_i)&lt;=0.0026\\) 时，该单词 100% 会被选择 \\(P(w_i)=0.5\\)，当单词词频 \\(z(w_i)&lt;=0.00746\\) 时，该单词有 50% 可能性会被选择 \\(P(w_i)=0.033\\)，当单词词频 \\(z(w_i)&lt;=1.0\\) 时，该单词 3.3% 会被选择 即整个语料库由该单词组成 重采样后，频繁出现的单词在采样时被选择的概率降低，而稀有词在采样时不会被忽略 加速训练 由于输出层的\\(softmax\\)函数会输出整个词汇表中所有单词的的概率，计算量非常大，两种方法加快训练速度： - Hierarchical Softmax - Negative sampling Hierarchical Softmax 基于层级\\(softmax\\)的模型，\\(softmax\\)的输出不再是词汇表所有单词的概率分布，而是一颗Huffman树，其中叶子节点共N个，对应于N个单词，非叶子节点N-1个 霍夫曼树： 霍夫曼树(Huffman Tree)，一种满二叉树，其带权路径长最小，也称为最优二叉树 带权路径长，指的是所有叶子节点的权值与路径长的乘积之和； 若叶子节点的权值都是已知的，使权值越大的叶子节点路径越小，则整棵树的带权路径长最小。 构造霍夫曼树的目的是为了完成霍夫曼编码，变长、极少多余编码方案，使得频率高的字符对应的二进制位较短，频率低的字符对应的二进制位较长。相对于等长编码，将文件中每个字符转换为固定个数的二进制位 霍夫曼树的构造，给定包含权重值为 \\((w_1,w_2,...w_n)\\) 的节点列表： - 将节点列表按权重进行升序排列，并构建一颗空树 - 遍历排序后的节点，依次将其添加到树中，添加规则如下： - 若树为空，则作为根节点 - 若权重大于根节点权重，则该节点作为根节点右兄弟，生成一个新的根节点，权重为两者之和 - 若权重不大于根节点权重，则该节点作为根节点左兄弟，生成一个新的根节点，权重为两者之和 - 约定霍夫曼树左子树编码为0，右子树编码为1，左子树的权重小于右子树的权重； - 权重越高的节点编码值越短，权重越低的编码值越长；保证了越常用的单词拥有越短的编码。 词汇表构建的霍夫曼树 首先根据语料库，构建词典\\(V\\)，然后以每个单词的词频为权重构建霍夫曼树， 出现越频繁的单词在树中深度越浅，越少见的词在越深；每个单词有其对应的由‘01’组成的编码 如下图所示： 树中叶子节点即为每个单词，内部节点为模型参数 在训练词对 ”深度：学习“ 时，目标为 “学习”，其路径为 643，编码为 101； 因此输入 ”深度“ 的词向量，经节点 6 处理，需要输出值 1；经节点 4 处理需要输出值 0；经过节点 3 处理，需要输出值 1 输入词向量与 目标单词 的霍夫曼树路径中每个节点参数求 点积，然后sigmoid激活 处理得到的结果，要与每个编码值相同 训练完成后，每个单词对应的词向量即为目标数据，霍夫曼树的节点参数被丢弃 训练完成后，再次输入单词 ”深度“ 的词向量，经过节点 6 处理得到概率 0.55； 因为训练时，输入为 “深度”，目标单词会有很多，概率 0.55，说明 “深度”的目标单词中 55% 出现在节点 6 的右边，45% 出现在左边； “深度：的” 词对占所有 “深度”词对的 0.45*0.75 Hierarchical Softmax: CBOW 模型中，给定上下文词 o，最大化中心词 c 的条件概率 \\(P(c|o)\\) \\(X_o\\)：上下文词向量平均后的向量，作为树的输入 \\(p\\) ：表示从根节点出发到中心词 c 所代表的叶节点的路径 \\(l\\)：表示路径包含的节点的个数 \\(p_1, p_2, ...p_l\\)：表示路径 \\(p\\) 中的节点 \\(d_2 ,d_3, ...d_l \\in{\\{0,1\\}}\\)：表示路径 \\(p\\) 中每个节点的编码，0 或 1，根节点无编码 \\(\\theta_1, \\theta_2,...\\theta_{l-1}\\)：表示路径 \\(p\\) 中非叶节点对应的参数向量 中心词的条件概率，可以看作从根节点到中心词对应的叶子节点的路径的概率，该路径是唯一的；在树中选择左分支还是右分支，即可看作一个二分类问题，选择每个节点的概率： \\[ \\begin{align}P(d_j|X_o,\\theta_{j-1})&amp;=\\begin{cases}\\sigma(X_o^T\\theta_{j-1}), &amp;d_j=0 \\\\1-\\sigma(X_o^T\\theta_{j-1}), &amp;d_j=1\\end{cases}\\\\&amp;=\\left[\\sigma(X_o^T\\theta_{j-1})\\right]^{1-d_j}\\cdot \\left[1-\\sigma(X_o^T\\theta_{j-1})\\right]^{d_j}\\end{align} \\] 中性词的概率，即为到中心词节点的路径中所有节点的概率的连乘： \\[ P(c|o)=\\prod_{j=2}^{l}P(d_j|X_o,\\theta_{j-1}) \\] 最大化模型的目标函数： \\[ \\begin{align}L&amp;=\\sum_{c\\in V}{logP(c|o)}\\\\&amp;=\\sum_{c\\in V}\\sum_{j=2}^{l}\\Big\\{(1-d_j)\\cdot log\\left[\\sigma(X_o^T\\theta_{j-1})\\right]+d_j\\cdot log\\left[1-\\sigma(X_o^T\\theta_{j-1})\\right]\\Big\\}\\\\&amp;=\\sum_{c\\in V}\\sum_{j=2}^{l}L(o,j)\\end{align} \\] 最大化目标函数中的每一项，两个参数，节点参数 \\(\\theta_{j-1}\\)，输入的向量 \\(X_o\\)，设定学习速率 \\(\\eta\\)。梯度下降法更新这两个参数： \\[ \\begin{align}\\theta_{j-1}&amp;:\\theta_{j-1}+\\eta\\cdot\\frac{\\partial L(o,j)}{\\partial\\theta_{j-1}}\\\\X_o&amp;:X_o+\\eta\\cdot\\sum_{j=2}^{l}\\frac{\\partial L(o,j)}{\\partial X_o}\\end{align} \\] \\(X_o\\) 为多个上下文词向量的平均，word2vec 直接将更新量应用到对应的那几个上下文词向量上 Negative Sampling 输出时，不再是计算整个词汇表的概率分布；而是从词汇表中选择一定量的负样本，再加上目标单词，计算这些单词的概率分布，训练时也只会更新这些单词对应的权重参数 - 如(fox,quick)词对，输出层矩阵\\(300\\times 10000\\)处理输入中心词fox词向量时，只从输出层矩阵中选择上下文词quick对应的那列权重向量(positive word)，以及随机选择的另外 5 个单词对应的权重向量，来进行更新，此时只需要更新 (\\(300\\times 6\\)) 个参数 如何选择负样本(Negative Samples)： - 可以通过词频均匀采样：\\(P(w_i)=\\frac{f(w_i)}{\\sum_{j=0}^{n}(f(w_j))}\\) - 效果最好的是 3/4 指数采样：\\(P(w_i)=\\frac{f(w_i)^{3/4}}{\\sum_{j=0}^{n}(f(w_j)^{3/4})}\\) - \\(f(w_i)\\) 为单词 \\(w_i\\) 在语料库中出现的概率 底层 C 语言实现了包含 100M 元素的数组，称为 unigram table。向表中填充单词对应的索引，该索引出现的次数由 \\(P(w_i)\\cdot 表的尺寸\\) 决定。选择负样本时，只要生成 \\(0-100M\\) 内的 5 个随机数 参考文章： 1.http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ 2.http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ 3.https://ruder.io/word-embeddings-softmax/","link":"/posts/61928.html"},{"title":"条件随机场","text":"1．概率无向图模型是由无向图表示的联合概率分布。无向图上的结点之间的连接关系表示了联合分布的随机变量集合之间的条件独立性，即马尔可夫性。因此，概率无向图模型也称为马尔可夫随机场。 概率无向图模型或马尔可夫随机场的联合概率分布可以分解为无向图最大团上的正值函数的乘积的形式。 2．条件随机场是给定输入随机变量\\(X\\)条件下，输出随机变量\\(Y\\)的条件概率分布模型， 其形式为参数化的对数线性模型。条件随机场的最大特点是假设输出变量之间的联合概率分布构成概率无向图模型，即马尔可夫随机场。条件随机场是判别模型。 3．线性链条件随机场是定义在观测序列与标记序列上的条件随机场。线性链条件随机场一般表示为给定观测序列条件下的标记序列的条件概率分布，由参数化的对数线性模型表示。模型包含特征及相应的权值，特征是定义在线性链的边与结点上的。线性链条件随机场的数学表达式是 \\[ P(y | x)=\\frac{1}{Z(x)} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) \\] 其中， \\[ Z(x)=\\sum_{y} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) \\] 4．线性链条件随机场的概率计算通常利用前向-后向算法。 5．条件随机场的学习方法通常是极大似然估计方法或正则化的极大似然估计，即在给定训练数据下，通过极大化训练数据的对数似然函数以估计模型参数。具体的算法有改进的迭代尺度算法、梯度下降法、拟牛顿法等。 6．线性链条件随机场的一个重要应用是标注。维特比算法是给定观测序列求条件概率最大的标记序列的方法。 例11.1 1from numpy import * 123456789101112131415#这里定义T为转移矩阵列代表前一个y(ij)代表由状态i转到状态j的概率,Tx矩阵x对应于时间序列#这里将书上的转移特征转换为如下以时间轴为区别的三个多维列表，维度为输出的维度T1 = [[0.6, 1], [1, 0]]T2 = [[0, 1], [1, 0.2]]#将书上的状态特征同样转换成列表,第一个是为y1的未规划概率，第二个为y2的未规划概率S0 = [1, 0.5]S1 = [0.8, 0.5]S2 = [0.8, 0.5]Y = [1, 2, 2] #即书上例一需要计算的非规划条件概率的标记序列Y = array(Y) - 1 #这里为了将数与索引相对应即从零开始P = exp(S0[Y[0]])for i in range(1, len(Y)): P *= exp((eval('S%d' % i)[Y[i]]) + eval('T%d' % i)[Y[i - 1]][Y[i]])print(P)print(exp(3.2)) 24.532530197109345 24.532530197109352 例11.2 1234567891011121314#这里根据例11.2的启发整合为一个矩阵F0 = S0F1 = T1 + array(S1 * len(T1)).reshape(shape(T1))F2 = T2 + array(S2 * len(T2)).reshape(shape(T2))Y = [1, 2, 2] #即书上例一需要计算的非规划条件概率的标记序列Y = array(Y) - 1P = exp(F0[Y[0]])Sum = Pfor i in range(1, len(Y)): PIter = exp((eval('F%d' % i)[Y[i - 1]][Y[i]])) P *= PIter Sum += PIterprint('非规范化概率', P) 非规范化概率 24.532530197109345 第11章条件随机场-习题 习题11.1 写出图11.3中无向图描述的概率图模型的因子分解式。 图11.3 无向图的团和最大团 解答： 图11.3表示由4个结点组成的无向图。图中由2个结点组成的团有5个：\\(\\{Y_1,Y_2\\},\\{Y_2,Y_3\\},\\{Y_3,Y_4\\},\\{Y_4,Y_2\\}\\)和\\(\\{Y_1,Y_3\\}\\)，有2个最大团：\\(\\{Y_1,Y_2,Y_3\\}\\)和\\(\\{Y_2,Y_3,Y_4\\}\\)，而\\(\\{Y_1,Y_2,Y_3,Y_4\\}\\)不是一个团，因为\\(Y_1\\)和\\(Y_4\\)没有边连接。 根据概率图模型的因子分解定义：将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作。公式在书中(11.5)，(11.6)。 \\[P(Y)=\\frac{\\Psi_{(1,2,3)}(Y_{(1,2,3)})\\cdot\\Psi_{(2,3,4)}(Y_{(2,3,4)})}{\\displaystyle \\sum_Y \\left[ \\Psi_{(1,2,3)}(Y_{(1,2,3)})\\cdot\\Psi_{(2,3,4)}(Y_{(2,3,4)})\\right]}\\] 习题11.2 证明\\(Z(x)=a_n^T(x) \\cdot \\boldsymbol{1} = \\boldsymbol{1}^T\\cdot\\beta_1(x)\\)，其中\\(\\boldsymbol{1}\\)是元素均为1的\\(m\\)维列向量。 解答： 第1步：证明\\(Z(x)=a_n^T(x) \\cdot \\boldsymbol{1}\\) 根据条件随机场的矩阵形式：\\[(M_{n+1}(x))_{i,j}=\\begin{cases} 1,&amp;j=\\text{stop}\\\\ 0,&amp;\\text{otherwise} \\end{cases}\\]根据前向向量的定义：\\[\\alpha_0(y|x)=\\begin{cases} 1,&amp;y=\\text{start} \\\\ 0,&amp;\\text{otherwise} \\end{cases}\\] \\(\\begin{aligned} \\therefore Z_n(x) &amp;= \\left(M_1(x)M_2(x){\\cdots}M_{n+1}(x)\\right)_{(\\text{start},\\text{stop})} \\\\ &amp;= \\alpha_0(x)^T M_1(x)M_2(x){\\cdots}M_n(x) \\cdot 1\\\\ &amp;=\\alpha_n(x)^T\\cdot \\boldsymbol{1} \\end{aligned}\\) 第2步：证明\\(Z(x)=\\boldsymbol{1}^T \\cdot \\beta_1(x)\\) 根据条件随机场的矩阵形式：\\[(M_{n+1}(x))_{i,j}=\\begin{cases} 1,&amp;j=\\text{stop}\\\\ 0,&amp;\\text{otherwise} \\end{cases}\\]根据后向向量定义：\\[\\beta_{n+1}(y_{n+1}|x)= \\begin{cases} 1,&amp; y_{n+1}=\\text{stop} \\\\ 0,&amp; \\text{otherwise} \\end{cases}\\] \\(\\begin{aligned} \\therefore Z_n(x) &amp;= (M_1(x)M_2(x) \\cdots M_{n+1}(x))_{(\\text{start},\\text{stop})} \\\\ &amp;= (M_1(x)M_2(x) \\cdots M_n(x) \\beta_{n+1}(x))_{\\text{start}} \\\\ &amp;=(\\beta_1(x))_{\\text{start}} \\\\ &amp;=\\boldsymbol{1}^T \\cdot \\beta_1(x) \\end{aligned}\\) 综上所述：\\(Z(x)=a_n^T(x) \\cdot \\boldsymbol{1} = \\boldsymbol{1}^T \\cdot \\beta_1(x)\\)，命题得证。 习题11.3 写出条件随机场模型学习的梯度下降法。 解答： 条件随机场的对数极大似然函数为：\\[L(w)=\\sum^N_{j=1} \\sum^K_{k=1} w_k f_k(y_j,x_j)-\\sum^N_{j=1} \\log{Z_w(x_j)}\\]梯度下降算法的目标函数是\\(f(w)=-L(w)\\) 目标函数的梯度为：\\[g(w)=\\nabla{f(w^{(k)})}=\\left(\\frac{\\partial{f(w)}}{\\partial{w_1}},\\frac{\\partial{f(w)}}{\\partial{w_2}},\\cdots,\\frac{\\partial{f(w)}}{\\partial{w_k}}\\right)\\]其中\\[\\begin{aligned} \\frac{\\partial{f(w)}}{\\partial{w_i}} &amp;= -\\sum^N_{j=1} w_i f_i(y_j,x_j) + \\sum^N_{j=1} \\frac{1}{Z_w(x_j)} \\cdot \\frac{\\partial{Z_w(x_j)}}{\\partial{w_i}}\\\\ &amp;= -\\sum^N_{j=1}w_if_i(y_j,x_j)+\\sum^N_{j=1}\\frac{1}{Z_w(x_j)}\\sum_y(\\exp{\\sum^K_{k=1}w_kf_k(y,x_j))}w_if_i(y,x_j) \\end{aligned}\\] 根据梯度下降算法： 1. 取初始值\\(w^{(0)} \\in \\mathbf{R}^n\\)，置\\(k=0\\) 2. 计算\\(f(w^{(k)})\\) 3. 计算梯度\\(g_k=g(w^{(k)})\\)，当\\(\\|g_k\\|&lt;\\varepsilon\\)时，停止迭代，令\\(w^*=w^{(k)}\\)；否则令\\(p_k=-g(w^{(k)})\\)，求\\(\\lambda_k\\)，使\\[ f(w^{(k)}+\\lambda_k p_k)=\\min_{\\lambda \\geqslant 0}{f(w^{(k)}+\\lambda p_k)}\\] 4. 置\\(w^{(k+1)}=w^{(k)}+\\lambda_k p_k\\)，计算\\(f(w^{(k+1)})\\) 当\\(\\|f(w^{(k+1)})-f(w^{(k)})\\| &lt; \\epsilon\\)或\\(\\|w^{(k+1)}-w^{(k)}\\| &lt; \\epsilon\\)时，停止迭代，令\\(w^*=w^{(k+1)}\\) 5. 否则，置\\(k=k+1\\)，转(3). 习题11.4 参考图11.6的状态路径图，假设随机矩阵\\(M_1(x),M_2(x),M_3(x),M_4(x)\\)分别是 \\[M_1(x)=\\begin{bmatrix}0&amp;0\\\\0.5&amp;0.5\\end{bmatrix} , M_2(x)=\\begin{bmatrix}0.3&amp;0.7\\\\0.7&amp;0.3\\end{bmatrix}\\] \\[ M_3(x)=\\begin{bmatrix}0.5&amp;0.5\\\\0.6&amp;0.4\\end{bmatrix}, M_4(x)=\\begin{bmatrix}0&amp;1\\\\0&amp;1\\end{bmatrix}\\] 求以\\(start=2\\)为起点\\(stop=2\\)为终点的所有路径的状态序列\\(y\\)的概率及概率最大的状态序列。 解答： 123456789import numpy as np# 创建随机矩阵M1 = [[0, 0], [0.5, 0.5]]M2 = [[0.3, 0.7], [0.7, 0.3]]M3 = [[0.5, 0.5], [0.6, 0.4]]M4 = [[0, 1], [0, 1]]M = [M1, M2, M3, M4]print(M) [[[0, 0], [0.5, 0.5]], [[0.3, 0.7], [0.7, 0.3]], [[0.5, 0.5], [0.6, 0.4]], [[0, 1], [0, 1]]] 123456789101112# 生成路径path = [2]for i in range(1, 4): paths = [] for _, r in enumerate(path): temp = np.transpose(r) paths.append(np.append(temp, 1)) paths.append(np.append(temp, 2)) path = paths.copy()path = [np.append(r, 2) for _, r in enumerate(path)]print(path) [array([2, 1, 1, 1, 2]), array([2, 1, 1, 2, 2]), array([2, 1, 2, 1, 2]), array([2, 1, 2, 2, 2]), array([2, 2, 1, 1, 2]), array([2, 2, 1, 2, 2]), array([2, 2, 2, 1, 2]), array([2, 2, 2, 2, 2])] 123456789101112# 计算概率pr = []for _, row in enumerate(path): p = 1 for i in range(len(row) - 1): a = row[i] b = row[i + 1] p *= M[i][a - 1][b - 1] pr.append((row.tolist(), p))pr = sorted(pr, key=lambda x: x[1], reverse=True)print(pr) [([2, 1, 2, 1, 2], 0.21), ([2, 2, 1, 1, 2], 0.175), ([2, 2, 1, 2, 2], 0.175), ([2, 1, 2, 2, 2], 0.13999999999999999), ([2, 2, 2, 1, 2], 0.09), ([2, 1, 1, 1, 2], 0.075), ([2, 1, 1, 2, 2], 0.075), ([2, 2, 2, 2, 2], 0.06)] 1234567# 打印结果print(&quot;以start=2为起点stop=2为终点的所有路径的状态序列y的概率为：&quot;)for path, p in pr: print(&quot; 路径为：&quot; + &quot;-&gt;&quot;.join([str(x) for x in path]), end=&quot; &quot;) print(&quot;概率为：&quot; + str(p))print(&quot;概率[&quot; + str(pr[0][1]) + &quot;]最大的状态序列为:&quot;, &quot;-&gt;&quot;.join([str(x) for x in pr[0][0]])) 以start=2为起点stop=2为终点的所有路径的状态序列y的概率为： 路径为：2-&gt;1-&gt;2-&gt;1-&gt;2 概率为：0.21 路径为：2-&gt;2-&gt;1-&gt;1-&gt;2 概率为：0.175 路径为：2-&gt;2-&gt;1-&gt;2-&gt;2 概率为：0.175 路径为：2-&gt;1-&gt;2-&gt;2-&gt;2 概率为：0.13999999999999999 路径为：2-&gt;2-&gt;2-&gt;1-&gt;2 概率为：0.09 路径为：2-&gt;1-&gt;1-&gt;1-&gt;2 概率为：0.075 路径为：2-&gt;1-&gt;1-&gt;2-&gt;2 概率为：0.075 路径为：2-&gt;2-&gt;2-&gt;2-&gt;2 概率为：0.06 概率[0.21]最大的状态序列为: 2-&gt;1-&gt;2-&gt;1-&gt;2","link":"/posts/57809.html"},{"title":"NLP基础","text":"解决 NLP 问题的一般思路 123这个问题人类可以做好么？ - 可以 -&gt; 记录自己的思路 -&gt; 设计流程让机器完成你的思路 - 很难 -&gt; 尝试从计算机的角度来思考问题 NLP 的历史进程 规则系统 正则表达式/自动机 规则是固定的 搜索引擎 12345“豆瓣酱用英语怎么说？”规则：“xx用英语怎么说？” =&gt; translate(XX, English)“我饿了”规则：“我饿（死）了” =&gt; recommend(饭店，地点) 概率系统 规则从数据中抽取 规则是有概率的 概率系统的一般工作方式 1234567流程设计收集训练数据 预处理 特征工程 分类器（机器学习算法） 预测 评价 最重要的部分：数据收集、预处理、特征工程 示例 12345678910111213141516171819202122232425262728293031323334任务：“豆瓣酱用英语怎么说” =&gt; translate(豆瓣酱，Eng)流程设计（序列标注）：子任务1： 找出目标语言 “豆瓣酱用 **英语** 怎么说”子任务2： 找出翻译目标 “ **豆瓣酱** 用英语怎么说”收集训练数据：（子任务1）“豆瓣酱用英语怎么说”“茄子用英语怎么说”“黄瓜怎么翻译成英语”预处理：分词：“豆瓣酱 用 英语 怎么说”抽取特征：（前后各一个词）0 茄子： &lt; _ 用0 用： 豆瓣酱 _ 英语1 英语： 用 _ 怎么说0 怎么说： 英语 _ &gt;分类器：SVM/CRF/HMM/RNN预测：0.1 茄子： &lt; _ 用0.1 用： 豆瓣酱 _ 英语0.7 英语： 用 _ 怎么说0.1 怎么说： 英语 _ &gt;评价：准确率 概率系统的优/缺点 + 规则更加贴近于真实事件中的规则，因而效果往往比较好 - 特征是由专家/人指定的； - 流程是由专家/人设计的； - 存在独立的子任务 深度学习 深度学习相对概率模型的优势 特征是由专家指定的 -&gt; 特征是由深度学习自己提取的 流程是由专家设计的 -&gt; 模型结构是由专家设计的 存在独立的子任务 -&gt; End-to-End Training Seq2Seq 模型 大部分自然语言问题都可以使用 Seq2Seq 模型解决 “万物”皆 Seq2Seq 评价机制 困惑度 (Perplexity, PPX) Perplexity - Wikipedia - 在信息论中，perplexity 用于度量一个概率分布或概率模型预测样本的好坏程度 &gt; ../机器学习/[信息论](../A-机器学习/A-机器学习基础#信息论) 基本公式 概率分布（离散）的困惑度 其中 H(p) 即信息熵 概率模型的困惑度 通常 b=2 指数部分也可以是交叉熵的形式，此时困惑度相当于交叉熵的指数形式 其中 p~ 为测试集中的经验分布——p~(x) = n/N，其中 n 为 x 的出现次数，N 为测试集的大小 语言模型中的 PPX - 在 NLP 中，困惑度常作为语言模型的评价指标 直观来说，就是下一个候选词数目的期望值—— 如果不使用任何模型，那么下一个候选词的数量就是整个词表的数量；通过使用 bi-gram语言模型，可以将整个数量限制到 200 左右 BLEU 一种机器翻译的评价准则——BLEU - CSDN博客 - 机器翻译评价准则 - 计算公式 其中 &gt; `c` 为生成句子的长度；`r` 为参考句子的长度——目的是**惩罚**长度过短的候选句子 为了计算方便，会加一层 log 通常 N=4, w_n=1/4 ROUGE 自动文摘评测方法：Rouge-1、Rouge-2、Rouge-L、Rouge-S - CSDN博客 - 一种机器翻译/自动摘要的评价准则 BLEU，ROUGE，METEOR，ROUGE-浅述自然语言处理机器翻译常用评价度量 - CSDN博客 语言模型 XX 模型的含义 如果能使用某个方法对 XX 打分（Score），那么就可以把这个方法称为 “XX 模型” 篮球明星模型: Score(库里)、Score(詹姆斯) 话题模型——对一段话是否在谈论某一话题的打分 12Score( NLP | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.8Score( ACM | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.05 概率/统计语言模型 (PLM, SLM) 语言模型是一种对语言打分的方法；而概率语言模型把语言的“得分”通过概率来体现 具体来说，概率语言模型计算的是一个序列作为一句话可能的概率 12Score(&quot;什么 是 语言 模型&quot;) --&gt; 0.05 # 比较常见的说法，得分比较高Score(&quot;什么 有 语言 模型&quot;) --&gt; 0.01 # 不太常见的说法，得分比较低 以上过程可以形式化为： 根据贝叶斯公式，有 其中每个条件概率就是模型的参数；如果这个参数都是已知的，那么就能得到整个序列的概率了 参数的规模 设词表的大小为 N，考虑长度为 T 的句子，理论上有 N^T 种可能的句子，每个句子中有 T 个参数，那么参数的数量将达到 O(T*N^T) 可用的概率模型 统计语言模型实际上是一个概率模型，所以常见的概率模型都可以用于求解这些参数 常见的概率模型有：N-gram 模型、决策树、最大熵模型、隐马尔可夫模型、条件随机场、神经网络等 目前常用于语言模型的是 N-gram 模型和神经语言模型（下面介绍） N-gram 语言模型 马尔可夫(Markov)假设——未来的事件，只取决于有限的历史 基于马尔可夫假设，N-gram 语言模型认为一个词出现的概率只与它前面的 n-1 个词相关 根据条件概率公式与大数定律，当语料的规模足够大时，有 以 n=2 即 bi-gram 为例，有 假设词表的规模 N=200000（汉语的词汇量），模型参数与 `n· 的关系表 可靠性与可区别性 假设没有计算和存储限制，n 是不是越大越好？ 早期因为计算性能的限制，一般最大取到 n=4；如今，即使 n&gt;10 也没有问题， 但是，随着 n 的增大，模型的性能增大却不显著，这里涉及了可靠性与可区别性的问题 参数越多，模型的可区别性越好，但是可靠性却在下降——因为语料的规模是有限的，导致 count(W) 的实例数量不够，从而降低了可靠性 OOV 问题 OOV 即 Out Of Vocabulary，也就是序列中出现了词表外词，或称为未登录词 或者说在测试集和验证集上出现了训练集中没有过的词 一般解决方案： 设置一个词频阈值，只有高于该阈值的词才会加入词表 所有低于阈值的词替换为 UNK（一个特殊符号） 无论是统计语言模型还是神经语言模型都是类似的处理方式 &gt; NPLM 中的 OOV 问题 平滑处理 TODO count(W) = 0 是怎么办？ 平滑方法（层层递进）： Add-one Smoothing (Laplace) Add-k Smoothing (k&lt;1) Back-off （回退） Interpolation （插值法） Absolute Discounting （绝对折扣法） Kneser-Ney Smoothing （KN） Modified Kneser-Ney &gt; 自然语言处理中N-Gram模型的Smoothing算法 - CSDN博客 神经概率语言模型 (NPLM) 专题-词向量 - 神经概率语言模型依然是一个概率语言模型，它通过神经网络来计算概率语言模型中每个参数 - 其中 `g` 表示神经网络，`i_w` 为 `w` 在词表中的序号，`context(w)` 为 `w` 的上下文，`V_context` 为上下文构成的特征向量。 - `V_context` 由上下文的**词向量**进一步组合而成 N-gram 神经语言模型 A Neural Probabilistic Language Model (Bengio, et al., 2003) - 这是一个经典的神经概率语言模型，它沿用了 N-gram 模型中的思路，将 w 的前 n-1 个词作为 w 的上下文 context(w)，而 V_context 由这 n-1 个词的词向量拼接而成，即 - 其中 `c(w)` 表示 `w` 的词向量 - 不同的神经语言模型中 `context(w)` 可能不同，比如 Word2Vec 中的 CBOW 模型 每个训练样本是形如 (context(w), w) 的二元对，其中 context(w) 取 w 的前 n-1 个词；当不足 n-1，用特殊符号填充 同一个网络只能训练特定的 n，不同的 n 需要训练不同的神经网络 N-gram 神经语言模型的网络结构 【输入层】首先，将 context(w) 中的每个词映射为一个长为 m 的词向量，词向量在训练开始时是随机的，并参与训练； 【投影层】将所有上下文词向量拼接为一个长向量，作为 w 的特征向量，该向量的维度为 m(n-1) 【隐藏层】拼接后的向量会经过一个规模为 h 隐藏层，该隐层使用的激活函数为 tanh 【输出层】最后会经过一个规模为 N 的 Softmax 输出层，从而得到词表中每个词作为下一个词的概率分布 &gt; 其中 m, n, h 为超参数，N 为词表大小，视训练集规模而定，也可以人为设置阈值 训练时，使用交叉熵作为损失函数 当训练完成时，就得到了 N-gram 神经语言模型，以及副产品词向量 整个模型可以概括为如下公式： 原文的模型还考虑了投影层与输出层有有边相连的情形，因而会多一个权重矩阵，但本质上是一致的： &gt; &gt; 模型参数的规模与运算量 模型的超参数：m, n, h, N m 为词向量的维度，通常在 10^1 ~ 10^2 n 为 n-gram 的规模，一般小于 5 h 为隐藏的单元数，一般在 10^2 N 位词表的数量，一般在 10^4 ~ 10^5，甚至 10^6 网络参数包括两部分 词向量 C: 一个 N * m 的矩阵——其中 N 为词表大小，m 为词向量的维度 网络参数 W, U, p, q： 1234- W: h * m(n-1) 的矩阵- p: h * 1 的矩阵- U: N * h 的矩阵- q: N * 1 的矩阵 模型的运算量 主要集中在隐藏层和输出层的矩阵运算以及 SoftMax 的归一化计算 此后的相关研究中，主要是针对这一部分进行优化，其中就包括 Word2Vec 的工作 相比 N-gram 模型，NPLM 的优势 单词之间的相似性可以通过词向量来体现 &gt; 相比神经语言模型本身，作为其副产品的词向量反而是更大的惊喜 &gt; &gt; 词向量的理解 自带平滑处理 NPLM 中的 OOV 问题 在处理语料阶段，与 N-gram 中的处理方式是一样的——将不满阈值的词全部替换为 UNK 神经网络中，一般有如下几种处理 UNK 的思路 为 UNK 分配一个随机初始化的 embedding，并参与训练 &gt; 最终得到的 embedding 会有一定的语义信息，但具体好坏未知 把 UNK 都初始化成 0 向量，不参与训练 &gt; UNK 共享相同的语义信息 每次都把 UNK 初始化成一个新的随机向量，不参与训练 &gt; 常用的方法——因为本身每个 UNK 都不同，随机更符合对 UNK 基于最大熵的估计 &gt;&gt; How to add new embeddings for unknown words in Tensorflow (training &amp; pre-set for testing) - Stack Overflow &gt;&gt; &gt;&gt; Initializing Out of Vocabulary (OOV) tokens - Stack Overflow 基于 Char-Level 的方法 &gt; PaperWeekly 第七期 -- 基于Char-level的NMT OOV解决方案 {\"mode\":\"full\",\"isActive\":false}","link":"/posts/62790.html"},{"title":"EM算法及其推广","text":"Expectation Maximization algorithm Maximum likehood function likehood &amp; maximum likehood 1．EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法。含有隐变量的概率模型的数据表示为\\(\\theta\\) )。这里，\\(Y\\)是观测变量的数据，\\(Z\\)是隐变量的数据，\\(\\theta\\) 是模型参数。EM算法通过迭代求解观测数据的对数似然函数\\({L}(\\theta)=\\log {P}(\\mathrm{Y} | \\theta)\\)的极大化，实现极大似然估计。每次迭代包括两步： \\(E\\)步，求期望，即求\\(logP\\left(Z | Y, \\theta\\right)\\) )关于$ P(Z | Y, ^{(i)})$)的期望： \\[Q\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{Z} \\log P(Y, Z | \\theta) P\\left(Z | Y, \\theta^{(i)}\\right)\\] 称为\\(Q\\)函数，这里\\(\\theta^{(i)}\\)是参数的现估计值； \\(M\\)步，求极大，即极大化\\(Q\\)函数得到参数的新估计值： \\[\\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\\] 在构建具体的EM算法时，重要的是定义\\(Q\\)函数。每次迭代中，EM算法通过极大化\\(Q\\)函数来增大对数似然函数\\({L}(\\theta)\\)。 2．EM算法在每次迭代后均提高观测数据的似然函数值，即 \\[P\\left(Y | \\theta^{(i+1)}\\right) \\geqslant P\\left(Y | \\theta^{(i)}\\right)\\] 在一般条件下EM算法是收敛的，但不能保证收敛到全局最优。 3．EM算法应用极其广泛，主要应用于含有隐变量的概率模型的学习。高斯混合模型的参数估计是EM算法的一个重要应用，下一章将要介绍的隐马尔可夫模型的非监督学习也是EM算法的一个重要应用。 4．EM算法还可以解释为\\(F\\)函数的极大-极大算法。EM算法有许多变形，如GEM算法。GEM算法的特点是每次迭代增加\\(F\\)函数值（并不一定是极大化\\(F\\)函数），从而增加似然函数值。 在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们运用出现的结果来判断这个事情本身的性质（参数），也就是似然。 \\[P(Y|\\theta) = \\prod[\\pi p^{y_i}(1-p)^{1-y_i}+(1-\\pi) q^{y_i}(1-q)^{1-y_i}]\\] E step: \\[\\mu^{i+1}=\\frac{\\pi (p^i)^{y_i}(1-(p^i))^{1-y_i}}{\\pi (p^i)^{y_i}(1-(p^i))^{1-y_i}+(1-\\pi) (q^i)^{y_i}(1-(q^i))^{1-y_i}}\\] 12import numpy as npimport math 123456789pro_A, pro_B, por_C = 0.5, 0.5, 0.5def pmf(i, pro_A, pro_B, por_C): pro_1 = pro_A * math.pow(pro_B, data[i]) * math.pow( (1 - pro_B), 1 - data[i]) pro_2 = pro_A * math.pow(pro_C, data[i]) * math.pow( (1 - pro_C), 1 - data[i]) return pro_1 / (pro_1 + pro_2) M step: \\[\\pi^{i+1}=\\frac{1}{n}\\sum_{j=1}^n\\mu^{i+1}_j\\] \\[p^{i+1}=\\frac{\\sum_{j=1}^n\\mu^{i+1}_jy_i}{\\sum_{j=1}^n\\mu^{i+1}_j}\\] \\[q^{i+1}=\\frac{\\sum_{j=1}^n(1-\\mu^{i+1}_jy_i)}{\\sum_{j=1}^n(1-\\mu^{i+1}_j)}\\] 12345678910111213141516171819202122232425262728293031class EM: def __init__(self, prob): self.pro_A, self.pro_B, self.pro_C = prob # e_step def pmf(self, i): pro_1 = self.pro_A * math.pow(self.pro_B, data[i]) * math.pow( (1 - self.pro_B), 1 - data[i]) pro_2 = (1 - self.pro_A) * math.pow(self.pro_C, data[i]) * math.pow( (1 - self.pro_C), 1 - data[i]) return pro_1 / (pro_1 + pro_2) # m_step def fit(self, data): count = len(data) print('init prob:{}, {}, {}'.format(self.pro_A, self.pro_B, self.pro_C)) for d in range(count): _ = yield _pmf = [self.pmf(k) for k in range(count)] pro_A = 1 / count * sum(_pmf) pro_B = sum([_pmf[k] * data[k] for k in range(count)]) / sum( [_pmf[k] for k in range(count)]) pro_C = sum([(1 - _pmf[k]) * data[k] for k in range(count)]) / sum([(1 - _pmf[k]) for k in range(count)]) print('{}/{} pro_a:{:.3f}, pro_b:{:.3f}, pro_c:{:.3f}'.format( d + 1, count, pro_A, pro_B, pro_C)) self.pro_A = pro_A self.pro_B = pro_B self.pro_C = pro_C 1data=[1,1,0,1,0,0,1,0,1,1] 123em = EM(prob=[0.5, 0.5, 0.5])f = em.fit(data)next(f) init prob:0.5, 0.5, 0.5 12# 第一次迭代f.send(1) 1/10 pro_a:0.500, pro_b:0.600, pro_c:0.600 12# 第二次f.send(2) 2/10 pro_a:0.500, pro_b:0.600, pro_c:0.600 123em = EM(prob=[0.4, 0.6, 0.7])f2 = em.fit(data)next(f2) init prob:0.4, 0.6, 0.7 1f2.send(1) 1/10 pro_a:0.406, pro_b:0.537, pro_c:0.643 1f2.send(2) 2/10 pro_a:0.406, pro_b:0.537, pro_c:0.643 第9章EM算法及其推广-习题 习题9.1 如例9.1的三硬币模型，假设观测数据不变，试选择不同的处置，例如，\\(\\pi^{(0)}=0.46,p^{(0)}=0.55,q^{(0)}=0.67\\)，求模型参数为\\(\\theta=(\\pi,p,q)\\)的极大似然估计。 解答： 123456789101112131415161718192021222324252627282930313233343536373839%matplotlib inlineimport numpy as npimport mathclass EM: def __init__(self, prob): self.pro_A, self.pro_B, self.pro_C = prob def pmf(self, i): pro_1 = self.pro_A * math.pow(self.pro_B, data[i]) * math.pow( (1 - self.pro_B), 1 - data[i]) pro_2 = (1 - self.pro_A) * math.pow(self.pro_C, data[i]) * math.pow( (1 - self.pro_C), 1 - data[i]) return pro_1 / (pro_1 + pro_2) def fit(self, data): print('init prob:{}, {}, {}'.format(self.pro_A, self.pro_B, self.pro_C)) count = len(data) theta = 1 d = 0 while (theta &gt; 0.00001): # 迭代阻塞 _pmf = [self.pmf(k) for k in range(count)] pro_A = 1 / count * sum(_pmf) pro_B = sum([_pmf[k] * data[k] for k in range(count)]) / sum( [_pmf[k] for k in range(count)]) pro_C = sum([(1 - _pmf[k]) * data[k] for k in range(count)]) / sum([(1 - _pmf[k]) for k in range(count)]) d += 1 print('{} pro_a:{:.4f}, pro_b:{:.4f}, pro_c:{:.4f}'.format( d, pro_A, pro_B, pro_C)) theta = abs(self.pro_A - pro_A) + abs(self.pro_B - pro_B) + abs(self.pro_C - pro_C) self.pro_A = pro_A self.pro_B = pro_B self.pro_C = pro_C 12345# 加载数据data = [1, 1, 0, 1, 0, 0, 1, 0, 1, 1]em = EM(prob=[0.46, 0.55, 0.67])f = em.fit(data) init prob:0.46, 0.55, 0.67 1 pro_a:0.4619, pro_b:0.5346, pro_c:0.6561 2 pro_a:0.4619, pro_b:0.5346, pro_c:0.6561 可见通过两次迭代，参数已经收敛，三个硬币的概率分别为0.4619，0.5346，0.6561 习题9.2 证明引理9.2。 引理9.2：若\\(\\tilde{P}_{\\theta}(Z)=P(Z | Y, \\theta)\\)，则\\[F(\\tilde{P}, \\theta)=\\log P(Y|\\theta)\\] 证明： 由\\(F\\)函数的定义（定义9.3）可得：\\[F(\\tilde{P}, \\theta)=E_{\\tilde{P}}[\\log P(Y,Z|\\theta)] + H(\\tilde{P})\\]其中，\\(H(\\tilde{P})=-E_{\\tilde{P}} \\log \\tilde{P}(Z)\\) \\(\\begin{aligned} \\therefore F(\\tilde{P}, \\theta) &amp;= E_{\\tilde{P}}[\\log P(Y,Z|\\theta)] -E_{\\tilde{P}} \\log \\tilde{P}(Z) \\\\ &amp;= \\sum_Z \\log P(Y,Z|\\theta) \\tilde{P}_{\\theta}(Z) - \\sum_Z \\log \\tilde{P}(Z) \\cdot \\tilde{P}(Z) \\\\ &amp;= \\sum_Z \\log P(Y,Z|\\theta) P(Z|Y,\\theta) - \\sum_Z \\log P(Z|Y,\\theta) \\cdot P(Z|Y,\\theta) \\\\ &amp;= \\sum_Z P(Z|Y,\\theta) \\left[ \\log P(Y,Z|\\theta) - \\log P(Z|Y,\\theta) \\right] \\\\ &amp;= \\sum_Z P(Z|Y,\\theta) \\log \\frac{P(Y,Z|\\theta)}{P(Z|Y,\\theta)} \\\\ &amp;= \\sum_Z P(Z|Y,\\theta) \\log P(Y|\\theta) \\\\ &amp;= \\log P(Y|\\theta) \\sum_Z P(Z|Y,\\theta) \\end{aligned}\\) \\(\\displaystyle \\because \\sum_Z \\tilde{P}_{\\theta}(Z) = P(Z|Y, \\theta) = 1\\) \\(\\therefore F(\\tilde{P}, \\theta) = \\log P(Y|\\theta)\\)，引理9.2得证。 习题9.3 已知观测数据 -67，-48，6，8，14，16，23，24，28，29，41，49，56，60，75 试估计两个分量的高斯混合模型的5个参数。 解答： 12345678910111213from sklearn.mixture import GaussianMixtureimport numpy as npimport matplotlib.pyplot as plt# 初始化观测数据data = np.array([-67, -48, 6, 8, 14, 16, 23, 24, 28, 29, 41, 49, 56, 60, 75]).reshape(-1, 1)# 聚类gmmModel = GaussianMixture(n_components=2)gmmModel.fit(data)labels = gmmModel.predict(data)print(&quot;labels =&quot;, labels) labels = [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0] 123456789101112for i in range(0, len(labels)): if labels[i] == 0: plt.scatter(i, data.take(i), s=15, c='red') elif labels[i] == 1: plt.scatter(i, data.take(i), s=15, c='blue')plt.title('Gaussian Mixture Model')plt.xlabel('x')plt.ylabel('y')plt.show()print(&quot;means =&quot;, gmmModel.means_.reshape(1, -1))print(&quot;covariances =&quot;, gmmModel.covariances_.reshape(1, -1))print(&quot;weights = &quot;, gmmModel.weights_.reshape(1, -1)) means = [[ 32.98489643 -57.51107027]] covariances = [[429.45764867 90.24987882]] weights = [[0.86682762 0.13317238]] 习题9.4 EM算法可以用到朴素贝叶斯法的非监督学习，试写出其算法。 解答： &gt; EM算法的一般化： E步骤：根据参数初始化或上一次迭代的模型参数来计算出隐变量的后验概率，其实就是隐变量的期望。作为隐变量的现估计值：\\[w_j^{(i)}=Q_{i}(z^{(i)}=j) := p(z^{(i)}=j | x^{(i)} ; \\theta)\\] M步骤：将似然函数最大化以获得新的参数值：\\[ \\theta :=\\arg \\max_{\\theta} \\sum_i \\sum_{z^{(i)}} Q_i (z^{(i)}) \\log \\frac{p(x^{(i)}, z^{(i)} ; \\theta)}{Q_i (z^{(i)})} \\] 使用NBMM（朴素贝叶斯的混合模型）中的\\(\\phi_z,\\phi_{j|z^{(i)}=1},\\phi_{j|z^{(i)}=0}\\)参数替换一般化的EM算法中的\\(\\theta\\)参数，然后依次求解\\(w_j^{(i)}\\)与\\(\\phi_z,\\phi_{j|z^{(i)}=1},\\phi_{j|z^{(i)}=0}\\)参数的更新问题。 NBMM的EM算法： E步骤： \\[w_j^{(i)}:=P\\left(z^{(i)}=1 | x^{(i)} ; \\phi_z, \\phi_{j | z^{(i)}=1}, \\phi_{j | z^{(i)}=0}\\right)\\]M步骤：\\[ \\phi_{j | z^{(i)}=1} :=\\frac{\\displaystyle \\sum_{i=1}^{m} w^{(i)} I(x_{j}^{(i)}=1)}{\\displaystyle \\sum_{i=1}^{m} w^{(i)}} \\\\ \\phi_{j | z^{(i)}=0}:= \\frac{\\displaystyle \\sum_{i=1}^{m}\\left(1-w^{(i)}\\right) I(x_{j}^{(i)}=1)}{ \\displaystyle \\sum_{i=1}^{m}\\left(1-w^{(i)}\\right)} \\\\ \\phi_{z^{(i)}} :=\\frac{\\displaystyle \\sum_{i=1}^{m} w^{(i)}}{m} \\]","link":"/posts/19440.html"},{"title":"函数式编程","text":"函数是Python内建支持的一种封装，我们通过把大段代码拆成函数，通过一层一层的函数调用，就可以把复杂任务分解成简单的任务，这种分解可以称之为面向过程的程序设计。函数就是面向过程的程序设计的基本单元。 函数式编程（请注意多了一个“式”字）——Functional Programming，虽然也可以归结到面向过程的程序设计，但其思想更接近数学计算。 我们首先要搞明白计算机（Computer）和计算（Compute）的概念。 在计算机的层次上，CPU执行的是加减乘除的指令代码，以及各种条件判断和跳转指令，所以汇编语言是最贴近计算机的语言。 计算则是指数学意义上的计算，越是抽象的计算，离计算机硬件越远。 对应到编程语言，就是越低级的语言，越贴近计算机，抽象程度低，执行效率高，比如C语言；越高级的语言，越贴近计算，抽象程度高，执行效率低，比如Lisp语言。 归纳一下： 低级语言 高级语言 特点 贴近计算机 贴近计算（数学意义上） 抽象程度 低 高 执行效率 高 低 例子 汇编和C Lisp 函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。 函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！ Python仅对函数式编程提供部分支持。由于Python允许使用变量，因此，Python不是纯函数式编程语言。 目录 函数式编程的三大特性 函数式编程的几个技术 函数式编程的几个好处 简单举例 高阶函数 变量可以指向函数 函数名也是变量 传入函数 map/reduce filter sorted 返回函数 函数作为返回值 闭包 匿名函数 装饰器 带参数的decorator 属性复制 练习 偏函数 函数式编程的三大特性 immutable data 变量不可变，或者说没有变量，只有常量。 函数式编程输入确定时输出就是确定的，函数内部的变量和函数外部的没有关系，不会受到外部操作的影响。 first class functions 第一类函数(也称高阶函数)，意思是函数可以向变量一样用，可以像变量一样创建、修改、传递和返回。 这就允许我们把大段代码拆成函数一层层地调用，这种面向过程的写法相比循环更加直观。 尾递归优化 之前一章的递归函数中已经提及过了，就是递归时返回函数本身而非表达式。 可惜Python中没有这个特性。 函数式编程的几个技术 map &amp; reduce 函数式编程最常见的技术就是对一个集合做Map和Reduce操作。这比起传统的面向过程的写法来说，在代码上要更容易阅读（不需要使用一堆for、while循环来倒腾数据，而是使用更抽象的Map函数和Reduce函数）。 pipeline 这个技术的意思是把函数实例成一个一个的action，然后把一组action放到一个数组或是列表中组成一个action list，然后把数据传给这个action list，数据就像通过一个pipeline一样顺序地被各个函数所操作，最终得到我们想要的结果。 recursing 递归最大的好处就简化代码，它可以把一个复杂的问题用很简单的代码描述出来。注意：递归的精髓是描述问题，而这正是函数式编程的精髓。 currying 把一个函数的多个参数分解成多个函数， 然后把函数多层封装起来，每层函数都返回一个函数去接收下一个参数这样，可以简化函数的多个参数（减少函数的参数数目）。 higher order function 高阶函数：所谓高阶函数就是函数当参数，把传入的函数做一个封装，然后返回这个封装函数。现象上就是函数传进传出。 对currying进行一点补充，举个例子： 12345def pow(i, j): return i**jdef square(i): return pow(i, 2) 这里就是把原本平方函数square的参数j分解了，它返回幂函数pow函数，把幂次封装在里面，从而减少了求平方时所需用到的参数。 关于函数式编程的一些概念理解可以看傻瓜函数式编程或者英文原版的Functional Programming For The Rest of Us。 函数式编程的几个好处 parallelization 并行 在并行环境下，各个线程之间不需要同步或互斥(变量都是内部的，不需要共享)。 lazy evaluation 惰性求值 表达式不在它被绑定到变量之后就立即求值，而是在该值被取用的时候求值。 determinism 确定性 输入是确定的，输出就是确定的。 简单举例 以往面向过程式的编程需要引入额外的逻辑变量以及使用循环： 1234upname =['HAO', 'CHEN', 'COOLSHELL']lowname =[]for i in range(len(upname)): lowname.append( upname[i].lower() ) 而函数式编程则非常简洁易懂： 12345def toUpper(item): return item.upper()upper_name = map(toUpper, [&quot;hao&quot;, &quot;chen&quot;, &quot;coolshell&quot;])print upper_name 再看一个计算一个列表中所有正数的平均数的例子： 123456789101112num =[2, -5, 9, 7, -2, 5, 3, 1, 0, -3, 8]positive_num_cnt = 0positive_num_sum = 0for i in range(len(num)): if num[i] &gt; 0: positive_num_cnt += 1 positive_num_sum += num[i]if positive_num_cnt &gt; 0: average = positive_num_sum / positive_num_cntprint average 如果采用函数式编程： 12positive_num = filter(lambda x: x&gt;0, num)average = reduce(lambda x,y: x+y, positive_num) / len( positive_num ) 可以看到函数式编程减少了变量的使用，也就减少了出Bug的可能，维护更加方便。可读性更高，代码更简洁。 更多的例子和解析详见函数式编程。 高阶函数 前面已经提到了函数式编程中的高阶函数特性，这一节将针对Python中的使用方式进行更详细的描述。 变量可以指向函数 1234567&gt;&gt;&gt; abs&lt;built-in function abs&gt;&gt;&gt;&gt; f = abs&gt;&gt;&gt; f&lt;built-in function abs&gt;&gt;&gt;&gt; f(-10)10 这个例子表明在Python中变量是可以指向函数的，并且这样赋值的变量能够作为函数的别名使用。 函数名也是变量 12345&gt;&gt;&gt; abs = 10&gt;&gt;&gt; abs(-10)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: 'int' object is not callable 这里把abs函数赋值为10，这样赋值以后abs就变成一个整形变量，指向int型对象10而不指向原本的函数了。所以无法再作为函数使用。 想恢复abs函数要重启Python交互环境。 abs函数定义在 __builtin__ 模块中，要让修改abs变量的指向在其它模块也生效，用 __builtin__.abs = 10 就可以了。 当然实际写代码绝对不应该这样做.. 传入函数 函数能够作为参数传递，接收这样的参数的函数就称为高阶函数。 简单举例： 12345def add(x, y, f): return f(x) + f(y)&gt;&gt;&gt; add(-5, 6, abs)11 这里abs函数可以作为一个参数传入我们编写的add函数中，add函数就是一个高阶函数。 map/reduce map()函数和reduce()函数是Python的两个内建函数(BIF)。 map函数 map()函数接收两个参数，一个是函数，一个是Iterable对象，map将传入的函数依次作用到序列的每个元素，并把结果作为Iterator对象（惰性序列，可以用list转换为列表输出）返回。例如： 123456&gt;&gt;&gt; def f(x):... return x * x...&gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; list(r)[1, 4, 9, 16, 25, 36, 49, 64, 81] 这里直接使用list()函数将迭代器对象转换为一个列表。 写循环也能达到同样效果，但是显然没有map()函数直观。 map()函数作为高阶函数，大大简化了代码，更易理解。 12&gt;&gt;&gt; list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9]))['1', '2', '3', '4', '5', '6', '7', '8', '9'] 将一个整数列表转换为字符列表仅仅需要一行代码。 reduce函数 reduce接收两个参数，一个是函数（假设该函数称为f），一个是Iterable对象（假设是l）。函数f必须接收两个参数，reduce函数每次会把上一次函数f返回的值和l的下一个元素传入到f中，直到l中所有元素都参与过运算时返回函数f最后返回的值（第一次传入时传入l的头两个元素）。 123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def add(x, y):... return x + y...&gt;&gt;&gt; reduce(add, [1, 3, 5, 7, 9])25 这里举了一个最简单的序列求和作例子(当然实际上我们直接用sum()函数更方便，这里只是为了举例子)。 这里reduce函数每次将add作用于序列的前两个元素，并把结果返回序列的头部，直到序列只剩下1个元素就返回结果（这样理解可能更直观一些）。 123456789&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fn(x, y):... return x * 10 + y...&gt;&gt;&gt; def char2num(s):... return {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}[s] #字符对应整数的dict，返回传入字符对应的整数...&gt;&gt;&gt; reduce(fn, map(char2num, '13579'))13579 可以整理一下，作为一个整体的str2int函数： 123456def str2int(s): def fn(x, y): return x * 10 + y def char2num(s): return {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}[s] return reduce(fn, map(char2num, s)) 使用lambda匿名函数还可以进一步简化： 12345def char2num(s): return {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}[s]def str2int(s): return reduce(lambda x, y: x * 10 + y, map(char2num, s)) 练习 1.利用map()函数，把不规范的英文名字变为首字母大写其他字母小写的规范名字。 Hint: 字符串支持切片操作，并且可以用加号做字符串拼接。 转换大写用upper函数，转换小写用lower函数。 123456def normalize(name): return name[0].upper()+name[1:].lower()L1 = ['adam', 'LISA', 'barT']L2 = list(map(normalize, L1))print(L2) 2.编写一个prod()函数，可以接受一个list并利用reduce()求积。 Hint: 用匿名函数做两数相乘 用reduce函数做归约，得到列表元素连乘之积。 12345from functools import reducedef prod(L): return reduce(lambda x,y: x*y,L)print('3 * 5 * 7 * 9 =', prod([3, 5, 7, 9])) 3.利用map和reduce编写一个str2float函数，把字符串'123.456'转换成浮点数123.45。 Hint: 这题的思路是找到小数点的位置i(从个位开始数i个数字之后)，然后让转换出的整数除以10的i次方。 另外一种思路是在转换时遇到小数点后，改变转换的方式由 num*10+当前数字 变为 num+当前数字/point。 point初始为1，每次加入新数字前除以10。 12345678910from functools import reducefrom math import powdef chr2num(s): return {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}[s]def str2float(s): return reduce(lambda x,y:x*10+y,map(chr2num,s.replace('.',''))) / pow(10,len(s)-s.find('.')-1)print(str2float('985.64785')) filter filter()函数同样是内建函数，用于过滤序列。 filter()接收一个函数和一个Iterable对象。 和map()不同的时，filter()把传入的函数依次作用于每个元素，然后根据函数返回值是True还是False决定保留还是丢弃该元素。 简单的奇数筛选例子： 12345def is_odd(n): return n % 2 == 1list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))# 结果: [1, 5, 9, 15] 筛掉列表的空字符串： 12345def not_empty(s): return s and s.strip()list(filter(not_empty, ['A', '', 'B', None, 'C', ' ']))# 结果: ['A', 'B', 'C'] 其中，strip 函数用于删除字符串中特定字符，格式为：s.strip(rm)，删除s字符串中开头、结尾处的包含在rm删除序列中的字符。 rm为空时默认删除空白符(包括'', ', ', ' ')。 注意到 filter() 函数返回的是一个 Iterator对象，也就是一个惰性序列，所以要强迫 filter() 完成计算结果，需要用 list() 函数获得所有结果并返回list。 filter函数最重要的一点就是正确地定义一个筛选函数（即传入filter作为参数的那个函数)。 练习 1.用filter筛选素数 这里使用埃氏筛法。 首先，列出从2开始的所有自然数，构造一个序列： 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ... 取序列的第一个数2，它一定是素数，然后用2把序列的2的倍数筛掉： 3, 5, 7, 9, 11, 13, 15, 17, 19, ... 取新序列的第一个数3，它一定是素数，然后用3把序列的3的倍数筛掉： 5, 7, 11, 13, 17, 19, ... 以此类推... 首先构造一个生成器，输出3开始的奇数序列: 12345def _odd_iter(): n = 1 while True: n = n + 2 yield n 然后定义一个筛选函数，传入n，判断x能否除尽n： 12def _not_divisible(n): return lambda x: x % n &gt; 0 这里x是匿名函数的参数，由外部提供。 然后就是定义返回素数的生成器了。 首先输出素数2，然后初始化奇数队列，每次输出队首(必然是素数，因为前一轮的过滤已经排除了比当前队首小且非素数的数)。 构造新的队列，每次用当前序列最小的数作为除数，检验后面的数是否素数。 定义如下： 1234567def primes(): yield 2 it = _odd_iter() # 初始序列 while True: n = next(it) # 返回序列的第一个数 yield n it = filter(_not_divisible(n), it) # 构造新序列 这里因为it是一个迭代器，每次使用next就得到队列的下一个元素，实际上就类似队列的出列操作，挤掉队首，不用担心重复。 然后这里filter的原理，就是把当前it队列的每个数都放进_not_divisible(n)中检测一下，注意不是作为参数n传入而是作为匿名函数的参数x传入！ _not_divisible(n) 实际是作为一个整体来看的，它返回一个自带参数n的函数(也即那个匿名函数)，然后filter再把列表每一个元素一一传返回的匿名函数中。一定要搞清楚这一点。 最后，因为primes产生的也是一个无限长的惰性序列，我们一般不需要求那么多，简单写个循环用作退出即可： 123456# 打印1000以内的素数:for n in primes(): if n &lt; 1000: print(n) else: break 2.用filter筛选回文数 Hint: str可以把整数转换为字符串 [::-1]可以得到逆序的列表。 1234def is_palindrome(n): return str(n) == str(n)[::-1]print(list(filter(is_palindrome, range(0,1001)))) sorted Python内置的 sorted() 函数就可以对list进行排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21])[-21, -12, 5, 9, 36] 并且 sorted() 作为高阶函数还允许接受一个key函数用于自定义排序，例如： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)[5, 9, -12, -21, 36] key指定的函数将作用于list的每一个元素上，并根据key函数返回(映射)的结果进行排序，最后对应回列表中的元素进行输出。 再看一个字符串排序例子： 12&gt;&gt;&gt; sorted(['bob', 'about', 'Zoo', 'Credit'])['Credit', 'Zoo', 'about', 'bob'] 默认情况下是按ASCII码排序的，但我们往往希望按照字典序来排，思路就是把字符串变为全小写/全大写再排： 12&gt;&gt;&gt; sorted(['bob', 'about', 'Zoo', 'Credit'], key=str.lower)['about', 'bob', 'Credit', 'Zoo'] 默认排序是由小到大，要反相排序只需把reverse参数设为True。 温习前面的知识，这里reverse参数是一个命名关键字参数。 12&gt;&gt;&gt; sorted(['bob', 'about', 'Zoo', 'Credit'], key=str.lower, reverse=True)['Zoo', 'Credit', 'bob', 'about'] 使用好sorted函数的关键就是定义好一个映射函数。 练习 给出成绩表，分别按姓名和成绩进行排序。 1234567&gt;&gt;&gt; L = [('Bob', 75), ('Adam', 92), ('Bart', 66), ('Lisa', 88)]&gt;&gt;&gt; L2 = sorted(L, key = lambda x:x[0]) #按姓名排序&gt;&gt;&gt; L2[('Adam', 92), ('Bart', 66), ('Bob', 75), ('Lisa', 88)]&gt;&gt;&gt; L3 = sorted(L, key = lambda x:x[1]) #按成绩排序&gt;&gt;&gt; L3[('Bart', 66), ('Bob', 75), ('Lisa', 88), ('Adam', 92)] 返回函数 函数作为返回值 高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。 比方说我们想实现一个对可变参数求和的函数，可以这样写： 12345def calc_sum(*args): ax = 0 for n in args: ax = ax + n return ax 调用时可以传入任意个数字，并得到这些数字的和。而如果我们不需要立即求和，而是后面再根据需要来进行求和，可以写为返回求和函数的形式： 1234567def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum 在调用 lazy_sum 时，返回一个sum函数，但sum函数内部的求和代码没有执行： 123&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt; 当我们再调用返回的这个sum函数时，就能得到和值了： 12&gt;&gt;&gt; f()25 注意！每一次调用 lazy_sum 返回的函数都是不同的！即使传入相同的参数，返回函数也是不同的！举个例子： 1234&gt;&gt;&gt; f1 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f2 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f1==f2False f1和f2是不同的两个函数，虽然调用它们得到同样的结果，但它们是互不影响的。 闭包 在Python中，从表现形式上来讲，闭包可以定义为：如果在一个内部函数里，对外部作用域（非全局作用域）的变量进行了引用，那么这个内部函数就被认为是闭包(closure)。 如上面例子中的f就是一个闭包，它调用了变量i，变量i属于外面的循环体而不是全局变量。 看一个例子： 1234567891011def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count() 三个返回函数的调用结果是： 123456&gt;&gt;&gt; f1()9&gt;&gt;&gt; f2()9&gt;&gt;&gt; f3()9 解析一下，这里count函数是一个返回三个函数的函数，里面使用了一个循环体来产生要返回的三个函数。从i为1开始到i等于3，每次产生一个函数f，返回i的平方值。如果按照平常的思路来看，可能会觉得返回的三个函数f1、f2、f3应该分别输出1、4、9。 但实际上并不是这样的，这是因为返回一个函数时，函数内部的代码是没有执行的！ 只有在调用这个返回的函数时才会执行！ 调用count函数时，实际上返回了3个新的函数，循环变量i的值也变为3。在调用这3个返回的函数时，它们的代码才会执行，这时引用的i的值就都是3。 如果一定要在闭包中用到外部的循环变量，要怎么办呢？ 我们先定义一个函数，用它的参数绑定循环变量，然后再在它的里面定义要返回的函数。 这样无论后面循环变量怎么变，已经绑定到参数的值是不会变的，就能得到我们期望的结果了。也即把上面的例子改写为： 1234567891011def count(): def f(j): def g(): return j*j return g fs = [] for i in range(1, 4): fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f() return fs 调用结果： 1234567&gt;&gt;&gt; f1, f2, f3 = count()&gt;&gt;&gt; f1()1&gt;&gt;&gt; f2()4&gt;&gt;&gt; f3()9 这里闭包g用到的变量j是外部作用域f的，并且j作为参数绑定在f中不再改变，不同于外部作用域count函数中的变量i。 所以执行count返回的3个函数，每个的结果都不同。 总结 返回闭包时，不要在闭包的代码中引用外部作用域的循环变量或者外部作用域中会变化的变量。 不应该在闭包中修改外部作用域的局部变量。 匿名函数 当我们在使用函数作为参数时，有些时候，不需要预先显式地定义函数，直接传入一个匿名函数更方便。 举个简单例子，计算 f(x)=x² 时，不需要显示定义f(x)，使用匿名函数可以直接一行解决，这样写非常简洁。 12&gt;&gt;&gt; list(map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9]))[1, 4, 9, 16, 25, 36, 49, 64, 81] 关键字lambda表示要定义一个匿名函数，冒号前面的x表示函数的参数。 匿名函数有个限制，就是只能包含一个表达式，不用写return，返回值就是该表达式的结果。 所以上面这个匿名函数就是：x作为参数传入，返回 x*x 的结果。 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数： 12345&gt;&gt;&gt; f = lambda x: x * x&gt;&gt;&gt; f&lt;function &lt;lambda&gt; at 0x101c6ef28&gt;&gt;&gt;&gt; f(5)25 并且匿名函数作为一个函数对象，也能被函数返回(像上一节那样)： 12def build(x, y): return lambda: x * x + y * y 这里返回的函数是一个匿名函数，它没有参数，里面调用的变量x和变量y是绑定在外部作用域build中的参数。所以调用build时会根据使用的参数返回这个匿名函数，调用返回的这个函数时使用的变量x和变量y就是调用build时用的参数。 装饰器 有时候我们希望为函数增加一些额外的功能，比如在调用函数的前后自动打印某些信息，但又不希望修改定义函数的代码。这时就可以使用装饰器（Decorator）了，它是python中对装饰器模式的实现，可以在代码运行期间动态增加功能（装饰器的代码和要装饰的函数的代码还是要写好，这里只是说对要装饰的函数使用装饰器后，在运行时要装饰的函数会被重新包装一遍，使得它具有了装饰器中定义的功能）。 比方说我们要实现一个打印函数名的额外功能，它是通过调用函数对象的 __name__ 属性获得的： 12345&gt;&gt;&gt; def now():... print('2015-3-25')...&gt;&gt;&gt; now.__name__'now' 如果我们不想在每个函数中都重复写实现这个功能的代码，可以把它写为装饰器的形式，然后为每个函数添加这个装饰器。装饰器本身也是一个函数，这个例子可以写为： 12345def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 它像正常函数一样定义（没有特别的语法），接收一个函数作为参数，并且返回一个函数 wrapper（Python中函数也是对象，既可以作为参数，也能被返回）。回顾前一节的内容，可以看出 wrapper 函数是一个闭包，它本身接收可变参数 *args 和关键字参数 **kw，并且引用了外部作用域中绑定在 log 函数参数中的 func 变量。 使用装饰器时，借助Python的@语法，把装饰器放在函数定义的上一行即可： 123@logdef now(): print('2016-2-10') 运行时： 123&gt;&gt;&gt; now()call now():2015-3-25 原理是这样的，把 @log 放在 now() 函数的定义前，运行代码的时候，实际上是在函数定义后执行了： 1now = log(now) 也即执行了 log 函数，并把 now 这个变量名赋值为 log(now) 返回的 wrapper(*args, **kw) 函数（也即 now 引用的函数对象变了）。此时查看 now 变量指向的函数对象的名字，会发现已经变成了 wrapper： 12&gt;&gt;&gt; now.__name__'wrapper' 此时我们调用这个新的 now() 函数时，实际上执行的就是 wrapper 函数中的代码，打印出函数信息，然后再调用原来的 now 函数。要注意 wrapper 调用的 now 函数和我们调用的 now 函数是不同的两个函数，我们调用的 now 函数已经变成了 wrapper 函数，而 wrapper 函数调用的则是绑定在 log 函数参数中的原本的 now 函数。 简单归纳一下： 装饰器也是一个函数 装饰器实际上是把传入的函数进行一层包装，返回一个新函数 要为函数添加装饰器时，在函数定义前使用 @装饰器名 即可 装饰器的原理部分如果还有不清楚的，不妨看看知乎上李冬的答案，讲得比较浅显和清楚。 带参数的decorator 前面提到装饰器可以用于为函数提供增强功能而无须修改函数本身的代码，在装饰器函数中，闭包 wrapper 接收的参数就是函数的参数。但是，如果我们希望在使用装饰器时可以更灵活一些，为不同的函数添加功能类似但又略有不同的装饰器呢？这时我们可以使用带参数的装饰器来实现（装饰器本身也是函数，是可以传入参数的）。 比方说要实现一个自定义打印文本的功能： 1234567def log(text): def decorator(func): def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 注意到这里在 wrapper 和 log 之间又套了一层函数，现在变为了 log 接收参数 text 并返回一个装饰器 decorator。这个 decorator 接收一个函数对象，输出文本 text 和函数对象的名字，理解起来其实不难。 使用这个装饰器： 123@log('execute')def now(): print('2015-3-25') 执行结果如下： 123&gt;&gt;&gt; now()execute now():2015-3-25 事实上，把 @log 放在 now() 函数的定义前，运行代码时实际上在函数定义后执行了： 1&gt;&gt;&gt; now = log('execute')(now) 也即是先调用 log 函数，传入参数 log('execute')，这时返回了 decorator 这个装饰器，然后传入了 now 函数，最后返回包装好的 now 函数（也即 wrapper 函数）。 属性复制 前面已经提到使用 @语法 之后，now变量指向的函数名字等属性都改变了，变成了 wrapper 函数的，实际上，我们希望变量 now 的属性依然是原本 now() 函数的属性，这时就需要进行属性复制。 我们不需要编写类似 wrapper.__name__ = func.__name__ 这样的代码来逐个把原函数的属性复制给 wrapper，Python内置的 functools.wraps 装饰器可以满足我们的需求。方法如下： 12345678import functoolsdef log(func): @functools.wraps(func) def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 和原来定义装饰器的代码对比，唯一修改的就是加上了 @functools.wraps(func) 这一句。 当然，还要注意先导入functools模块。 练习 习题1 编写一个decorator，能在函数调用的前后分别打印出 'begin call' 和 'end call' 的日志。 解析： 这题很简单，在 wrapper 调用原函数之前，各编写一条打印语句就可以了。 代码： 1234567891011def decorator(func): def wrapper(*args,**kw): print(&quot;begin call&quot;) a = func(*args,**kw) print(&quot;end call&quot;) return a return wrapper@decoratordef now(): print(&quot;haha&quot;) 执行结果： 123456&gt;&gt;&gt; now # now是一个函数&lt;function decorator.&lt;locals&gt;.wrapper at 0x00000254B45D8EA0&gt;&gt;&gt;&gt; now() # 调用now函数begin callhahaend call 习题2 写出一个@log的decorator，使它既支持： 123@logdef f(): pass 又支持： 123@log('execute')def f(): pass 解析： 思路很简单，我们知道使用不带参数的装饰器时，传入装饰器函数（即这里的 log）的参数就是要装饰的函数（比方说 now 函数）；而带参数的装饰器接收的参数则不是要装饰的函数而是别的（比方说一个字符串）。所以呀，我们可以依然使用带参数的装饰器作为原型，但在里面加入对参数类型的判断，如果接收到字符串参数则表示这次调用的是有参数的装饰器，否则就是调用不带参数的装饰器。 代码： 1234567891011121314151617181920212223import functoolsdef log(text): # 默认参数，没有参数时，text就是空字符串 def decorator(func): @functools.wraps(func) # 属性复制 def wrapper(*args,**kw): if isinstance(text, str): print('%s %s():' % (text, func.__name__)) else: print('%s():' % func.__name__) return func(*args, **kw) return wrapper if isinstance(text, str): # 接收到字符串后返回decorator函数 return decorator else: return decorator(text) # 接收到函数则直接返回wrapper函数@log('execute') # 带参数text的decoratordef now1(): print('2016-2-10')@log # 不带参数text的decoratordef now2(): print('2016-2-10') 执行结果： 123456&gt;&gt;&gt; now1()execute now1():2016-2-10&gt;&gt;&gt; now2()now2():2016-2-10 偏函数 Python的functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。 functools.partial(f, *args, **kw) 的作用就是创建一个偏函数，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 举个例子，字符串转整型数的函数int，可以使用关键字参数base，指定字符串的进制是多少，然后转换为int的时候会按照base进行进制转换，把字符串转换成十进制整数。 如： 12&gt;&gt;&gt; int('1000000', base=2)64 如果有大量的二进制字符串要转换，每次都写base=2很麻烦，我们就会希望定义一个新函数，把base参数固定为2，无须每次指定： 12def int2(x, base=2): return int(x, base) 实际上我们不需要自己定义，使用 functools.partial 就可以轻松创建偏函数： 123456&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2('1000000')64&gt;&gt;&gt; int2('1010101')85 运行int('1000000')实际相当于： 12kw = { 'base': 2 }int('1000000', **kw) Notice： 这里创建偏函数只是设定了默认值为2，调用偏函数时依然可以把参数设置为其他值。 12&gt;&gt;&gt; int2('1000000', base=10)1000000 functools.partial 不但可以接收关键字参数，还可以接收可变参数 *args，如： 123&gt;&gt;&gt; max2 = functools.partial(max, 10)&gt;&gt;&gt; max2(5, 6, 7)10 相当于max函数每次接收到若干数字时，都默认再放入一个整数10，然后找其中的最大值。","link":"/posts/62985.html"},{"title":"RNN原理","text":"Recurrent Neural Network，循环神经网络 SimpleRNN SimpleRNN其结构如下图所示： 输入为一个向量序列\\(\\{x_0,x_1,x_2...x_n\\}\\) ； 在时间步 \\(t\\)，序列的元素 \\(x_t\\) 和上一时间步的输出 $h_{t-1} $一起，经过RNN单元处理，产生输出 \\(h_t\\); \\[h_t=ϕ(Wx_t+Uh_{t−1})\\] \\[y_t=Vh_t\\] \\(h_t\\) 为隐藏层状态，携带了序列截止时间步 \\(t\\) 的信息；\\(y_t\\) 为时间步 \\(t\\) 的输出；\\(h_t\\) 继续作为下一时间步的输入 整个序列被处理完，最终的输出 \\(y_n\\) 即为RNN的输出；根据情况，也可返回所有的输出序列 \\(\\{y_0,y_1,y_2...y_n\\}\\) 序列的每个元素是经过同一个RNN处理，因此待学习的参数只有一组：\\(W,U,V\\) 序列元素依次经过RNN的激活(sigmoid/tanh)函数的处理，存在信息丢失；并且在训练时反向传播会导致梯度消失，因此只能储存短期记忆 例如训练单词it对应的向量时，只能利用time和is对应的信息，而what对应的信息丢失 1 1 LSTM LSTM原理 Long Short-Term Memory，其框架如下所示，LSTM单元利用当前输入、短期记忆和长期记忆，更新长期和短期记忆，并产生输出 LSTM结构 LSTM的结构如下图所示，包含四个门：forget gate,learn gate,remember gate,ouput(use) gate forget gate：决定长期记忆\\(c_t\\)中哪些信息该保留，哪些该忘记 首先整合当前输入\\(x_t\\)和短期记忆\\(h_{t-1}\\)，输出一个向量\\(f_t\\)； \\(f_t\\)的值介于\\(0-1\\)之间，每一位对应于长期记忆的一个数字，\\(1\\)表示完全保留，\\(0\\)表示完全丢弃 \\[f_t=\\sigma(W_f[h_{t-1},x_t]+b_f)\\] \\[Out_f = c_{t-1}\\cdot f_t\\] learn gate：决定短期记忆和当前输入中学到的信息 首先整合 \\(x_t\\) 和短期记忆 \\(h_{t-1}\\) 的信息 \\(\\hat c_t\\) 然后通过 \\(x_t\\) 和 \\(h_{t-1}\\) 获得一个遗忘因子 \\(i_t\\)，其值位于\\(0-1\\)之间 再将上两步的结果结合 \\[\\hat c_t=tanh(W_n[h_{t-1},x_t]+b_n)\\] \\[i_t=\\sigma(W_i[h_{t-1},x_t]+b_i)\\] \\[Out_n = i_t\\cdot \\hat c_t\\] remember gate：整合上一步的长短期记忆，更新长期记忆 \\[c_t = Out_f+Out_n\\] output(use) gate：整合上一步的长短期记忆，更新短期记忆 \\[o_t=\\sigma(W_o[h_{t-1},x_t]+b_o)\\] \\[h_t=o_t\\cdot tanh(c_t)\\] 短期记忆\\(h_t\\)，即为LSTM当前时间步\\(t\\)的输出 综上LSTM单元的训练参数有四组：forget gate参数\\(\\{W_f,b_f\\}\\)，learn gate参数\\(\\{W_n,b_n\\}\\)和\\(\\{W_i,b_i\\}\\)，output gate参数\\(\\{W_o,b_o\\}\\) LSTM中不同位置处sigmoid和tanh激活函数的选择，向量相加加或相乘的确定，具有一定的随意性。之所以选择现结构，是因为在实践中有效 peephole机制 门机制中的sigmoid激活函数，将输入转化成\\(0-1\\)数值；sigmoid乘以另一向量，即可决定保留该向量的哪些信息； 上述LSTM结构中三个sigmoid函数的输入都是当前输入和短期记忆\\([h_{t_1},x_t]\\)，即决定LSTM单元保留哪些信息的都是短期记忆； peephole connections：将长期记忆也加入到sigmoid激活函数的输入中，其在LSTM中的决策参与度提高了，即长期和短期记忆共同决定保留哪些信息、丢弃哪些信息 \\[f_t=\\sigma(W_f[c_{t-1},h_{t-1},x_t]+b_f)\\] \\[i_t=\\sigma(W_i[c_{t-1},h_{t-1},x_t]+b_i)\\] \\[o_t=\\sigma(W_o[c_{t-1},h_{t-1},x_t]+b_o)\\] 1 1 GRU Gated Recurrent Unit，将forget gate和learn gate整合成单个的update gate，单元状态(长期记忆)\\(c_{t}\\)与隐藏状态(短期记忆)\\(h_t\\)合并 1 RNN实现 123456import torchimport torch.nn as nnimport numpy as npimport matplotlib.pyplot as plt%matplotlib inline SimpleRNN层 对输入序列的每个向量\\(x_t\\)，进行如下计算： \\(h_t=tanh(W_{ih}x_t+b_{ih}+W_{hh}h_{(t−1)}+b_{hh})\\) 12345678910111213141516# 指定输入的特征量，隐藏状态的长度，rnn的层数rnn = nn.RNN( input_size=6, hidden_size=10, num_layers=2, batch_first=True, # 输入和输出张量形状：batch,seq,feature bidirectional=False, # 双向RNN)# 输入张量：input = torch.randn(5, 3, 6) # batch,seq,featureh0 = torch.randn(2, 5, 10) # num_layers,batch,hidden_sizeprint(&quot;输入形状：&quot;, input.shape)output, hn = rnn(input, h0)print(&quot;输出形状：&quot;, output.shape, &quot; 隐藏层状态形状：&quot;, hn.shape) 输入形状： torch.Size([5, 3, 6]) 输出形状： torch.Size([5, 3, 10]) 隐藏层状态形状： torch.Size([2, 5, 10]) 12print(&quot;输入向量对应的权重 W_ih：&quot;,rnn.weight_ih_l0.shape)print(&quot;隐藏状态对应的权重 W_hh：&quot;,rnn.weight_hh_l0.shape) 输入向量对应的权重 W_ih： torch.Size([10, 6]) 隐藏状态对应的权重 W_hh： torch.Size([10, 10]) LSTM层 123456789101112131415rnn = nn.LSTM( input_size=6, hidden_size=10, num_layers=2, batch_first=True, # 输入和输出张量形状：batch,seq,feature bidirectional=False,)input = torch.randn(5, 3, 6)h0 = torch.randn(2, 5, 10) # num_layer,batch,hiddenc0 = torch.randn(2, 5, 10)output, (hn, cn) = rnn(input, (h0, c0))print(&quot;输出形状：&quot;, output.shape)print(&quot;hidden state：&quot;, hn.shape)print(&quot;cell state：&quot;, cn.shape) 输出形状： torch.Size([5, 3, 10]) hidden state： torch.Size([2, 5, 10]) cell state： torch.Size([2, 5, 10]) 1 GRU层 1234rnn = nn.GRU(10, 20, 2)input = torch.randn(5, 3, 10)h0 = torch.randn(2, 3, 20)output, hn = rnn(input, h0) 1h0.shape torch.Size([2, 3, 15]) 1 RNN训练流程 训练数据 1234567891011121314151617plt.figure(figsize=(8, 5))# 序列数据seq_length = 20time_steps = np.linspace(0, np.pi, seq_length + 1)data = np.sin(time_steps)data.resize((seq_length + 1, 1))x = data[:-1] # 数据y = data[1:] # 标签# 图示数据plt.plot(time_steps[1:], x, 'r.', label='input, x')plt.plot(time_steps[1:], y, 'b.', label='target, y')plt.legend(loc='best')plt.show() 定义模型 12345678910111213class RNN(nn.Module): def __init__(self, input_size, output_size, hidden_dim, n_layers): super(RNN, self).__init__() self.hidden_dim = hidden_dim self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True) self.fc = nn.Linear(hidden_dim, output_size) def forward(self, x, hidden): batch_size = x.size(0) r_out, hidden = self.rnn(x, hidden) r_out = r_out.view(-1, self.hidden_dim) output = self.fc(r_out) return output, hidden 123456789# 检测正确建模test_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=2)test_input = torch.Tensor(data).unsqueeze(0)print('Input size:', test_input.size())test_out, test_h = test_rnn(test_input, None)print('Output size:', test_out.size())print('Hidden state size:', test_h.size()) Input size: torch.Size([1, 21, 1]) Output size: torch.Size([21, 1]) Hidden state size: torch.Size([2, 1, 10]) 训练模型 12345# 参数input_size = 1output_size = 1hidden_dim = 32n_layers = 1 123# 初始化模型rnn = RNN(input_size, output_size, hidden_dim, n_layers)print(rnn) RNN( (rnn): RNN(1, 32, batch_first=True) (fc): Linear(in_features=32, out_features=1, bias=True) ) 123# 损失函数和优化器criterion = nn.MSELoss()optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01) 1234567891011121314151617181920212223242526272829# 训练模型def train(rnn, n_steps, print_every): hidden = None for batch_i, step in enumerate(range(n_steps)): x_tensor = torch.Tensor(x).unsqueeze(0) y_tensor = torch.Tensor(y) # 前向推理 prediction, hidden = rnn(x_tensor, hidden) hidden = hidden.data # 损失函数 loss = criterion(prediction, y_tensor) # 梯度归零 optimizer.zero_grad() # 反向传播 loss.backward() # 更新梯度 optimizer.step() if batch_i % print_every == 0: print('Loss: ', loss.item()) plt.plot(time_steps[1:], x, 'r.') plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.') plt.show() return rnn 12345n_steps = 75print_every = 15# 训练trained_rnn = train(rnn, n_steps, print_every) Loss: 0.40589970350265503 Loss: 0.035483404994010925 Loss: 0.012853428721427917 Loss: 0.00824706070125103 Loss: 0.010340889915823936 1 RNN示例：字符级文本生成 1234import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as F 数据集 1234with open('datasets/anna.txt', 'r') as f: text = f.read() text[:100] 'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin' 12345678# 文本向量化chars = tuple(set(text))int2char = dict(enumerate(chars))char2int = {ch: ii for ii, ch in int2char.items()}encoded = np.array([char2int[ch] for ch in text])encoded[:20] array([77, 41, 28, 66, 7, 21, 47, 58, 4, 35, 35, 35, 23, 28, 66, 66, 31, 58, 9, 28]) 数据预处理 12345def one_hot_encode(arr, n_labels): one_hot = np.zeros((arr.size, n_labels), dtype=np.float32) one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1. one_hot = one_hot.reshape((*arr.shape, n_labels)) return one_hot 123test_seq = np.array([[3, 5, 1]])one_hot = one_hot_encode(test_seq, 8)print(one_hot) [[[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]]] 12345678910111213141516171819202122# 创建批量数据def get_batches(arr, batch_size, seq_length): batch_size_total = batch_size * seq_length n_batches = len(arr) // batch_size_total arr = arr[:n_batches * batch_size_total] arr = arr.reshape((batch_size, -1)) for n in range(0, arr.shape[1], seq_length): x = arr[:, n:n + seq_length] y = np.zeros_like(x) try: y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seq_length] except IndexError: y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0] yield x, ybatches = get_batches(encoded, 8, 50)x, y = next(batches)print('x\\n', x[:4, :10])print('\\ny\\n', y[:4, :10]) x [[77 41 28 66 7 21 47 58 4 35] [39 81 37 58 7 41 28 7 58 28] [21 37 67 58 81 47 58 28 58 9] [39 58 7 41 21 58 11 41 12 21]] y [[41 28 66 7 21 47 58 4 35 35] [81 37 58 7 41 28 7 58 28 7] [37 67 58 81 47 58 28 58 9 81] [58 7 41 21 58 11 41 12 21 9]] 创建模型 12345678# gpu 可用train_on_gpu = torch.cuda.is_available()if (train_on_gpu): print('Training on GPU!')else: print( 'No GPU available, training on CPU; consider making n_epochs very small.' ) Training on GPU! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 创建模型class CharRNN(nn.Module): def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001): super(CharRNN, self).__init__() self.drop_prob = drop_prob self.n_layers = n_layers self.n_hidden = n_hidden self.lr = lr self.chars = tokens self.int2char = dict(enumerate(self.chars)) self.char2int = {ch: ii for ii, ch in self.int2char.items()} self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True) self.dropout = nn.Dropout(drop_prob) self.fc = nn.Linear(n_hidden, len(self.chars)) def forward(self, x, hidden): r_output, hidden = self.lstm(x, hidden) out = self.dropout(r_output) out = out.contiguous().view(-1, self.n_hidden) out = self.fc(out) return out, hidden def init_hidden(self, batch_size): weight = next(self.parameters()).data if train_on_gpu: hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda()) else: hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_()) return hidden 训练模型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374def train(net, data, epochs=10, batch_size=10, seq_length=10, lr=0.001, clip=5, val_frac=0.1, print_every=10): net.train() opt = torch.optim.Adam(net.parameters(), lr=lr) criterion = nn.CrossEntropyLoss() val_idx = int(len(data) * (1 - val_frac)) data, val_data = data[:val_idx], data[val_idx:] if train_on_gpu: net.cuda() counter = 0 n_chars = len(net.chars) for e in range(epochs): h = net.init_hidden(batch_size) for x, y in get_batches(data, batch_size, seq_length): counter += 1 x = one_hot_encode(x, n_chars) inputs, targets = torch.from_numpy(x), torch.from_numpy(y) if train_on_gpu: inputs, targets = inputs.cuda(), targets.cuda() h = tuple([each.data for each in h]) net.zero_grad() output, h = net(inputs, h) loss = criterion(output, targets.view(batch_size * seq_length).long()) loss.backward() nn.utils.clip_grad_norm_(net.parameters(), clip) opt.step() if counter % print_every == 0: val_h = net.init_hidden(batch_size) val_losses = [] net.eval() for x, y in get_batches(val_data, batch_size, seq_length): x = one_hot_encode(x, n_chars) x, y = torch.from_numpy(x), torch.from_numpy(y) val_h = tuple([each.data for each in val_h]) inputs, targets = x, y if (train_on_gpu): inputs, targets = inputs.cuda(), targets.cuda() output, val_h = net(inputs, val_h) val_loss = criterion( output, targets.view(batch_size * seq_length).long()) val_losses.append(val_loss.item()) net.train() print(&quot;Epoch: {}/{}...&quot;.format(e + 1, epochs), &quot;Step: {}...&quot;.format(counter), &quot;Loss: {:.4f}...&quot;.format(loss.item()), &quot;Val Loss: {:.4f}&quot;.format(np.mean(val_losses))) 12345n_hidden = 512n_layers = 2net = CharRNN(chars, n_hidden, n_layers)print(net) CharRNN( (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5) (dropout): Dropout(p=0.5, inplace=False) (fc): Linear(in_features=512, out_features=83, bias=True) ) 1234567891011batch_size = 128seq_length = 100n_epochs = 20train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10) Epoch: 1/20... Step: 10... Loss: 3.2684... Val Loss: 3.2099 Epoch: 1/20... Step: 20... Loss: 3.1553... Val Loss: 3.1399 Epoch: 1/20... Step: 30... Loss: 3.1438... Val Loss: 3.1250 Epoch: 1/20... Step: 40... Loss: 3.1109... Val Loss: 3.1204 Epoch: 1/20... Step: 50... Loss: 3.1416... Val Loss: 3.1175 Epoch: 1/20... Step: 60... Loss: 3.1164... Val Loss: 3.1145 Epoch: 1/20... Step: 70... Loss: 3.1047... Val Loss: 3.1109 Epoch: 1/20... Step: 80... Loss: 3.1169... Val Loss: 3.1029 Epoch: 1/20... Step: 90... Loss: 3.1012... Val Loss: 3.0809 Epoch: 1/20... Step: 100... Loss: 3.0419... Val Loss: 3.0239 Epoch: 1/20... Step: 110... Loss: 2.9835... Val Loss: 2.9680 Epoch: 1/20... Step: 120... Loss: 2.8329... Val Loss: 2.8223 Epoch: 1/20... Step: 130... Loss: 2.8383... Val Loss: 2.8556 Epoch: 2/20... Step: 140... Loss: 2.7164... Val Loss: 2.6663 Epoch: 2/20... Step: 150... Loss: 2.6229... Val Loss: 2.5753 Epoch: 2/20... Step: 160... Loss: 2.5504... Val Loss: 2.5114 Epoch: 2/20... Step: 170... Loss: 2.4817... Val Loss: 2.4664 Epoch: 2/20... Step: 180... Loss: 2.4539... Val Loss: 2.4292 Epoch: 2/20... Step: 190... Loss: 2.4003... Val Loss: 2.3941 Epoch: 2/20... Step: 200... Loss: 2.4008... Val Loss: 2.3660 Epoch: 2/20... Step: 210... Loss: 2.3633... Val Loss: 2.3358 Epoch: 2/20... Step: 220... Loss: 2.3302... Val Loss: 2.3069 Epoch: 2/20... Step: 230... Loss: 2.3156... Val Loss: 2.2811 Epoch: 2/20... Step: 240... Loss: 2.2904... Val Loss: 2.2567 Epoch: 2/20... Step: 250... Loss: 2.2315... Val Loss: 2.2277 Epoch: 2/20... Step: 260... Loss: 2.1987... Val Loss: 2.1988 Epoch: 2/20... Step: 270... Loss: 2.2062... Val Loss: 2.1753 Epoch: 3/20... Step: 280... Loss: 2.2010... Val Loss: 2.1486 Epoch: 3/20... Step: 290... Loss: 2.1719... Val Loss: 2.1246 Epoch: 3/20... Step: 300... Loss: 2.1350... Val Loss: 2.1097 Epoch: 3/20... Step: 310... Loss: 2.1090... Val Loss: 2.0875 Epoch: 3/20... Step: 320... Loss: 2.0769... Val Loss: 2.0644 Epoch: 3/20... Step: 330... Loss: 2.0504... Val Loss: 2.0463 Epoch: 3/20... Step: 340... Loss: 2.0679... Val Loss: 2.0248 Epoch: 3/20... Step: 350... Loss: 2.0545... Val Loss: 2.0122 Epoch: 3/20... Step: 360... Loss: 1.9831... Val Loss: 1.9931 Epoch: 3/20... Step: 370... Loss: 2.0144... Val Loss: 1.9765 Epoch: 3/20... Step: 380... Loss: 1.9839... Val Loss: 1.9569 Epoch: 3/20... Step: 390... Loss: 1.9620... Val Loss: 1.9429 Epoch: 3/20... Step: 400... Loss: 1.9336... Val Loss: 1.9274 Epoch: 3/20... Step: 410... Loss: 1.9439... Val Loss: 1.9143 Epoch: 4/20... Step: 420... Loss: 1.9417... Val Loss: 1.8957 Epoch: 4/20... Step: 430... Loss: 1.9184... Val Loss: 1.8853 Epoch: 4/20... Step: 440... Loss: 1.9016... Val Loss: 1.8775 Epoch: 4/20... Step: 450... Loss: 1.8396... Val Loss: 1.8572 Epoch: 4/20... Step: 460... Loss: 1.8320... Val Loss: 1.8479 Epoch: 4/20... Step: 470... Loss: 1.8746... Val Loss: 1.8396 Epoch: 4/20... Step: 480... Loss: 1.8527... Val Loss: 1.8245 Epoch: 4/20... Step: 490... Loss: 1.8512... Val Loss: 1.8171 Epoch: 4/20... Step: 500... Loss: 1.8388... Val Loss: 1.8028 Epoch: 4/20... Step: 510... Loss: 1.8212... Val Loss: 1.7942 Epoch: 4/20... Step: 520... Loss: 1.8299... Val Loss: 1.7832 Epoch: 4/20... Step: 530... Loss: 1.7957... Val Loss: 1.7752 Epoch: 4/20... Step: 540... Loss: 1.7593... Val Loss: 1.7640 Epoch: 4/20... Step: 550... Loss: 1.8066... Val Loss: 1.7529 Epoch: 5/20... Step: 560... Loss: 1.7772... Val Loss: 1.7441 Epoch: 5/20... Step: 570... Loss: 1.7592... Val Loss: 1.7396 Epoch: 5/20... Step: 580... Loss: 1.7381... Val Loss: 1.7289 Epoch: 5/20... Step: 590... Loss: 1.7341... Val Loss: 1.7203 Epoch: 5/20... Step: 600... Loss: 1.7233... Val Loss: 1.7146 Epoch: 5/20... Step: 610... Loss: 1.7124... Val Loss: 1.7046 Epoch: 5/20... Step: 620... Loss: 1.7138... Val Loss: 1.7029 Epoch: 5/20... Step: 630... Loss: 1.7224... Val Loss: 1.6903 Epoch: 5/20... Step: 640... Loss: 1.6983... Val Loss: 1.6863 Epoch: 5/20... Step: 650... Loss: 1.6905... Val Loss: 1.6752 Epoch: 5/20... Step: 660... Loss: 1.6594... Val Loss: 1.6704 Epoch: 5/20... Step: 670... Loss: 1.6819... Val Loss: 1.6640 Epoch: 5/20... Step: 680... Loss: 1.6872... Val Loss: 1.6568 Epoch: 5/20... Step: 690... Loss: 1.6595... Val Loss: 1.6552 Epoch: 6/20... Step: 700... Loss: 1.6551... Val Loss: 1.6462 Epoch: 6/20... Step: 710... Loss: 1.6496... Val Loss: 1.6408 Epoch: 6/20... Step: 720... Loss: 1.6318... Val Loss: 1.6312 Epoch: 6/20... Step: 730... Loss: 1.6589... Val Loss: 1.6292 Epoch: 6/20... Step: 740... Loss: 1.6186... Val Loss: 1.6267 Epoch: 6/20... Step: 750... Loss: 1.6037... Val Loss: 1.6149 Epoch: 6/20... Step: 760... Loss: 1.6439... Val Loss: 1.6133 Epoch: 6/20... Step: 770... Loss: 1.6214... Val Loss: 1.6056 Epoch: 6/20... Step: 780... Loss: 1.6137... Val Loss: 1.6016 Epoch: 6/20... Step: 790... Loss: 1.5932... Val Loss: 1.5931 Epoch: 6/20... Step: 800... Loss: 1.6040... Val Loss: 1.5969 Epoch: 6/20... Step: 810... Loss: 1.5964... Val Loss: 1.5870 Epoch: 6/20... Step: 820... Loss: 1.5575... Val Loss: 1.5828 Epoch: 6/20... Step: 830... Loss: 1.6043... Val Loss: 1.5756 Epoch: 7/20... Step: 840... Loss: 1.5554... Val Loss: 1.5712 Epoch: 7/20... Step: 850... Loss: 1.5727... Val Loss: 1.5696 Epoch: 7/20... Step: 860... Loss: 1.5676... Val Loss: 1.5616 Epoch: 7/20... Step: 870... Loss: 1.5734... Val Loss: 1.5599 Epoch: 7/20... Step: 880... Loss: 1.5708... Val Loss: 1.5537 Epoch: 7/20... Step: 890... Loss: 1.5672... Val Loss: 1.5536 Epoch: 7/20... Step: 900... Loss: 1.5418... Val Loss: 1.5477 Epoch: 7/20... Step: 910... Loss: 1.5274... Val Loss: 1.5427 Epoch: 7/20... Step: 920... Loss: 1.5348... Val Loss: 1.5420 Epoch: 7/20... Step: 930... Loss: 1.5371... Val Loss: 1.5350 Epoch: 7/20... Step: 940... Loss: 1.5318... Val Loss: 1.5341 Epoch: 7/20... Step: 950... Loss: 1.5469... Val Loss: 1.5285 Epoch: 7/20... Step: 960... Loss: 1.5396... Val Loss: 1.5269 Epoch: 7/20... Step: 970... Loss: 1.5491... Val Loss: 1.5230 Epoch: 8/20... Step: 980... Loss: 1.5196... Val Loss: 1.5151 Epoch: 8/20... Step: 990... Loss: 1.5156... Val Loss: 1.5145 Epoch: 8/20... Step: 1000... Loss: 1.5137... Val Loss: 1.5077 Epoch: 8/20... Step: 1010... Loss: 1.5480... Val Loss: 1.5094 Epoch: 8/20... Step: 1020... Loss: 1.5163... Val Loss: 1.5056 Epoch: 8/20... Step: 1030... Loss: 1.4934... Val Loss: 1.5022 Epoch: 8/20... Step: 1040... Loss: 1.5129... Val Loss: 1.5031 Epoch: 8/20... Step: 1050... Loss: 1.4812... Val Loss: 1.4964 Epoch: 8/20... Step: 1060... Loss: 1.4982... Val Loss: 1.4925 Epoch: 8/20... Step: 1070... Loss: 1.4994... Val Loss: 1.4875 Epoch: 8/20... Step: 1080... Loss: 1.4974... Val Loss: 1.4881 Epoch: 8/20... Step: 1090... Loss: 1.4706... Val Loss: 1.4843 Epoch: 8/20... Step: 1100... Loss: 1.4743... Val Loss: 1.4796 Epoch: 8/20... Step: 1110... Loss: 1.4622... Val Loss: 1.4781 Epoch: 9/20... Step: 1120... Loss: 1.4796... Val Loss: 1.4796 Epoch: 9/20... Step: 1130... Loss: 1.4893... Val Loss: 1.4764 Epoch: 9/20... Step: 1140... Loss: 1.4759... Val Loss: 1.4679 Epoch: 9/20... Step: 1150... Loss: 1.4925... Val Loss: 1.4718 Epoch: 9/20... Step: 1160... Loss: 1.4514... Val Loss: 1.4657 Epoch: 9/20... Step: 1170... Loss: 1.4605... Val Loss: 1.4629 Epoch: 9/20... Step: 1180... Loss: 1.4495... Val Loss: 1.4687 Epoch: 9/20... Step: 1190... Loss: 1.4862... Val Loss: 1.4622 Epoch: 9/20... Step: 1200... Loss: 1.4313... Val Loss: 1.4548 Epoch: 9/20... Step: 1210... Loss: 1.4401... Val Loss: 1.4512 Epoch: 9/20... Step: 1220... Loss: 1.4504... Val Loss: 1.4553 Epoch: 9/20... Step: 1230... Loss: 1.4249... Val Loss: 1.4508 Epoch: 9/20... Step: 1240... Loss: 1.4332... Val Loss: 1.4452 Epoch: 9/20... Step: 1250... Loss: 1.4445... Val Loss: 1.4436 Epoch: 10/20... Step: 1260... Loss: 1.4480... Val Loss: 1.4434 Epoch: 10/20... Step: 1270... Loss: 1.4383... Val Loss: 1.4392 Epoch: 10/20... Step: 1280... Loss: 1.4405... Val Loss: 1.4332 Epoch: 10/20... Step: 1290... Loss: 1.4421... Val Loss: 1.4338 Epoch: 10/20... Step: 1300... Loss: 1.4320... Val Loss: 1.4349 Epoch: 10/20... Step: 1310... Loss: 1.4420... Val Loss: 1.4338 Epoch: 10/20... Step: 1320... Loss: 1.4085... Val Loss: 1.4336 Epoch: 10/20... Step: 1330... Loss: 1.4142... Val Loss: 1.4289 Epoch: 10/20... Step: 1340... Loss: 1.3981... Val Loss: 1.4257 Epoch: 10/20... Step: 1350... Loss: 1.3931... Val Loss: 1.4213 Epoch: 10/20... Step: 1360... Loss: 1.3841... Val Loss: 1.4259 Epoch: 10/20... Step: 1370... Loss: 1.3755... Val Loss: 1.4218 Epoch: 10/20... Step: 1380... Loss: 1.4250... Val Loss: 1.4148 Epoch: 10/20... Step: 1390... Loss: 1.4314... Val Loss: 1.4170 Epoch: 11/20... Step: 1400... Loss: 1.4284... Val Loss: 1.4185 Epoch: 11/20... Step: 1410... Loss: 1.4427... Val Loss: 1.4147 Epoch: 11/20... Step: 1420... Loss: 1.4309... Val Loss: 1.4073 Epoch: 11/20... Step: 1430... Loss: 1.3992... Val Loss: 1.4105 Epoch: 11/20... Step: 1440... Loss: 1.4362... Val Loss: 1.4060 Epoch: 11/20... Step: 1450... Loss: 1.3561... Val Loss: 1.4054 Epoch: 11/20... Step: 1460... Loss: 1.3756... Val Loss: 1.4073 Epoch: 11/20... Step: 1470... Loss: 1.3766... Val Loss: 1.4047 Epoch: 11/20... Step: 1480... Loss: 1.3972... Val Loss: 1.3992 Epoch: 11/20... Step: 1490... Loss: 1.3747... Val Loss: 1.3967 Epoch: 11/20... Step: 1500... Loss: 1.3645... Val Loss: 1.3986 Epoch: 11/20... Step: 1510... Loss: 1.3538... Val Loss: 1.3992 Epoch: 11/20... Step: 1520... Loss: 1.3887... Val Loss: 1.3923 Epoch: 12/20... Step: 1530... Loss: 1.4452... Val Loss: 1.3915 Epoch: 12/20... Step: 1540... Loss: 1.3953... Val Loss: 1.3887 Epoch: 12/20... Step: 1550... Loss: 1.3962... Val Loss: 1.3859 Epoch: 12/20... Step: 1560... Loss: 1.4023... Val Loss: 1.3845 Epoch: 12/20... Step: 1570... Loss: 1.3618... Val Loss: 1.3864 Epoch: 12/20... Step: 1580... Loss: 1.3288... Val Loss: 1.3844 Epoch: 12/20... Step: 1590... Loss: 1.3300... Val Loss: 1.3844 Epoch: 12/20... Step: 1600... Loss: 1.3490... Val Loss: 1.3835 Epoch: 12/20... Step: 1610... Loss: 1.3478... Val Loss: 1.3864 Epoch: 12/20... Step: 1620... Loss: 1.3535... Val Loss: 1.3791 Epoch: 12/20... Step: 1630... Loss: 1.3670... Val Loss: 1.3753 Epoch: 12/20... Step: 1640... Loss: 1.3440... Val Loss: 1.3791 Epoch: 12/20... Step: 1650... Loss: 1.3304... Val Loss: 1.3763 Epoch: 12/20... Step: 1660... Loss: 1.3709... Val Loss: 1.3695 Epoch: 13/20... Step: 1670... Loss: 1.3404... Val Loss: 1.3753 Epoch: 13/20... Step: 1680... Loss: 1.3551... Val Loss: 1.3698 Epoch: 13/20... Step: 1690... Loss: 1.3301... Val Loss: 1.3665 Epoch: 13/20... Step: 1700... Loss: 1.3306... Val Loss: 1.3616 Epoch: 13/20... Step: 1710... Loss: 1.3086... Val Loss: 1.3666 Epoch: 13/20... Step: 1720... Loss: 1.3270... Val Loss: 1.3703 Epoch: 13/20... Step: 1730... Loss: 1.3601... Val Loss: 1.3625 Epoch: 13/20... Step: 1740... Loss: 1.3294... Val Loss: 1.3621 Epoch: 13/20... Step: 1750... Loss: 1.2962... Val Loss: 1.3605 Epoch: 13/20... Step: 1760... Loss: 1.3291... Val Loss: 1.3585 Epoch: 13/20... Step: 1770... Loss: 1.3347... Val Loss: 1.3585 Epoch: 13/20... Step: 1780... Loss: 1.3094... Val Loss: 1.3519 Epoch: 13/20... Step: 1790... Loss: 1.3037... Val Loss: 1.3556 Epoch: 13/20... Step: 1800... Loss: 1.3259... Val Loss: 1.3502 Epoch: 14/20... Step: 1810... Loss: 1.3300... Val Loss: 1.3498 Epoch: 14/20... Step: 1820... Loss: 1.3115... Val Loss: 1.3474 Epoch: 14/20... Step: 1830... Loss: 1.3319... Val Loss: 1.3431 Epoch: 14/20... Step: 1840... Loss: 1.2688... Val Loss: 1.3421 Epoch: 14/20... Step: 1850... Loss: 1.2637... Val Loss: 1.3406 Epoch: 14/20... Step: 1860... Loss: 1.3170... Val Loss: 1.3437 Epoch: 14/20... Step: 1870... Loss: 1.3217... Val Loss: 1.3407 Epoch: 14/20... Step: 1880... Loss: 1.3195... Val Loss: 1.3401 Epoch: 14/20... Step: 1890... Loss: 1.3369... Val Loss: 1.3401 Epoch: 14/20... Step: 1900... Loss: 1.2999... Val Loss: 1.3382 Epoch: 14/20... Step: 1910... Loss: 1.3153... Val Loss: 1.3349 Epoch: 14/20... Step: 1920... Loss: 1.3041... Val Loss: 1.3391 Epoch: 14/20... Step: 1930... Loss: 1.2676... Val Loss: 1.3367 Epoch: 14/20... Step: 1940... Loss: 1.3331... Val Loss: 1.3396 Epoch: 15/20... Step: 1950... Loss: 1.3017... Val Loss: 1.3355 Epoch: 15/20... Step: 1960... Loss: 1.3021... Val Loss: 1.3367 Epoch: 15/20... Step: 1970... Loss: 1.2851... Val Loss: 1.3269 Epoch: 15/20... Step: 1980... Loss: 1.2824... Val Loss: 1.3319 Epoch: 15/20... Step: 1990... Loss: 1.2737... Val Loss: 1.3260 Epoch: 15/20... Step: 2000... Loss: 1.2606... Val Loss: 1.3242 Epoch: 15/20... Step: 2010... Loss: 1.2767... Val Loss: 1.3311 Epoch: 15/20... Step: 2020... Loss: 1.3080... Val Loss: 1.3249 Epoch: 15/20... Step: 2030... Loss: 1.2721... Val Loss: 1.3248 Epoch: 15/20... Step: 2040... Loss: 1.2891... Val Loss: 1.3206 Epoch: 15/20... Step: 2050... Loss: 1.2806... Val Loss: 1.3202 Epoch: 15/20... Step: 2060... Loss: 1.2827... Val Loss: 1.3219 Epoch: 15/20... Step: 2070... Loss: 1.2918... Val Loss: 1.3219 Epoch: 15/20... Step: 2080... Loss: 1.2858... Val Loss: 1.3213 Epoch: 16/20... Step: 2090... Loss: 1.2936... Val Loss: 1.3203 Epoch: 16/20... Step: 2100... Loss: 1.2737... Val Loss: 1.3163 Epoch: 16/20... Step: 2110... Loss: 1.2669... Val Loss: 1.3130 Epoch: 16/20... Step: 2120... Loss: 1.2843... Val Loss: 1.3172 Epoch: 16/20... Step: 2130... Loss: 1.2545... Val Loss: 1.3134 Epoch: 16/20... Step: 2140... Loss: 1.2673... Val Loss: 1.3119 Epoch: 16/20... Step: 2150... Loss: 1.2944... Val Loss: 1.3089 Epoch: 16/20... Step: 2160... Loss: 1.2658... Val Loss: 1.3128 Epoch: 16/20... Step: 2170... Loss: 1.2693... Val Loss: 1.3130 Epoch: 16/20... Step: 2180... Loss: 1.2581... Val Loss: 1.3123 Epoch: 16/20... Step: 2190... Loss: 1.2856... Val Loss: 1.3108 Epoch: 16/20... Step: 2200... Loss: 1.2443... Val Loss: 1.3063 Epoch: 16/20... Step: 2210... Loss: 1.2143... Val Loss: 1.3135 Epoch: 16/20... Step: 2220... Loss: 1.2763... Val Loss: 1.3081 Epoch: 17/20... Step: 2230... Loss: 1.2509... Val Loss: 1.3117 Epoch: 17/20... Step: 2240... Loss: 1.2526... Val Loss: 1.3090 Epoch: 17/20... Step: 2250... Loss: 1.2455... Val Loss: 1.3028 Epoch: 17/20... Step: 2260... Loss: 1.2519... Val Loss: 1.3079 Epoch: 17/20... Step: 2270... Loss: 1.2622... Val Loss: 1.3007 Epoch: 17/20... Step: 2280... Loss: 1.2646... Val Loss: 1.2985 Epoch: 17/20... Step: 2290... Loss: 1.2591... Val Loss: 1.3006 Epoch: 17/20... Step: 2300... Loss: 1.2187... Val Loss: 1.3061 Epoch: 17/20... Step: 2310... Loss: 1.2488... Val Loss: 1.3003 Epoch: 17/20... Step: 2320... Loss: 1.2377... Val Loss: 1.3023 Epoch: 17/20... Step: 2330... Loss: 1.2464... Val Loss: 1.3072 Epoch: 17/20... Step: 2340... Loss: 1.2496... Val Loss: 1.2971 Epoch: 17/20... Step: 2350... Loss: 1.2634... Val Loss: 1.2999 Epoch: 17/20... Step: 2360... Loss: 1.2586... Val Loss: 1.2965 Epoch: 18/20... Step: 2370... Loss: 1.2422... Val Loss: 1.2945 Epoch: 18/20... Step: 2380... Loss: 1.2481... Val Loss: 1.2942 Epoch: 18/20... Step: 2390... Loss: 1.2385... Val Loss: 1.3051 Epoch: 18/20... Step: 2400... Loss: 1.2693... Val Loss: 1.2974 Epoch: 18/20... Step: 2410... Loss: 1.2627... Val Loss: 1.2962 Epoch: 18/20... Step: 2420... Loss: 1.2337... Val Loss: 1.2883 Epoch: 18/20... Step: 2430... Loss: 1.2447... Val Loss: 1.2922 Epoch: 18/20... Step: 2440... Loss: 1.2341... Val Loss: 1.2936 Epoch: 18/20... Step: 2450... Loss: 1.2323... Val Loss: 1.2899 Epoch: 18/20... Step: 2460... Loss: 1.2457... Val Loss: 1.2914 Epoch: 18/20... Step: 2470... Loss: 1.2332... Val Loss: 1.2993 Epoch: 18/20... Step: 2480... Loss: 1.2231... Val Loss: 1.2909 Epoch: 18/20... Step: 2490... Loss: 1.2181... Val Loss: 1.2913 Epoch: 18/20... Step: 2500... Loss: 1.2346... Val Loss: 1.2918 Epoch: 19/20... Step: 2510... Loss: 1.2335... Val Loss: 1.2922 Epoch: 19/20... Step: 2520... Loss: 1.2471... Val Loss: 1.2908 Epoch: 19/20... Step: 2530... Loss: 1.2490... Val Loss: 1.2854 Epoch: 19/20... Step: 2540... Loss: 1.2635... Val Loss: 1.2854 Epoch: 19/20... Step: 2550... Loss: 1.2124... Val Loss: 1.2876 Epoch: 19/20... Step: 2560... Loss: 1.2234... Val Loss: 1.2851 Epoch: 19/20... Step: 2570... Loss: 1.2193... Val Loss: 1.2875 Epoch: 19/20... Step: 2580... Loss: 1.2569... Val Loss: 1.2992 Epoch: 19/20... Step: 2590... Loss: 1.2133... Val Loss: 1.2876 Epoch: 19/20... Step: 2600... Loss: 1.2198... Val Loss: 1.2921 Epoch: 19/20... Step: 2610... Loss: 1.2273... Val Loss: 1.2813 Epoch: 19/20... Step: 2620... Loss: 1.2029... Val Loss: 1.2822 Epoch: 19/20... Step: 2630... Loss: 1.2044... Val Loss: 1.2786 Epoch: 19/20... Step: 2640... Loss: 1.2234... Val Loss: 1.2819 Epoch: 20/20... Step: 2650... Loss: 1.2344... Val Loss: 1.2802 Epoch: 20/20... Step: 2660... Loss: 1.2336... Val Loss: 1.2819 Epoch: 20/20... Step: 2670... Loss: 1.2329... Val Loss: 1.2765 Epoch: 20/20... Step: 2680... Loss: 1.2183... Val Loss: 1.2775 Epoch: 20/20... Step: 2690... Loss: 1.2269... Val Loss: 1.2758 Epoch: 20/20... Step: 2700... Loss: 1.2301... Val Loss: 1.2785 Epoch: 20/20... Step: 2710... Loss: 1.1988... Val Loss: 1.2817 Epoch: 20/20... Step: 2720... Loss: 1.2052... Val Loss: 1.2792 Epoch: 20/20... Step: 2730... Loss: 1.1917... Val Loss: 1.2770 Epoch: 20/20... Step: 2740... Loss: 1.1990... Val Loss: 1.2773 Epoch: 20/20... Step: 2750... Loss: 1.1967... Val Loss: 1.2780 Epoch: 20/20... Step: 2760... Loss: 1.1961... Val Loss: 1.2748 Epoch: 20/20... Step: 2770... Loss: 1.2353... Val Loss: 1.2739 Epoch: 20/20... Step: 2780... Loss: 1.2497... Val Loss: 1.2741 1 获取最优模型 过拟合与欠拟合 实时监控训练和验证损失，如果训练损失远远低于验证损失，则模型过拟合；添加正则化、dropout，或使用更小的模型； 如果训练和验证损失相近，则过拟合，可以增加模型的尺寸 超参数 模型定义时：隐藏层神经元数量n_hidden，LSTM的层数n_layers n_layers建议设置值2或3，模型的总参数量与训练数据量处于同样的量级；如100MB的数据，当模型150K参数，模型会严重欠拟合，而10MB数量模型10M参数，模型会欠拟合，增大dropout参数 总是训练较大的模型，然后尝试不同的dropout 模型训练时：batch_size,seq_length,lr,及数据拆分为训练集及验证集的拆分比列 尝试不同的超参数组合，选择性能最佳模型 1 保存模型 123456789model_name = 'rnn_20_epoch.net'checkpoint = {'n_hidden': net.n_hidden, 'n_layers': net.n_layers, 'state_dict': net.state_dict(), 'tokens': net.chars}with open(model_name, 'wb') as f: torch.save(checkpoint, f) 1 文本生成 1234567891011121314151617181920212223242526272829303132def predict(net, char, h=None, top_k=None): # tensor inputs x = np.array([[net.char2int[char]]]) x = one_hot_encode(x, len(net.chars)) inputs = torch.from_numpy(x) if(train_on_gpu): inputs = inputs.cuda() # detach hidden state from history h = tuple([each.data for each in h]) # get the output of the model out, h = net(inputs, h) # get the character probabilities p = F.softmax(out, dim=1).data if(train_on_gpu): p = p.cpu() # move to cpu # topK采样 if top_k is None: top_ch = np.arange(len(net.chars)) else: p, top_ch = p.topk(top_k) top_ch = top_ch.numpy().squeeze() # select the likely next character with some element of randomness p = p.numpy().squeeze() char = np.random.choice(top_ch, p=p/p.sum()) # return the encoded value of the predicted char and the hidden state return net.int2char[char], h 1234567891011121314151617181920212223242526# 文本生成def sample(net, size, prime='The', top_k=None): if (train_on_gpu): net.cuda() else: net.cpu() net.eval() # eval mode # First off, run through the prime characters chars = [ch for ch in prime] h = net.init_hidden(1) for ch in prime: char, h = predict(net, ch, h, top_k=top_k) chars.append(char) # Now pass in the previous character and get a new one for ii in range(size): char, h = predict(net, chars[-1], h, top_k=top_k) chars.append(char) return ''.join(chars)print(sample(net, 1000, prime='Anna', top_k=5)) Anna with a smile to holding a person was, and a line white sheel, who did not know. His father was a long while, and her friend, and the secundes, time and some of or that some made a dress so as happy, and to see it. To her that he was not finished, he went into the same time, the princess had always taken up to the corridor, was in which the prevent position he was an expression to the table, and she could do to triem to herself what they seemed to the cletched of the coup of the sick man are a sinting state of charming head, was not to such her for his brother's woman that when they were since he was sitting to the soft might have been seening that her family and with the proviss, which she had been set off the same thing, who had been disagreeable and had sore of the propersy of always. And he set her. He spoke tran out of the stranger and her husband who had no supported that he had not heard the face of her starts, began to say, the pissons were far in shame, and her heart, their sho 1 1","link":"/posts/28867.html"},{"title":"错误、调试与测试","text":"简述 在程序运行过程中我们总会遇到各种各样的错误。有的错误是程序编写有问题造成的，比如本来应该输出整数结果输出了字符串，这种错误我们通常称之为bug，bug是必须修复的；有的错误是用户输入造成的，比如让用户输入email地址，结果得到一个空字符串，这种错误可以通过检查用户输入来做相应的处理；还有一类错误是完全无法预测的，比如写入文件的时候，磁盘满了，写不进去了，或者从网络抓取数据，网络突然断掉了。这类错误也称为异常，在程序中通常是必须处理的，否则，程序会因为各种问题终止并退出。Python内置了一套异常处理机制，可以帮助我们处理这些错误。 此外，在编写代码时，我们可能会需要跟踪程序的执行，查看变量的值是否正确，然后再进行调整或者下一步操作，这个过程称为调试。Python的pdb可以让我们以单步方式执行代码，从而方便地调试程序。 最后，编写测试也很重要。编写好测试文件，这样当我们改动了代码或者实现了新的功能时，只需再运行一遍测试，就能知道原来的功能有没有出错，程序是否依然能输出我们期望的结果了。 错误处理 错误码 在程序运行的过程中，如果发生了错误，可以返回一个事先约定的错误代码，这样，就可以知道是否有错，以及出错的原因。在操作系统提供的调用中，返回错误码非常常见。比如打开文件的函数 open()，成功时返回文件描述符（就是一个整数），出错时返回-1。 用错误码来表示是否出错十分不便，因为函数本身既可能返回正常结果又可能返回错误码，所以调用者不得不用大量的代码来判断属于哪一种情况。例如： 12345678910111213def foo(): r = some_function() if r==(-1): return (-1) # do something return rdef bar(): r = foo() if r==(-1): print('Error') else: pass 函数 foo 既可能返回正常结果又可能返回错误码，因此调用 foo 的函数 bar 就不得不先进行判断，检查返回的是正常结果还是错误码。这种情况在有多种错误码时显得更为麻烦。 还有一个很大的缺点是，使用错误码时，一旦出错，就必须把这个错误码一级一级上报，直到某个函数可以处理该错误（比如，给用户输出一个错误信息）。假如上面例子中 bar 函数无法处理错误，就必须继续返回错误码给调用 bar 的上级函数，以此类推。并且在返回的过程中，我们在每个中间函数中都要对错误码进行判断，这样写出来的程序有“半壁江山”都被处理错误的逻辑占据了，着实可怕。。。 有没有可以替代错误码又能处理错误的方案呢？有的！基本上，所有高级语言都内置了一套 try...except...finally... 的错误处理机制，Python也不例外，在下一小节中将介绍这种错误处理机制。 try...except...finally try...except...finally... 机制的工作方式是这样的： 当我们认为某段代码可能会出错时，可以用 try 来运行这段代码，如果运行出错，则这段代码会终止在错误出现的地方； 如果后续代码中 except 语句成功捕获到错误，程序就会执行 except 语句块内的代码处理错误。如果没有捕获到，则错误没有得到处理，程序就会停止运行； 最后，无论是否出错，无论是否成功捕获到错误，finally 语句块内的代码都会被执行。 try...except...finally... 机制中，我们可以不使用 finally 语句块，但 try 和 except 是一定要同时出现的，except 不一定能成功捕获 try 语句块内的错误，如果捕获不成功，程序就会终止运行。 接下来看一个使用 try...except...finally... 机制处理错误的具体案例： 123456789try: print('try...') r = 10 / 0 print('result:', r)except ZeroDivisionError as e: print('except:', e)finally: print('finally...')print('END') 上面的代码在计算 10 / 0 时会产生一个除零错误，得到输出： 1234try...except: division by zerofinally...END 从输出可以看到，当错误发生时，后续语句 print('result:', r) 不会被执行，由于 except 语句捕获到这个 ZeroDivisionError 错误，因此 except 语句块里的代码会被执行。最后，finally 语句块里的代码也会被执行。又因为错误得到了处理，所以之后程序会继续运行后续代码，输出 END。 如果把除数0改成2，则执行结果如下： 1234try...result: 5finally...END 由于没有错误发生，所以 except 语句块不会被执行，但是 finally 语句块只要存在，就一定会被执行。 除了上面出现的 ZeroDivisionError 错误，在实际运行中，还有可能出现各种不同类型的错误。不同类型的错误应该由不同的 except 语句块进行处理。我们可以使用多个 except 语句来捕获不同类型的错误： 1234567891011try: print('try...') r = 10 / int('a') print('result:', r)except ValueError as e: print('ValueError:', e)except ZeroDivisionError as e: print('ZeroDivisionError:', e)finally: print('finally...')print('END') 因为当 int() 函数无法把参数转换为 int 类型时会抛出 ValueError 错误，我们用一个 except 来捕获和处理 ValueError，用另一个 except 来捕获并处理做除法可能产生的 ZeroDivisionError。 特别地，我们还可以在 except 语句块后面加一个 else 语句块。当错误没有发生时，就会执行 else 语句内的代码： 12345678910111213try: print('try...') r = 10 / int('2') print('result:', r)except ValueError as e: print('ValueError:', e)except ZeroDivisionError as e: print('ZeroDivisionError:', e)else: print('no error!')finally: print('finally...')print('END') 我们常说，在Python中一切皆对象。其实呀，Python中的错误也是采用面向对象实现的，每一种错误都是一个类，BaseException 类是所有错误类型最顶级的父类。在使用 except 时需要注意，它不但会捕获所指定类型的错误，还把属于该类型子类的错误一并捕获。比如： 123456try: foo()except ValueError as e: print('ValueError')except UnicodeError as e: print('UnicodeError') 这里的第二个 except 永远也不会捕获到 UnicodeError，因为 UnicodeError 是 ValueError 的子类，如果出现了 UnicodeError 就一定会被第一个 except 语句捕获。 常见的错误类型和继承关系看这里： https://docs.python.org/3/library/exceptions.html#exception-hierarchy 在上一小节中，我们说到了使用错误码处理错误有两大缺点，一是函数既可能返回正常结果也可能返回错误码，二是一旦发生错误必须层层上报。那么使用 try...except...finally... 机制是否能克服这两个缺点呢？答案是肯定的！举个例子： 12345678910111213def foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar('0') except Exception as e: print('Error:', e) finally: print('finally...') 这里我们在 main 函数中调用 bar 函数，在 bar 函数中调用 foo 函数。我们使用 try 模块来运行调用代码，当 foo 函数发生错误时，我们不需要返回错误码，也不需要一级级上报，程序会自动寻找对应的 except 语句进行错误处理。也即是说，不需要在每个可能出错的地方去捕获错误，只要在合适的层次去捕获错误就可以了。这样一来，我们就能使用非常简洁的方式来处理程序运行中可能出现的错误了。 错误的调用链 如果错误没有被捕获，就会一直往上抛，最后被Python解释器捕获，打印出错误信息，然后程序终止运行。 编写一个包含如下代码的 err.py 文件： 1234567891011# err.py:def foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): bar('0')main() 执行该文件，结果如下： 1234567891011$ python3 err.pyTraceback (most recent call last): File &quot;err.py&quot;, line 11, in &lt;module&gt; main() File &quot;err.py&quot;, line 9, in main bar('0') File &quot;err.py&quot;, line 6, in bar return foo(s) * 2 File &quot;err.py&quot;, line 3, in foo return 10 / int(s)ZeroDivisionError: division by zero 出错并不可怕，可怕的是不知道哪里出错了。解读错误信息是定位错误的关键。我们从上往下可以看到整个错误的函数调用链： 错误信息的第1行： 1Traceback (most recent call last): 这句话告诉我们下面是错误的跟踪信息。 错误信息的第2~3行： 12File &quot;err.py&quot;, line 11, in &lt;module&gt; main() 告诉我们调用 main() 出错了，具体是在代码文件 err.py 的第11行代码。 错误信息的第4~5行： 12File &quot;err.py&quot;, line 9, in main bar('0') 告诉我们调用 bar('0') 出错了，具体是在代码文件 err.py 的第9行代码。 错误信息的第6~7行： 12File &quot;err.py&quot;, line 6, in bar return foo(s) * 2 告诉我们调用 foo(s) 出错了，具体是在代码文件 err.py 的第6行代码。 错误信息的第8~9行： 12File &quot;err.py&quot;, line 3, in foo return 10 / int(s) 告诉我们语句 return 10 / int(s) 出错了，具体是在代码文件 err.py 的第3行代码。这是错误的源头，因为下面打印了具体的错误原因： 1ZeroDivisionError: integer division or modulo by zero 根据错误类型 ZeroDivisionError，我们可以判断 int(s) 本身并没有出错，但是 int(s)返回了0，在计算 10 / 0 时程序出错了。这和我们使用 except 来捕获错误信息时打印出的内容是一样的。 记录错误 上一小节讲到，如果不在代码中进行错误处理，Python解释器最终会捕获错误并打印出错误调用链，但同时程序也会终止运行。那么，有没有既能打印出错误调用链，帮助我们分析出错的原因和源头，同时又能让程序继续运行的方法呢？有的，Python内置的 logging 模块可以帮助我们非常容易地记录错误信息。 这里举一个简单的例子，首先编写 err_logging.py 文件： 12345678910111213141516import loggingdef foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar('0') except Exception as e: logging.exception(e) # 使用logging模块的exception方法打印错误信息main()print('END') 同样是打印出错误调用链，但程序打印完错误信息后会继续运行，并正常结束： 1234567891011$ python3 err_logging.pyERROR:root:division by zeroTraceback (most recent call last): File &quot;err_logging.py&quot;, line 13, in main bar('0') File &quot;err_logging.py&quot;, line 9, in bar return foo(s) * 2 File &quot;err_logging.py&quot;, line 6, in foo return 10 / int(s)ZeroDivisionError: division by zeroEND 此外，我们还可以借助 logging 模块把错误信息记录到日志文件里，方便事后排查，这里不作举例了。 抛出错误 前面我们说到，在Python中错误都是通过类来实现的，捕获一个错误就是捕获到该类的一个实例。错误并不是凭空产生的，而是有意地创建并抛出的。Python的内置函数会抛出很多不同类型的错误，我们自己编写函数时也可以这样做。 举一个简单的例子，首先编写 err_raise.py 文件： 12345678910class FooError(ValueError): passdef foo(s): n = int(s) if n==0: raise FooError('invalid value: %s' % s) return 10 / nfoo('0') 这里我们自定义了一个错误类型 FooError，继承自 ValueError。使用 raise 语句抛出一个错误的实例。执行 err_raise.py，最终可以跟踪到我们自定义的错误类型： 1234567$ python3 err_raise.pyTraceback (most recent call last): File &quot;err_throw.py&quot;, line 11, in &lt;module&gt; foo('0') File &quot;err_throw.py&quot;, line 8, in foo raise FooError('invalid value: %s' % s)__main__.FooError: invalid value: 0 只有在必要的时候才自定义错误类型。如果可以使用Python内置的错误类型（比如ValueError，TypeError等等），就应尽量使用Python内置的错误类型。 最后，我们来看另一种错误处理的方式，首先编写 err_reraise.py 文件： 1234567891011121314def foo(s): n = int(s) if n==0: raise ValueError('invalid value: %s' % s) return 10 / ndef bar(): try: foo('0') except ValueError as e: print('ValueError!') raisebar() 在 bar() 函数中，我们明明已经捕获了错误，但是，打印一个 ValueError 之后，又把错误通过 raise 语句再次抛出去了，为什么呢？ 其实这种错误处理方式并没有错，而且相当常见。有时候，捕获错误的目的只是记录一下，便于后续追踪。如果当前函数没有处理该错误的逻辑，最恰当的方式就是继续往上抛，让顶层调用者去处理。好比一个员工处理不了一个问题时，就把问题抛给他的老板，如果他的老板也处理不了，就一直往上抛，最终抛给CEO去处理。 特别地，当 raise 语句不带参数时，会把当前错误原样抛出。但既然我们可以在 except 语句块中使用 raise 语句，那就可以轻易地抛出一个别的错误，从而把一种错误类型转换成另一种错误类型。例如： 1234try: 10 / 0except ZeroDivisionError: raise ValueError('input error!') 当然，我们不能滥用这样的功能，只有在有必要进行转换时才进行合理的转换。 小结 使用Python内置的 try...except...finally 机制可以十分方便地处理错误。但出错时，会分析错误信息并定位错误发生的位置才是最关键的。 我们编写模块时可以在代码中主动抛出错误，让调用者来处理相应的错误。但是，我们应当在模块的文档中写清楚可能会抛出哪些错误，以及错误产生的原因。 调试 程序运行总会有各种各样的bug，有的bug很简单，看看错误信息就知道；但有的bug很复杂，我们不但需要知道错误类型和出错的地方，还需要知道一些变量的值才能做出准确的推断。跟踪程序的执行，查看变量的值这个过程就称为调试，这一节会介绍各种调试程序的手段。 直接打印 直接打印是一种直接粗暴但十分有效的方法，简单来说就是使用 print() 把可能有问题的变量打印出来看看： 123456789def foo(s): n = int(s) print('n = %d' % n) return 10 / ndef main(): foo('0')main() 执行后在输出中查找打印的变量值： 12345$ python3 err.pyn = 0Traceback (most recent call last): ...ZeroDivisionError: integer division or modulo by zero 这样我们就知道除零错误是因为变量 n 的值不合理而产生的了。 但是使用直接打印来调试有一个很大的缺点，在完成调试后，我们还得删掉代码里用于输出变量值的 pinrt()，如果我们要观察很多变量的值，那么代码里就会到处都是 print()，运行结果也会包含很多垃圾信息，删除的时候就会很麻烦。 断言 我们可以用断言（assert）来替代 print()，例如： 1234567def foo(s): n = int(s) assert n != 0, 'n is zero!' return 10 / ndef main(): foo('0') 我们看到，使用 assert 的方法是在它后面接一个表达式以及一个字符串，如果表达式不为 True，则断言失败，此时会抛出 AssertionError 错误，并输出自定义的错误信息（跟在表达式后面的字符串）： 1234$ python3 err.pyTraceback (most recent call last): ...AssertionError: n is zero! 程序中如果到处充斥着 assert 语句，似乎和使用 print() 相比也没有什么不同。但是，我们可以在启动Python解释器时可以用 -O 参数来关闭 assert： 1234$ python3 -O err.pyTraceback (most recent call last): ...ZeroDivisionError: division by zero 关闭后，可以把所有的 assert 语句当成 pass 来看，此时断言就不会发挥作用了。 logging 我们还可以把 print() 替换为 logging。和 assert 相比，使用 logging 不会抛出错误，而且不但能打印信息还能方便地保存到日志中。这里简单举个例子，首先编写 err.py 文件： 123456import loggings = '0'n = int(s)logging.info('n = %d' % n)print(10 / n) logging.info() 可以输出一段文本。但运行上述代码，发现除了抛出 ZeroDivisionError 错误之外，没有任何信息。怎么回事呢？ 别急，在 import logging 之后添加一行配置再试试： 12import logginglogging.basicConfig(level=logging.INFO) 再次运行，此时能看到输出了： 123456$ python3 err.pyINFO:root:n = 0Traceback (most recent call last): File &quot;err.py&quot;, line 8, in &lt;module&gt; print(10 / n)ZeroDivisionError: division by zero 这就是使用 logging 的好处了，它允许开发者指定记录信息的级别，按程度由低到高有 debug, info, waring, error 几个级别。当我们指定 level=INFO 时，logging.debug 就不起作用了。同理，指定 level=WARNING 后，debug 和 info 就不起作用了。这样一来，我们可以很方便地统一控制输出哪个级别的信息，而不用担心调试完还要删除的问题了。 使用 logging 还有另一个好处就是可以通过很简单的配置，把一条语句同时输出到不同的地方，比如命令行和文件。 pdb 前面几种方式都需要插入额外的代码，有没有不需要插入代码的调试方式呢？有的！我们可以启动Python自带的pdb调试器，让程序以单步方式运行，可以随时查看运行到某一步时各个变量的值。首先编写好 err.py 文件： 1234# err.pys = '0'n = int(s)print(10 / n) 以参数 -m pdb 来启动pdb调试环境： 123C:\\Users\\Administrator\\Desktop&gt;python -m pdb err.py&gt; c:\\users\\administrator\\desktop\\err.py(2)&lt;module&gt;()-&gt; s = '0' 此时输出有两行，第一行表示下一步执行的代码属于哪一个代码文件的哪一行（这里是 err.py 的第2行）；第二行则是下一步执行的代码。我们可以输入命令 l 来查看这行代码的上下文： 123456(Pdb) l 1 # err.py 2 -&gt; s = '0' 3 n = int(s) 4 print(10 / n)[EOF] 输入命令 n 可以单步执行该行代码： 123456(Pdb) n&gt; c:\\users\\administrator\\desktop\\err.py(3)&lt;module&gt;()-&gt; n = int(s)(Pdb) n&gt; c:\\users\\administrator\\desktop\\err.py(4)&lt;module&gt;()-&gt; print(10 / n) 执行后pdb会自动指向下一行代码。特别地，任何时候我们都可以通过输入命令 p 变量名 来查看一个变量的值： 1234(Pdb) p s'0'(Pdb) p n0 继续执行： 1234(Pdb) nZeroDivisionError: division by zero&gt; c:\\users\\administrator\\desktop\\err.py(4)&lt;module&gt;()-&gt; print(10 / n) 此时执行到了出错代码，pdb会报错并停止在这一行，我们可以看到它指向的地方没有发生变化。 输入命令 q 可以结束调试，退出pdb： 123(Pdb) qC:\\Users\\Administrator\\Desktop&gt; 这种通过pdb在命令行调试的方法在理论上是万能的，但实在是太麻烦了，打个比方，如果代码文件中有1000行代码，要运行到第999行就得敲999次命令 n，这样还不如执行在代码文件插入代码呢。 pdb.set_trace 有没有可以直接调到我们需要检查的地方再进行单步调试的调试方式呢？有的！同样是基于pdb，这次我们在代码文件中 import pdb，然后，在可能出错的地方插入一句 pdb.set_trace()，这就设置了一个断点： 1234567# err.pyimport pdbs = '0'n = int(s)pdb.set_trace() # 运行到这里会自动暂停print(10 / n) 直接运行代码，这时程序会自动在 pdb.set_trace() 暂停并进入pdb调试环境，可以用命令 p 查看变量，用命令 n 单步执行，或者用命令 c 继续运行（如果有下一个断点就会停在下一个断点，否则直接运行到程序结束）： 123456789101112C:\\Users\\Administrator\\Desktop&gt;python err.py&gt; c:\\users\\administrator\\desktop\\err.py(7)&lt;module&gt;()-&gt; print(10 / n)(Pdb) p n0(Pdb) cTraceback (most recent call last): File &quot;err.py&quot;, line 7, in &lt;module&gt; print(10 / n)ZeroDivisionError: division by zeroC:\\Users\\Administrator\\Desktop&gt; 这种方式的效率要比直接启动pdb进行单步调试更高。 IDE 除了上面介绍到的方式之外，使用IDE附带的调试功能也是很常见的。通常IDE会自带一些快捷键，允许我们方便地设置断点、单步执行、查看变量值等等。PyCharm是一个不错的选择。 小结 写程序最痛苦的事情莫过于调试，程序往往会以你意想不到的流程来运行，你期待执行的语句其实根本没有执行，这时候，就需要调试了。 虽然用IDE调试起来比较方便，但是最后你会发现，logging 才是终极武器。 单元测试 什么是单元测试 如果你听说过测试驱动开发（TDD：Test-Driven Development），单元测试就不陌生。 单元测试是用来对一个模块、一个函数或者一个类来进行正确性检验的测试工作。 比如我们实现了一个求绝对值的函数 abs()，则测试用例需要包含以下这些情况： 输入正数，比如1、1.2、0.99，期待返回值与输入相同； 输入负数，比如-1、-1.2、-0.99，期待返回值与输入相反； 输入0，期待返回0； 输入非数值类型，比如None、[]、{}，期待抛出TypeError。 把上面的测试用例放到一个测试模块里，就得到了一个完整的单元测试。 如果单元测试通过，说明我们测试的代码能够正常工作。如果单元测试不通过，要么代码有bug，要么单元测试没有编写好，总之，需要修复代码使单元测试能够通过。 单元测试通过后有什么意义呢？如果我们对 abs() 函数代码做了修改，只需要再跑一遍单元测试，如果通过，说明我们的修改不会对 abs() 函数原有的行为造成影响，如果测试不通过，说明我们的修改与原有行为不一致，此时我们要么修改代码，要么修改测试。 这种以测试为驱动的开发模式最大的好处就是确保一个程序模块的行为符合我们设计的测试用例。在将来修改的时候，可以极大程度地保证该模块行为仍然是正确的。 编写一个单元测试 假设我们要编写一个 Dict 类，这个类的行为和 dict 一致，但是可以通过属性来访问，可以像下面这样使用： 12345&gt;&gt;&gt; d = Dict(a=1, b=2)&gt;&gt;&gt; d['a']1&gt;&gt;&gt; d.a1 把类定义写在 mydict.py 文件中： 12345678910111213class Dict(dict): def __init__(self, **kw): super().__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&quot;'Dict' object has no attribute '%s'&quot; % key) def __setattr__(self, key, value): self[key] = value 为了编写单元测试，我们需要引入Python自带的 unittest 模块，把单元测试写在 mydict_test.py 文件中： 12345678910111213141516171819202122232425262728293031import unittest # 导入Python自带的单元测试模块unittestfrom mydict import Dict # 导入我们要进行单元测试的模块/类/函数等等class TestDict(unittest.TestCase): def test_init(self): d = Dict(a=1, b='test') self.assertEqual(d.a, 1) self.assertEqual(d.b, 'test') self.assertTrue(isinstance(d, dict)) def test_key(self): d = Dict() d['key'] = 'value' self.assertEqual(d.key, 'value') def test_attr(self): d = Dict() d.key = 'value' self.assertTrue('key' in d) self.assertEqual(d['key'], 'value') def test_keyerror(self): d = Dict() with self.assertRaises(KeyError): value = d['empty'] def test_attrerror(self): d = Dict() with self.assertRaises(AttributeError): value = d.empty 我们使用一个测试类来实现单元测试，把所有类型的测试用例都封装为该类的方法。测试类继承自 unittest 模块的 TestCase 类。注意，所有测试方法都必须以 test 开头，不以 test 开头的方法不被认为是测试方法，测试的时候不会被执行。 每一类测试样例都需要编写一个 test_xxx() 方法。由于 unittest.TestCase 提供了很多内置的条件判断方法，我们只需要调用这些方法就可以断言输出是否符合我们的期望。最常用的断言就是 assertEqual()： 1self.assertEqual(abs(-1), 1) # 断言函数返回的结果与1相等 另一种重要的断言就是期待抛出指定类型的Error，比如通过 d['empty'] 访问不存在的 key 时，断言会抛出 KeyError： 12with self.assertRaises(KeyError): value = d['empty'] 而通过 d.empty 访问不存在的 key 时，我们期待抛出 AttributeError： 12with self.assertRaises(AttributeError): value = d.empty 当这些断言输出是否符合我们的期望时，测试用例通过，否则测试用例失败。这一小节知识说明怎样编写单元测试，具体怎么进行测试会在后续的小结中详细说明。 补充说明 这里补充一下 with 语法和 assertRaises 方法的说明。 使用with的语法 关于 with 语句的相关概念可以看看浅谈 Python 的 with 语句这篇文章。使用 with 的语法一般如下： 12with ContextExpression [as alias]: with-body 例如： 1234with open(r'somefileName') as f: for line in f: print(line) # ...more code 跟在 with 关键字后的表达式称为上下文表达式，它必须能返回一个上下文管理器（Context Manager）对象。with 语句包裹起来的代码块则称为 with-语句体（with-body）。如果我们在语句体中不需要用到上下文管理器对象，就不需要为这个对象取别名（alias），也即方括号 [] 内的是可忽略的。比方说前面编写测试类的时候就不需要，而上面例子中由于我们需要使用文件对象中，所以取了别名 f。 上下文管理器对象都实现了 __enter__() 和 __exit__() 这两个特殊方法。执行 __enter__() 方法会进行运行时上下文（runtime context），执行 __exit__() 方法则会退出。我们可以直接调用这两个方法来管理运行时上下文，也可以使用 with 语句进行管理。在执行 with-语句体的代码之前，__enter__() 方法会被自动调用，而执行完 with-语句体的代码之后，__exit__() 方法会被调用来退出运行时上下文。 assertRaises方法 接下来说说 assertRaises 方法，它有两种使用方法： assertRaises(exception, callable, *args, **kwds) assertRaises(exception, msg=None) 方法1测试我们使用参数 *args 和 **kwds 调用 callable 对象（可能是某个函数/方法）时，是否会出现 exception 异常，如果是则测试用例通过，否则测试失败。 方法2同样是测试一个异常是否出现，但当我们只传入异常时，assertRaises 方法会返回一个上下文管理器对象，所以我们可以用 with 来管理，从而实现判断运行某一段代码（放在 with-语句体中）时，是否出现某种异常的测试用例。 特别地，这些** TestCase 类提供的断言方法都支持传入一个关键字参数 msg**，我们可以使用它自定义断言失败时提示的错误信息。 没指定 msg 参数时断言失败的报错： 1AssertionError: KeyError not raised 指定了 msg 参数（假设指定 msg = '1234'）时断言失败的报错： 1AssertionError: KeyError not raised : 1234 运行单元测试 一旦编写好单元测试，我们就可以运行单元测试，具体有两种实现方法。 第一次方法是直接在单元测试文件 mydict_test.py 的最后加上两行代码： 12if __name__ == '__main__': unittest.main() 这样只要把 mydict_test.py 当成普通Python脚本来运行就可以了，运行时就会直接跑单元测试了： 123456C:\\Users\\Administrator\\Desktop&gt;python mydict_test.py.....----------------------------------------------------------------------Ran 5 tests in 0.000sOK 第二种方法是在命令行通过参数 -m unittest 来运行单元测试： 123456C:\\Users\\Administrator\\Desktop&gt;python3 -m unittest mydict_test.....----------------------------------------------------------------------Ran 5 tests in 0.000sOK 第二种方法更为推荐，因为这样可以一次批量运行多个单元测试，比方说： 123456C:\\Users\\Administrator\\Desktop&gt;python -m unittest mydict_test.py mydict_test.py..........----------------------------------------------------------------------Ran 10 tests in 0.000sOK 此外，还有很多工具可以自动来运行这些单元测试。 前面都是举单元测试运行通过的例子，接下来补充一个运行不通过的例子，看看有测试用例不通过时，运行单元测试会返回什么。比方说把 test_keyerror(self) 方法中的 value = d['empty'] 语句换为 pass，这样语句体就不会返回 KeyError 了，断言会失败。看看此时运行单元测试的结果： 1234567891011121314C:\\Users\\Administrator\\Desktop&gt;python -m unittest mydict_test.py....F======================================================================FAIL: test_keyerror (mydict_test.TestDict)----------------------------------------------------------------------Traceback (most recent call last): File &quot;C:\\Users\\Administrator\\Desktop\\mydict_test.py&quot;, line 27, in test_keyerror passAssertionError: KeyError not raised----------------------------------------------------------------------Ran 5 tests in 0.000sFAILED (failures=1) 可以看到这里汇报了失败的源头是 test_keyerror 这个测试方法，原因是这个方法的 pass 语句没有引起 KeyError，使得断言失败。最后还汇报了运行了5个测试、总共运行的时间、单元测试失败、失败的测试数为1。 setUp与tearDown 在测试类中除了定义 test_xxx() 这样的测试方法，我们还可以编写两个特殊的 setUp() 和 tearDown() 方法。这两个方法分别在每次调用一个测试方法的前后被执行。 那么这两个方法有什么实际意义呢？假设测试时需要启动一个数据库，如果我们在 setUp() 方法中编写连接数据库的代码，在 tearDown() 方法中编写关闭数据库，这样我们就不必在每个测试方法中重复编写相同的代码了，也即把功能封装起来： 123456789class TestDict(unittest.TestCase): ... def setUp(self): print('setUp') def tearDown(self): print('tearDown') 再次运行单元测试： 12345678910111213141516C:\\Users\\Administrator\\Desktop&gt;python -m unittest mydict_test.pysetUptearDown.setUptearDown.setUptearDown.setUptearDown.setUptearDown.----------------------------------------------------------------------Ran 5 tests in 0.016sOK 这里看到多出了一些句号 .，它们是每个测试方法通过之后会打印的。 小结 单元测试可以有效地测试某个程序模块的行为，是未来重构代码的信心保证。 单元测试的测试用例要覆盖常用的输入组合、边界条件和异常。 单元测试代码要非常简单，如果测试代码太复杂，那么测试代码本身就可能有bug。 单元测试通过了并不意味着程序就没有bug了，但是不通过程序肯定有bug。 除了Python自带的 unittest 模块，不妨再了解一下 Nose 和 PyTest 这两个第三方模块。 文档测试 使用文档测试 如果你经常阅读Python的官方文档，可以看到很多官方文档都带有示例代码。比如 re 模块的官方文档就带了很多示例代码，例如： 1234&gt;&gt;&gt; import re&gt;&gt;&gt; m = re.search('(?&lt;=abc)def', 'abcdef')&gt;&gt;&gt; m.group(0)'def' 可以把这些示例代码在Python的交互式环境下输入并执行，结果与文档中的示例代码显示的一致。 这些代码与其他说明可以写在注释中，然后，由一些工具来自动生成文档。既然这些代码本身就可以复制出来直接运行，那么，可不可以自动执行写在注释中的代码呢？ 答案是肯定的，Python内置的 “文档测试”（doctest）模块 可以提取出注释中的代码并执行测试。 当我们编写注释时，如果写上这样的注释： 1234567891011121314def abs(n): ''' Function to get absolute value of number. Example: &gt;&gt;&gt; abs(1) 1 &gt;&gt;&gt; abs(-1) 1 &gt;&gt;&gt; abs(0) 0 ''' return n if n &gt;= 0 else (-n) 无疑更明确地向函数的调用者说明了该函数的期望输入和输出。 doctest 严格按照Python交互式命令行的输入和输出来判断测试结果是否正确。只有测试异常的时候，可以用 ... 来代替发生异常时Traceback的部分（毕竟实在是太长了..）。 不妨用文档测试 doctest 来重新实现上一节中为 Dict 类编写的单元测试，编写 mydict.py 文件： 12345678910111213141516171819202122232425262728293031323334353637383940# mydict2.pyclass Dict(dict): ''' Simple dict but also support access as x.y style. # 以下为文档注释中的代码部分 &gt;&gt;&gt; d1 = Dict() &gt;&gt;&gt; d1['x'] = 100 &gt;&gt;&gt; d1.x 100 &gt;&gt;&gt; d1.y = 200 &gt;&gt;&gt; d1['y'] 200 &gt;&gt;&gt; d2 = Dict(a=1, b=2, c='3') &gt;&gt;&gt; d2.c '3' &gt;&gt;&gt; d2['empty'] # 注意这里我们使用省略号...来替换了Traceback的细节 Traceback (most recent call last): ... KeyError: 'empty' &gt;&gt;&gt; d2.empty Traceback (most recent call last): ... AttributeError: 'Dict' object has no attribute 'empty' ''' # 以下为该类的方法 def __init__(self, **kw): super(Dict, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&quot;'Dict' object has no attribute '%s'&quot; % key) def __setattr__(self, key, value): self[key] = valueif __name__=='__main__': import doctest doctest.testmod() # 使用doctest模块的testmod函数来进行文档测试 注意前面我们说的是注释，但这个注释并非使用 # 号标识的那种注释，而是文档注释，也即文档字符串。按PEP257的定义： A docstring is a string literal that occurs as the first statement in a module, function, class, or method definition. Such a docstring becomes the doc special attribute of that object. 所以这里 Dict 类的第一个字符串就是 Dict 类的文档注释，我们把用于文档测试的示例代码按照Python交互式命令行的输入和输出的标准来书写即可（只有测试异常时可以用 ... 替换掉Traceback的部分）。 运行 mydict.py： 12C:\\Users\\Administrator\\Desktop&gt;python mydict.py 文档测试通过时，程序不会有任何输出。接下来我们试试把 __getattr__() 方法注释掉（这样就不能通过把字典的key作为属性来访问了），此时再运行 mydict.py： 123456789101112131415161718192021222324252627C:\\Users\\Administrator\\Desktop&gt;python mydict.py**********************************************************************File &quot;mydict.py&quot;, line 7, in __main__.DictFailed example: d1.xException raised: Traceback (most recent call last): File &quot;F:\\Anaconda3\\lib\\doctest.py&quot;, line 1320, in __run compileflags, 1), test.globs) File &quot;&lt;doctest __main__.Dict[2]&gt;&quot;, line 1, in &lt;module&gt; d1.x AttributeError: 'Dict' object has no attribute 'x'**********************************************************************File &quot;mydict.py&quot;, line 13, in __main__.DictFailed example: d2.cException raised: Traceback (most recent call last): File &quot;F:\\Anaconda3\\lib\\doctest.py&quot;, line 1320, in __run compileflags, 1), test.globs) File &quot;&lt;doctest __main__.Dict[6]&gt;&quot;, line 1, in &lt;module&gt; d2.c AttributeError: 'Dict' object has no attribute 'c'**********************************************************************1 items had failures: 2 of 9 in __main__.Dict***Test Failed*** 2 failures. 可以看到因为没有实现把key作为属性访问的功能，此时文档注释中的两个example（即 d1.x 和 d2.c 这两行输入）出错了，而文档注释中总共包含9对输入输出example。 注意到，我们只在 if __name__=='__main__': 代码块内写了执行文档测试的逻辑，也即只有在命令行中直接运行（python mydict.py）时会进行文档测试。而使用者使用这个类，在别的模块中导入该类（from mydict import Dict）时，文档测试是不会被执行的。因此，我们不必担心文档测试会在非测试环境下被执行，编写文档测试并不会影响到使用者使用该模块。 练习 对函数 fact(n) 编写文档测试并执行： 1234567891011121314151617181920def fact(n): ''' &gt;&gt;&gt; fact(1) 1 &gt;&gt;&gt; fact(5) 120 &gt;&gt;&gt; fact(0) Traceback (most recent call last): ... ValueError ''' if n &lt; 1: raise ValueError() if n == 1: return 1 return n * fact(n - 1)if __name__ == '__main__': import doctest doctest.testmod() 小结 文档测试非常有用，不但可以用来测试，还可以直接作为示例代码。通过某些文档生成工具，就可以自动把包含文档测试的注释提取出来。用户看文档的时候，同时也能看到文档测试。","link":"/posts/38807.html"},{"title":"IO编程","text":"什么是IO IO在计算机中指输入和输出（Input/Output）。由于程序运行时，数据是在内存中驻留，并由CPU这个超快的计算核心来进行处理的（处理时会把数据从内存载入到CPU的高速缓存中），而涉及到数据交换的操作，比如磁盘读写、网络传输等的时候，就需要使用IO接口来协调了。 比如你打开浏览器，访问新浪首页，浏览器这个程序就需要通过网络IO获取新浪的网页。浏览器首先会发送数据给新浪服务器，告诉它我想要首页的HTML，这个动作是往外发数据，叫Output，随后新浪服务器把网页发过来，这个动作是从外面接收数据，叫Input。所以，通常，程序完成IO操作会有Input和Output两个数据流。当然也有只用一个的情况，比如，从磁盘读取文件到内存，就只有Input操作，反过来，把数据写到磁盘文件里，就只是一个Output操作。 IO编程中，流（Stream）是一个很重要的概念，可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream就是数据从外面（磁盘、网络）流进内存，Output Stream就是数据从内存流到外面去。对于浏览网页来说，浏览器程序和新浪服务器之间至少需要建立两根水管，才可以既能发数据，又能收数据。 由于CPU和内存的速度远远高于外设的速度，所以，在IO编程中，就存在速度严重不匹配的问题。举个例子来说，比如要把100M的数据写入磁盘，CPU输出100M的数据只需要0.01秒，可是磁盘要接收这100M数据可能需要10秒，怎么办呢？有两种办法： 第一种方法是让CPU等待，也就是程序暂停执行后续代码，等100M的数据在10秒后写入磁盘，再接着往下执行，这种模式称为同步IO； 第二种方法是CPU不等待，只是告诉磁盘，“您老慢慢写，不着急，我接着干别的事去了”，于是，后续代码可以继续执行，这种模式称为异步IO。 同步和异步的区别就在于是否等待IO执行的结果。好比你去麦当劳点餐，你说“来个汉堡”，服务员告诉你，对不起，汉堡要现做，需要等5分钟，于是你站在收银台前面等了5分钟，拿到汉堡再去逛商场，这是同步IO。 你说“来个汉堡”，服务员告诉你，汉堡需要等5分钟，你可以先去逛商场，等做好了，我们再通知你，这样你可以立刻去干别的事情（逛商场），这是异步IO。 很明显，使用异步IO来编写程序性能会远远高于同步IO，但是异步IO的缺点是编程模型复杂。想想看，你得知道什么时候通知你“汉堡做好了”，而通知你的方法也各不相同。如果是服务员亲自跑过来找到你，这是回调模式，如果服务员发短信通知你，你就得不停地检查手机，这是轮询模式。总之，异步IO的复杂度远远高于同步IO。 操作IO的能力都是由操作系统提供的，编程语言所做的只是把操作系统提供的低级C接口封装起来方便使用，Python也不例外。后面的小节中会详细讨论Python的IO编程接口。 注意，本章的IO编程都是同步模式，异步IO由于复杂度太高，后续涉及到服务器端程序开发时会再作讨论。 文件读写 读写文件是最常见的IO操作。Python内置了读写文件的函数，用法和C是兼容的。 读写文件前，我们先必须了解一下，在磁盘上读写文件的功能都是由操作系统提供的，现代操作系统不允许普通的程序直接操作磁盘，所以，读写文件就是请求操作系统打开一个文件对象（通常称为文件描述符），然后，通过操作系统提供的接口从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件）。 读文件 要以读文件的模式打开一个文件对象，可以使用Python内置的 open() 函数，传入文件名（如果文件和代码文件在相同文件夹下就可以省略路径）和标示符 'r'： 1&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'r') 标示符 'r' 表示读，这样，我们就成功地打开了一个文件。 如果文件不存在，open() 函数就会抛出一个 IOError 的错误，并且给出错误码和详细的信息告诉你文件不存在： 1234&gt;&gt;&gt; f=open('/Users/michael/notfound.txt', 'r')Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;FileNotFoundError: [Errno 2] No such file or directory: '/Users/michael/notfound.txt' 如果文件打开成功，我们就可以调用 read() 方法来一次读取文件的全部内容，Python会把为文件的内容读到内存，返回的是一个 str 对象： 12&gt;&gt;&gt; f.read()'Hello, world!' 读取完毕后，如果不需要继续操作文件对象，我们就应当调用 close() 方法来关闭它。因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的： 1&gt;&gt;&gt; f.close() 由于读写文件都有可能产生 IOError，一旦出错，后面的 f.close() 就不会调用。所以，为了保证无论是否出错都能正确地关闭文件，我们可以使用 try ... finally 来实现： 123456try: f = open('/path/to/file', 'r') print(f.read())finally: if f: f.close() 但是每次都这么写实在太繁琐，所以，Python引入了 with 语句来自动帮我们调用 close() 方法（（在上一章中有 with 语句使用及原理的介绍））： 12with open('/path/to/file', 'r') as f: print(f.read()) 这和前面的 try ... finally 实现的效果是一样的，但是代码更简洁，并且我们不必调用 f.close() 方法。 调用 read() 方法可以一次性读取文件的全部内容。但如果文件有10G，内存就爆了，所以，为了保险起见，我们可以多次调用 read(size) 方法，每次最多读取size个字节的内容。 但是有时候文件不一定有严格的格式，比方说读取一篇文章，这时按字节读取就不太合适了。但我们可以调用 readline() 方法，readline() 方法每次读取文件的一行内容。而调用 readlines() 方法则会一次读取文件的所有内容并按行返回一个 list 对象。我们可以： 12for line in f.readlines(): print(line.strip()) # 把末尾的换行符'\\n'删掉再打印 写文件 写文件和读文件是一样的，唯一区别是调用 open()函数时，传入标识符 'w' 或者 'wb' 表示写文本文件或写二进制文件： 123&gt;&gt;&gt; f = open('/Users/michael/test.txt', 'w')&gt;&gt;&gt; f.write('Hello, world!')&gt;&gt;&gt; f.close() 你可以多次调用 write() 来写入文件，但是最后一定要调用 f.close() 来关闭文件。当我们写文件时，操作系统往往不会立刻把数据写入磁盘，而是放在内存中缓存起来，空闲的时候再慢慢写入。只有调用 close() 方法时，操作系统才会保证把没有写入的数据全部写入磁盘。忘记 close() 的后果是数据可能只有一部分写到了磁盘，剩下的丢失了。为了避免这样的情况发生，类似上一节所介绍的，我们可以使用 with 语句自动管理上下文： 12with open('/Users/michael/test.txt', 'w') as f: f.write('Hello, world!') 如果要写入特定编码的文本文件，还可以给 open() 函数传入 encoding 参数，将要写入的字符串自动转换成指定编码。 file-like Object 在Python中，除了文件对象之外，内存中的字节流，网络流，自定义流等等，拥有 read() 方法的对象统称为 file-like Object。file-like Object 不需要继承自特定的类，只要有 read() 方法就行（文件对象的其他方法不一定都需要实现，可以看看官方说明），这得益于Python鸭子类型的实现。StringIO 就是在内存中创建的 file-like Object，常用作临时缓冲。 二进制文件 前面讲的默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用 'rb' 模式打开文件即可： 123&gt;&gt;&gt; f = open('/Users/michael/test.jpg', 'rb')&gt;&gt;&gt; f.read()b'\\xff\\xd8\\xff\\xe1\\x00\\x18Exif\\x00\\x00...' # 十六进制表示的字节 字符编码 要读取非UTF-8编码的文本文件，可以给 open() 函数传入 encoding 参数，例如，读取GBK编码的文件： 123&gt;&gt;&gt; f = open('/Users/michael/gbk.txt', 'r', encoding='gbk')&gt;&gt;&gt; f.read()'测试' 遇到有些编码不规范的文件，你可能会遇到 UnicodeDecodeError，因为在文本文件中可能夹杂了一些非法编码的字符。遇到这种情况，open() 函数还接收一个 errors 参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略： 1&gt;&gt;&gt; f = open('/Users/michael/gbk.txt', 'r', encoding='gbk', errors='ignore') 小结 在Python中，文件读写是通过 open() 函数打开的文件对象完成的。使用 with 语句操作文件IO是个好习惯。 StringIO和BytesIO StringIO 很多时候，数据读写不一定是对文件进行的，我们也可以在内存中进行读写操作。 StringIO 顾名思义就是在内存中读写 str。 要把 str 写入 StringIO，我们需要先创建一个 StringIO 对象，然后，像文件一样写入即可： 12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write('hello')5&gt;&gt;&gt; f.write(' ')1&gt;&gt;&gt; f.write('world!')6&gt;&gt;&gt; print(f.getvalue())hello world! getvalue() 方法用于获得IO流中的全部内容。 读取 StringIO 的方法也和读文件类似： 1234567891011&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; f = StringIO('Hello!\\nHi!\\nGoodbye!') # 用一个str初始化StringIO&gt;&gt;&gt; while True:... s = f.readline()... if s == '': # 读取完毕，跳出循环... break... print(s.strip()) # 去掉当前行首尾的空格再打印...Hello!Hi!Goodbye! BytesIO StringIO 操作的只能是 str，如果要操作二进制数据，就需要使用 BytesIO。 BytesIO 实现了在内存中读写 bytes，我们创建一个 BytesIO，然后写入一些 bytes： 123456&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO()&gt;&gt;&gt; f.write('中文'.encode('utf-8'))6&gt;&gt;&gt; print(f.getvalue())b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 请注意，写入的不是 str，而是经过UTF-8编码的 bytes。 和 StringIO 类似，读取 BytesIO： 1234&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO(b'\\xe4\\xb8\\xad\\xe6\\x96\\x87') # 用一个bytes初始化BytesIO&gt;&gt;&gt; f.read()b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' 为什么使用StringIO和BytesIO 这个问题在Stackoverflow上有回答，为什么我们不直接使用 str 和 bytes，而要这么别扭地在内存中使用 StringIO 和 BytesIO 呢？其实呀，主要是因为 StringIO 和 BytesIO 都是 file-like Object，可以像文件对象那样使用，当某些库的函数支持文件对象时，我们可以传入 StringIO 和 BytesIO，也能使用，这一点 str 和 bytes 是没法做到的。 读写IO需要注意的地方 以 StringIO 为例，前面我们将到读取 StringIO 时，是先使用字符串进行初始化，然后再读取： 123&gt;&gt;&gt; f = StringIO('Hello!\\nHi!\\nGoodbye!') # 用一个str初始化StringIO&gt;&gt;&gt; f.readlines()['Hello!\\n', 'Hi!\\n', 'Goodbye!'] 但如果我们没有进行初始化，而是对一个空的 StringIO 进行写入，然后再读取呢？这时就会像下面这样： 123456789&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write('Hello!\\n')7&gt;&gt;&gt; f.write('Hi!\\n')4&gt;&gt;&gt; f.write('Goodbye!')8&gt;&gt;&gt; f.readlines()[] 我们发现此时读取不到写入的字符串了，这是为什么呢？其实呀，这时因为当前所处流的位置（Stream Position）在末尾，所以读取不到东西了。那怎么知道当前处在流的什么位置呢？我们可以使用 tell() 方法。对比一下： 使用字符串初始化 StringIO： 123&gt;&gt;&gt; f = StringIO('Hello!\\nHi!\\nGoodbye!')&gt;&gt;&gt; f.tell()0 写入 StringIO： 123456789&gt;&gt;&gt; f = StringIO()&gt;&gt;&gt; f.write('Hello!\\n')7&gt;&gt;&gt; f.write('Hi!\\n')4&gt;&gt;&gt; f.write('Goodbye!')8&gt;&gt;&gt; f.tell()19 可以发现，使用字符串初始化时，位置会保持在流的开头，而使用 write() 方法对流进行写入操作后，位置会移动到写入结束的地方。那有没有办法在写入以后进行读取呢？有！可以使用前面提到的 getvalue() 方法读取IO流中的全部内容，另外也可以使用 seek() 方法回到前面的某一位置，然后读取该位置后的内容： 123456&gt;&gt;&gt; f.tell()19&gt;&gt;&gt; f.seek(0) # 回到流的开头位置0&gt;&gt;&gt; f.tell()0 有时候呀，我们可能会在初始化一个 StringIO 之后，想要对其进行写入操作，这时会发生一个问题： 1234567&gt;&gt;&gt; f = StringIO('Hello!')&gt;&gt;&gt; f.getvalue()'Hello!'&gt;&gt;&gt; f.write('Hi!')3&gt;&gt;&gt; f.getvalue()'Hi!lo!' 可以看到初始化的内容被写入的内容覆盖了，这显然不是我们所希望的。为什么会这样呢？其实呀，跟前面说的问题是一样的，举一反三，都是因为Stream Position引起的。初始化一个 StringIO 后，位置在流的开头，此时写入就会从流的开头写入，而不是像我们所希望的那样从流的末尾写入，稍微改动一下就好了： 1234567&gt;&gt;&gt; f = StringIO('Hello!')&gt;&gt;&gt; f.seek(0, 2) # 移动到流的末尾6&gt;&gt;&gt; f.write('Hi!')3&gt;&gt;&gt; f.getvalue()'Hello!Hi!' 除了移动到流的末尾，也能移动到某个位置，看看 seek() 方法的描述： 12345678910Help on built-in function seek:seek(pos, whence=0, /) method of _io.StringIO instance Change stream position. Seek to character offset pos relative to position indicated by whence: 0 Start of stream (the default). pos should be &gt;= 0; 1 Current position - pos must be 0; 2 End of stream - pos must be 0. Returns the new absolute position. 可以看到 seek() 方法有必选参数 pos 和 可选参数 whence，前者是移动多少，后者是从哪里开始移动。whence 默认为0，也即默认从流的开头移动 pos 个位置。 小结 StringIO 和 BytesIO 是在内存中操作 str 和 bytes 的方法，和读写文件具有一致的接口。 操作目录和文件 简述 在命令行下，我们可以通过输入操作系统提供的各种命令，比如dir、cp等，来操作目录和文件。这些命令的本质其实就是简单地调用了操作系统提供的接口函数。 那如果想在Python程序中操作目录和文件该怎么办呢？Python内置的 os 模块同样给与我们调用操作系统提供的接口函数的能力。 打开Python交互式命令行，首先看看如何使用 os 模块的基本功能： 123&gt;&gt;&gt; import os&gt;&gt;&gt; os.name'posix' Linux、Unix和Mac OS X系统返回的是 posix，Windows系统返回的则是 nt。 要获取详细的系统信息，可以调用 uname() 函数： 12&gt;&gt;&gt; os.uname()posix.uname_result(sysname='Darwin', nodename='MichaelMacPro.local', release='14.3.0', version='Darwin Kernel Version 14.3.0: Mon Mar 23 11:59:05 PDT 2015; root:xnu-2782.20.48~5/RELEASE_X86_64', machine='x86_64') 注意，uname() 函数在Windows系统上不提供，也就是说，os 模块的能否使用某些函数取决于使用者的操作系统。 环境变量 在操作系统中定义的环境变量，全部保存在 os.environ 变量中。我们可以直接查看操作系统的所有环境变量： 12&gt;&gt;&gt; os.environenviron({'VERSIONER_PYTHON_PREFER_32_BIT': 'no', 'TERM_PROGRAM_VERSION': '326', 'LOGNAME': 'michael', 'USER': 'michael', 'PATH': '/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin', ...}) 要获取某个环境变量的值，可以调用使用 os.environ.get('key') 的方式： 1234&gt;&gt;&gt; os.environ.get('PATH')'/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin'&gt;&gt;&gt; os.environ.get('x', 'default')'default' 传入某个环境变量的名称，得到对应的路径。除此之外还可以传入一个字符串作为默认路径（没有可返回的路径时会返回默认路径）。 操作目录和文件 除了前面的 os 模块中，操作目录和文件的函数还有一部分放在 os.path 模块中。比方说用于生成绝对路径的 abspath() 函数： 1234&gt;&gt;&gt; os.path.abspath('.') # 点符代表当前工作路径'F:\\\\Python35'&gt;&gt;&gt; os.path.abspath('Tools\\\\demos')'F:\\\\Python35\\\\Tools\\\\demos' 在05模块中归纳过文件搜索路径的一些知识，当我们在程序中需要用到某个文件时，可以使用两种方式来让程序查找到这个文件： 一是使用绝对路径，也即完整的路径； 二是使用相对路径（相对当前工作路径而言的路径），并且可以使用点符 . 来替代当前工作路径。 注意，使用相对路径时是可以不使用点符的，所以上面代码中，为 Tools\\\\demos 生成绝对路径也同样可行。 接下来我们试试创建目录和删除目录： 1234567# 在某个目录下创建一个新目录，首先生成新目录的完整路径:&gt;&gt;&gt; os.path.join('/Users/michael', 'testdir')'/Users/michael/testdir'# 然后创建一个目录:&gt;&gt;&gt; os.mkdir('/Users/michael/testdir')# 删掉一个目录:&gt;&gt;&gt; os.rmdir('/Users/michael/testdir') 把两个路径合成一个时，不要直接拼字符串，而要通过 os.path.join() 函数，这样可以正确处理不同操作系统的路径分隔符。在Linux/Unix/Mac下，os.path.join('part1','part2') 返回这样的字符串： 1part-1/part-2 而Windows下会返回这样的字符串： 1part-1\\part-2 同样的道理，要拆分路径时，也不要直接去拆字符串，而要通过 os.path.split() 函数，这样可以把一个路径拆分为两部分，后一部分总是最后级别的目录或文件名： 12&gt;&gt;&gt; os.path.split('/Users/michael/testdir/file.txt')('/Users/michael/testdir', 'file.txt') os.path.splitext() 函数可以用来获取文件扩展名，很多时候非常方便： 12&gt;&gt;&gt; os.path.splitext('/path/to/file.txt')('/path/to/file', '.txt') 这些合并、拆分路径的函数并不要求目录和文件真实存在，它们只是对字符串进行操作。 文件操作使用下面的函数。假定当前目录下有一个 test.txt 文件： 1234# 对文件重命名:&gt;&gt;&gt; os.rename('test.txt', 'test.py')# 删掉文件:&gt;&gt;&gt; os.remove('test.py') 但是 os 模块中不存在复制文件的函数！原因是复制文件并非是由操作系统提供的系统调用。理论上讲，我们通过上一节的读写文件可以完成文件复制，只不过要多写很多代码。 幸运的是 shutil 模块提供了 copyfile() 的函数，你还可以在 shutil 模块中找到很多实用函数，它们可以看做是对 os 模块的补充。 最后看看如何利用Python的特性来过滤文件。比如我们要列出当前目录下的所有目录，只需要一行代码： 12&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isdir(x)]['.lein', '.local', '.m2', '.npm', '.ssh', '.Trash', '.vim', 'Applications', 'Desktop', ...] 要列出所有的 .py 文件，也只需一行代码： 12&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']['apis.py', 'config.py', 'models.py', 'pymonitor.py', 'test_db.py', 'urls.py', 'wsgiapp.py'] 是不是非常简洁？ 小结 Python的 os 模块封装了操作系统的目录和文件操作，要注意这些函数有的在 os 模块中，有的在 os.path 模块中。 练习 习题1 利用 os 模块编写一个能实现 ls -l 输出的程序。 先看看 ls -l 做的是什么： 12345ubuntu@ubuntu:~/HumanFaceRecognitionWithNN$ ls -ltotal 344-rw-rw-r-- 1 ubuntu ubuntu 10301 Dec 14 22:28 face_recognition.pydrwxrwxr-x 3 ubuntu ubuntu 4096 Dec 21 15:50 test-rw-rw-r-- 1 ubuntu ubuntu 328506 Dec 10 15:39 yaleB_face_dataset.mat 注意这是在Linux上执行的，我们想查看当前路径下有什么文件和文件夹可以使用 ls 或者 dir 命令，而如果我们想了解更详细的信息则可以用 ls -l 或者dir -l 命令。 这里稍微解析一下返回的信息吧，以下面这一条为例： field1 field2 field3 field4 field5 field6 field7 field8 field9 field10 - rw- rw- r-- 1 ubuntu ubuntu 10301 Dec 14 22:28 face_recognition.py field1 是 File type flag，标识文件类型，如果是 - 则表明是普通文件，是 d 则表明是一个目录； field2、field3、field4 依次是拥有者、拥有者所在的用户组以及其他用户对该文件/文件夹的操作权限，r 表示可读，w 表示可写， x 表示可执行。 field5 是所含链接数，如果该项是一个文件，则链接数为1；如果该项是一个目录，则一般为该目录下子文件夹数+2。为什么呢？因为当前目录（该项的父目录）有一条指向该项的链接，而对文件夹来说，除了父目录的链接之外，它本身还有一条 . 链接指向自身，并且它的子目录都有一条 .. 链接指向它； field6 是拥有者的名字； field7 是拥有者所在的用户组； field8 是该项的大小（多少bytes）； field9 是最后修改该项的日期和时间； field10 是该项的名字。 我们注意到，除了每一项的详细信息之外，最前面还有一行输出 total 344，这个344是什么呢？它指的是当前目录所有文件和文件夹所使用的块（block）的数目，块是一个操作系统的概念，这里不详细展开。如果我们想知道当前目录下每一项所使用的块的数目，可以使用 ls -s命令： 123ubuntu@VM-173-69-ubuntu:~/HumanFaceRecognitionWithNN$ ls -stotal 344 12 face_recognition.py 4 test 328 yaleB_face_dataset.mat 加起来总数正是 344。 以上内容参考了以下几个链接： What is that “total” in the very first line after ls -l? command ls -l output explained What do the fields in ls -al output mean? What does each part of the ls -la output mean? 题目要求实现Python版的 ls -l，理论上应该是可行的，但上面的内容只涉及到 os 和 os.path 模块中很少的函数，其他的还有待发掘。我暂时没有时间去琢磨，所以先略过这一题。 习题2 编写一个程序，能在当前目录以及当前目录的所有子目录下查找文件名包含指定字符串的文件，并打印出相对路径。 12345678910111213141516import osdef search(s, path=os.getcwd()): filelst = [x for x in os.listdir(path)] for filename in filelst: filepath = os.path.join(path, filename) # print('Searching: ', path, '\\nWith: ', filepath) if os.path.isfile(filepath): if s in filename: print(os.path.relpath(filepath)) elif os.path.isdir(filepath): search(s, filepath)if __name__ == '__main__': s = input('Enter the string: ') search(s) 这题还是挺有意思的，用户自己定义搜索的字符串，我们不仅要找出当前目录下包含该字符串的文件，还要搜索所有的子目录。我们可以把搜索一个目录的过程封装为 search 函数，并采用递归的方式来实现其子目录的搜索。思路如下： 获取当前目录的所有文件&amp;目录名，使用 os.listdir() 函数可以实现，把这些名称放在一个列表里保存； 接下来逐个遍历并判断列表中的元素是文件还是目录，可以使用 os.path.isfile() 和 os.path.isdir() 函数； 如果当前遍历到的元素是文件，则使用 os.path.relpath() 函数输出文件的相对路径； 如果当前遍历到的元素是目录，则将该目录的路径传入 search 函数。 特别地，我们要注意这些函数应该输入什么和会输出什么。os.path.relpath() 函数接收一条完整的绝对路径，并输出相对于当前工作路径（在命令行中执行该Python文件时所处的路径）的相对路径，所以我们要先构造出正确的绝对路径，才能获取正确的相对路径。 os.path.isdir() 可以接收相对路径也可以接收绝对路径，因为我们使用 os.listdir() 只能获得文件或目录的名称，在搜索子目录时，这些名称并不是相对于当前工作路径的相对路径，所以不能直接传入 os.listdir() 中，必须先构造绝对路径，然后再判断。 为什么不使用 os.path.abspath() 函数来生成绝对路径呢？因为传入 os.path.abspath() 函数的必须是一条正确的相对路径，才会得到正确的相对路径。举个例子，当前工作路径是 C:\\Users\\Administrator\\Desktop，其子目录 test1 中有一个文件 test2.py，如果我们使用 os.path.abspath('test2.py')，那么得到的绝对路径就变成了 C:\\Users\\Administrator\\Desktop\\test2.py，显然是不对的。 序列化 序列化和反序列化 在程序运行的过程中，所有的变量都保存在内存中，而一旦程序结束，变量所占用的内存就会被操作系统全部回收。但是，有时候，我们希望通过程序修改了某个变量的值之后，能够让另一个程序能调用这个变量。比方说在程序1中定义了一个 list，并且经过某些高开销的操作修改了这个 list 的值。如果我们想在程序2中使用程序1中修改后的 list，按之前的做法就只能把程序1作为一个模块，在程序2中执行程序1的代码，这样一来，就必须重复执行高开销的操作了。有没有解决这个问题的方法呢？有的，答案就是序列化。 我们把将变量从内存中保存的格式变成可存储或可传输的格式这个过程称之为序列化，在Python中叫 pickling，在其他语言中也被称之为 serialization，marshalling，flattening 等等，都是一个意思。经过序列化之后，内存中的变量就由原来的格式（某种数据结构/类型）转换为特定的格式，从而可以被存储或传输，这样另一个程序需要用到时就可以直接读取，而不必重复计算了。 反过来，把将变量内容从序列化的对象重新读到内存里还原为原来的格式这一过程称之为反序列化，即 unpickling。 序列化（pickle） 在01Python基础中，我们就知道传输和存储都是以字节（bytes）为单位的，所以这节首先介绍一种将变量序列化为 bytes 对象的方法，Python提供了 pickle 模块来实现这一功能。 以 dict 为例，将一个 dict 类型的对象序列化并写入文件： 1234&gt;&gt;&gt; import pickle&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)&gt;&gt;&gt; pickle.dumps(d)b'\\x80\\x03}q\\x00(X\\x03\\x00\\x00\\x00ageq\\x01K\\x14X\\x05\\x00\\x00\\x00scoreq\\x02KXX\\x04\\x00\\x00\\x00nameq\\x03X\\x03\\x00\\x00\\x00Bobq\\x04u.' pickle.dumps() 函数可以把任意变量序列化为一个 bytes 对象。我们可以把这个 bytes 对象写入文件。此外，我们也可以用 pickle.dump() 函数直接把对象序列化后写入一个 file-like Object： 123&gt;&gt;&gt; f = open('dump.txt', 'wb')&gt;&gt;&gt; pickle.dump(d, f)&gt;&gt;&gt; f.close() 打开 dump.txt 文件，我们会看到一堆乱七八糟无法阅读的内容，这些都是Python保存的对象信息。 当我们需要还原变量，也即把对象从磁盘读到内存时，可以先把内容读入到一个 bytes 对象中，然后用 pickle.loads() 方法反序列化出对象，也可以直接用 pickle.load() 方法从一个 file-like Object 中直接反序列化出对象。打开另一个Python命令行，试试反序列化刚才保存的对象： 12345&gt;&gt;&gt; f = open('dump.txt', 'rb')&gt;&gt;&gt; d = pickle.load(f)&gt;&gt;&gt; f.close()&gt;&gt;&gt; d{'age': 20, 'score': 88, 'name': 'Bob'} 可以看到我们成功地还原了这个 dict 类型的对象。 有了 Pickle 之后，我们可以方便地在Python中进行序列化和反序列化。但是，和所有其他编程语言的序列化问题一样，Pickle 是一种Python特有的序列化解决方案，它只能用于Python，甚至不同版本的Python彼此都可能不兼容。如果我们使用Python写程序，而别人使用其他语言，比如Java，C++等，它们没有 Pickle 模块也就没办法进行反序列化了。 JSON 如果我们要在不同的编程语言之间传递对象，就必须把对象序列化为通用的标准格式，比如序列化XML（Extensible Markup Language，可扩展标记语言）。但更好的方法是序列化为JSON（JavaScript Object Notation，JavaScript对象表示法）。因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，还可以直接在Web页面中读取，非常方便。 比较一下Python内置的数据类型和JSON中的表示方式： Python类型 JSON表示 dict {} list [] str \"string\" int, float 10, 1234.56 True/False true/false None null Python内置的 json 模块提供了非常完善的Python对象到JSON格式的转换方法。同样对一个 dict 进行序列化，方法如下： 1234&gt;&gt;&gt; import json&gt;&gt;&gt; d = dict(name='Bob', age=20, score=88)&gt;&gt;&gt; json.dumps(d)'{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}' dumps() 方法返回一个 str，内容就是标准的JSON。类似的，dump() 方法可以直接把JSON写入一个 file-like Object。 要把JSON反序列化为Python对象，用 loads() 或者对应的 load()方法，前者把JSON的字符串反序列化，后者从 file-like Object 中读取字符串并反序列化： 123&gt;&gt;&gt; json_str = '{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}'&gt;&gt;&gt; json.loads(json_str){'age': 20, 'score': 88, 'name': 'Bob'} 由于JSON标准规定JSON编码是UTF-8，所以我们总是能正确地在Python的 str 与JSON之间的转换。 JSON进阶 Python的 dict 对象可以直接序列化为JSON的 {}，不过很多时候，Python自带的数据结构并不足以实现我们的需求，此时我们会使用自定义的类来表示对象。比如定义一个Student类，并尝试序列化该类的实例： 12345678910import jsonclass Student(object): def __init__(self, name, age, score): self.name = name self.age = age self.score = scores = Student('Bob', 20, 88)print(json.dumps(s)) 运行代码，毫不留情地得到一个 TypeError： 123Traceback (most recent call last): ...TypeError: &lt;__main__.Student object at 0x10603cc50&gt; is not JSON serializable 错误的原因是Student对象不是一个可序列化为JSON的对象。 这样看来还是不够实用呀，别急，我们再仔细看看 dumps() 方法的参数列表，可以发现，除了第一个必须的 obj 参数外，dumps() 方法还提供了一大堆的可选参数： 1234567&gt;&gt;&gt; help(json.dumps)Help on function dumps in module json:dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, cls=None, indent=None, separators=None, default=None, sort_keys=False, **kw)... 这些可选参数就是允许我们对JSON序列化进行定制的。前面的代码之所以无法把Student类实例序列化为JSON，是因为默认情况下，dumps() 方法不知道如何将Student实例变为一个JSON的 {} 对象。 可选参数 default 允许我们传入一个可以把传入对象变得可序列化的函数，我们只需要为Student专门写一个转换函数，再把函数传进去即可，例如定义： 123456def student2dict(std): return { 'name': std.name, 'age': std.age, 'score': std.score } 这样，Student实例首先被 student2dict() 函数转换成 dict，然后再被序列化为JSON： 12&gt;&gt;&gt; print(json.dumps(s, default=student2dict)){&quot;age&quot;: 20, &quot;name&quot;: &quot;Bob&quot;, &quot;score&quot;: 88} 不过，下次如果遇到一个Teacher类的实例，我们还是无法把Teacher类的实例序列化为JSON。有没有更方便的做法呢？有的，我们可以偷个懒，利用 __dict__ 属性即可： 1print(json.dumps(s, default=lambda obj: obj.__dict__)) 通常类的实例都有一个 __dict__ 属性，它就是一个 dict。但也有少数例外，比如定义了 __slots__ 的类（这样的类没有 __dict__ 属性）。 同样的道理，如果我们要把JSON反序列化为一个Student对象实例，loads()方法会首先转换出一个 dict 对象，然后，参数 object_hook 则允许我们传入一个函数，负责把 dict 转换为Student实例： 12def dict2student(d): return Student(d['name'], d['age'], d['score']) 运行结果如下： 123&gt;&gt;&gt; json_str = '{&quot;age&quot;: 20, &quot;score&quot;: 88, &quot;name&quot;: &quot;Bob&quot;}'&gt;&gt;&gt; print(json.loads(json_str, object_hook=dict2student))&lt;__main__.Student object at 0x10cd3c190&gt; 打印出的是反序列化后的Student实例对象。 小结 Python语言特定的序列化模块是 pickle，但如果想把序列化做得更通用、更符合Web标准，可以使用 json 模块。 json 模块的 dumps() 和 loads() 函数是定义得非常好的接口的典范。当我们使用时，只需要传入一个必须的参数。但是，当默认的序列化或反序列机制不满足我们的要求时，我们又可以传入更多的参数来定制序列化或反序列化的规则，既做到了接口简单易用，又做到了充分的扩展性和灵活性。","link":"/posts/35486.html"},{"title":"进程和线程","text":"简介 多任务 现代操作系统包括Mac OS X，UNIX，Linux，Windows等，它们都是支持“多任务”的操作系统。 什么叫“多任务”呢？简单地说，就是操作系统可以同时运行多个任务。打个比方，你一边在用浏览器上网，一边在听MP3，一边在用Word赶作业，这就是多任务，至少同时有3个任务正在运行。还有很多任务悄悄地在后台同时运行着，只是桌面上没有显示而已。 现在多核CPU已经非常普及了，使用不同的核执行不同任务自然也不是什么难事。但是，即使是过去的单核CPU，也可以执行多任务。我们知道，CPU执行代码都是顺序执行的，那么单核CPU是怎么执行多任务的呢？ 答案就是操作系统轮流让各个任务交替执行，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒…… 这样反复执行下去。表面上每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们实际感觉到的就变成了所有任务都在同时执行。 真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量往往是远多于CPU的核心数量的，要给每个任务都分配一个CPU内核是不现实的，所以实际上操作系统会自动把很多任务轮流调度到每个核心上执行。 进程和线程的含义 对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。 有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的“子任务”称为线程（Thread）。 由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正同时执行多线程需要多核CPU才可能实现。 我们前面编写的所有的Python程序，都是执行单任务的进程，并且只有一个线程。如果我们要同时执行多个任务怎么办？有三种解决方案： 多进程模式：启动多个进程，每个进程只有一个线程，但多个进程可以一块执行多个任务； 多线程模式：启动一个进程，在一个进程内启动多个线程，多个线程可以一块执行多个任务； 多进程+多线程模式：启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。 注意，同时执行的多个任务之间可能是关联的，需要相互通信和协调。比方说，任务1必须先暂停等待任务2完成后才能继续执行，而任务3和任务4需要操作同一个文件所以不能同时执行。因此，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。 因为复杂度高，调试困难，所以，不是迫不得已，我们也不想编写多任务。但是，有很多时候，没有多任务还真不行。想想在电脑上看电影，就必须由一个线程播放视频，另一个线程播放音频，否则，单线程实现的话就只能先把视频播放完再播放音频，或者先把音频播放完再播放视频，这显然是不行的。 Python既支持多进程，又支持多线程，在接下来的这一章，我们会讨论如何编写这两种多任务程序。 小结 线程是最小的执行单元，进程由至少一个线程组成。如何调度进程和线程，完全由操作系统决定，程序自己不能决定自己什么时候被执行，执行多长时间。 多进程和多线程的程序涉及到同步、数据共享等问题，编写起来更复杂。 多进程 fork函数 要让Python程序实现多进程（multiprocessing），我们先得了解操作系统的相关知识。 Unix/Linux操作系统提供了一个 fork() 系统调用函数，它非常特殊。普通的函数在被调用时，调用一次只会返回一次。但是 fork() 函数调用一次会返回两次，因为此时操作系统会自动把当前进程（称为父进程）复制一份（称为子进程），然后分别在父进程和子进程内进行返回。 子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用 getppid() 就可以拿到父进程的ID。 Python的 os 模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程： 123456789import osprint('Process (%s) start...' % os.getpid())# Only works on Unix/Linux/Mac:pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I (%s) just created a child process (%s).' % (os.getpid(), pid)) 运行结果如下： 123Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876. 这段代码在执行第一个print时只有一个进程（876），因此只打印一次。然后在执行 fork() 之后，当前进程（876）被复制出一个子进程（877）。当前进程会先返回，返回子进程ID（877），然后往下走进入if-else代码块，进入else语句进行打印；其后子进程（877）返回0，同样往下走，进入if-else代码块，进入if语句进行打印。 由于Windows没有fork调用，上面的代码在Windows上无法运行。而Mac系统是基于BSD（Unix的一种）内核，所以，在Mac下运行是没有问题的~ 有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。 multiprocessing 如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？ 由于Python是跨平台的，自然也应该提供跨平台的多进程支持。multiprocessing 模块就是跨平台版本的多进程模块。 multiprocessing 模块提供了一个 Process 类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束： 1234567891011121314from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('Child process will start.') p.start() p.join() print('Child process end.') 执行结果如下： 1234Parent process 928.Process will start.Run child process test (929)...Process end. 创建子进程时，只需要传入一个执行函数和该函数的参数就可以创建一个 Process 实例。用 start() 方法就可以启动这个子进程，这样创建进程要比 fork() 更简单和灵活。 join() 方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 进程池 如果要启动大量的子进程，我们可以用进程池的方式来批量创建子进程： 1234567891011121314151617181920from multiprocessing import Poolimport os, time, random# 子进程执行的任务def long_time_task(name): print('Run task %s (%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random() * 3) end = time.time() print('Task %s runs %0.2f seconds.' % (name, (end - start)))if __name__=='__main__': print('Parent process %s.' % os.getpid()) p = Pool(4) # 创建一个大小为4的进程池 for i in range(5): # 依次创建5个子进程 p.apply_async(long_time_task, args=(i,)) print('Waiting for all subprocesses done...') p.close() p.join() print('All subprocesses done.') 执行结果如下： 12345678910111213Parent process 669.Waiting for all subprocesses done...Run task 0 (671)...Run task 1 (672)...Run task 2 (673)...Run task 3 (674)...Task 2 runs 0.14 seconds.Run task 4 (673)...Task 1 runs 0.27 seconds.Task 3 runs 0.86 seconds.Task 0 runs 1.41 seconds.Task 4 runs 1.91 seconds.All subprocesses done. 代码解读： 对 Pool 对象调用 join() 方法会等待所有子进程执行完毕，调用 join() 之前必须先调用 close()，调用 close() 之后就不能往进程池中继续添加新的 Process 了。 请注意输出的结果，task 0，1，2，3这四个子进程是立刻执行的，而task 4要等待前面某个task完成后才执行，这是因为我们定义进程池时定义了大小为4，也即最多同时执行4个进程，所以第5个进程就要等进程池里有空位了才能开始。如果改成： 1p = Pool(5) 就可以同时跑5个进程。 当没有传入参数时，Pool的默认大小是CPU的核数，所以如果电脑是4核CPU则默认进程池大小为4，如果电脑是8核CPU则默认进程池大小为8。 子进程的输入和输出 很多时候，子进程和父进程要执行的不是同一个任务。我们创建了子进程后，还需要控制子进程的输入和输出。 subprocess 模块可以让我们非常方便地启动一个子进程，并且控制其输入和输出。 下面的例子演示了如何在Python代码中运行命令 nslookup www.python.org，这和命令行直接运行的效果是一样的： 12345import subprocessprint('$ nslookup www.python.org')r = subprocess.call(['nslookup', 'www.python.org'])print('Exit code:', r) 运行结果： 12345678910$ nslookup www.python.orgServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:www.python.org canonical name = python.map.fastly.net.Name: python.map.fastly.netAddress: 199.27.79.223Exit code: 0 看看帮助文档中 subprocess 模块的 call() 函数的描述： Run command with arguments. Wait for command to complete or timeout, then return the returncode attribute. 它可以帮助我们建立一个子进程来执行系统调用函数，并且允许传入函数，调用后会等待运行结束并最终返回调用函数的返回值，之后才继续执行后续代码。可以只传入一个列表，列表第一个元素为系统调用的名称，第二个元素为参数。 如果子进程执行的过程中还需要其他输入，我们使用 subprocess 模块的 Popen 类初始化子进程，并通过它的 communicate() 方法来实现进程执行过程中的多次输入： 1234567import subprocessprint('$ nslookup')p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)output, err = p.communicate(b'set q=mx\\npython.org\\nexit\\n') # 每次输入之间用换行符隔开print(output.decode('utf-8')) # 返回的是字节流，所以要先解码print('Exit code:', p.returncode) 上面的代码相当于在命令行执行命令 nslookup，然后手动输入： 123set q=mxpython.orgexit 运行结果如下： 12345678910111213$ nslookupServer: 192.168.19.4Address: 192.168.19.4#53Non-authoritative answer:python.org mail exchanger = 50 mail.python.org.Authoritative answers can be found from:mail.python.org internet address = 82.94.164.166mail.python.org has AAAA address 2001:888:2000:d::a6Exit code: 0 进程间通信 Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的 multiprocessing 模块包装了底层的机制，提供了 Queue、Pipes 等多种方式来交换数据。 我们以 Queue 为例，在父进程中创建两个子进程，第一个子进程往 Queue 里写数据，第二个子进程从 Queue 里读数据： 12345678910111213141516171819202122232425262728293031from multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q): print('Process to write: %s' % os.getpid()) for value in ['A', 'B', 'C']: print('Put %s to queue...' % value) q.put(value) time.sleep(random.random())# 读数据进程执行的代码:def read(q): print('Process to read: %s' % os.getpid()) while True: value = q.get(True) print('Get %s from queue.' % value)if __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 启动子进程pr，读取: pr.start() # 等待pw结束: pw.join() # pr进程里是死循环，无法等待其结束，只能强行终止: pr.terminate() 运行结果如下： 12345678Process to write: 50563Put A to queue...Process to read: 50564Get A from queue.Put B to queue...Get B from queue.Put C to queue...Get C from queue. 在Unix/Linux下，multiprocessing 模块封装了 fork() 调用，使我们不需要关注 fork() 的细节。由于Windows没有fork调用，因此，multiprocessing 需要“模拟”出fork的效果，父进程的所有Python对象都必须先通过 pickle 序列化再传到子进程去。所以，如果 multiprocessing 在Windows下调用失败了，就要先考虑是不是序列化失败了。 小结 在Unix/Linux下，可以使用 fork() 调用实现多进程。 要实现跨平台的多进程，可以使用 multiprocessing 模块。 进程间通信可以通过 Queue、Pipes 等实现的。 多线程 多线程简单实现 多任务可以由多进程完成，也可以由一个进程内的多线程完成。 我们前面提到了进程是由若干线程组成的，一个进程至少有一个线程。 由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。 Python的标准库提供了两个模块：_thread 和 threading，_thread 是低级模块，threading是高级模块，对 _thread 进行了封装。绝大多数情况下，我们只需要使用 threading 这个高级模块。 使用 threading 时，我们可以创建 Thread 实例来建立新的线程，然后调用 start() 启动该线程： 1234567891011121314151617import time, threading# 新线程执行的代码:def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name)print('thread %s is running...' % threading.current_thread().name)t = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print('thread %s ended.' % threading.current_thread().name) 执行结果如下： 123456789thread MainThread is running...thread LoopThread is running...thread LoopThread &gt;&gt;&gt; 1thread LoopThread &gt;&gt;&gt; 2thread LoopThread &gt;&gt;&gt; 3thread LoopThread &gt;&gt;&gt; 4thread LoopThread &gt;&gt;&gt; 5thread LoopThread ended.thread MainThread ended. 由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的 threading 模块有个 current_thread() 函数，它永远返回当前线程的实例。主线程实例的名字叫 MainThread，子线程的名字在创建时指定，我们用 LoopThread 命名子线程。名字仅仅在打印时用来显示，没有其他意义，如果不起名字Python就会自动给线程命名为 Thread-1，Thread-2…… 线程锁 多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响。而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。 来看看多个线程同时操作一个变量怎么把内容给改乱了： 12345678910111213141516171819202122import time, threading# 假定这是你的银行存款:balance = 0def change_it(n): # 先加后减，结果应该为0: global balance balance = balance + n balance = balance - ndef run_thread(n): for i in range(100000): change_it(n)t1 = threading.Thread(target=run_thread, args=(5,))t2 = threading.Thread(target=run_thread, args=(8,))t1.start()t2.start()t1.join()t2.join()print(balance) 注意，change_it() 函数中，使用了global关键字来声明使用的 balance 变量是在函数定义外部的，这样在函数内部的修改也会反映到函数外部。 balance，初始值为0，并且启动两个线程，先加后减，理论上结果应该为0，但是，由于线程的调度是由操作系统决定的，当t1、t2交替执行时，只要循环次数足够多，在某个时刻，线程t1和t2同时对 balance 变量进行修改，那么最终结果就不一定是0了。比方说： 时间 线程 balance 操作 00 t1 0 +5 01 t1 5 -5 02 t2 0 +8 03 t2 8 -8 04 t1 0 +5 05 t2 5 +8 06 t1 13 -8 07 t2 5 -5 08 t1,t2 0 +5,+8 09 t1 8 -5 10 t2 3 -8 11 t1 -5 +5 12 t2 0 +8 13 t1 8 -5 14 t2 3 -8 假设在08时刻，t1和t2同时执行 balance = balance + n 这个语句，右边的 balance 都取0，那么t1执行完该语句时 balance 为5，而t2执行完时 balance 为8，假设t2后执行完毕，那么 balance 就取8，这时就产生错误了，并且错误会继续累积，当循环次数达到一定规模时，这种错误的累积会非常可怕。 为什么会产生这样的错误呢？其实 呀，这时因为高级语言的一条语句在CPU执行时需要转换为多条汇编指令，即使一个简单的计算： 1balance = balance + n 也需要分为两步： 计算 balance + n，存入临时变量中； 将临时变量的值赋给 balance。 可以看成： 12x = balance + nbalance = x 临时变量 x 是一个局部变量，两个线程各自都有自己的 x，所以当代码正常执行时： 初始值 balance = 0 123456789t1: x1 = balance + 5 # x1 = 0 + 5 = 5t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t2: x2 = balance - 8 # x2 = 8 - 8 = 0t2: balance = x2 # balance = 0 结果 balance = 0 但是t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2： 初始值 balance = 0 1234567891011t1: x1 = balance + 5 # x1 = 0 + 5 = 5t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance - 8 # x2 = 0 - 8 = -8t2: balance = x2 # balance = -8 结果 balance = -8，自然就不对了 究其原因，是因为修改 balance 需要多条语句，而执行这几条语句时，线程可能中断，其他线程可能也会对 balance 进行了修改，从而导致多个线程把同一个对象的内容改乱了。所以，我们必须确保一个线程在修改 balance 的时候，别的线程一定不能改。 如果我们要确保 balance 计算正确，就要给 change_it() 上一把锁，当某个线程开始执行 change_it() 时，我们说，该线程获得了锁，因此其他线程不能同时执行 change_it()，只能等待锁被释放，线程获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁可以通过 threading.Lock() 来实现： 12345678910111213balance = 0lock = threading.Lock()def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() 当多个线程同时执行 lock.acquire() 时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。 获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用 try...finally 来确保锁一定会被释放。 锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以构造多个不同的锁，不同的线程持有不同的锁，在试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。 死循环与多核CPU 如果你拥有一个多核CPU，肯定就会想到不同的核应该可以同时执行不同的多个线程。 如果写一个死循环的话，会出现什么情况呢？ 打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。我们可以监控到一个死循环线程会100%占用一个CPU。如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。 试试用Python写个死循环： 12345678910import threading, multiprocessingdef loop(): x = 0 while True: x = x ^ 1for i in range(multiprocessing.cpu_count()): t = threading.Thread(target=loop) t.start() 启动与CPU核心数量相同的N个线程，可以监控到CPU占用率仅有102%，也就是仅使用了一核。但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？ 因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。 GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。 不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 小结 多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。 Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程并发在Python中就是一个美丽的梦。 ThreadLocal 局部变量的传递 在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。 但是局部变量也有问题，就是在函数调用的时候，传递起来很麻烦： 12345678910111213def process_student(name): std = Student(name) # std是局部变量，但是每个函数都要用它，因此必须传进去： do_task_1(std) do_task_2(std)def do_task_1(std): do_subtask_1(std) do_subtask_2(std)def do_task_2(std): do_subtask_2(std) do_subtask_2(std) 每个函数一层一层调用都这么传参数那还得了？用全局变量？也不行，因为每个线程处理不同的 Student 对象，所以不能共享。 如果用一个全局的 dict 存放所有的 Student 对象，以线程自身作为 key 和存取对应的 Student 对象如何呢？ 123456789101112131415161718global_dict = {}def std_thread(name): std = Student(name) # 把std放到全局变量global_dict中： global_dict[threading.current_thread()] = std do_task_1() do_task_2()def do_task_1(): # 不传入std，而是根据当前线程查找： std = global_dict[threading.current_thread()] ...def do_task_2(): # 任何函数都可以查找出当前线程的std变量： std = global_dict[threading.current_thread()] ... 这种方式理论上是可行的，它最大的优点是消除了 std 对象在每层函数中的传递问题，但是，每个函数获取 std 的代码有点丑。 ThreadLocal的用法 有没有更简单的方式？ThreadLocal 应运而生，不用查找 dict，ThreadLocal 可以自动帮我们做这件事： 123456789101112131415161718192021import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): # 获取当前线程关联的student: std = local_school.student print('Hello, %s (in %s)' % (std, threading.current_thread().name))def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 执行结果： 12Hello, Alice (in Thread-A)Hello, Bob (in Thread-B) 全局变量 local_school 就是一个 ThreadLocal 对象，每个线程都可以对它进行读写，线程之间互不影响。你可以把 local_school 看成全局变量，但每个属性如 local_school.student 都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal 内部会处理。也可以把 local_school 理解为一个全局的 dict，不但可以绑定 local_school.student，还可以绑定其他变量，如 local_school.teacher 等等。 ThreadLocal 最常用的地方就是为每个线程绑定一个不同的数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 小结 ThreadLocal 虽然是全局的，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。 进程 vs. 线程 比较多进程和多线程 我们介绍了多进程和多线程，这是实现多任务最常用的两种方式。现在，我们来讨论一下这两种方式的优缺点。 首先，要实现多任务，通常我们会设计主从模式（Master-Worker模式），Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。 如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。 如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。 多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。 多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。 多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。 在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。 效率问题 无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？ 我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。 如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时。这种每次只完成一个任务的方式称为单任务模型，或者批处理任务模型。 假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。 但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。 所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。 计算密集型 vs. IO密集型 是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。 计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。 计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。 第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。 IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 异步IO 考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。 现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。 所谓异步IO，其实这个异步是指使用者发起IO请求后并不马上得到结果。比方说有一些需要处理的数据放在磁盘上，使用者预先知道这些数据的位置，所以预先发起异步IO读请求。而等到真正需要用到这些数据的时候，再等待异步IO完成，对数据进行处理。这样做的话，在发起IO请求到实际使用数据这段时间内，程序还可以继续做其他事情。 对应到Python语言，单进程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。 分布式进程 简介 在线程和进程中，应当优先选择进程，因为进程更稳定，而且，进程可以分布到多台机器上，而线程最多只能分布到同一台机器的多个CPU上。 Python的 multiprocessing 模块不但支持多进程，其中 managers 子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于 managers 模块封装很好，我们不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。 举个例子：如果我们已经有一个通过 Queue 通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？ 原有的 Queue 可以继续使用，但是，通过 managers 模块把 Queue 通过网络暴露出去，就可以让其他机器的进程访问 Queue 了。 实现方法 我们先看服务进程，服务进程负责启动 Queue，把 Queue 注册到网络上，然后往 Queue 里面写入任务： 12345678910111213141516171819202122232425262728293031323334353637# task_master.pyimport random, time, queuefrom multiprocessing.managers import BaseManager# 发送任务的队列:task_queue = queue.Queue()# 接收结果的队列:result_queue = queue.Queue()# 从BaseManager继承的QueueManager:class QueueManager(BaseManager): pass# 把两个Queue都注册到网络上, callable参数关联了Queue对象:QueueManager.register('get_task_queue', callable=lambda: task_queue)QueueManager.register('get_result_queue', callable=lambda: result_queue)# 绑定端口5000, 设置验证码'abc':manager = QueueManager(address=('', 5000), authkey=b'abc')# 启动Queue:manager.start()# 获得通过网络访问的Queue对象:task = manager.get_task_queue()result = manager.get_result_queue()# 放几个任务进去:for i in range(10): n = random.randint(0, 10000) print('Put task %d...' % n) task.put(n)# 从result队列读取结果:print('Try get results...')for i in range(10): r = result.get(timeout=10) print('Result: %s' % r)# 关闭:manager.shutdown()print('master exit.') 请注意，当我们在一台机器上写多进程程序时，创建的 Queue 可以直接拿来用，但是，在分布式多进程环境下，添加任务到 Queue 不可以直接对原始的 task_queue 进行操作，那样就绕过了 QueueManager 的封装，必须通过 manager.get_task_queue() 获得的 Queue 接口添加任务。 然后，在另一台机器上启动任务进程（本机上启动也可以）： 1234567891011121314151617181920212223242526272829303132333435# task_worker.pyimport time, sys, queuefrom multiprocessing.managers import BaseManager# 创建类似的QueueManager:class QueueManager(BaseManager): pass# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:QueueManager.register('get_task_queue')QueueManager.register('get_result_queue')# 连接到服务器，也就是运行task_master.py的机器:server_addr = '127.0.0.1' # 这里因为worker用的也是本机，所以写回送地址就可以了print('Connect to server %s...' % server_addr)# 端口和验证码注意保持与task_master.py设置的完全一致:m = QueueManager(address=(server_addr, 5000), authkey=b'abc')# 从网络连接:m.connect()# 获取Queue的对象:task = m.get_task_queue()result = m.get_result_queue()# 从task队列取任务,并把结果写入result队列:for i in range(10): try: n = task.get(timeout=1) print('run task %d * %d...' % (n, n)) r = '%d * %d = %d' % (n, n, n*n) time.sleep(1) result.put(r) except Queue.Empty: print('task queue is empty.')# 处理结束:print('worker exit.') 任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。 现在，可以试试分布式进程的工作效果了。先启动 task_master.py 服务进程： 123456789101112$ python3 task_master.pyPut task 3411...Put task 1605...Put task 1398...Put task 4729...Put task 5300...Put task 7471...Put task 68...Put task 4219...Put task 339...Put task 7866...Try get results... task_master.py 进程发送完任务后，开始等待 result 队列的结果。现在启动 task_worker.py 进程： 12345678910111213$ python3 task_worker.pyConnect to server 127.0.0.1...run task 3411 * 3411...run task 1605 * 1605...run task 1398 * 1398...run task 4729 * 4729...run task 5300 * 5300...run task 7471 * 7471...run task 68 * 68...run task 4219 * 4219...run task 339 * 339...run task 7866 * 7866...worker exit. task_worker.py 进程结束，在 task_master.py 进程中会继续打印出结果： 12345678910Result: 3411 * 3411 = 11634921Result: 1605 * 1605 = 2576025Result: 1398 * 1398 = 1954404Result: 4729 * 4729 = 22363441Result: 5300 * 5300 = 28090000Result: 7471 * 7471 = 55815841Result: 68 * 68 = 4624Result: 4219 * 4219 = 17799961Result: 339 * 339 = 114921Result: 7866 * 7866 = 61873956 这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算 n*n 的代码换成发送邮件，就实现了邮件队列的异步发送。 Queue 对象存储在哪？注意到 task_worker.py 中根本没有创建 Queue 的代码，所以，Queue 对象是存储在 task_master.py 进程中的： task_master_worker 而 Queue 之所以能通过网络访问，就是通过 QueueManager 实现的。由于 QueueManager 管理的不止一个 Queue，所以，要给每个 Queue 的网络调用接口起个名字，比如 get_task_queue。 authkey 有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果 task_worker.py 的 authkey 和 task_master.py 的 authkey 不一致，肯定连接不上。 小结 Python的分布式进程接口简单，封装良好，适合需要把繁重任务分布到多台机器的环境下。 注意 Queue 的作用是用来传递任务和接收结果，每个任务的描述数据要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件。","link":"/posts/4339.html"},{"title":"常用内建模块（下）","text":"Python之所以自称 “batteries included”，就是因为内置了许多非常有用的模块，无需额外安装和配置，即可直接使用。 本章将介绍一些常用的内建模块。 itertools 简介 Python的内建模块 itertools 提供了非常有用的用于操作迭代对象的函数。我们首先看看 itertools 提供的几个“无限”迭代器： count count() 返回的是一个无限的迭代器，默认初始值为0，步长为1（按Python的传参规则，只传入一个参数时，传入的参数被视作初始值） 123456789&gt;&gt;&gt; import itertools&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; for n in natuals:... print(n)...123... 上述代码会打印出自然数序列，但问题是它根本停不下来，只能按 Ctrl+C 退出。 cycle cycle() 会把传入的一个序列无限重复下去： 123456789101112&gt;&gt;&gt; import itertools&gt;&gt;&gt; cs = itertools.cycle('ABC') # 注意字符串也是序列的一种，还可以是列表、元组等&gt;&gt;&gt; for c in cs:... print(c)...'A''B''C''A''B''C'... 同样停不下来。 repeat repeat() 负责把一个元素无限重复下去，不过 repeat() 提供了第二个参数，用于限定重复的次数： 1234567&gt;&gt;&gt; ns = itertools.repeat('A', 3)&gt;&gt;&gt; for n in ns:... print(n)...AAA takewhile 前面介绍了几种产生“无限”迭代器的方法，有没有办法对它们进行控制呢？有的~我们可以通过 takewhile()等函数，根据条件判断来截取出有限的序列： 1234&gt;&gt;&gt; natuals = itertools.count(1)&gt;&gt;&gt; ns = itertools.takewhile(lambda x: x &lt;= 10, natuals)&gt;&gt;&gt; list(ns)[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 这里我们只截取序列中小于等于10的数，所以迭代器产生的数不符合该条件时就会停止迭代，也就不会无限地排列下去了。 除了 takewhile() 之外，itertools 还提供了一些非常有用的迭代器操作函数，在后面几个小节中会进行介绍。 chain chain() 可以把一组迭代对象串联起来，形成一个更大的迭代器： 123456789&gt;&gt;&gt; for c in itertools.chain('ABC', 'XYZ'):... print(c)...ABCXYZ groupby groupby() 可以把迭代器中相邻的重复元素挑出来放在一起： 1234567&gt;&gt;&gt; for key, group in itertools.groupby('AAABBBCCAAA'):... print(key, list(group))...A ['A', 'A', 'A']B ['B', 'B', 'B']C ['C', 'C']A ['A', 'A', 'A'] 实际上挑选规则是通过函数完成的，只要作用于函数的两个元素返回的值相等，这两个元素就被认为是同一组的，而函数返回值将作为该组的key。如果我们想忽略大小写来分组，可以让元素 'A' 和 'a' 都返回相同的key： 1234567&gt;&gt;&gt; for key, group in itertools.groupby('AaaBBbcCAAa', lambda c: c.upper()):... print(key, list(group))...A ['A', 'a', 'a']B ['B', 'B', 'b']C ['c', 'C']A ['A', 'A', 'a'] 小结 itertools 模块提供的全部是处理迭代功能的函数，它们的返回值不是 list，而是 Iterator，也即它们的值不是立刻计算出放在内存中的，只有进行迭代时（例如使用 for 循环）才会被真正计算出来。 contextlib 引言 在Python中，读写文件要注意使用完毕后必须进行关闭（文件对象占用大量资源并且同一时间操作系统只能打开有限数量的文件）。在09IO编程中，已经介绍了利用 try...finally 机制关闭文件资源的方法： 123456try: f = open('/path/to/file', 'r') f.read()finally: if f: f.close() 但是，写 try...finally 非常繁琐，所以后续又介绍了使用 with 语句的方法。with 语句允许我们非常方便地使用资源，而不必担心资源没有关闭。使用 with 语句改写后，上面的代码就可以简化为： 12with open('/path/to/file', 'r') as f: f.read() 事实上，并不是只有 open() 函数返回的文件对象才能使用 with 语句。任何对象，只要正确实现了上下文管理，就可以用于 with 语句。 上下文管理的实现 上下文管理是通过 __enter__ 和 __exit__ 这两个方法实现的。下面的类就实现了这两个方法： 1234567891011121314151617class Query(object): def __init__(self, name): self.name = name def __enter__(self): print('Begin') return self def __exit__(self, exc_type, exc_value, traceback): if exc_type: print('Error') else: print('End') def query(self): print('Query info about %s...' % self.name) 这样我们就可以把自己写的资源对象用于 with 语句： 12with Query('Bob') as q: q.query() @contextmanager装饰器 编写 __enter__ 和 __exit__ 还是太繁琐了，有没有更简单的办法呢？有！Python的标准库 contextlib 提供了更简单的写法，借助它，上面的代码可以改写为： 1234567891011121314from contextlib import contextmanagerclass Query(object): def __init__(self, name): self.name = name def query(self): print('Query info about %s...' % self.name)@contextmanagerdef create_query(name): print('Begin') q = Query(name) yield q print('End') 简单解析一下，我们定义一个简单的 Query 类，只有一个 query() 方法。同时我们定义了一个 create_query() 函数，由于这个函数包含 yield 关键字，所以实际上它是一个生成器。不过这个生成器只生成和抛出一个 Query 类的对象。 @contextmanager 这个装饰器接收一个生成器，并为生成器抛出的对象添加上下文管理的功能。这样 with 语句就可以正常地工作了： 12with create_query('Bob') as q: q.query() 很多时候，我们希望在某段代码执行前后自动执行特定代码，也可以用 @contextmanager 实现。例如： 123456789@contextmanagerdef tag(name): print(&quot;&lt;%s&gt;&quot; % name) yield print(&quot;&lt;/%s&gt;&quot; % name)with tag(&quot;h1&quot;): print(&quot;hello&quot;) print(&quot;world&quot;) 上述代码执行结果为： 1234&lt;h1&gt;helloworld&lt;/h1&gt; 代码的执行顺序是： with 语句首先执行 yield 前面的语句，因此打印出 &lt;h1&gt;； yield 之后会跳出生成器（tag() 函数），执行 with 语句内部的所有语句，因此打印出 hello 和 world； 执行完 with 语句内部的所有语句继续回到生成器； 执行 yield 后面的语句，打印出 &lt;/h1&gt;； 此时生成器所有语句执行完毕，不再生成，结束上下文。 借助 @contextmanager 装饰器，我们能够更加方便地实现上下文管理。 closing函数 前面一节介绍了如何为一个对象实现上下文管理功能，使得它能被作用于 with 语句。但是，得自己编写一个生成器还是很麻烦！有没有更更方便的办法呢？有！我们可以用 closing() 方法！ closing() 的本质如下： 123456@contextmanagerdef closing(thing): try: yield thing finally: thing.close() 其实它就是一个经过 @contextmanager 装饰的生成器，它的作用就是把任意对象变为上下文对象，使其支持 with 语句。 再改写一次上面 Query 的例子： 12345678910from contextlib import closingclass Query(object): def __init__(self, name): self.name = name def query(self): print('Query info about %s...' % self.name)with closing(Query('Bob')) as q: q.query() 这次更加简单了~ @contextlib 还有一些其他装饰器，可以帮助我们编写更简洁的代码。 XML 简介 XML虽然比JSON复杂，在Web中应用也不如以前多了，不过仍然有很多地方会用到XML，所以我们有必要了解如何在Python中如何处理XML。 DOM vs SAX 一般来说，处理XML有两种方法，即DOM和SAX： DOM会先把整个XML读入内存，然后解析为树，因此DOM占用的内存大，解析慢。优点是可以任意遍历树的节点。 SAX则是流模式，边读边解析，占用内存小，解析快，缺点是我们需要自己处理事件。 正常情况下，优先考虑SAX，因为DOM实在太占内存。 在Python中使用SAX 在Python中使用SAX解析XML非常简洁，通常我们需要关心3个事件：start_element，end_element 和 char_data，准备好处理这3个事件的函数后就可以解析XML了。那么这些事件到底是什么意思呢？举个例子，当SAX解析器读到一个节点时： 1&lt;a href=&quot;/&quot;&gt;python&lt;/a&gt; 会产生3个事件： start_element 事件：读取 &lt;a href=\"/\"&gt; 时； char_data 事件：读取 python 时； end_element 事件：读取 &lt;/a&gt; 时。 首先实现好处理这3个事件的函数： 12345678910111213from xml.parsers.expat import ParserCreatedef start_element(name, attrs): print('Start element:', name, 'with attributes:', attrs)def end_element(name): print('End element:', name)# 使用repr()函数可以将字符串转换为可打印的表示方式# 这样就能更清楚地观察到空白字符了def char_data(data): print('Character data:', repr(data)) 然后创建解析器： 1234p = ParserCreate()p.StartElementHandler = start_elementp.EndElementHandler = end_elementp.CharacterDataHandler = char_data 尝试解析一个XML字符串： 12345678xml = r'''&lt;?xml version=&quot;1.0&quot;?&gt;&lt;ol&gt; &lt;li&gt;&lt;a href=&quot;/python&quot;&gt;Python&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/ruby&quot;&gt;Ruby&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;'''print(p.Parse(xml)) 执行结果： 1234567891011121314151617Start element: ol with attributes: {}Character data: '\\n'Character data: ' 'Start element: li with attributes: {}Start element: a with attributes: {'href': '/python'}Character data: 'Python'End element: aEnd element: liCharacter data: '\\n'Character data: ' 'Start element: li with attributes: {}Start element: a with attributes: {'href': '/ruby'}Character data: 'Ruby'End element: aEnd element: liCharacter data: '\\n'End element: ol 注意，遇到换行符之后，即使后续还有其他内容，char_data 事件也会结束再被触发。因此，读取一大段文本时，CharacterDataHandler 可能会被多次调用，如果我们想要将文本放在一起输出而非分开输出，就要先保存下来，在 EndElementHandler 中再进行合并。 除了解析XML外，我们要如何生成XML呢？99%的情况下需要生成的XML结构都是非常简单的，因此，最简单也最有效的生成XML的方法就是拼接字符串： 123456L = []L.append(r'&lt;?xml version=&quot;1.0&quot;?&gt;')L.append(r'&lt;root&gt;')L.append(encode('some &amp; data'))L.append(r'&lt;/root&gt;')return ''.join(L) 注意，在使用XML字符串时，我们最好使用 r 表示该字符串不进行转义，三引号表示保留换行，从而避免一些不必要的错误和麻烦。 如果要生成复杂的XML呢？这时建议不要用XML，而是改成用JSON。 小结 解析XML时，注意找出自己感兴趣的节点，响应事件时，可以先把节点中的数据保存起来，等待解析完毕后，再进行处理。对这一章所用模块知识感兴趣的话可以查看官方文档。此外，觉得自带的XML库不够给力的话可以使用更为强大的第三方库lxml。 练习 编写程序使用SAX解析Yahoo天气RSS的XML格式天气预报，获取地点、当天天气和次日天气： 由于现在Yahoo天气已经不再提供这个RSS服务了，所以链接已经失效了。这里我们直接解析一个廖老师提供好的XML字符串： 123456789101112131415161718192021222324data = r'''&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot; ?&gt;&lt;rss version=&quot;2.0&quot; xmlns:yweather=&quot;http://xml.weather.yahoo.com/ns/rss/1.0&quot; xmlns:geo=&quot;http://www.w3.org/2003/01/geo/wgs84_pos#&quot;&gt; &lt;channel&gt; &lt;title&gt;Yahoo! Weather - Beijing, CN&lt;/title&gt; &lt;lastBuildDate&gt;Wed, 27 May 2015 11:00 am CST&lt;/lastBuildDate&gt; &lt;yweather:location city=&quot;Beijing&quot; region=&quot;&quot; country=&quot;China&quot;/&gt; &lt;yweather:units temperature=&quot;C&quot; distance=&quot;km&quot; pressure=&quot;mb&quot; speed=&quot;km/h&quot;/&gt; &lt;yweather:wind chill=&quot;28&quot; direction=&quot;180&quot; speed=&quot;14.48&quot; /&gt; &lt;yweather:atmosphere humidity=&quot;53&quot; visibility=&quot;2.61&quot; pressure=&quot;1006.1&quot; rising=&quot;0&quot; /&gt; &lt;yweather:astronomy sunrise=&quot;4:51 am&quot; sunset=&quot;7:32 pm&quot;/&gt; &lt;item&gt; &lt;geo:lat&gt;39.91&lt;/geo:lat&gt; &lt;geo:long&gt;116.39&lt;/geo:long&gt; &lt;pubDate&gt;Wed, 27 May 2015 11:00 am CST&lt;/pubDate&gt; &lt;yweather:condition text=&quot;Haze&quot; code=&quot;21&quot; temp=&quot;28&quot; date=&quot;Wed, 27 May 2015 11:00 am CST&quot; /&gt; &lt;yweather:forecast day=&quot;Wed&quot; date=&quot;27 May 2015&quot; low=&quot;20&quot; high=&quot;33&quot; text=&quot;Partly Cloudy&quot; code=&quot;30&quot; /&gt; &lt;yweather:forecast day=&quot;Thu&quot; date=&quot;28 May 2015&quot; low=&quot;21&quot; high=&quot;34&quot; text=&quot;Sunny&quot; code=&quot;32&quot; /&gt; &lt;yweather:forecast day=&quot;Fri&quot; date=&quot;29 May 2015&quot; low=&quot;18&quot; high=&quot;25&quot; text=&quot;AM Showers&quot; code=&quot;39&quot; /&gt; &lt;yweather:forecast day=&quot;Sat&quot; date=&quot;30 May 2015&quot; low=&quot;18&quot; high=&quot;32&quot; text=&quot;Sunny&quot; code=&quot;32&quot; /&gt; &lt;yweather:forecast day=&quot;Sun&quot; date=&quot;31 May 2015&quot; low=&quot;20&quot; high=&quot;37&quot; text=&quot;Sunny&quot; code=&quot;32&quot; /&gt; &lt;/item&gt; &lt;/channel&gt;&lt;/rss&gt;''' 我们需要的信息有三样，分别是地点、当天天气和次日天气。在这段XML中，地点可以从 yweather:location 标签的 city 属性和 country 属性中获得。当天天气从第一个 yweather:forecast 标签的 text、low 和 high 这三个属性获得。次日天气则在第二个 yweather:forecast 标签中。并且注意到，我们只需要编写处理 start_element 事件的函数就可以取出所有这些信息了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445# -*- coding:utf-8 -*-from xml.parsers.expat import ParserCreateclass WeatherSaxHandler(object): def __init__(self): self.result = dict() self.count = 0 self.result['forecast'] = dict() def start_element(self, name, attrs): if name == 'yweather:location': self.result['city'] = attrs['city'] self.result['country'] = attrs['country'] elif name == 'yweather:forecast': if self.count == 0: self.result['forecast']['today'] = attrs self.count += 1 elif self.count == 1: self.result['forecast']['tomorrow'] = attrs self.count += 1def parse_weather(data): handler = WeatherSaxHandler() p = ParserCreate() p.StartElementHandler = handler.start_element p.Parse(data) return { 'city': handler.result['city'], 'country': handler.result['country'], 'today': { 'text': handler.result['forecast']['today']['text'], 'low': int(handler.result['forecast']['today']['low']), 'high': int(handler.result['forecast']['today']['high']) }, 'tomorrow': { 'text': handler.result['forecast']['tomorrow']['text'], 'low': int(handler.result['forecast']['tomorrow']['low']), 'high': int(handler.result['forecast']['tomorrow']['high']) } } 123456789101112# 测试:weather = parse_weather(data)assert weather['city'] == 'Beijing', weather['city']assert weather['country'] == 'China', weather['country']assert weather['today']['text'] == 'Partly Cloudy', weather['today']['text']assert weather['today']['low'] == 20, weather['today']['low']assert weather['today']['high'] == 33, weather['today']['high']assert weather['tomorrow']['text'] == 'Sunny', weather['tomorrow']['text']assert weather['tomorrow']['low'] == 21, weather['tomorrow']['low']assert weather['tomorrow']['high'] == 34, weather['tomorrow']['high']print('Weather:', str(weather)) HTMLParser 简介 如果我们要编写一个搜索引擎，第一步是用爬虫把目标网站的页面抓下来，第二步就是解析该HTML页面，看看里面的内容到底是新闻、图片还是视频。 假设第一步已经完成了，第二步我们应该如何解析HTML呢？ HTML本质上是XML的子集，但是HTML的语法没有XML那么严格，所以不能用标准的DOM或SAX来解析HTML。 好在Python提供了 HTMLParser 模块帮助我们解析HTML，非常方便，只需简单几行代码即可完成。 HTML字符实体 在学习 HTMLParser 之前，我们需要首先了解一下HTML需要注意的地方。在HTML中，某些字符是预留的。 在HTML中不能使用小于号（&lt;）和大于号（&gt;），这是因为浏览器会误认为它们是标签符号。 如果希望正确地显示预留字符，我们必须在HTML的源代码中使用字符实体（character entities）。 具体来说，常用的字符实体如下： 显示结果 描述 实体名称 实体编号 &lt;tr&gt; &lt;td&gt;&amp;nbsp;&lt;/td&gt; &lt;td&gt;空格&lt;/td&gt; &lt;td&gt;&amp;amp;nbsp;&lt;/td&gt; &lt;td&gt;&amp;amp;#160;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&amp;lt;&lt;/td&gt; &lt;td&gt;小于号&lt;/td&gt; &lt;td&gt;&amp;amp;lt;&lt;/td&gt; &lt;td&gt;&amp;amp;#60;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&amp;gt;&lt;/td&gt; &lt;td&gt;大于号&lt;/td&gt; &lt;td&gt;&amp;amp;gt;&lt;/td&gt; &lt;td&gt;&amp;amp;#62;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&amp;amp;&lt;/td&gt; &lt;td&gt;和号&lt;/td&gt; &lt;td&gt;&amp;amp;amp;&lt;/td&gt; &lt;td&gt;&amp;amp;#38;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&quot;&lt;/td&gt; &lt;td&gt;引号&lt;/td&gt; &lt;td&gt;&amp;amp;quot;&lt;/td&gt; &lt;td&gt;&amp;amp;#34;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;'&lt;/td&gt; &lt;td&gt;撇号&amp;nbsp;&lt;/td&gt; &lt;td&gt;&amp;amp;apos; (IE不支持)&lt;/td&gt; &lt;td&gt;&amp;amp;#39;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;￠&lt;/td&gt; &lt;td&gt;分（cent）&lt;/td&gt; &lt;td&gt;&amp;amp;cent;&lt;/td&gt; &lt;td&gt;&amp;amp;#162;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;£&lt;/td&gt; &lt;td&gt;镑（pound）&lt;/td&gt; &lt;td&gt;&amp;amp;pound;&lt;/td&gt; &lt;td&gt;&amp;amp;#163;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;¥&lt;/td&gt; &lt;td&gt;元（yen）&lt;/td&gt; &lt;td&gt;&amp;amp;yen;&lt;/td&gt; &lt;td&gt;&amp;amp;#165;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;€&lt;/td&gt; &lt;td&gt;欧元（euro）&lt;/td&gt; &lt;td&gt;&amp;amp;euro;&lt;/td&gt; &lt;td&gt;&amp;amp;#8364;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;§&lt;/td&gt; &lt;td&gt;小节&lt;/td&gt; &lt;td&gt;&amp;amp;sect;&lt;/td&gt; &lt;td&gt;&amp;amp;#167;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;©&lt;/td&gt; &lt;td&gt;版权（copyright）&lt;/td&gt; &lt;td&gt;&amp;amp;copy;&lt;/td&gt; &lt;td&gt;&amp;amp;#169;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;®&lt;/td&gt; &lt;td&gt;注册商标&lt;/td&gt; &lt;td&gt;&amp;amp;reg;&lt;/td&gt; &lt;td&gt;&amp;amp;#174;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;™&lt;/td&gt; &lt;td&gt;商标&lt;/td&gt; &lt;td&gt;&amp;amp;trade;&lt;/td&gt; &lt;td&gt;&amp;amp;#8482;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;×&lt;/td&gt; &lt;td&gt;乘号&lt;/td&gt; &lt;td&gt;&amp;amp;times;&lt;/td&gt; &lt;td&gt;&amp;amp;#215;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;÷&lt;/td&gt; &lt;td&gt;除号&lt;/td&gt; &lt;td&gt;&amp;amp;divide;&lt;/td&gt; &lt;td&gt;&amp;amp;#247;&lt;/td&gt; &lt;/tr&gt; 完整的字符实体表可以查看W3School的HTML 实体符号参考手册。 注意到表格中有实体名称和实体编号两种形式，在编写HTML代码时这两种形式都是可以使用的。即字符实体既可以写作 &amp;entity_name; 的形式，也可以写作 &amp;#entity_number; 的形式。比如需要显示小于号时可以写作 &amp;lt; 也可以写作 &amp;#60;。使用实体名而不是编号的好处是，名称更易于记忆。不过坏处是，浏览器也许并不支持所有实体名称（但对实体编号的支持却很好）。特别地，实体编号可以写作十进制形式，也可以写作十六进制形式，&amp;#60 等价于 &amp;#x3C。 使用HTMLParser解析HTML 类似于XML的SAX解析方法，使用 HTMLParser 解析HTML时，我们只需要为不同的事件编写相应的处理函数就可以了。以下面的代码为例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243from html.parser import HTMLParserfrom html.entities import name2codepointclass MyHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): print('This is a start tag: %s' % tag) def handle_endtag(self, tag): print('This is an end tag: %s' % tag) def handle_startendtag(self, tag, attrs): print('This is a start-end tag: %s' % tag) def handle_data(self, data): print('This is data:', repr(data)) def handle_comment(self, data): print('This is a comment:', data) def handle_entityref(self, name): print('This is a named character reference: %s' % chr(name2codepoint[name])) def handle_charref(self, name): if name.startswith('x'): print('This is a numeric character reference: %s' % chr(int(name[1:], 16))) else: print('This is a numeric character reference: %s' % chr(int(name)))parser = MyHTMLParser(convert_charrefs=False)parser.feed(r'''&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Test&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- test html parser --&gt; &lt;p&gt;My personal website is &lt;a href=&quot;http://www.2wildkids.com/&quot;&gt;www.2wildkids.com&lt;/a&gt;. Welcome to visit it.&lt;/p&gt; &lt;img src=&quot;http://oe0e8k1nf.bkt.clouddn.com/avator-lion.jpg&quot; /&gt; &lt;p&gt;&amp;times; is a named character reference and &amp;#215; is a numeric character reference. &lt;/body&gt;&lt;/html&gt;''') 运行结果： 12345678910111213141516171819202122232425262728293031323334This is data: '\\n'This is data: '\\n'This is a start tag: htmlThis is data: '\\n 'This is a start tag: headThis is data: '\\n 'This is a start tag: titleThis is data: 'Test'This is an end tag: titleThis is data: '\\n 'This is an end tag: headThis is data: '\\n 'This is a start tag: bodyThis is data: '\\n 'This is a comment: test html parserThis is data: '\\n 'This is a start tag: pThis is data: 'My personal website is 'This is a start tag: aThis is data: 'www.2wildkids.com'This is an end tag: aThis is data: '. Welcome to visit it.'This is an end tag: pThis is data: '\\n 'This is a start-end tag: imgThis is data: '\\n 'This is a start tag: pThis is a named character reference: ×This is data: ' is a named character reference and 'This is a numeric character reference: ×;This is data: ' is a numeric character reference.\\n 'This is an end tag: bodyThis is data: '\\n'This is an end tag: html feed() 方法可以多次调用，所以HTML字符串可以一部分一部分地塞进去，而无需一次传入完整的HTML文档。 代码比较简单，不需要过多地讲解。有几个小知识点需要注意一下： HTML中每个标签可能会有一些属性，比如 &lt;img&gt; 标签的 src 属性还有大小属性等等，这些属性传入事件处理函数时，会被整合到一个元组（attrs 参数）中，每个属性会以键值对的形式被存放在这个元组里。 处理实体名称需要 name2codepoint 这个字典，注意导入的方式。它可以将实体名称映射为十进制code point，然后再使用 chr() 函数就能得到对应的Unicode字符了。 处理实体编号需要先判断使用了十进制表示形式还是十六进制表示形式。 还有一个问题，因为廖老师教程中使用的是Python3的早期版本，在早期版本中，HTMLParser类初始化时 convert_charrefs 参数默认是 False，不会把HTML字符串中的字符实体转换为Unicode字符。而在Python3.5版本中，这一参数被修改为默认是 True，所以传入HTML字符串时会自动进行转换，转换后自然就无法触发 handle_entityref() 事件和 handle_charref() 事件了。这个小问题刚开始也让我小卡了一下，在查看官方文档后终于解决了问题。 小结 借助 HTMLParser 模块，我们可以非常方便地把网页中的文本、图像等解析出来。此外，我们也可以使用更为强大的第三方库 BeautifulSoup。 练习 查看Python官网的新闻页，用浏览器查看该网页的源码，尝试解析出Python官网发布的会议名称、时间和地点。 由于源码较长，所以就不在笔记中展示出来了，源码文件 Our Events _ Python.org.html 放在Res目录下。 首先分析一下源码，会议名称、时间和地点在源码中是这样表示的： 会议名称： 1&lt;h3 class=&quot;event-title&quot;&gt;&lt;a href=&quot;https://www.python.org/events/python-events/491/&quot;&gt;PyCon Belarus 2017&lt;/a&gt;&lt;/h3&gt; 会议时间： 1&lt;time datetime=&quot;2017-02-04T00:00:00+00:00&quot;&gt;04 Feb. – 05 Feb. &lt;span class=&quot;say-no-more&quot;&gt; 2017&lt;/span&gt;&lt;/time&gt; 会议地点： 1&lt;span class=&quot;event-location&quot;&gt;Minsk, Belarus&lt;/span&gt; 并且，出现会议名称后一定会紧接着出现相应的会议时间和会议地点。因此，提取策略可以分为以下步骤： 提取会议名称：在 h3 标签触发 handle_starttag 事件时，判断其 class 属性是否为 event-title ，是则紧接着触发的一次 handle_data 事件所得的就是会议名称。 提取会议时间：在 time 标签触发 handle_starttag 事件时，则紧接着触发的两次 handle_data 事件所得的分别是会议日期和年份。 提取会议地点：在 span 标签触发 handle_starttag 事件时，判断其 class 属性是否为 event-location ，是则紧接着触发的一次 handle_data 事件所得的就是会议地点。 我们可以设置一个 flag 变量来标记触发事件的标签，方便在 handle_data 时判断处理。又因为会议日期和年份是分开的，我们可以使用一个中间变量 date 来暂存日期，待和年份合并后再进行输出。 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243from html.parser import HTMLParserclass MyHTMLParser(HTMLParser): def __init__(self, **kw): super().__init__(**kw) self.flag = '' self.date = '' def handle_starttag(self, tag, attrs): if tag == 'h3': for attr in attrs: if attr[0] == 'class' and attr[1] == 'event-title': self.flag = 'title' elif tag == 'time': self.flag = 'time' elif tag == 'span': for attr in attrs: if attr[0] == 'class' and attr[1] == 'event-location': self.flag = 'location' def handle_data(self, data): if self.flag == 'title': print('会议名称:', data) self.flag = '' elif self.flag == 'time': if self.date == '': self.date = self.date + data else: self.date = self.date + data print('会议时间:', self.date) self.date = '' self.flag = '' elif self.flag == 'location': print('会议地点:', data, '\\n') self.flag = ''with open(r'E:\\wheels\\learnpython\\My_Python_Notebook\\Res\\Our Events _ Python.org.html', 'r', encoding='utf-8') as f: html = f.read() parser = MyHTMLParser(convert_charrefs=False) parser.feed(html) 运行结果： 12345678910111213141516171819202122232425262728会议名称: PyCon Belarus 2017会议时间: 04 Feb. – 05 Feb. 2017会议地点: Minsk, Belarus会议名称: PyTennessee 2017会议时间: 04 Feb. – 06 Feb. 2017会议地点: Nashville, Tennessee, USA会议名称: PythonFOSDEM 2017会议时间: 04 Feb. – 06 Feb. 2017会议地点: Université Libre de Bruxelles, Franklin Rooseveltlaan 50, 1050 Brussel, Belgium会议名称: FOSDEM 2017会议时间: 04 Feb. – 06 Feb. 2017会议地点: Université Libre de Bruxelles, Franklin Rooseveltlaan 50, 1050 Brussel, Belgium会议名称: PyCon Colombia 2017会议时间: 10 Feb. – 12 Feb. 2017会议地点: Bogota, Colombia会议名称: PyCon Pune 2017会议时间: 16 Feb. – 20 Feb. 2017会议地点: COEP, Pune, India会议名称: PyCon Cameroon会议时间: 20 Jan. – 23 Jan. 2017会议地点: Molyko Buea,Cameroon urllib 简介 urllib 库提供了一系列用于操作URL的功能。 Get urllib 的 request 模块可以非常方便地抓取URL内容，urlopen() 函数首先发送一个GET请求到指定的页面，然后返回HTTP的响应。比方说，对豆瓣的一个URL（https://api.douban.com/v2/book/2129650）进行抓取，并返回响应： 12345678from urllib import requestwith request.urlopen('https://api.douban.com/v2/book/2129650') as f: data = f.read() print('Status:', f.status, f.reason) for k, v in f.getheaders(): print('%s: %s' % (k, v)) print('Data:', data.decode('utf-8')) 可以看到HTTP响应状态，header，以及返回的JSON数据： 1234567891011Status: 200 OKServer: nginxDate: Tue, 26 May 2015 10:02:27 GMTContent-Type: application/json; charset=utf-8Content-Length: 2049Connection: closeExpires: Sun, 1 Jan 2006 01:00:00 GMTPragma: no-cacheCache-Control: must-revalidate, no-cache, privateX-DAE-Node: pidl1Data: {&quot;rating&quot;:{&quot;max&quot;:10,&quot;numRaters&quot;:16,&quot;average&quot;:&quot;7.4&quot;,&quot;min&quot;:0},&quot;subtitle&quot;:&quot;&quot;,&quot;author&quot;:[&quot;廖雪峰编著&quot;],&quot;pubdate&quot;:&quot;2007-6&quot;,&quot;tags&quot;:[{&quot;count&quot;:20,&quot;name&quot;:&quot;spring&quot;,&quot;title&quot;:&quot;spring&quot;}...} 如果我们要想模拟浏览器发送GET请求，就需要使用 Request 对象，通过往 Request 对象添加HTTP头，我们就可以把请求伪装成浏览器。例如，模拟iPhone 6去请求豆瓣首页： 123456789from urllib import requestreq = request.Request('http://www.douban.com/')req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')with request.urlopen(req) as f: print('Status:', f.status, f.reason) for k, v in f.getheaders(): print('%s: %s' % (k, v)) print('Data:', f.read().decode('utf-8')) 这样豆瓣会返回适合iPhone的移动版网页： 12345... &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0&quot;&gt; &lt;meta name=&quot;format-detection&quot; content=&quot;telephone=no&quot;&gt; &lt;link rel=&quot;apple-touch-icon&quot; sizes=&quot;57x57&quot; href=&quot;http://img4.douban.com/pics/cardkit/launcher/57.png&quot; /&gt;... Post 如果要以POST方式发送一个请求，就需要把参数数据以bytes的形式传入。 比方说，我们模拟微博登录，需要先读取用于登录的邮箱和口令，然后按照 weibo.cn 登录页的格式以 username=xxx&amp;password=xxx 的方式来传入： 12345678910111213141516171819202122232425from urllib import request, parseprint('Login to weibo.cn...')email = input('Email: ')passwd = input('Password: ')login_data = parse.urlencode([ ('username', email), ('password', passwd), ('entry', 'mweibo'), ('client_id', ''), ('savestate', '1'), ('ec', ''), ('pagerefer', 'https://passport.weibo.cn/signin/welcome?entry=mweibo&amp;r=http%3A%2F%2Fm.weibo.cn%2F')])req = request.Request('https://passport.weibo.cn/sso/login')req.add_header('Origin', 'https://passport.weibo.cn')req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')req.add_header('Referer', 'https://passport.weibo.cn/signin/login?entry=mweibo&amp;res=wel&amp;wm=3349&amp;r=http%3A%2F%2Fm.weibo.cn%2F')with request.urlopen(req, data=login_data.encode('utf-8')) as f: print('Status:', f.status, f.reason) for k, v in f.getheaders(): print('%s: %s' % (k, v)) print('Data:', f.read().decode('utf-8')) 如果登录成功，我们获得的响应如下： 123456Status: 200 OKServer: nginx/1.2.0...Set-Cookie: SSOLoginState=1432620126; path=/; domain=weibo.cn...Data: {&quot;retcode&quot;:20000000,&quot;msg&quot;:&quot;&quot;,&quot;data&quot;:{...,&quot;uid&quot;:&quot;1658384301&quot;}} 如果登录失败，我们获得的响应如下： 123...Data: {&quot;retcode&quot;:50011015,&quot;msg&quot;:&quot;\\u7528\\u6237\\u540d\\u6216\\u5bc6\\u7801\\u9519\\u8bef&quot;,&quot;data&quot;:{&quot;username&quot;:&quot;example@python.org&quot;,&quot;errline&quot;:536}}Handler 如果还需要更复杂的控制，比如通过一个代理服务器(Proxy) 去访问网站，我们需要利用 ProxyHandler 来处理，示例代码如下： 123456proxy_handler = urllib.request.ProxyHandler({'http': 'http://www.example.com:3128/'})proxy_auth_handler = urllib.request.ProxyBasicAuthHandler()proxy_auth_handler.add_password('realm', 'host', 'username', 'password')opener = urllib.request.build_opener(proxy_handler, proxy_auth_handler)with opener.open('http://www.example.com/login.html') as f: pass 小结 urllib 提供的功能就是利用程序去执行各种HTTP请求。如果要模拟浏览器完成特定功能，需要把请求伪装成浏览器。伪装的方法是先监控浏览器发出的请求，再根据浏览器的发出的请求中的header来伪装，User-Agent就是用来标识浏览器的。 练习 利用 urllib 读取XML，将XML一节的天气预报数据由硬编码改为由 urllib 获取 由于雅虎天气API已经跪了，这里改用百度天气API来尝试。 接口例子：http://api.map.baidu.com/telematics/v3/weather?location=广州&amp;ak=8IoIaU655sQrs95uMWRWPDIa 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from urllib import requestfrom xml.parsers.expat import ParserCreateclass WeatherSaxHandler(object): def __init__(self): self.result = dict() self.currentTag = '' # self.flag = True def start_element(self, name, attrs): self.currentTag = name def char_data(self, data): if self.flag == True: if self.currentTag == 'currentCity': self.result['城市'] = data elif self.currentTag == 'date': self.result['当前'] = data elif self.currentTag == 'weather': self.result['天气'] = data elif self.currentTag == 'wind': self.result['风速'] = data elif self.currentTag == 'temperature': self.result['气温'] = data self.flag = False self.currentTag = '' # 记得每次解析完信息要重置def parse_weather(data): handler = WeatherSaxHandler() p = ParserCreate() p.StartElementHandler = handler.start_element p.CharacterDataHandler = handler.char_data p.Parse(data) return handler.resultdef fetch_xml(url): with request.urlopen(url) as f: data = f.read().decode('utf-8') print('Status:', f.status, f.reason) return parse_weather(data)# 测试print(fetch_xml('http://api.map.baidu.com/telematics/v3/weather?location=guangzhou&amp;ak=8IoIaU655sQrs95uMWRWPDIa')) 运行结果： 12Status: 200 OK{'气温': '21 ~ 14℃', '城市': 'guangzhou', '风速': '微风', '当前': '周六 02月04日 (实时：19℃)', '天气': '小雨'}","link":"/posts/32532.html"},{"title":"交换最小二乘","text":"什么是ALS ALS是交替最小二乘（alternating least squares）的简称。在机器学习中，ALS特指使用交替最小二乘求解的一个协同推荐算法。它通过观察到的所有用户给商品的打分，来推断每个用户的喜好并向用户推荐适合的商品。举个例子，我们看下面一个8*8的用户打分矩阵。 这个矩阵的每一行代表一个用户（u1,u2,…,u8）、每一列代表一个商品（v1,v2,…,v8）、用户的打分为1-9分。这个矩阵只显示了观察到的打分，我们需要推测没有观察到的打分。比如（u6，v5）打分多少？如果以数独的方式来解决这个问题，可以得到唯一的结果。 因为数独的规则很强，每添加一条规则，就让整个系统的自由度下降一个量级。当我们满足所有的规则时，整个系统的自由度就降为1了，也就得出了唯一的结果。对于上面的打分矩阵，如果我们不添加任何条件的话，也即打分之间是相互独立的，我们就没法得到（u6，v5）的打分。 所以在这个用户打分矩阵的基础上，我们需要提出一个限制其自由度的合理假设，使得我们可以通过观察已有打分来猜测未知打分。 ALS的核心就是这样一个假设：打分矩阵是近似低秩的。换句话说，就是一个m*n的打分矩阵可以由分解的两个小矩阵U（m*k）和V（k*n）的乘积来近似，即\\(A=U{V}^{T},k &lt;= m,n\\)。这就是ALS的矩阵分解方法。这样我们把系统的自由度从O(mn)降到了O((m+n)k)。 那么ALS的低秩假设为什么是合理的呢？我们描述一个人的喜好经常是在一个抽象的低维空间上进行的，并不需要一一列出他喜好的事物。例如，我喜好看侦探影片，可能代表我喜欢《神探夏洛特》、《神探狄仁杰》等。这些影片都符合我对自己喜好的描述，也就是说他们在这个抽象的低维空间的投影和我的喜好相似。 再抽象一些来描述这个问题，我们把某个人的喜好映射到了低维向量ui上，同时将某个影片的特征映射到了维度相同的向量vj上，那么这个人和这个影片的相似度就可以表述成这两个向量之间的内积\\(u_{i}^{T}v_{j}\\) 。 我们把打分理解成相似度，那么打分矩阵A就可以由用户喜好矩阵和产品特征矩阵的乘积$ U{V}^{T} $来近似了。 低维空间的选取是一个问题。这个低维空间要能够很好的区分事物，那么就需要一个明确的可量化目标，这就是重构误差。在ALS中我们使用F范数来量化重构误差，就是每个元素重构误差的平方和。这里存在一个问题，我们只观察到部分打分，A中的大量未知元是我们想推断的，所以这个重构误差是包含未知数的。 解决方案很简单：只计算已知打分的重构误差。 后面的章节我们将从原理上讲解spark中实现的ALS模型。 spark中ALS的实现原理 Spark利用交换最小二乘解决矩阵分解问题分两种情况：数据集是显式反馈和数据集是隐式反馈。由于隐式反馈算法的原理是在显示反馈算法原理的基础上作的修改，所以我们在此只会具体讲解数据集为隐式反馈的算法。 算法实现所依据的文献见参考文献【1】。 介绍 从广义上讲，推荐系统基于两种不同的策略：基于内容的方法和基于协同过滤的方法。Spark中使用协同过滤的方式。协同过滤分析用户以及用户相关的产品的相关性，用以识别新的用户-产品相关性。协同过滤系统需要的唯一信息是用户过去的行为信息，比如对产品的评价信息。协同过滤是领域无关的，所以它可以方便解决基于内容方法难以解决的许多问题。 推荐系统依赖不同类型的输入数据，最方便的是高质量的显式反馈数据，它们包含用户对感兴趣商品明确的评价。例如，Netflix收集的用户对电影评价的星星等级数据。但是显式反馈数据不一定总是找得到，因此推荐系统可以从更丰富的隐式反馈信息中推测用户的偏好。 隐式反馈类型包括购买历史、浏览历史、搜索模式甚至鼠标动作。例如，购买同一个作者许多书的用户可能喜欢这个作者。 许多研究都集中在处理显式反馈，然而在很多应用场景下，应用程序重点关注隐式反馈数据。因为可能用户不愿意评价商品或者由于系统限制我们不能收集显式反馈数据。在隐式模型中，一旦用户允许收集可用的数据，在客户端并不需要额外的显式数据。文献中的系统避免主动地向用户收集显式反馈信息，所以系统仅仅依靠隐式信息。 了解隐式反馈的特点非常重要，因为这些特质使我们避免了直接调用基于显式反馈的算法。最主要的特点有如下几种： （1） 没有负反馈。通过观察用户行为，我们可以推测那个商品他可能喜欢，然后购买，但是我们很难推测哪个商品用户不喜欢。这在显式反馈算法中并不存在，因为用户明确告诉了我们哪些他喜欢哪些他不喜欢。 （2） 隐式反馈是内在的噪音。虽然我们拼命的追踪用户行为，但是我们仅仅只是猜测他们的偏好和真实动机。例如，我们可能知道一个人的购买行为，但是这并不能完全说明偏好和动机，因为这个商品可能作为礼物被购买而用户并不喜欢它。 （3） 显示反馈的数值值表示偏好（preference），隐式回馈的数值值表示信任（confidence）。基于显示反馈的系统用星星等级让用户表达他们的喜好程度，例如一颗星表示很不喜欢，五颗星表示非常喜欢。基于隐式反馈的数值值描述的是动作的频率，例如用户购买特定商品的次数。一个较大的值并不能表明更多的偏爱。但是这个值是有用的，它描述了在一个特定观察中的信任度。 一个发生一次的事件可能对用户偏爱没有用，但是一个周期性事件更可能反映一个用户的选择。 （4） 评价隐式反馈推荐系统需要合适的手段。 显式反馈模型 潜在因素模型由一个针对协同过滤的交替方法组成，它以一个更加全面的方式发现潜在特征来解释观察的ratings数据。我们关注的模型由奇异值分解（SVD）推演而来。一个典型的模型将每个用户u（包含一个用户-因素向量ui）和每个商品v（包含一个商品-因素向量vj）联系起来。 预测通过内积\\(r_{ij}=u_{i}^{T}v_{j}\\)来实现。另一个需要关注的地方是参数估计。许多当前的工作都应用到了显式反馈数据集中，这些模型仅仅基于观察到的rating数据直接建模，同时通过一个适当的正则化来避免过拟合。公式如下： 在公式(2.1)中，lambda是正则化的参数。正规化是为了防止过拟合的情况发生，具体参见文献【3】。这样，我们用最小化重构误差来解决协同推荐问题。我们也成功将推荐问题转换为了最优化问题。 隐式反馈模型 在显式反馈的基础上，我们需要做一些改动得到我们的隐式反馈模型。首先，我们需要形式化由\\(r_{ij}\\)变量衡量的信任度的概念。我们引入了一组二元变量\\(p_{ij}\\) ，它表示用户u对商品v的偏好。\\(p_{ij}\\)的公式如下： 换句话说，如果用户购买了商品，我们认为用户喜欢该商品，否则我们认为用户不喜欢该商品。然而我们的信念（beliefs）与变化的信任（confidence）等级息息相关。首先，很自然的，\\(p_{ij}\\)的值为0和低信任有关。用户对一个商品没有得到一个正的偏好可能源于多方面的原因，并不一定是不喜欢该商品。例如，用户可能并不知道该商品的存在。 另外，用户购买一个商品也并不一定是用户喜欢它。因此我们需要一个新的信任等级来显示用户偏爱某个商品。一般情况下，\\(r_{ij}\\)越大，越能暗示用户喜欢某个商品。因此，我们引入了一组变量\\(c_{ij}\\)，它衡量了我们观察到\\(p_{ij}\\)的信任度。\\(c_{ij}\\)一个合理的选择如下所示： 按照这种方式，我们存在最小限度的信任度，并且随着我们观察到的正偏向的证据越来越多，信任度也会越来越大。 我们的目的是找到用户向量ui以及商品向量vj来表明用户偏好。这些向量分别是用户因素（特征）向量和商品因素（特征）向量。本质上，这些向量将用户和商品映射到一个公用的隐式因素空间，从而使它们可以直接比较。这和用于显式数据集的矩阵分解技术类似，但是包含两点不一样的地方： （1）我们需要考虑不同的信任度，（2）最优化需要考虑所有可能的u，v对，而不仅仅是和观察数据相关的u，v对。显性反馈的矩阵分解优化时，对于missing data(没有评分)，是不会当做训练数据输入到模型的，优化时针对已知评分数据优化。而这里隐性反馈，是利用所有可能的u,i键值对，所以总的数据是m*n，其中m是用户数量，n是物品数量。这里没有所谓的missing data，因为假如u对i没有任何动作，我们就认为偏好值为0，只不过置信度较低而已。因此，通过最小化下面的损失函数来计算相关因素（factors）。 \\[min_{u,v}\\sum _{i,j}c_{ij}(p_{ij}-u_{i}^{T}v_{j})^{2} + \\lambda (\\sum_{i}\\left \\| u_{i} \\right \\|^{2} + \\sum_{j}\\left \\|v_{j} \\right \\|^{2})\\] 求解最小化损失函数 考虑到损失函数包含m*n个元素，m是用户的数量，n是商品的数量。一般情况下，m*n可以到达几百亿。这么多的元素应该避免使用随机梯度下降法来求解，因此，spark选择使用交替最优化方式求解。 公式（2.1）和公式（2.4）是非凸函数，无法求解最优解。但是，固定公式中的用户-特征向量或者商品-特征向量，公式就会变成二次方程，可以求出全局的极小值。交替最小二乘的计算过程是：交替的重新计算用户-特征向量和商品-特征向量，每一步都保证降低损失函数的值，直到找到极小值。 交替最小二乘法的处理过程如下所示： ALS在spark中的实现 在spark的源代码中，ALS算法实现于org.apache.spark.ml.recommendation.ALS.scala文件中。我们以官方文档中的例子为起点，来分析ALS算法的分布式实现。下面是官方的例子： 123456789//处理训练数据val data = sc.textFile(&quot;data/mllib/als/test.data&quot;)val ratings = data.map(_.split(',') match { case Array(user, item, rate) =&gt; Rating(user.toInt, item.toInt, rate.toDouble)})// 使用ALS训练推荐模型val rank = 10val numIterations = 10val model = ALS.train(ratings, rank, numIterations, 0.01) 从代码中我们知道，训练模型用到了ALS.scala文件中的train方法，下面我们将详细介绍train方法的实现。在此之前，我们先了解一下train方法的参数表示的含义。 1234567891011121314def train( ratings: RDD[Rating[ID]], //训练数据 rank: Int = 10, //隐含特征数 numUserBlocks: Int = 10, //分区数 numItemBlocks: Int = 10, maxIter: Int = 10, //迭代次数 regParam: Double = 1.0, implicitPrefs: Boolean = false, alpha: Double = 1.0, nonnegative: Boolean = false, intermediateRDDStorageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK, finalRDDStorageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK, checkpointInterval: Int = 10, seed: Long = 0L): MatrixFactorizationModel 以上定义中，ratings指用户提供的训练数据，它包括用户id集、商品id集以及相应的打分集。rank表示隐含因素的数量，也即特征的数量。numUserBlocks和numItemBlocks分别指用户和商品的块数量，即分区数量。maxIter表示迭代次数。regParam表示最小二乘法中lambda值的大小。 implicitPrefs表示我们的训练数据是否是隐式反馈数据。Nonnegative表示求解的最小二乘的值是否是非负,根据Nonnegative的值的不同，spark使用了不同的求解方法。 下面我们分步骤分析train方法的处理流程。 (1) 初始化ALSPartitioner和LocalIndexEncoder。 ALSPartitioner实现了基于hash的分区，它根据用户或者商品id的hash值来进行分区。LocalIndexEncoder对（blockid，localindex）即（分区id，分区内索引）进行编码，并将其转换为一个整数，这个整数在高位存分区ID，在低位存对应分区的索引，在空间上尽量做到了不浪费。 同时也可以根据这个转换的整数分别获得blockid和localindex。这两个对象在后续的代码中会用到。 12345678910111213141516171819202122232425262728293031323334353637383940414243val userPart = new ALSPartitioner(numUserBlocks)val itemPart = new ALSPartitioner(numItemBlocks)val userLocalIndexEncoder = new LocalIndexEncoder(userPart.numPartitions)val itemLocalIndexEncoder = new LocalIndexEncoder(itemPart.numPartitions)//ALSPartitioner即HashPartitionerclass HashPartitioner(partitions: Int) extends Partitioner { def numPartitions: Int = partitions def getPartition(key: Any): Int = key match { case null =&gt; 0 case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions) } override def equals(other: Any): Boolean = other match { case h: HashPartitioner =&gt; h.numPartitions == numPartitions case _ =&gt; false } override def hashCode: Int = numPartitions}//LocalIndexEncoderprivate[recommendation] class LocalIndexEncoder(numBlocks: Int) extends Serializable { private[this] final val numLocalIndexBits = math.min(java.lang.Integer.numberOfLeadingZeros(numBlocks - 1), 31) //左移（&lt;&lt;,相当于乘2），右移（&gt;&gt;，相当于除2）和无符号右移（&gt;&gt;&gt;，无符号右移，忽略符号位，空位都以0补齐） private[this] final val localIndexMask = (1 &lt;&lt; numLocalIndexBits) - 1 //encodeIndex高位存分区ID，在低位存对应分区的索引 def encode(blockId: Int, localIndex: Int): Int = { (blockId &lt;&lt; numLocalIndexBits) | localIndex } @inline def blockId(encoded: Int): Int = { encoded &gt;&gt;&gt; numLocalIndexBits } @inline def localIndex(encoded: Int): Int = { encoded &amp; localIndexMask } } (2) 根据nonnegative参数选择解决矩阵分解的方法。 如果需要解的值为非负,即nonnegative为true，那么用非负最小二乘（NNLS）来解，如果没有这个限制，用乔里斯基（Cholesky）分解来解。 1val solver = if (nonnegative) new NNLSSolver else new CholeskySolver 乔里斯基分解分解是把一个对称正定的矩阵表示成一个上三角矩阵U的转置和其本身的乘积的分解。在ml代码中，直接调用netlib-java封装的dppsv方法实现。 1lapack.dppsv(“u”, k, 1, ne.ata, ne.atb, k, info) 可以深入dppsv代码（Fortran代码）了解更深的细节。我们分析的重点是非负正则化最小二乘的实现，因为在某些情况下，方程组的解为负数是没有意义的。虽然方程组可以得到精确解，但却不能取负值解。在这种情况下，其非负最小二乘解比方程的精确解更有意义。`NNLS在最优化模块会作详细讲解。 (3) 将ratings数据转换为分区的格式。 将ratings数据转换为分区的形式，即（（用户分区id，商品分区id），分区数据集blocks））的形式，并缓存到内存中。其中分区id的计算是通过ALSPartitioner的getPartitions方法获得的，分区数据集由RatingBlock组成， 它表示（用户分区id，商品分区id ）对所对应的用户id集，商品id集，以及打分集，即（用户id集，商品id集，打分集）。 1234567891011121314151617181920212223242526272829303132333435363738val blockRatings = partitionRatings(ratings, userPart, itemPart) .persist(intermediateRDDStorageLevel) //以下是partitionRatings的实现 //默认是10*10 val numPartitions = srcPart.numPartitions * dstPart.numPartitions ratings.mapPartitions { iter =&gt; val builders = Array.fill(numPartitions)(new RatingBlockBuilder[ID]) iter.flatMap { r =&gt; val srcBlockId = srcPart.getPartition(r.user) val dstBlockId = dstPart.getPartition(r.item) //当前builder的索引位置 val idx = srcBlockId + srcPart.numPartitions * dstBlockId val builder = builders(idx) builder.add(r) //如果某个builder的数量大于2048，那么构建一个分区 if (builder.size &gt;= 2048) { // 2048 * (3 * 4) = 24k builders(idx) = new RatingBlockBuilder //单元素集合 Iterator.single(((srcBlockId, dstBlockId), builder.build())) } else { Iterator.empty } } ++ { builders.view.zipWithIndex.filter(_._1.size &gt; 0).map { case (block, idx) =&gt; //用户分区id val srcBlockId = idx % srcPart.numPartitions //商品分区id val dstBlockId = idx / srcPart.numPartitions ((srcBlockId, dstBlockId), block.build()) } } }.groupByKey().mapValues { blocks =&gt; val builder = new RatingBlockBuilder[ID] blocks.foreach(builder.merge) builder.build() }.setName(&quot;ratingBlocks&quot;) } （4）获取inblocks和outblocks数据。 获取inblocks和outblocks数据是数据处理的重点。我们知道，通信复杂度是分布式实现一个算法时要重点考虑的问题，不同的实现可能会对性能产生很大的影响。我们假设最坏的情况：即求解商品需要的所有用户特征都需要从其它节点获得。 如下图3.1所示，求解v1需要获得u1,u2，求解v2需要获得u1,u2,u3等，在这种假设下，每步迭代所需的交换数据量是O(m*rank)，其中m表示所有观察到的打分集大小，rank表示特征数量。 从图3.1中，我们知道，如果计算v1和v2是在同一个分区上进行的，那么我们只需要把u1和u2一次发给这个分区就好了，而不需要将u2分别发给v1,v2，这样就省掉了不必要的数据传输。 图3.2描述了如何在分区的情况下通过U来求解V，注意节点之间的数据交换量减少了。使用这种分区结构，我们需要在原始打分数据的基础上额外保存一些信息。 在Q1中，我们需要知道和v1相关联的用户向量及其对应的打分，从而构建最小二乘问题并求解。这部分数据不仅包含原始打分数据，还包含从每个用户分区收到的向量排序信息，在代码里称作InBlock。在P1中，我们要知道把u1,u2 发给Q1。我们可以查看和u1相关联的所有产品来确定需要把u1发给谁，但每次迭代都扫一遍数据很不划算，所以在spark的实现中只计算一次这个信息，然后把结果通过RDD缓存起来重复使用。这部分数据我们在代码里称作OutBlock。 所以从U求解V，我们需要通过用户的OutBlock信息把用户向量发给商品分区，然后通过商品的InBlock信息构建最小二乘问题并求解。从V求解U，我们需要商品的OutBlock信息和用户的InBlock信息。所有的InBlock和OutBlock信息在迭代过程中都通过RDD缓存。打分数据在用户的InBlock和商品的InBlock各存了一份，但分区方式不同。这么做可以避免在迭代过程中原始数据的交换。 下面介绍获取InBlock和OutBlock的方法。下面的代码用来分别获取用户和商品的InBlock和OutBlock。 123456789val (userInBlocks, userOutBlocks) =makeBlocks(&quot;user&quot;, blockRatings, userPart, itemPart,intermediateRDDStorageLevel)//交换userBlockId和itemBlockId以及其对应的数据val swappedBlockRatings = blockRatings.map { case ((userBlockId, itemBlockId), RatingBlock(userIds, itemIds, localRatings)) =&gt; ((itemBlockId, userBlockId), RatingBlock(itemIds, userIds, localRatings))}val (itemInBlocks, itemOutBlocks) =makeBlocks(&quot;item&quot;, swappedBlockRatings, itemPart, userPart,intermediateRDDStorageLevel) 我们会以求商品的InBlock以及用户的OutBlock为例来分析makeBlocks方法。因为在第（5）步中构建最小二乘的讲解中，我们会用到这两部分数据。 下面的代码用来求商品的InBlock信息。 12345678910111213141516171819202122232425262728293031323334353637val inBlocks = ratingBlocks.map { case ((srcBlockId, dstBlockId), RatingBlock(srcIds, dstIds, ratings)) =&gt; val start = System.nanoTime() val dstIdSet = new OpenHashSet[ID](1 &lt;&lt; 20) //将用户id保存到hashset中，用来去重 dstIds.foreach(dstIdSet.add) val sortedDstIds = new Array[ID](dstIdSet.size) var i = 0 var pos = dstIdSet.nextPos(0) while (pos != -1) { sortedDstIds(i) = dstIdSet.getValue(pos) pos = dstIdSet.nextPos(pos + 1) i += 1 } //对用户id进行排序 Sorting.quickSort(sortedDstIds) val dstIdToLocalIndex = new OpenHashMap[ID, Int](sortedDstIds.length) i = 0 while (i &lt; sortedDstIds.length) { dstIdToLocalIndex.update(sortedDstIds(i), i) i += 1 } //求取块内，用户id的本地位置 val dstLocalIndices = dstIds.map(dstIdToLocalIndex.apply) //返回数据集 (srcBlockId, (dstBlockId, srcIds, dstLocalIndices, ratings))}.groupByKey(new ALSPartitioner(srcPart.numPartitions)) .mapValues { iter =&gt; val builder = new UncompressedInBlockBuilder[ID](new LocalIndexEncoder(dstPart.numPartitions)) iter.foreach { case (dstBlockId, srcIds, dstLocalIndices, ratings) =&gt; builder.add(dstBlockId, srcIds, dstLocalIndices, ratings) } //构建非压缩块，并压缩为InBlock builder.build().compress() }.setName(prefix + &quot;InBlocks&quot;) .persist(storageLevel) 这段代码首先对ratingBlocks数据集作map操作，将ratingBlocks转换成（商品分区id，（用户分区id，商品集合，用户id在分区中相对应的位置，打分）这样的集合形式。然后对这个数据集作groupByKey操作，以商品分区id为key值，处理key对应的值，将数据集转换成（商品分区id，InBlocks）的形式。 这里值得我们去分析的是输入块（InBlock）的结构。为简单起见，我们用图3.2为例来说明输入块的结构。 以Q1为例，我们需要知道关于v1和v2的所有打分：(v1, u1, r11)，(v2, u1, r12)， (v1, u2, r21)， (v2, u2, r22)， (v2, u3, r32)，把这些项以Tuple的形式存储会存在问题，第一，Tuple有额外开销，每个Tuple实例都需要一个指针，而每个Tuple所存的数据不过是两个ID和一个打分； 第二，存储大量的Tuple会降低垃圾回收的效率。所以spark实现中，是使用三个数组来存储打分的，如([v1, v2, v1, v2, v2], [u1, u1, u2, u2, u3], [r11, r12, r21, r22, r32])。这样不仅大幅减少了实例数量，还有效地利用了连续内存。 但是，光这么做并不够，spark代码实现中，并没有存储用户的真实id，而是存储的使用LocalIndexEncoder生成的编码，这样节省了空间，格式为UncompressedInBlock:（商品id集，用户id集对应的编码集，打分集）， 如，([v1, v2, v1, v2, v2], [ui1, ui1, ui2, ui2, ui3], [r11, r12, r21, r22, r32])。这种结构仍旧有压缩的空间，spark调用compress方法将商品id进行排序（排序有两个好处，除了压缩以外，后文构建最小二乘也会因此受益）， 并且转换为（不重复的有序的商品id集，商品位置偏移集，用户id集对应的编码集，打分集）的形式，以获得更优的存储效率（代码中就是将矩阵的coo格式转换为csc格式，你可以更进一步了解矩阵存储，以获得更多信息）。 以这样的格式修改([v1, v2, v1, v2, v2], [ui1, ui1, ui2, ui2, ui3], [r11, r12, r21, r22, r32])，得到的结果是([v1, v2], [0, 2, 5], [ui1, ui2, ui1, ui2, ui3], [r11, r21, r12, r22, r32])。其中[0, 2]指v1对应的打分的区间是[0, 2]，[2, 5]指v2对应的打分的区间是[2, 5]。 Compress方法利用spark内置的Timsort算法将UncompressedInBlock进行排序并转换为InBlock。代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344def compress(): InBlock[ID] = { val sz = length //Timsort排序 sort() val uniqueSrcIdsBuilder = mutable.ArrayBuilder.make[ID] val dstCountsBuilder = mutable.ArrayBuilder.make[Int] var preSrcId = srcIds(0) uniqueSrcIdsBuilder += preSrcId var curCount = 1 var i = 1 var j = 0 while (i &lt; sz) { val srcId = srcIds(i) if (srcId != preSrcId) { uniqueSrcIdsBuilder += srcId dstCountsBuilder += curCount preSrcId = srcId j += 1 curCount = 0 } curCount += 1 i += 1 } dstCountsBuilder += curCount val uniqueSrcIds = uniqueSrcIdsBuilder.result() val numUniqueSrdIds = uniqueSrcIds.length val dstCounts = dstCountsBuilder.result() val dstPtrs = new Array[Int](numUniqueSrdIds + 1) var sum = 0 i = 0 //计算偏移量 while (i &lt; numUniqueSrdIds) { sum += dstCounts(i) i += 1 dstPtrs(i) = sum } InBlock(uniqueSrcIds, dstPtrs, dstEncodedIndices, ratings)}private def sort(): Unit = { val sz = length val sortId = Utils.random.nextInt() val sorter = new Sorter(new UncompressedInBlockSort[ID]) sorter.sort(this, 0, length, Ordering[KeyWrapper[ID]]) } 下面的代码用来求用户的OutBlock信息。 1234567891011121314151617181920212223val outBlocks = inBlocks.mapValues { case InBlock(srcIds, dstPtrs, dstEncodedIndices, _) =&gt; val encoder = new LocalIndexEncoder(dstPart.numPartitions) val activeIds = Array.fill(dstPart.numPartitions)(mutable.ArrayBuilder.make[Int]) var i = 0 val seen = new Array[Boolean](dstPart.numPartitions) while (i &lt; srcIds.length) { var j = dstPtrs(i) ju.Arrays.fill(seen, false) while (j &lt; dstPtrs(i + 1)) { val dstBlockId = encoder.blockId(dstEncodedIndices(j)) if (!seen(dstBlockId)) { activeIds(dstBlockId) += i seen(dstBlockId) = true } j += 1 } i += 1 } activeIds.map { x =&gt; x.result() }}.setName(prefix + &quot;OutBlocks&quot;) .persist(storageLevel) 这段代码中，inBlocks表示用户的输入分区块，格式为（用户分区id，（不重复的用户id集，用户位置偏移集，商品id集对应的编码集，打分集））。 activeIds表示商品分区中涉及的用户id集，也即上文所说的需要发送给确定的商品分区的用户信息。activeIds是一个二维数组，第一维表示分区，第二维表示用户id集。用户OutBlocks的最终格式是（用户分区id，OutBlocks）。 通过用户的OutBlock把用户信息发给商品分区，然后结合商品的InBlock信息构建最小二乘问题，我们就可以借此解得商品的极小解。反之，通过商品OutBlock把商品信息发送给用户分区，然后结合用户的InBlock信息构建最小二乘问题，我们就可以解得用户解。 第（6）步会详细介绍如何构建最小二乘。 （5）初始化用户特征矩阵和商品特征矩阵。 交换最小二乘算法是分别固定用户特征矩阵和商品特征矩阵来交替计算下一次迭代的商品特征矩阵和用户特征矩阵。通过下面的代码初始化第一次迭代的特征矩阵。 12var userFactors = initialize(userInBlocks, rank, seedGen.nextLong())var itemFactors = initialize(itemInBlocks, rank, seedGen.nextLong()) 初始化后的userFactors的格式是（用户分区id，用户特征矩阵factors），其中factors是一个二维数组，第一维的长度是用户数，第二维的长度是rank数。初始化的值是异或随机数的F范式。itemFactors的初始化与此类似。 （6）利用inblock和outblock信息构建最小二乘。 构建最小二乘的方法是在computeFactors方法中实现的。我们以商品inblock信息结合用户outblock信息构建最小二乘为例来说明这个过程。代码首先用用户outblock与userFactor进行join操作，然后以商品分区id为key进行分组。 每一个商品分区包含一组所需的用户分区及其对应的用户factor信息，格式即（用户分区id集，用户分区对应的factor集）。紧接着，用商品inblock信息与merged进行join操作，得到商品分区所需要的所有信息，即（商品inblock，（用户分区id集，用户分区对应的factor集））。 有了这些信息，构建最小二乘的数据就齐全了。详细代码如下： 12345678val srcOut = srcOutBlocks.join(srcFactorBlocks).flatMap { case (srcBlockId, (srcOutBlock, srcFactors)) =&gt; srcOutBlock.view.zipWithIndex.map { case (activeIndices, dstBlockId) =&gt; (dstBlockId, (srcBlockId, activeIndices.map(idx =&gt; srcFactors(idx)))) }}val merged = srcOut.groupByKey(new ALSPartitioner(dstInBlocks.partitions.length))dstInBlocks.join(merged) 我们知道求解商品值时，我们需要通过所有和商品关联的用户向量信息来构建最小二乘问题。这里有两个选择，第一是扫一遍InBlock信息，同时对所有的产品构建对应的最小二乘问题； 第二是对于每一个产品，扫描InBlock信息，构建并求解其对应的最小二乘问题。第一种方式复杂度较高，具体的复杂度计算在此不作推导。spark选取第二种方法求解最小二乘问题，同时也做了一些优化。 做优化的原因是二种方法针对每个商品，都会扫描一遍InBlock信息，这会浪费较多时间，为此，将InBlock按照商品id进行排序（前文已经提到过），我们通过一次扫描就可以创建所有的最小二乘问题并求解。 构建代码如下所示： 1234567891011121314151617while (j &lt; dstIds.length) { ls.reset() var i = srcPtrs(j) var numExplicits = 0 while (i &lt; srcPtrs(j + 1)) { val encoded = srcEncodedIndices(i) val blockId = srcEncoder.blockId(encoded) val localIndex = srcEncoder.localIndex(encoded) val srcFactor = sortedSrcFactors(blockId)(localIndex) val rating = ratings(i) ls.add(srcFactor, rating) numExplicits += 1 i += 1 } dstFactors(j) = solver.solve(ls, numExplicits * regParam) j += 1} 到了这一步，构建显式反馈算法的最小二乘就结束了。隐式反馈算法的实现与此类似，不同的地方是它将YtY这个值预先计算了（可以参考文献【1】了解更多信息），而不用在每次迭代中都计算一遍。代码如下： 123456789101112131415161718//在循环之外计算val YtY = if (implicitPrefs) Some(computeYtY(srcFactorBlocks, rank)) else None//在每个循环内if (implicitPrefs) { ls.merge(YtY.get)}if (implicitPrefs) { // Extension to the original paper to handle b &lt; 0. confidence is a function of |b| // instead so that it is never negative. c1 is confidence - 1.0. val c1 = alpha * math.abs(rating) // For rating &lt;= 0, the corresponding preference is 0. So the term below is only added // for rating &gt; 0. Because YtY is already added, we need to adjust the scaling here. if (rating &gt; 0) { numExplicits += 1 ls.add(srcFactor, (c1 + 1.0) / c1, c1) }} 后面的问题就如何求解最小二乘了。我们会在最优化章节介绍spark版本的NNLS。 4 参考文献 【1】Yifan Hu，Yehuda Koren∗，Chris Volinsky. Collaborative Filtering for Implicit Feedback Datasets 【2】 Yehuda Koren, Robert Bell and Chris Volinsky. Matrix Factorization Techniques for Recommender Systems 【3】 Yunhong Zhou, Dennis Wilkinson, Robert Schreiber and Rong Pan. Large-scale Parallel Collaborative Filtering for the Netflix Prize","link":"/posts/1032586435.html"},{"title":"C++Primer","text":"变量 类型 算术类型 整形 包括char和bool在内 浮点型 单精度 双精度 扩展精度 空类型（void） 使用建议： 使用int执行整数运算，超过范围用long long，因为long一般和int大小一样 浮点运算用double，float通常精度不够而且双精度和单精度的计算代价相差无几。long double提供的精度通常没必要，而且运算时的消耗也不容忽视 大小 字节：内存可寻址的最小块，大多数计算机将内存中的每个字节与一个数字(地址)关联起来。C++中，一个字节要至少能容纳机器基本字符集中的字符； 字：一般是32比特(4字节)或64比特(8字节) 在不同机器上有所差别，对于C++标准(P30)： 一个char的大小和机器字节一样； ​bool大小未定义； int至少和short一样大； long至少和int一样大； long long(C++11)至少和long一样大 signed与unsigned 除了bool和扩展字符型外，都可以分为signed和unsigned；​char可以表现为signed char和unsigned char，具体由编译器决定； unsigned减去一个数必须保证结果不能是一个负值​，否则结果是取模后的值（比如，很多字符串的长度为无符号型，在for循环非常容易出现str.length() - i &gt;= 0这种表达，如果i比字符串长度大，那么就会引发错误） signed会转化为unsigned（切勿混用signed和unsigned) 溢出 赋给unsigned超过范围的值：结果是初始值对无符号类型表示值总数取模后的余数 赋给signed超过范围的值：结果未定义，可能继续工作、崩溃、生成垃圾数据 类型转换 隐式转换与显式转换 隐式转换 整形的隐式转换：多数表达式中，比int小的整形首先提升为较大整形 数组转成指针 指针的转换：0,nullptr转成任意指针，任意指针转void 转换时机： 拷贝初始化 算术或关系运算 函数调用时 显示转换 命名强制类型转换 cast-name&lt;type&gt;(expression) static_cast：只要不包含底层const，都可以使用。适合将较大算术类型转换成较小算术类型 const_cast：​只能改变底层const，例如指向const的指针(指向的对象不一定是常量，但是无法通过指针修改)​，如果指向的对象是常量，则这种转换在修改对象时，结果未定义 reinterpret_cast：通常为算术对象的位模式提供较低层次上的重新解释。如将int*转换成char*。很危险！ dynamic_cast：一种动态类型识别。转换的目标类型，即type，是指针或者左右值引用，主要用于基类指针转换成派生类类型的指针(或引用)，通常需要知道转换源和转换目标的类型。如果​​转换失败，返回0（转换目标类型为指针类型时）或抛出bad_cast异常（转换目标类型为引用类型时） 旧式强制类型转换 type (expr) 或 (type) expr​ 旧式强制类型转换与const_cast，static_cast，reinterpret_cast拥有相似行为，如果换成const_cast，static_cast也合法，则其行为与对应命名转换一致。不合法，则执行与reinterpret_cast类似的行为 算术转换 既有浮点型也有整形时，整形将转换成相应浮点型 整形提升：bool,char,signed char,unsigned char,short,unsigned short所有可能值能存于int则提升为int，否则提升为unsigned int signed类型相同则转换成相同signed类型中的较大类型 unsigned类型大于等于signed类型时，signed转换成unsigned unsigned类型小于signed类型时： 如果unsigned类型所有值能存在signed类型中，则转换成signed类型 如果不能，则signed类型转换成unsigned类型​​ 初始化与赋值 很多语言中二者的区别几乎可以忽略，即使在C++中有时这种区别也无关紧要，所以特别容易把二者混为一谈 C++中初始化和赋值是2个完全不同的操作 显示初始化：创建变量时的赋值行为 拷贝初始化：int a = 0; 直接初始化：int a(0); 初始值列表：int a = {0}; 或 int a{0}; 默认初始化（程序） 局部变量 non-static：内置类型非静态局部变量不会执行默认初始化 static：如果没有初始值则使用值初始化 全局变量：内置类型全局变量初始化为0 值初始化 内置类型的值初始化为0​ container&lt;T&gt; c(n) 只指定了容器的大小，未指定初始值，此时容器内的元素进行值初始化 使用初始值列表时，未提供的元素会进行值初始化 静态局部变量会使用值初始化 声明与定义 声明：extern 类型 变量名字; 声明 + 定义：类型 变量名字; extern 类型 变量名字 = 值;（如果在函数内则会报错） 声明不会分配存储空间，定义会分配存储空间 作用域 访问被同名局部变量覆盖的全局变量：::变量名（不管有多少层覆盖，都是访问全局） 复合类型 引用 本质：引用并非对象，它只是为对象起了另一个名字 形式：int &amp;a = b; 理解与使用： 非常量引用不能绑定到字面值或表达式的计算结果 一般来说，引用类型和绑定的对象类型需严格匹配 程序把引用和其初始值绑定到一起（对引用的操作都在其绑定的对象上进行）因此一旦初始化完成，无法另引用重新绑定到另外一个对象。因此必须初始化 引用本身并非对象，故不能定义引用的引用 指针 指针不同与引用，指针本身就是一个对象 因为引用不是对象，没有实际地址，所以不能定义指向引用的指针 指针是一个对象，所以存在对指针的引用 一般来说，指针类型和指向的对象类型也需严格匹配 编译器并不负责检查试图拷贝或以其它方式访问无效指针 和试图使用未经初始化的变量一样，使用未经无效指针的后果无法估计 空指针：不指向任何对象（不要混淆空指针和空类型(void)的指针） int *p1 = nullptr; (C++11) int *p2 = 0; int *p3 = NULL; //#include cstdlib 把int变量直接赋给指针是错误的，即使变量的值恰好等于0 空类型(void)指针用于存放任意对象的地址 复合类型的声明 非数组与复合类型的声明 从右到左分析 1​int *&amp;r = p; //r是一个引用，引用一个int指针p 变量的定义包括一个基本数据类型和一组声明符。同一条语句中，虽然基本数据类型只有一个，但是声明的形式却可以不同： 1int* p1, p2; //p1是一个int*，p2是一个int 数组与复合类型的复杂申明 从数组名字开始，由内到外分析（数组的维度紧跟着被声明的名字，所以由内到外阅读比从右到左好多了​ ） 数组与指针的复杂申明 1int (*Parray)[10] = &amp;arr; //Parry是一个指针，指向一个含有10个int的数组​ 数组与引用的复杂申明 1int (&amp;arrRef)[10] = arr; //arrRef是一个引用，引用一个含有10个int的数组 数组与指针及引用的混合复杂申明 1​int *(&amp;arry)[10] = ptrs; //arry是一个引用，引用一个包含10个int指针的数组 const const对象 const对象必须初始化，因为创建后const对象的值就不能再改变，初始值可以是任意复杂的表达式 12const int i = get_size(); //运行时初始化const int j = 42; //编译时初始化 只能在const类型的对象上执行不改变其内容的操作 当以编译时初始化的方式定义一个const对象时，编译器将在编译过程中把用到该对象的地方替换成对应值 默认状态下，const对象仅在文件内有效。多个文件的同名const对象等同于在不同文件中定义了独立的变量 要在多个文件之间共享同一个const对象，需在定义和声明时都加上extern const的引用（常量引用） 不能修改所绑定的对象 和非常量引用不同，常量引用可以使用字面值或任意表达式作为初始值（原因:绑定了一个临时量常量） 指针与const 指向常量的指针(并不一定要指向常量，只是为了说明无法修改所指的对象) 1const int *a = &amp;b; const指针(常量指针)：不能修改指针，将一直指向一个地址，因此必须初始化。但是指向的对象不是常量的话，可以修改指向的对象 12int *const a = &amp;b;const double *const pip = &amp;pi; //pip是一个常量指针，指向的对象是一个双精度浮点型常量 顶层const与底层const 顶层const：无法修改指针本身（顶层是一种直接的关系） 12const int ci = 123; int *const a = &amp;b; 底层const：无法修改所指的对象（底层是一种间接的关系） 用于声明引用的const都是底层const constexpr与常量表达式 常量表达式：在“编译过程”就能确定结果的表达式。 包括： 字面值 常量表达式初始化的const对象 以下不是常量表达式 12int s = 123;const int sz = get_size(); constexpr变量(C++11)：变量声明为contexpr类型，编译器会检查变量的值是否是个常量表达式 123constexpr int mf = 20 //20是常量表达式constexpr int limit = mf + 1; //mf + 1是常量表达式const int sz = size(); //只有当size是一个constexpr函数时，声明才正确 constexpr函数：这种函数足够简单以使编译时就可以计算其结果 字面值类型：能使用constexpr声明的类型应该足够简单，称为字面值类型 包括 算数类型 引用 &amp; 指针 constexpr的指针初始值必须是nullptr，0或存储于某个固定地址中的对象 一般来说全局变量和静态局部变量的地址不变 constexpr指针，constexpr只对指针有效，与指针所指对象无关 constexpr const int *p = &amp;i //p是常量指针，指向整形常量i 不包括 自定义类型 I/O 库 string字符串 类型别名 typedef：typedef double wages; using（C++11)：using SI = Sales_item; const与指针的类型别名使用时，还原别名来理解const的限定是错误的 auto 编译器根据初始值判断变量类型 必须初始化 一条语句声明多个变量（只能有一个基本类型，const int和int不算1个类型） 12auto i = 0, *p = &amp;i; //正确auto sz = 0, pi = 3.14 //错误 初始值为引用时，类型为所引对象的类型 auto一般会忽略掉顶层const，底层const会保留下来 如果希望判断出的auto是一个顶层const，在auto前加const 还可以将引用的类型设为auto，此时原来的初始化规则仍然适用 decltype 希望根据表达式判定变量类型，但不用表达式的值初始化变量 decltype(f()) sum = x; f()并不会被调用，sum为f()的返回类型 引用从来都作为其所指对象的同义词出现，只有在decltype处是一个例外 如果表达式的结果对象能作为一条赋值语句的左值，则表达式将向decltype返回一个引用类型 1decltype(*p) c; //错误，c是int &amp;，必须初始化 变量加上括号后会被编译器视为一个表达式 1decltype((i)) d; //错误，d是int &amp;，必须初始化 模板与泛型编程 模板函数 1234567template &lt;typename T&gt;int compare (const T &amp;v1,const T &amp;v2)​{ if(v1 &lt; v2) return -1; if(v2 &lt; v1) return 1; return 0;​​​}​​ 当调用一个函数模板时，编译器(通常)用函数实参来为我们推断模板实参。编译器用推断出的模板参数为我们实例化一个特定版本的函数，这些编译器生成的版本通常被称为模板的实例 ​上面的模板函数说明了编写泛型代码的两个重要原则： 模板中的函数参数是const的引用（保证了函数可以用于不能拷贝的类型。同时，如果compare用于处理大对象，这种设计策略还能使函数运行得更快） 函数体中的条件判断仅使用&lt;比较运算（如果编写代码时只使用&lt;运算符，就降低了compare函数对要处理的类型的要求。这种类型必须支持&lt;，但不必支持&gt;。实际上，如果真的关系类型无关和可移植性，应该用less，因为&lt;无法比较指针，但是less可以） 函数模板可以声明为inline或constexpr的，如同非模板函数一样。inline或constexpr说明符放在模板参数列表之后，返回类型之前 模板参数 在模板定义中，模板参数列表不能为空 模板参数的名字没有什么内在含义，通常将类型参数命名为T，但实际上可以使用任何名字 一个模板参数名的可用范围是在其声明之后，至模板声明或定义结束之前。模板参数会隐藏外层作用域中声明的相同名字，模板内不能重用模板参数名 与函数参数相同，声明中的模板参数的名字不必与定义中相同； typename和class并没有什么不同，typename可能更直观，因为class可能会让人觉得能使用的类型必须是类类型 模板类型参数 用来指定返回类型或函数类型，以及在函数体内用于变量声明或类型转换 1234567//T用作了返回类型、参数类型、变量类型template &lt;typename T&gt; T foo (T* p){ T tmp = *p; //... return tmp;​​​}​​​ 非类型模板参数 12345template&lt;unsigned N,unsigned M&gt;int compare(const char (&amp;p1)[N], const char (&amp;p2) [M]){ return strcmp(p1,p2);​} 第一个非类型模板参数表示第一个数组的长度 第二个非类型模板参数表示第二个数组的长度 当调用这个模板时，compare(\"hi\",\"mom\"); 编译器会使用字面常量的大小来代替N和M，从而实例化模板 非类型模板参数包括： 整形：绑定到非类型整形参数的实参必须是一个常量表达式 指针或引用：绑定到指针或引用非类型参数的实参必须具有静态的生存期，不能用一个普通局部变量或动态对象作为指针或引用非类型模板参数的实参。指针也可以用nullptr或一个值为0的常量表达式来实例化 函数形参 模板函数的形参中可以含有正常类型。即，不一定全必须是模板类型： 1234template &lt;typename T&gt; ostream &amp;print(ostream &amp;os,const T &amp;obj)​{ return os &lt;&lt; obj;}​​​ 成员模板 普通类的成员模板 12345678class DebugDelete {public: DebugDelete(std::ostream &amp;s = std::cerr) : os(s) { } template &lt;typename T​&gt; void operator( ) (T *p) const {os &lt;&lt; &quot;deleting unique_ptr&quot; &lt;&lt; std::endl;delete p;}private: std::ostream &amp;os；​​​​};​ 类模板的成员模板 类和成员各自有自己的独立的模板参数 123​template &lt;typename T&gt; class Blob { template &lt;typename It&gt; Blob(It b,It e);​}​ 当在类外定义成员模板时，必须同时为类模板和成员模板提供模板参数： 123template &lt;typename T&gt;template &lt;typename It&gt;Blob&lt;T&gt;::Blob(It b,It e) : data(...) {...}​​​​ 实例化成员模板： 123456int ia[ ] = {0,1,2,3,4,5,6,7,8,9};vector&lt;long&gt; vi = {0,1,2,3,...};list&lt;const char*&gt; w = {&quot;now&quot;,&quot;is&quot;,&quot;the&quot;};​​​Blob&lt;int&gt; a1(begin(ia),end(ia));​​Blob&lt;int&gt; a2(vi.begin( ),vi.end( ));Blob&lt;string&gt; a3(w.begin( ),w.end( ));​​ 类模板 12345template &lt;typename T&gt; class Blob { //typename告诉编译器size_type是一个类型而不是一个对象​ typedef typename std::vector&lt;T&gt;::size_type size_type ​//...​};​​​ 一个类模板的每个实例都形成一个独立的类： 12Blob&lt;string&gt; names;Blob&lt;double&gt; prices; 与模板函数的区别 编译器不能为类模板推断模板参数类型 使用时必须在模板名后的尖括号中提供额外信息 模板类名的使用 类内使用不需要指明 1BlobPtr&amp; operator++( ); 当处于一个类模板的作用域中时，编译器处理模板自身引用时就好像我们已经提供了与模板参数匹配的实参一样 ​ #### 类外使用需要指明 12345template &lt;typename T&gt;​BlobPtr&lt;T&gt; BlobPtr&lt;T&gt;::operator++(int)​{​ //...}​​ 由于位于类作用域外，必须指出返回类型是一个实例化的BlobPtr，它所用类型与类实例化所用类型一致 类模板的成员函数 类外定义成员函数时要加 template&lt;typename T&gt;。类模板的成员函数具有和模板相同的模板参数。因此，定义在类模板之外的成员函数就必须以关键字template开始，后接类模板参数列表： 12template &lt;typename T&gt;ret-type Blob&lt;T&gt;::member-name(parm-list) 对于​一个实例化了的类模板，其成员函数只有当程序用到它时才进行实例化 12//实例化Blob&lt;int&gt;和接受initializer_list&lt;int&gt;的构造函数Blob&lt;int&gt; squares = {0,1,2,3,4,5,6,7,8,9}​​； 如果一个成员函数没有被使用，则它不会被实例化，成员函数只有在被用到时才会进行实例化，这一特性使得即使某种类型不能完全符合模板操作的要求，我们仍然能用该类型实例化类 类型成员 假定T是一个模板类型参数，当编译器遇到类似T::mem这样的代码时，它不会知道mem是一个类型成员还是一个static数据成员，直至初始化时才会知道。但是，为了处理模板，编译器必须知道名字是否表示一个类型。例如，假定T是一个类型参数的名字，当编译器遇到如下形式的语句时： 1T::size_type *p; 它需要知道我们是整在定义一个名为p的变量还是一个名为size_type的static数据成员与名为p的变量相乘 默认情况下，C++假定通过作用域运算符访问的名字不是类型。因此，如果我们希望使用一个模板类型参数的类型成员，就必须显示告诉编译器该名字是一个类型。通过关键字typename来实现这一点 类模板和友元 普通类中将另一模板类声明为友元 12345678template &lt;typename T&gt; class Pal;​class C { //用类C实例化的Pal是C的一个友元​ friend class Pal&lt;C&gt;; //Pal2的所有实例都是C的友元​ template &lt;typename T&gt; friend class Pal2;​​};​​ 模板类中将另一模板类声明为友元 12345678template &lt;typename T&gt; class Pal;​template &lt;typename T&gt; class C2 { //C2的每个实例将相同实例化的Pal声明为友元​ friend class Pal&lt;T&gt;; //Pal2的所有实例都是C2的每个实例的友元 template &lt;typename X&gt; friend class Pal2;​​};​​ 为了让所有实例成为友元，友元声明中必须使用与类模板本身不同的模板参数（上面的X） 令模板自己的类型参数成为友元 1234template &lt;typename T&gt; class Bar{ //将访问权限授予用来实例化Bar的类型 ​friend T;​};​ 对于某个类型Foo，Foo将成为Bar的友元...​ 模板类型别名 类模板的一个实例化定义了一个类类型，可以定义一个typedef来引用实例化的类： 1typedef Blob&lt;string&gt; StrBlob; 由于模板不是一个类型，所以不能定义一个typedef引用一个模板。即，无法定义一个typedef引用Blob&lt;T&gt; 但是，新标准允许我们为类模板定义一个类型别名： 12template &lt;typename T&gt; using twin = pair&lt;T,T&gt;;​​​​​twin&lt;string&gt; authors; //authors是一个pair&lt;string,string&gt;； 定义一个模板类型别名时，可以固定一个或多个模板参数； 12template &lt;typename T&gt; using partNo = pair&lt;T,unsigned&gt;；partNo&lt;string&gt; books; //pair&lt;string,unsigned&gt;；​​​​ 类模板的static成员 static属于每个实例化的类类型，而不是类模板。即，每个实例化的类都有一个自己对应的static成员 模板类的每个static成员必须有且仅有一个定义。但是，类模板的每个实例都有一个独有的static对象 12template &lt;typename T&gt;size_t Foo&lt;T&gt;::ctr = 0;​​​ 可通过类类型对象或作用域运算符访问： 123Foo&lt;int&gt; f1;auto ct = Foo&lt;int&gt;::count( );ct = f1.count( );​​ 只有使用时才会实例化 模板编译 遇到模板时不生成代码，实例化时生成代码 函数模板和类模板成员函数的定义通常放在头文件中 实例化冗余：当模板被使用时才会进行实例化这一特性意味着，相同的实例可能出现在多个对象文件中。当两个或多个独立编译的源文件使用了相同的模板，并提供了相同的模板参数时，每个文件中就都会有该模板的一个实例 实例化声明 形式：extern template declaration 12extern template class Blob&lt;string&gt;;​extern template int compare(const int&amp;,const int&amp;); 当遇到extern模板声明时，不会在本文件中生成实例化代码。将一个实例化声明为extern就表示承诺在程序其他位置有该实例化的一个定义。对于一个给定的实例化版本，可能有多个extern声明，但必须只有一个定义 实例化声明可以有多个：即多个源文件可能含有相同声明 实例化声明必须出现在任何使用此实例化版本的代码之前。因为编译器在使用一个模板时会自动对其实例化 实例化定义 123template declarationtemplate int compare(const int &amp;,const int&amp;);​template class Blob&lt;string&gt;;​ 类模板的实例化定义会实例化该模板的所有成员 所用类型必须能用于模板的所有成员：与处理类模板的普通实例化不同，编译器会实例化该类的所有成员。即使我们不使用某个成员，它也会被实例化。因此，我们用来显式实例化一个类模板的类型，必须能用于模板的所有成员 模板参数 默认模板实参 为模板提供默认类型 模板函数 1234567template &lt;typename T,typename F = less&lt;T&gt;&gt;int compare(const T &amp;v1,const T &amp;v2,F f = F( )){ if(f(v1,v2)) return -1; if(f(v2,v1)) return 1; return 0;​​​}​​​ 和函数默认实参一样，所有提供了默认实参的形参右边的形参都需要提供默认实参​ 类模板 123456789template &lt;class T = int&gt; class Numbers {public: Numbers(T v = 0) : val(v) { } private: T val;​​​​​};​Numbers&lt;long double&gt; lots_of_precision;Numbers&lt;&gt; average_precision; //空&lt;&gt;表示希望使用默认类型；​​ 模板实参推断 函数模板的参数转换 模板类型参数的类型转换：将实参传递给带模板类型的函数形参时，能够自动应用的类型转换只有const转换及数组或函数到指针的转换 const的转换 可以将一个const对象传递给一个非const的非引用形参 1234template &lt;typename T&gt; fobj(T,T);string s1(&quot;a value&quot;);const string s2(&quot;another value&quot;);​​​​fobj(s1,s2); //正确； fobj调用中，传递了一个string和一个const string。虽然这些类型不严格匹配，但两个调用都是合法的，由于实参被拷贝，因此原对象是否是const没有关系；​ 可以将一个非const对象的引用(或指针)传递给一个const的引用(或指针)形参 1234template &lt;typename T&gt; fref(const T&amp;,const T&amp;);​string s1(&quot;a value&quot;);const string s2(&quot;another value&quot;);fref(s1,s2); //正确； 在fref调用中，​​​参数类型是const的引用。对于一个引用参数来说，转换为const是允许的，因此合法； 非引用类型形参可以对数组或函数指针应用正常的指针转换 12345template &lt;typename T&gt; fobj(T,T);template &lt;typename T&gt; fref(const T&amp;,const T&amp;);​int a[10],b[42];​fobj(a,b); //调用fobj(int*,int*)fref(a,b); //错误，数组类型不匹配； ​​​​​​在fobj调用中，数组大小不同无关紧要，两个数组都被转换为指针。fobj中的模板类型为Int*；但是，fref调用是不合法的，如果形参是一个引用，则数组不会转换为指针。a和b的类型不匹配 普通类型参数的类型转换：模板函数可以有用普通类型定义的参数，即，不涉及模板类型参数的类型。这种函数实参不进行特殊处理，这些实参执行正常类型的转换 显示实参 为什么需要显示实参？编译器无法推断出模板实参的类型。假设定义如下模板: 12​template &lt;typename T&gt;T sum(T,T);​ ​则调用sum时，必须要求传入相同类型的参数，否则会报错。因此可以按这种方式定义模板： 12​​template &lt;typename T1,typename T2,typename T3&gt;T1 sum(T2,T3);​​ 但是，这种情况下，无论传入什么函数实参，都无法推断T1的类型。因此，每次调用sum时，调用者必须为T1提供一个显示实参： 1auto val3 = sum&lt;long long&gt;(i,lng);​ 这个调用显示指定了T1的类型，而T2和T3的类型则由编译器从i和lng的类型判断出来 显示实参配对顺序：由左至右。只有尾部参数的显示模板实参可以忽略，但必须能推断出来 因此，如果按找这种形式定义模板： 12template &lt;typename T1,typename T2,typename T3&gt;T3 sum(T2,T1); 则总是必须为所有三个形参指定参数。希望控制模板实例化 对于sum模板，如果保留原有的设计：template T sum(T,T) 则当函数调用传入不同类型的参数时，我们必须放弃参数类型推断，采取控制模板实例化的方式来调用：sum&lt;int&gt;(long,1024); 这种情况下，会实例化一个int sum(int,int)的函数，传入的参数都会按照内置类型的转换规则转换为int 尾置返回类型与traits 当我们希望用户确定返回类型时，用显示模板实参表示模板函数的返回类型是很有效的。在其他情况下，要求显示指定模板实参会给用户增添额外负担，而且不会带来什么好处： 123456template &lt;typename It&gt;??? &amp;fcn(It beg,It end){ //处理序列 return *beg;}​​​​​​ 在这个例子中，并不知道返回结果的准确类型，但知道所需类型是所处理的序列的元素类型;我们知道函数应该返回*beg，可以使用decltype(*beg)来获取此表达式的类型。但是在编译器遇到函数的参数列表之前，beg是不存在的。所以必须使用尾置类型： ​​template &lt;typename It&gt; auto fcn(It beg,It end) -&gt; decltype(*beg) { //处理序列 return *beg;//返回序列中一个元素的引用 }​​​​​​ 也可以使用标准库的类型转换模板。​可以使用remove_reference来获得元素类型。这个模板有一个模板类型参数和一个名为type的成员。如果用一个引用类型实例化这个模板，则type将表示被引用的类型。如果实例化remove_reference&lt;int&amp;&gt;，则type成员将是int。因此，可以通过下列模板满足需求： 1234567template &lt;typename It&gt;auto fcn2(It beg,It end) -&gt;typename remove_reference&lt;decltype(*beg)&gt;::type{ ​ //处理序列 return *beg;}​​​​​​ 函数指针和实参推断 用一个函数模板初始化一个函数指针或为一个函数指针赋值时，编译器使用指针的类型来推断模板实参 12template &lt;typename T&gt; int compare(const T&amp;,const T&amp;);int (*pf1)(const int&amp;,const int&amp;) = compare; pf1中参数的类型决定了T的模板实参的类型。如果不能从函数指针类型确定模板实参，则产生错误： 123void func(int(*)(const string&amp;,const string&amp;));​​​​void func(int(*)(const int&amp;,const int&amp;));​​​​​func(compare); //错误，使用那个实例? 对于这种情况，可以使用显示模板实参： 1func(compare&lt;int&gt;);​​​ 引用与实参推断 非常重要的是记住两个规则： 编译器会应用正常的引用绑定规则； const是底层的，不是顶层的；​​ 当一个函数的参数是模板类型参数的一个普通(左值)引用时，绑定规则告诉我们，只能传递给它一个左值： 1234​template &lt;typename T&gt; void f1(T&amp;);f1(i); //i是int，T推断为int；f1(ci); //ci是const int，T推断为const int；f1(5);​​​ //错误 如果参数类型是const T&amp;，正常的绑定规则告诉我们可以传递给它任何类型的实参：一个对象，临时对象或字面值常量​： 1234​​template &lt;typename T&gt; void f2(const T&amp;);f2(i); //i是int，T推断为int；f2(ci); //ci是const int，但T推断为int；f2(5); //T推断为int；​ 当参数是一个右值引用时，正常绑定规则告诉我们可以传递给它一个右值： 12template &lt;typename T&gt; void f3(T&amp;&amp;);f3(42); //实参是int型的右值，T推断为int；​​ 引用折叠： ​1. 如果将一个左值传递给函数的右值引用参数，且此右值引用指向模板类型参数(如:T&amp;&amp;)时，编译器推断模板的类型参数为左值引用类型 2. 如果因为1.间接的创建了一个引用的引用，则引用形参了“折叠”、则： * 右值引用的右值引用会被折叠成右值引用 * 其它情况下都折叠成左值引用 因此，对于前面的f3： 12f3(i)​; //i是左值，T推断为int&amp;，T&amp;&amp;被折叠成int &amp;；f3(ci); //​​​​​ci是左值，T是const int&amp;； ​​因此，如果模板参数类型为右值引用，可以传递给它任意类型的实参 右值引用的问题：因为可以传递任意实参，引用折叠会导致T被推断为引用或非引用类型，所以函数内使用这个类型在传入不同参数时可能产生不同结果，此时，编写正确的代码就变得异常困难； 右值引用的使用场景：因为上述问题，所以右值引用主要应用于两个场景 模板转发其实参：当使用右值引用作为模板参数时，如果T被推断成普通类型(即非引用)，可以通过std::forward保持其右值属性，会返回一个T&amp;&amp;。如果被推断成一个(左值)引用，通过引用折叠，最终也还是会返回T&amp;；因此，当用于一个指向模板参数类型的右值引用函数参数(T&amp;&amp;)时，forward会保持实参类型的所有细节 模板被重载 重载与模板 包含模板的函数匹配规则： 候选函数包括所有模板实参推断成功的函数模板实例 12345template &lt;typename T&gt; string debug_rep(const T &amp;t) {...}template &lt;typename T&gt; string debug_rep(T *p) {...}​​string s(&quot;hi&quot;);//第二个模板实参推断失败，所以调用第一个模板；cout &lt;&lt; debug_rep(s) &lt;&lt; endl; 可行函数按类型转换来排序 如果恰好有一个比其他提供更好的匹配则使用该函数 12345678template &lt;typename T&gt; string debug_rep(const T &amp;t) {...}template &lt;typename T&gt; string debug_rep(T *p) {...}string s(&quot;hi&quot;);//两个模板都能匹配：//第一个模板实例化debug_rep(const string*&amp;)，T被绑定到string*；//第二个模板实例化debug_rep(string*)，T被绑定到string；​​​//但由于第一个实例化版本需要进行普通指针到const指针的转换，所以第二个更匹配；​cout &lt;&lt; debug_rep(&amp;s) &lt;&lt; endl;​​​​ 如果有多个函数提供“同样好的”匹配 同样好的函数中只有一个是非模板函数，则选择此函数 1234567891011121314151617181920212223 template &lt;typename T&gt; string debug_rep(const T &amp;t) {...} template &lt;typename T&gt; string debug_rep(T *p) {...} string debug_rep(const string &amp;s) {...}​​ ​​string s(&quot;hi&quot;); //以下调用有两个同样好的可行函数： //第一个模板实例化debug_rep&lt;string&gt;(​const string &amp;)，T被绑定到string； //非模板版本debug_rep(const string &amp;s)； //编译器会选择非模板版本，因为最特例化；​​ ​cout &lt;&lt; debug_rep(s) &lt;&lt; endl; ​```* 同样好的函数中全是模板函数，选择更“特例化的模板” ```c++ template &lt;typename T&gt; string debug_rep(const T &amp;t) {...} template &lt;typename T&gt; string debug_rep(T *p) {...} ​​string s(&quot;hi&quot;); ​const string​ *sp = &amp;s; //以下调用两个模板实例化的版本都能精确匹配： //第一个模板实例化debug_rep(​const string *&amp;)，T被绑定到string*； //第二个模板实例化debug_rep(const string *)，T被绑定到const string；​ //我们可能觉得这个调用是有歧义的。但是，根据重载函数模板的特殊规则，调用被解析为debug_rep(T*)，即更特例化的版本； //如果不这样设计，将无法对一个const的指针调用指针版本的debug_rep。 //问题在于模板debug_rep(const T&amp;)本质上可以用于任何类型，包括指针类型。此模板比debug_rep(T*)更通用，后者只能用于指针类型；​​ ​cout &lt;&lt; debug_rep(sp) &lt;&lt; endl; 否则，调用有歧义 可变参数模板 参数包： 模板参数包： template&lt;typename T,typename... Args&gt; Args为模板参数包，class...或typename...指出接下来的参数表示零个或多个类型的列表，一个类型名后面跟一个省略号表示零个或多个给定类型的非类型参数的列表；​​ 函数参数包 12template &lt;typename T,typename... Args&gt;void foo(const T &amp;t,const Args&amp; ... rest); rest为函数参数包 使用参数包： sizeof... 获取参数包大小。可以使用sizeof...运算符获取包中元素的数目 扩展包：扩展一个包就是将包分解为构成的元素，对每个元素应用模式，获得扩展后的列表，通过在模式右边放一个省略号来触发扩展操作： 123456template &lt;typename T,typename... Args&gt;ostream&amp; print(ostream &amp;os,const T &amp;t,const Args&amp;... rest) //扩展Args​​​​{ os &lt;&lt; t &lt;&lt; ​&quot;, &quot;; return print(os,rest...); //扩展rest}​​ 扩展中的模式会独立地应用于包中的每个元素： 12debug_res(rest)... 是对包rest的每一个元素调用debug_res；debug_res(rest...) 是调用一个参数数目和类型与rest中元素匹配的debug_rest；​​​ 转发包参数： 新标准下，可以组合使用可变参数模板与forward机制来编写函数，实现将其参数不变地传递给其他函数： 123456template &lt;typename... Args&gt;void fun(Args&amp;&amp;... args) //将Args扩展为一个右值引用的列表 { //work的实参既扩展Args又扩展args work(std::forward&lt;Args&gt;(args)...);​​}​​​​ 模板特例化 编写单一模板，使之对任何可能的模板实参都是最合适的，都能实例化，这并不总是能办到。当我们不能（或不希望）使用模板版本时，可以定义类或函数模板的一个特例化版本 一个特例化版本本质上是一个实例，而非函数名的一个重载版本。因此，特例化不影响函数匹配； 函数模板特例化 1234567891011121314151617 template &lt;typename T&gt; int compare(const T&amp;,const T&amp;); //compare函数模板的通用定义不适合字符指针的情况， //我们希望compare通过strcmp比较两个字符指针而非比较指针值； template &lt;&gt; int compare(const char* const &amp;p1,const char* const &amp;p2​) { return strcmp(p1,p2); ​}​​​​ ​``` 当定义一个特例化版本时，函数参数类型必须与一个先前声明的模板中对应的类型匹配。这个特例化版本中，`T`为`const char*`，​先前声明的模板要求一个指向此类型const版本的引用。一个指针类型的const版本是一个常量指针而不是指向const类型的指针。需要在特例化版本中使用的类型是`const char* const &amp;`，即一个指向`const char`的const指针的引用；* **类模板特例化** ```c++ template &lt;&gt; struct 模板类名&lt;Sales_data&gt; { ...​ }​​​ 定义了某个模板能处理Sales_data的特例化版本 类模板（偏特化）部分特例化：与函数模板不同，类模板的特例化不必为所有模板参数提供实参。可以只提供一部分而非所有模板参数，或是参数的一部分而非全部特性。部分特例化本身是一个模板，部分特例化版本的模板参数列表是原始模板的参数列表的一个子集或者是一个特例化版本 12345678910111213//原始的，最通用的版本template &lt;class T&gt; struct remove_reference​​ { typedef T type;​ };//部分特例化版本，将用于左值引用和右值引用template &lt;class T&gt; struct remove_reference&lt;T&amp;&gt;{ typedef T type; };​template &lt;class T&gt; struct remove_feference&lt;T&amp;&amp;&gt;{ typedef T type; };//用例int i;remove_reference&lt;decltype(42)&gt;::type a; //decltype(42)为int，使用原始模板​；remove_reference&lt;decltype(i)&gt;::type b; //decltype(i)为int&amp;，使用第一个部分特例化版本；​​remove_reference&lt;decltype(std::move(i))&gt;::type c;​​​​​​​​​​​​​​​ //decltype(std::move(i))为int&amp;&amp;，使用第二个部分特例化版本； 特例化成员而非类 123456789template &lt;&gt;void Foo&lt;int&gt;::Bar( ){ //进行应用于int的特例化处理；​​​}Foo&lt;string&gt; fs; //实例化Foo&lt;string&gt;::Foo( );fs.Bar( ); //实例化Foo&lt;string&gt;::Bar( );Foo&lt;int&gt; fi; //实例化Foo&lt;int&gt;::Foo( );fi.Bar( ); //使用特例化版本的Foo&lt;int&gt;::Bar( ); 内存管理 new和delete new 动态分配单个对象 初始化： 123456789int *pi1 = new int; //默认初始化int *pi2 = new int(); //值初始化int *pi2 = new int(1024); //直接初始化string *ps = new string(10,'9');//若obj是一个int，则p1是int*；//不能用{...}代替(obj)包含多个对象；​auto p1 = new auto(obj); 动态分配const对象： 必须进行初始化 不能修改指向的对象，但是能delete(销毁)这个动态分配的const对象 12const int *pci = new const int(1024);const string *pcs = new const string; //隐式初始化 内存耗尽： 内存不足时，new会失败 抛出类型为bad_alloc的异常 new (nothrow) T 可以阻止抛出异常（定位new） 动态分配多个对象 使用注意： 大多数应用应该使用标准库容器而不是动态分配的数组 动态分配数组的类必须定义自己版本的拷贝，复制，销毁对象的操作 理解： 通常称new T[]分配的内存为“动态数组”某种程度上有些误导 返回的并不是一个“数组类型”的对象，而是一个”数组元素类型“的指针 即使使用类型别名也不会分配一个数组类型的对象 不能创建大小为0的动态数组，但当[n]中n为0时，是合法的。此时new返回一个合法的非空指针，次指针保证与new返回的其它任何指针都不同，就像尾后指针一样，可以进行比较操作，加0，减0，不能解引用 初始化： 12345678910int *pia = new int[get_size()]; //维度不必是常量，但是必须是整形int *p1 = new int[42]; //未初始化//以下为上一行的等价调用​typedef int arrT[42]；int *p = new arrT;​int *p2 = new int[42](); //值初始化//初始值列表中没有给定初始值的元素进行”值初始化“，如果初始值列表中元素超出，new会失败int *p3 = new int[5]{1,2,3,4,5}; delete delete单个对象：delete p; delete动态数组：delete [] pa; 不管分配时有没有用类型别名，delete时都要加上[] 逆序销毁 []指示编译器指针指向的是一个数组的首元素 注意： 不要delete非new分配的对象 不要重复delete 可以delete空指针 可以delete动态分配的const对象 通常情况下，编译器不能分辨一个指针指向的是静态还是动态分配的对象。类似的，编译器也不能分辨一个指针所指向的内存是否已经被释放了。对于这些delete表达式，大多数编译器能通过，尽管它们是错误的。这些错误delete的结果是未定义的 空悬指针：指向原本存在数据现在已经无效的内存的指针 当delete一个动态分配的对象后，原本指向这个对象的指针就变成了空悬指针 防止使用空悬指针（只能保证这个指针不会再访问无效内存，但是可能也还有其它指针也指向这块动态分配的内存，它们在delete后也可能会访问） 在即将离开指针作用域时delete：这样之后，当离开作用域后这个指针就销毁了，而在delete前，指针指向的内存是有效的 delete后赋值为空指针nullptr 智能指针 通用操作 以下操作支持shared_ptr和unique_ptr 创建 123//默认初始化，保存一个空指针shared_ptr&lt;T&gt; sp;unique_prt&lt;T&gt; up; 作为条件：p 访问指向的对象：*p 获取保存指针：p.get() 不要delete get()返回的指针，假设delete没问题，在引用计数为0时，智能指针会重复delete 如果p是shared_ptr，不要用get()​​返回的指针初始化另一个shared_ptr，这样不会递增引用计数，当新建智能指针销毁后，这个动态对象就被释放了 交换 12swap(p,q);p.swap(q); shared_ptr 创建： 调用函数make_shared make_shared&lt;T&gt;(args)：推荐使用这种方式。args用于初始化指向的对象，不传参数时”值初始化“ 123shared_ptr&lt;int&gt; p1 = make_shared&lt;int&gt;(42);​​​ //动态对象初始化为42​​​shared_ptr&lt;string&gt; p2 = make_shared&lt;string&gt;(10,'9'); //动态对象初始化为&quot;9999999999&quot;shared_ptr&lt;int&gt; p3 = make_shared&lt;int&gt;(); //动态对象值初始化，0​​​​​​​ 使用构造函数 shared_ptr&lt;T&gt; p(q) q为shared_ptr时，会递增q的引用计数​ 构造函数为explicit，如果q不是一个智能指针，必须直接初始化​，此时q必须能转换为T*，如shared_ptr&lt;int&gt; p(new int(1024)) 如果q不是一个指向动态内存的指针，须自定义释放操作（shared_ptr默认使用delete释放所指动态对象，如果指针不指向动态内存，不能delete） q不是智能指针时，这种方式构建临时shared_ptr很危险（比如一个函数参数为shared_ptr，由于explicit，因此不能隐式转换。如果q是new int创建​的内置类型指针，则可能通过这个构造函数创建一个临时shared_ptr来满足调用要求，这样的话当函数返回时，两个shared_ptr(形参与实参)都被销毁，所以函数外部原本指针指向的动态对象会被释放掉，在函数调用之后再使用就是空悬指针，因此，最好使用make_shared来创建智能指针） shared_ptr&lt;T&gt; p(q,d)：d是可调用对象，用于代替delete执行释放操作，在这里q可以不指向动态内存 shared_ptr&lt;T&gt; p(p2,d)：p是shared_ptr p2的拷贝，但是使用可调用对象d代替delete执行释放操作 shared_ptr&lt;T&gt; p(u)：从unique_ptr u那里接管了对象的所以权，将u置为空 赋值 1p = q; //递增q引用计数，递减p引用计数 重置 123456789101112// 1）若p是唯一指向其对象的shared_ptr，则释放对象；// 2）​将p置为空；p.reset();// 1）若p是唯一指向其对象的shared_ptr，则释放对象；// 2）​p = q；p.reset(q);// 1）若p是唯一指向其对象的shared_ptr，则释放对象；// 2）​p = q；// 3）d代替delete执行释放操作；​p.reset(q,d); 状态 12345//返回与p共享对象的智能指针数量；可能很慢，主要用于调试p.use_count();//若use_count()为1则返回true，否则返回falsep.unique(); unique_ptr 初始化 12345unique_ptr&lt;T&gt; u1; //创建一个空的unique_ptrunique_ptr&lt;T D&gt; u2; //D为自定义释放操作的类型//D为自定义释放操作的类型，d为自定义释放操作的指针。这里没有传入指针参数，是一个空unique_ptrunique_ptr&lt;T,D&gt; u(d); unique_ptr&lt;T,D&gt; u(T*,d); 赋值与拷贝 只有在unique_ptr即将销毁时才能赋值或拷贝。如：当函数返回一个局部unique_ptr时 交出控制权 123456//返回指针，放弃对指针的控制权，并将u置为空//不会释放，主要目的在于切断与原来管理对象的联系，将其交由其它unique_ptr来管理u.release()p.release() //内存泄露auto pp = p.release() //要记得delete pp​​​​​ 释放 1234u = nullptr; //释放u指向的对象，将u置为空；u.reset(); //释放u指向的对象，并将u置为空；u.reset(q); //释放u指向的对象，转为控制指针p指向的对象u.reset(nullptr); //释放u指向的对象，并将u置为空； 管理动态数组 shared_ptr不直接管理动态数组，如果要用shared_ptr来管，须提供自定义的删除操作，因为默认情况下shared_ptr使用delete销毁所指对象。但即使如此，也不能用下标访问每个元素，需要用get()函数。unique_ptr可以用下标访问​ 12345unique_ptr&lt;int[]&gt; up(new int[10]); //创建up.release(); //放弃对指针的控制权，并将u置为空（不会释放。测试如此，和书本不同）up[i]; //返回位置i处的对象，左值； weak_ptr 初始化 12345//空weak_ptr，可以指向类型为T的对象weak_ptr&lt;T&gt; w;//与shared_ptr sp指向相同对象的weak_ptr，T必须能转换为sp指向的类型weak_ptr&lt;T&gt; w(sp); 赋值 1w = p; //p是shared_ptr或weak_ptr，赋值后w与p共享对象 重置 1w.reset(); //将w置为空（不会释放对象） 状态 12345//返回与w共享对象的“shared_ptr”的数量w.use_count();//如果共享对象的&quot;shared_ptr&quot;为0(没有共享对象的shared_ptr)，则返回true，否则返回falsew.expired(); 访问 12345//获取shared_ptr// 如果没有共享对象的shared_ptr，则返回一个空的shared_ptr；// 否则返回一个指向共享对象的shared_ptr；//这种访问方式提供了对动态对象的安全访问；​w.lock();","link":"/posts/1200417882.html"},{"title":"使效率倍增的Pandas使用技巧","text":"本文取自Analytics Vidhya的一个帖子12 Useful Pandas Techniques in Python for Data Manipulation，浏览原帖可直接点击链接，中文版可参见Datartisan的用 Python 做数据处理必看：12 个使效率倍增的 Pandas 技巧。这里主要对帖子内容进行检验并记录有用的知识点。 ## 数据集 首先这个帖子用到的数据集是datahack的贷款预测(load prediction)竞赛数据集，点击链接可以访问下载页面，如果失效只需要注册后搜索loan prediction竞赛就可以找到了。入坑的感兴趣的同学可以前往下载数据集，我就不传上来github了。 先简单看一看数据集结构(此处表格可左右拖动)： Loan_ID Gender Married Dependents Education Self_Employed ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History Property_Area Loan_Status LP001002 Male No 0 Graduate No 5849 0 360 1 Urban Y LP001003 Male Yes 1 Graduate No 4583 1508 128 360 1 Rural N LP001005 Male Yes 0 Graduate Yes 3000 0 66 360 1 Urban Y LP001006 Male Yes 0 Not Graduate No 2583 2358 120 360 1 Urban Y LP001008 Male No 0 Graduate No 6000 0 141 360 1 Urban Y 共614条数据记录，每条记录有十二个基本属性，一个目标属性(Loan_Status)。数据记录可能包含缺失值。 数据集很小，直接导入Python环境即可： 1234import pandas as pdimport numpy as npdata = pd.read_csv(r&quot;F:\\Datahack_Loan_Prediction\\Loan_Prediction_Train.csv&quot;, index_col=&quot;Loan_ID&quot;) 注意这里我们指定index_col为Loan_ID这一列，所以程序会把csv文件中Loan_ID这一列作为DataFrame的索引。默认情况下index_col为False，程序自动创建int型索引，从0开始。 1. 布尔索引(Boolean Indexing) 有时我们希望基于某些列的条件筛选出需要的记录，这时可以使用loc索引的布尔索引功能。比方说下面的代码筛选出全部无大学学历但有贷款的女性列表： 1data.loc[(data[&quot;Gender&quot;]==&quot;Female&quot;) &amp; (data[&quot;Education&quot;]==&quot;Not Graduate&quot;) &amp; (data[&quot;Loan_Status&quot;]==&quot;Y&quot;), [&quot;Gender&quot;,&quot;Education&quot;,&quot;Loan_Status&quot;]] Gender Education Loan_Status Loan_ID LP001155 Female Not Graduate Y LP001669 Female Not Graduate Y LP001692 Female Not Graduate Y LP001908 Female Not Graduate Y LP002300 Female Not Graduate Y LP002314 Female Not Graduate Y LP002407 Female Not Graduate Y LP002489 Female Not Graduate Y LP002502 Female Not Graduate Y LP002534 Female Not Graduate Y LP002582 Female Not Graduate Y LP002731 Female Not Graduate Y LP002757 Female Not Graduate Y LP002917 Female Not Graduate Y loc索引是优先采用label定位的，label可以理解为索引。前面我们定义了Loan_ID为索引，所以对这个DataFrame使用loc定位时就可以用Loan_ID定位： 1data.loc['LP001002'] 123456789101112131415Gender MaleMarried NoDependents 0Education GraduateSelf_Employed NoApplicantIncome 5849CoapplicantIncome 0LoanAmount 129.937Loan_Amount_Term 360Credit_History 1Property_Area UrbanLoan_Status YLoanAmount_Bin mediumLoan_Status_Coded 1Name: LP001002, dtype: object 可以看到我们指定了一个Loan_ID，就定位到这个Loan_ID对应的记录。 loc允许四种input方式: 指定一个label; label列表，比如['LP001002','LP001003','LP001004']; label切片，比如'LP001002':'LP001003'; 布尔数组 我们希望基于列值进行筛选就用到了第4种input方式-布尔索引。使用()括起筛选条件，多个筛选条件之间使用逻辑运算符&amp;,|,~与或非进行连接，特别注意，和我们平常使用Python不同，这里用and,or,not是行不通的。 此外，这四种input方式都支持第二个参数，使用一个columns的列表，表示只取出记录中的某些列。上面的例子就是只取出了Gender,Education,Loan_Status这三列，当然，获得的新DataFrame的索引依然是Loan_ID。 想了解更多请阅读 Pandas Selecting and Indexing。 2. Apply函数 Apply是一个方便我们处理数据的函数，可以把我们指定的一个函数应用到DataFrame的每一行或者每一列(使用参数axis设定，默认为0，即应用到每一列)。 如果要应用到特定的行和列只需要先提取出来再apply就可以了，比如data['Gender'].apply(func)。特别地，这里指定的函数可以是系统自带的，也可以是我们定义的(可以用匿名函数)。比如下面这个例子统计每一列的缺失值个数： 12345678910# 创建一个新函数:def num_missing(x): return sum(x.isnull())# Apply到每一列:print(&quot;Missing values per column:&quot;)print(data.apply(num_missing, axis=0)) # axis=0代表函数应用于每一列 1234567891011121314Missing values per column:Gender 13Married 3Dependents 15Education 0Self_Employed 32ApplicantIncome 0CoapplicantIncome 0LoanAmount 22Loan_Amount_Term 14Credit_History 50Property_Area 0Loan_Status 0dtype: int64 下面这个例子统计每一行的缺失值个数，因为行数太多，所以使用head函数仅打印出DataFrame的前5行： 1234# Apply到每一行:print(&quot;\\nMissing values per row:&quot;)print(data.apply(num_missing, axis=1).head()) # axis=1代表函数应用于每一行 12345678Missing values per row:Loan_IDLP001002 1LP001003 0LP001005 0LP001006 0LP001008 0dtype: int64 想了解更多请阅读 Pandas Reference (apply) 3. 替换缺失值 一般来说我们会把某一列的缺失值替换为所在列的平均值/众数/中位数。fillna()函数可以帮我们实现这个功能。但首先要从scipy库导入获取统计值的函数。 1234567# 首先导入一个寻找众数的函数：from scipy.stats import modeGenderMode = mode(data['Gender'].dropna())print(GenderMode)print(GenderMode.mode[0],':',GenderMode.count[0]) 12345ModeResult(mode=array(['Male'], dtype=object), count=array([489]))Male : 489F:\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:257: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored. &quot;values. nan values will be ignored.&quot;, RuntimeWarning) 可以看到对DataFrame的某一列使用mode函数可以得到这一列的众数以及它所出现的次数，由于众数可能不止一个，所以众数的结果是一个列表，对应地出现次数也是一个列表。可以使用.mode和.count提取出这两个列表。 特别留意，可能是版本原因，我使用的scipy (0.17.0)不支持原帖子中的代码，直接使用mode(data['Gender'])是会报错的: 12345678910F:\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:257: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored. &quot;values. nan values will be ignored.&quot;, RuntimeWarning)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;F:\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py&quot;, line 644, in mode scores = np.unique(np.ravel(a)) # get ALL unique values File &quot;F:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py&quot;, line 198, in unique ar.sort()TypeError: unorderable types: str() &gt; float() 必须使用mode(data['Gender'].dropna())，传入dropna()处理后的DataFrame才可以。虽然Warning仍然存在，但是可以执行得到结果。Stackoverflow里有这个问题的讨论：Scipy Stats Mode function gives Type Error unorderable types。指出scipy的mode函数无法处理列表中包含混合类型的情况，比方说上面的例子就是包含了缺失值NAN类型和字符串类型，所以无法直接处理。 同时也指出Pandas自带的mode函数是可以处理混合类型的，我测试了一下： 12from pandas import SeriesSeries.mode(data['Gender']) 120 Maledtype: object 确实没问题，不需要使用dropna()处理，但是只能获得众数，没有对应的出现次数。返回结果是一个Pandas的Series对象。可以有多个众数，索引是int类型，从0开始。 掌握了获取众数的方法后就可以使用fiilna替换缺失值了： 12345678#值替换:data['Gender'].fillna(GenderMode.mode[0], inplace=True)MarriedMode = mode(data['Married'].dropna())data['Married'].fillna(MarriedMode.mode[0], inplace=True)Self_EmployedMode = mode(data['Self_Employed'].dropna())data['Self_Employed'].fillna(Self_EmployedMode.mode[0], inplace=True) 先提取出某一列，然后用fillna把这一列的缺失值都替换为计算好的平均值/众数/中位数。inplace关键字用于指定是否直接对这个对象进行修改，默认是False，如果指定为True则直接在对象上进行修改，其他地方调用这个对象时也会收到影响。这里我们希望修改直接覆盖缺失值，所以指定为True。 12#再次检查缺失值以确认:print(data.apply(num_missing, axis=0)) 12345678910111213Gender 0Married 0Dependents 15Education 0Self_Employed 0ApplicantIncome 0CoapplicantIncome 0LoanAmount 22Loan_Amount_Term 14Credit_History 50Property_Area 0Loan_Status 0dtype: int64 可以看到Gender,Married,Self_Employed这几列的缺失值已经都替换成功了，所以缺失值的个数为0。 想了解更多请阅读 Pandas Reference (fillna) 4. 透视表 透视表(Pivot Table)是一种交互式的表，可以动态地改变它的版面布置，以便按照不同方式分析数据，也可以重新安排行号、列标、页字段，比如下面的Excel透视表： Excel透视表 可以自由选择用来做行号和列标的属性，非常便于我们做不同的分析。Pandas也提供类似的透视表的功能。例如LoanAmount这个重要的列有缺失值。我们不希望直接使用整体平均值来替换，这样太过笼统，不合理。 这时可以用先根据 Gender、Married、Self_Employed分组后，再按各组的均值来分别替换缺失值。每个组 LoanAmount的均值可以用如下方法确定： 12345#Determine pivot tableimpute_grps = data.pivot_table(values=[&quot;LoanAmount&quot;], index=[&quot;Gender&quot;,&quot;Married&quot;,&quot;Self_Employed&quot;], aggfunc=np.mean)print(impute_grps) 12345678910 LoanAmountGender Married Self_EmployedFemale No No 114.691176 Yes 125.800000 Yes No 134.222222 Yes 282.250000Male No No 129.936937 Yes 180.588235 Yes No 153.882736 Yes 169.395833 关键字values用于指定要使用集成函数(字段aggfunc)计算的列，选填，不进行指定时默认对所有index以外符合集成函数处理类型的列进行处理，比如这里使用mean作集成函数，则合适的类型就是数值类型。index是行标，可以是多重索引，处理时会按序分组。 想了解更多请阅读 Pandas Reference (Pivot Table) 5. 多重索引 不同于DataFrame对象，可以看到上一步得到的Pivot Table每个索引都是由三个值组合而成的，这就叫做多重索引。 从上一步中我们得到了每个分组的平均值，接下来我们就可以用来替换LoanAmount字段的缺失值了： 1234567#只在带有缺失值的行中迭代：for i,row in data.loc[data['LoanAmount'].isnull(),:].iterrows(): ind = tuple([row['Gender'],row['Married'],row['Self_Employed']]) data.loc[i,'LoanAmount'] = impute_grps.loc[ind].values[0] 特别注意对Pivot Table使用loc定位的方式，Pivot Table的索引是一个tuple。从上面的代码可以看到，先把DataFrame中所有LoanAmount字段缺失的数据记录取出，并且使用iterrows函数转为一个按行迭代的对象，每次迭代返回一个索引(DataFrame里的索引)以及对应行的内容。然后从行的内容中把 Gender、Married、Self_Employed三个字段的值提取出放入一个tuple里，这个tuple就可以用作前面定义的Pivot Table的索引了。 接下来的赋值对DataFrame使用loc定位根据索引定位，并且只抽取LoanAmount字段，把它赋值为在Pivot Table中对应分组的平均值。最后检查一下，这样我们又处理好一个列的缺失值了。 123#再次检查缺失值以确认：print(data.apply(num_missing, axis=0)) 12345678910111213Gender 0Married 0Dependents 15Education 0Self_Employed 0ApplicantIncome 0CoapplicantIncome 0LoanAmount 0Loan_Amount_Term 14Credit_History 50Property_Area 0Loan_Status 0dtype: int64 6. 二维表 二维表这个东西可以帮助我们快速验证一些基本假设，从而获取对数据表格的初始印象。例如，本例中Credit_History字段被认为对会否获得贷款有显著影响。可以用下面这个二维表进行验证： 1pd.crosstab(data[&quot;Credit_History&quot;],data[&quot;Loan_Status&quot;],margins=True) Loan_Status N Y All Credit_History 0.0 82 7 89 1.0 97 378 475 All 192 422 614 crosstab的第一个参数是index，用于行的分组；第二个参数是columns，用于列的分组。代码中还用到了一个margins参数，这个参数默认为False，启用后得到的二维表会包含汇总数据。 然而，相比起绝对数值，百分比更有助于快速了解数据。我们可以用apply函数达到目的： 1234def percConvert(ser): return ser/float(ser[-1])pd.crosstab(data[&quot;Credit_History&quot;],data[&quot;Loan_Status&quot;],margins=True).apply(percConvert, axis=1) Loan_Status N Y All Credit_History 0.0 0.921348 0.078652 1.0 1.0 0.204211 0.795789 1.0 All 0.312704 0.687296 1.0 从这个二维表中我们可以看到有信用记录 (Credit_History字段为1.0) 的人获得贷款的可能性更高。接近80%有信用记录的人都 获得了贷款，而没有信用记录的人只有大约8% 获得了贷款。令人惊讶的是，如果我们直接利用信用记录进行训练集的预测，在614条记录中我们能准确预测出460条记录 (不会获得贷款+会获得贷款：82+378) ，占总数足足75%。不过呢~在训练集上效果好并不代表在测试集上效果也一样好。有时即使提高0.001%的准确度也是相当困难的，这就是我们为什么需要建立模型进行预测的原因了。 想了解更多请阅读 Pandas Reference (crosstab) 7. 数据框合并 就像数据库有多个表的连接操作一样，当数据来源不同时，会产生把不同表格合并的需求。这里假设不同的房产类型有不同的房屋均价数据，定义一个新的表格，如下： 123prop_rates = pd.DataFrame([1000, 5000, 12000], index=['Rural','Semiurban','Urban'],columns=['rates'])prop_rates rates Rural 1000 Semiurban 5000 Urban 12000 农村房产均价只用1000，城郊这要5000，城镇内房产比较贵，均价为12000。我们获取到这个数据之后，希望把它连接到原始表格Loan_Prediction_Train.csv中以便观察房屋均价对预测的影响。在原始表格中有一列Property_Area就是表明贷款人居住的区域的，可以通过这一列进行表格连接： 123data_merged = data.merge(right=prop_rates, how='inner',left_on='Property_Area',right_index=True, sort=False)data_merged.pivot_table(values='Credit_History',index=['Property_Area','rates'], aggfunc=len) 12345Property_Area ratesRural 1000 179.0Semiurban 5000 233.0Urban 12000 202.0Name: Credit_History, dtype: float64 使用merge函数进行连接,解析一下各个参数： 参数right即连接操作右端表格； 参数how指示连接方式，默认是inner，即内连接。可选left、right、outer、inner； left: use only keys from left frame (SQL: left outer join) right: use only keys from right frame (SQL: right outer join) outer: use union of keys from both frames (SQL: full outer join) inner: use intersection of keys from both frames (SQL: inner join) 参数left_on用于指定连接的key的列名，即使key在两个表格中的列名不同，也可以通过left_on和right_on参数分别指定。 如果一样的话，使用on参数就可以了。可以是一个标签(单列)，也可以是一个列表（多列）； right_index默认为False，设置为True时会把连接操作右端表格的索引作为连接的key。同理还有left_index； sort参数默认为False，指示是否需要按key排序。 所以上面的代码是把data表格和prop_rates表格连接起来。连接时，data表格用于连接的key是Property_Area，而prop_rates表格用于连接的key是索引，它们的值域是相同的。 连接之后使用了第四小节透视表的方法检验新表格中Property_Area字段和rates字段的关系。后面跟着的数字表示出现的次数。 想了解更多请阅读 Pandas Reference (merge) 8. 给数据排序 Pandas可以轻松地基于多列进行排序，方法如下： 123data_sorted = data.sort_values(['ApplicantIncome','CoapplicantIncome'], ascending=False)data_sorted[['ApplicantIncome','CoapplicantIncome']].head(10) ApplicantIncome CoapplicantIncome Loan_ID LP002317 81000 0.0 LP002101 63337 0.0 LP001585 51763 0.0 LP001536 39999 0.0 LP001640 39147 4750.0 LP002422 37719 0.0 LP001637 33846 0.0 LP001448 23803 0.0 LP002624 20833 6667.0 LP001922 20667 0.0 Notice：Pandas 的sort函数现在已经不推荐使用，使用 sort_values函数代替。 这里传入了ApplicantIncome和CoapplicantIncome两个字段用于排序，Pandas会先按序进行。先根据ApplicantIncome进行排序，对于ApplicantIncome相同的记录再根据CoapplicantIncome进行排序。 ascending参数设为False，表示降序排列。不妨再看个简单的例子： 1234from pandas import DataFramedf_temp = DataFrame({'a':[1,2,3,4,3,2,1],'b':[0,1,1,0,1,0,1]})df_sorted = df_temp.sort_values(['a','b'],ascending=False)df_sorted a b 3 4 0 2 3 1 4 3 1 1 2 1 5 2 0 6 1 1 0 1 0 这里只有a和b两列，可以清晰地看到Pandas的多列排序是先按a列进行排序，a列的值相同则会再按b列的值排序。 想了解更多请阅读 Pandas Reference (sort_values) 9. 绘图（箱型图&amp;直方图） Pandas除了表格操作之外，还可以直接绘制箱型图和直方图且只需一行代码。这样就不必单独调用matplotlib了。 123%matplotlib inline# coding:utf-8data.boxplot(column=&quot;ApplicantIncome&quot;,by=&quot;Loan_Status&quot;) 箱型图1 ###箱型图 因为之前没怎么接触过箱型图，所以这里单独开一节简单归纳一下。 箱形图（英文：Box-plot），又称为盒须图、盒式图、盒状图或箱线图，是一种用作显示一组数据分散情况资料的统计图。因型状如箱子而得名。详细解析看维基百科。 因为上面那幅图不太容易看，用个简单点的例子来说，还是上一小节那个只有a列和b列的表格。按b列对a列进行分组： 1df_temp.boxplot(column=&quot;a&quot;,by=&quot;b&quot;) 箱型图2 定义b列值为0的分组为Group1，b列值为1的分组为Group2。Group1分组有4，2，1三个值，毫无疑问最大值4，最小值1，在箱型图中这两个值对应箱子发出的虚线顶端的两条实线。Group2分组有3，3，2，1四个值，由于最大值3和上四分位数3(箱子顶部)相同，所以重合了。 Group1中位数是2，而Group2的中位数则是中间两个数2和3的平均数，也即2.5。在箱型图中由箱子中间的有色线段表示。 ###四分位数 四分位数是统计学的一个概念。把所有数值由小到大排列好，然后分成四等份，处于三个分割点位置的数值就是四分位数，其中： 第一四分位数 (Q1)，又称“较小四分位数”或“下四分位数”，等于该样本中所有数值由小到大排列后，在四分之一位置的数。 第二四分位数 (Q2)，又称“中位数”，等于该样本中所有数值由小到大排列后，在二分之一位置的数。 第三四分位数 (Q3)，又称“较大四分位数”或“上四分位数”，等于该样本中所有数值由小到大排列后，在四分之三位置的数。 Notice：Q3与Q1的差距又称四分位距（InterQuartile Range, IQR）。 计算四分位数时首先计算位置，假设有n个数字，则： Q1位置 = (n-1) / 4 Q2位置 = 2 * (n-1) / 4 = (n-1) / 2 Q3位置 = 3 * (n-1) / 4 如果n-1恰好是4的倍数，那么数列中对应位置的就是各个四分位数了。但是，如果n-1不是4的倍数呢？ 这时位置会是一个带小数部分的数值，四分位数以距离该值最近的两个位置的加权平均值求出。其中，距离较近的数，权值为小数部分；而距离较远的数，权值为(1-小数部分)。 再看例子中的Group1，Q1位置为0.5，Q2位置为1，Q3位置为1.5。（注意：位置从下标0开始！），所以： Q1 = 0.5*1+0.5*2 = 1.5 Q2 = 2 Q3 = 0.5*2+0.5*4 = 3 而Group2中，Q1位置为0.75，Q2位置为1.5，Q3位置为2.25。 Q1 = 0.25*1+0.72*2 = 1.75 Q2 = 0.5*2+0.5*3 = 2.5 Q3 = 0.25*3+0.75*3 = 3 这样是否就清晰多了XD 然而，四分位数的取法还存在分歧，定义不一，我在学习这篇文章时也曾经很迷茫，直到阅读了源码！！ 因为Pandas库依赖numpy库，所以它计算四分位数的方式自然也是使用了numpy库的。而numpy中实现计算百分比数的函数为percentile，代码实现如下： 1234567891011121314151617181920def percentile(N, percent, key=lambda x:x): &quot;&quot;&quot; Find the percentile of a list of values. @parameter N - is a list of values. Note N MUST BE already sorted. @parameter percent - a float value from 0.0 to 1.0. @parameter key - optional key function to compute value from each element of N. @return - the percentile of the values &quot;&quot;&quot; if not N: return None k = (len(N)-1) * percent f = math.floor(k) c = math.ceil(k) if f == c: return key(N[int(k)]) d0 = key(N[int(f)]) * (c-k) d1 = key(N[int(c)]) * (k-f) return d0+d1 读一遍源码之后就更加清晰了。最后举个例子： 123456789101112131415161718192021222324252627282930313233343536373839from numpy import percentile, mean, mediantemp = [1,2,4] # 数列包含3个数print(min(temp))print(percentile(temp,25))print(percentile(temp,50))print(percentile(temp,75))print(max(temp))11.52.03.04temp2 = [1,2,3,3] # 数列包含4个数print(min(temp2))print(percentile(temp2,25))print(percentile(temp2,50))print(percentile(temp2,75))print(max(temp2))11.752.53.03temp3 = [1,2,3,4,5] # 数列包含5个数print(min(temp2))print(percentile(temp3,25))print(percentile(temp3,50))print(percentile(temp3,75))print(max(temp3))12.03.04.05 不熟悉的话就再手撸一遍！！！不要怕麻烦！！！这一小节到此Over~ ###直方图 1data.hist(column=&quot;ApplicantIncome&quot;,by=&quot;Loan_Status&quot;,bins=30) 直方图 直方图比箱型图熟悉一些，这里就不详细展开了。结合箱型图和直方图，我们可以看出获得贷款的人和未获得贷款的人没有明显的收入差异，也即收入不是决定性因素。 ###离群点 特别地，从直方图上我们可以看出这个数据集在收入字段上，比较集中于一个区间，区间外部有些散落的点，这些点我们称为离群点(Outlier)。前面将箱型图时没有细说，现在再回顾一下箱型图： 箱型图1 可以看到除了箱子以及最大最小值之外，还有很多横线，这些横线其实就表示离群点。那么可能又会有新的疑问了？怎么会有离群点在最大最小值外面呢？这样岂不是存在比最大值大，比最小值小的情况了吗？ 其实之前提到一下，有一个概念叫四分位距（IQR），数值上等于Q3-Q1，记作ΔQ。定义： 最大值区间： Q3+1.5ΔQ 最小值区间： Q1-1.5ΔQ 也就是说最大值必须出现在这两个区间内，区间外的值被视为离群点，并显示在图上。这样做我们可以避免被过分偏离的数据点带偏，更准确地观测到数据的真实状况，或者说普遍状况。 想了解更多请阅读 Pandas Reference (hist) | Pandas Reference (boxplot) ##10. 用Cut函数分箱 有时把数值聚集在一起更有意义。例如，如果我们要为交通状况（路上的汽车数量）根据时间（分钟数据）建模。具体的分钟可能不重要，而时段如“上午”“下午”“傍晚”“夜间”“深夜”更有利于预测。如此建模更直观，也能避免过度拟合。 这里我们定义一个简单的、可复用的函数，轻松为任意变量分箱。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#分箱:def binning(col, cut_points, labels=None): #Define min and max values: minval = col.min() maxval = col.max() #利用最大值和最小值创建分箱点的列表 break_points = [minval] + cut_points + [maxval] #如果没有标签，则使用默认标签0 ... (n-1) if not labels: labels = range(len(cut_points)+1) #使用pandas的cut功能分箱 colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True) return colBin#为年龄分箱:cut_points = [90,140,190]labels = [&quot;low&quot;,&quot;medium&quot;,&quot;high&quot;,&quot;very high&quot;]data[&quot;LoanAmount_Bin&quot;] = binning(data[&quot;LoanAmount&quot;], cut_points, labels)print(pd.value_counts(data[&quot;LoanAmount_Bin&quot;], sort=False))low 104medium 273high 146very high 91dtype: int64 解析以下这段代码，首先定义了一个cut_points列表，里面的三个点用于把LoanAmount字段分割成四个段，对应地，定义了labels列表，四个箱子(段)按贷款金额分别为低、中、高、非常高。然后把用于分箱的LoanAmount字段，cut_points，labels传入定义好的binning函数。 binning函数中，首先拿到分箱字段的最小值和最大值，把这两个点加入到break_points列表的一头一尾，这样用于切割的所有端点就准备好了。如果labels没有定义就默认按0~段数-1命名箱子。 最后借助pandas提供的cut函数进行分箱。 cut函数原型是cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False)，x是分箱字段，bins是区间端点，right指示区间右端是否闭合，labels不解释..retbins表示是否需要返回bins，precision是label存储和显示的精度，include_lowest指示第一个区间的左端是否闭合。 在把返回的列加入到data后，使用value_counts函数，统计了该列的各个离散值出现的次数，并且指定不需要对结果进行排序。 想了解更多请阅读 Pandas Reference (cut) 11.为分类变量编码 有时，我们会面对要改动分类变量的情况。原因可能是： 有些算法（如logistic回归）要求所有输入项目是数字形式。所以分类变量常被编码为0, 1….(n-1) 有时同一个分类变量可能会有两种表现方式。如，温度可能被标记为“High”， “Medium”， “Low”，“H”， “low”。这里 “High” 和 “H”都代表同一类别。同理， “Low” 和“low”也是同一类别。但Python会把它们当作不同的类别。 一些类别的频数非常低，把它们归为一类是个好主意。 这里我们定义了一个函数，以字典的方式输入数值，用‘replace’函数进行编码 1234567891011121314151617181920212223242526272829303132333435#使用Pandas replace函数定义新函数：def coding(col, codeDict): colCoded = pd.Series(col, copy=True) for key, value in codeDict.items(): colCoded.replace(key, value, inplace=True) return colCoded#把贷款状态LoanStatus编码为Y=1, N=0:print('Before Coding:')print(pd.value_counts(data[&quot;Loan_Status&quot;]))data[&quot;Loan_Status_Coded&quot;] = coding(data[&quot;Loan_Status&quot;], {'N':0,'Y':1})print('\\nAfter Coding:')print(pd.value_counts(data[&quot;Loan_Status_Coded&quot;]))Before Coding:Y 422N 192Name: Loan_Status, dtype: int64After Coding:1 4220 192Name: Loan_Status_Coded, dtype: int64 在coding函数中第二个参数是一个dict，定义了需要编码的字段中每个值对应的编码。 实现上首先把传入的列转换为Series对象，copy参数表示是否要进行复制，关于copy可以看我另一篇笔记：深拷贝与浅拷贝的区别。 copy得到的Series对象不会影响到原本DataFrame传入的列，所以可以放心修改，这里replace函数中的inplace参数表示是否直接在调用对象上作出更改，我们选择是，那么colCoded对像在调用replace后就会被修改为编码形式了。最后，把编码后的列加入到data中，比较编码前后的效果。 想了解更多请阅读 Pandas Reference (replace) 12. 在一个数据框的各行循环迭代 有时我们会需要用一个for循环来处理每行。比方说下面两种情况： 带数字的分类变量被当做数值。 带文字的数值变量被当做分类变量。 先看看data表格的数据类型： 123#检查当前数据类型：data.dtypes 结果： 123456789101112131415Gender objectMarried objectDependents objectEducation objectSelf_Employed objectApplicantIncome int64CoapplicantIncome float64LoanAmount float64Loan_Amount_Term float64Credit_History float64Property_Area objectLoan_Status objectLoanAmount_Bin categoryLoan_Status_Coded int64dtype: object 可以看到Credit_History这一列被当作浮点数，而实际上我们原意是分类变量。所以通常来说手动定义变量类型是个好主意。那这种情况下该怎么办呢？这时我们需要逐行迭代了。 首先创建一个包含变量名和类型的csv文件，读取该文件： 123#载入文件:colTypes = pd.read_csv(r'F:\\Datahack_Loan_Prediction\\datatypes.csv')print(colTypes) 1234567891011121314 feature type0 Loan_ID categorical1 Gender categorical2 Married categorical3 Dependents categorical4 Education categorical5 Self_Employed categorical6 ApplicantIncome continuous7 CoapplicantIncome continuous8 LoanAmount continuous9 Loan_Amount_Term continuous10 Credit_History categorical11 Property_Area categorical12 Loan_Status categorical 载入这个文件之后，我们能对它的逐行迭代，然后使用astype函数来设置表格的类型。这里把离散值字段都设置为categorical类型(对应np.object)，连续值字段都设置为continuous类型（对应np.float）。 123456789101112#迭代每行，指派变量类型。#注，astype函数用于指定变量类型。for i, row in colTypes.iterrows(): #i: dataframe索引; row: 连续的每行 if row['type']==&quot;categorical&quot; and row['feature']!=&quot;Loan_ID&quot;: data[row['feature']]=data[row['feature']].astype(np.object) elif row['type']==&quot;continuous&quot;: data[row['feature']]=data[row['feature']].astype(np.float)print(data.dtypes) 12345678910111213Gender objectMarried objectDependents objectEducation objectSelf_Employed objectApplicantIncome float64CoapplicantIncome float64LoanAmount float64Loan_Amount_Term float64Credit_History objectProperty_Area objectLoan_Status objectdtype: object 可以看到现在信用记录(Credit_History)这一列的类型已经变成了‘object’ ，这在Pandas中代表分类变量。 特别地，无论是中文教程还是原版英文教程这个地方都出错了.. 中文教材代码中判断条件是错的，英文教程中没有考虑到Loan_ID这个字段，由于它被设定为表格的索引，所以它的类型是不被考虑的。 想了解更多请阅读 Pandas Reference (iterrows) ##结语 嗷，暂时没想到写啥，这篇拖了蛮久的时间才把后半部分给补上，因为前阵子实在太忙了... 不过确实学习了这些Pandas技巧之后，使用python处理数据的效率高了很多！！最重要的还是多多练习呗~","link":"/posts/4991.html"},{"title":"Pandas 学习","text":"该笔记摘录自微信公众号“每天进步一点点2015”的文章《Python数据分析之pandas学习（一）》和《Python数据分析之pandas学习（二）》。我对代码和讲解中不够清晰的地方进行了一些改动和补充。 有关pandas模块的学习与应用主要介绍以下8个部分： 数据结构简介：DataFrame和Series 数据索引index 利用pandas查询数据 利用pandas的DataFrames进行统计分析 利用pandas实现SQL操作 利用pandas进行缺失值的处理 利用pandas实现Excel的数据透视表功能 多层索引的使用 数据结构介绍 在pandas中有两类非常重要的数据结构，即序列Series和数据框DataFrame。Series类似于numpy中的一维数组，除了通吃一维数组可用的函数或方法，而且其可通过索引标签的方式获取数据，还具有索引的自动对齐功能；DataFrame类似于numpy中的二维数组，同样可以通用numpy数组的函数和方法，而且还具有其他灵活应用，后续会介绍到。 1、Series的创建 序列的创建主要有三种方式： 1）通过一维数组创建序列 123456In [1]: import numpy as np, pandas as pdIn [2]: arr1 = np.arange(10)In [3]: arr1Out[3]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [4]: type(arr1)Out[4]: numpy.ndarray 返回的是数组类型。 12345678910111213141516In [5]: s1 = pd.Series(arr1)In [6]: s1Out[6]:0 01 12 23 34 45 56 67 78 89 9dtype: int32In [7]: type(s1)Out[7]: pandas.core.series.Series 返回的是序列类型。 2）通过字典的方式创建序列 这种情况下字典的key会被作为Series的索引。 12345In [8]: dic1 = {'a':10,'b':20,'c':30,'d':40,'e':50}In [9]: dic1Out[9]: {'a': 10, 'b': 20, 'c': 30, 'd': 40, 'e': 50}In [10]: type(dic1)Out[10]: dict 返回的是字典类型。 1234567891011In [11]: s2 = pd.Series(dic1)In [12]: s2Out[12]:a 10b 20c 30d 40e 50dtype: int64In [13]: type(s2)Out[13]: pandas.core.series.Series 返回的是序列类型。 补充一点，使用数组创建序列也是可以自定义索引的，通过index关键字传入同值数组一样大小的数组即可： 123456789&gt;&gt;&gt; a = np.arange(1,6)&gt;&gt;&gt; b = np.arange(2,7)&gt;&gt;&gt; pd.Series(a, index=b)2 13 24 35 46 5dtype: int32 3）通过DataFrame中的某一行或某一列创建序列 这部分内容我们放在后面讲，接下来就开始讲一讲如何构造一个DataFrame。 2、DataFrame的创建 数据框的创建主要有三种方式： 1）通过二维数组创建数据框 123456789In [14]: arr2 = np.arange(12).reshape(4,3)In [15]: arr2Out[15]:array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]])In [16]: type(arr2)Out[16]: numpy.ndarray 返回的是数组类型。 12345678910In [17]: df1 = pd.DataFrame(arr2)In [18]: df1Out[18]: 0 1 20 0 1 21 3 4 52 6 7 83 9 10 11In [19]: type(df1)Out[19]: pandas.core.frame.DataFrame 返回的数据框类型。 2）通过字典的方式创建数据框 以下以两种字典来创建数据框，一个是字典列表，一个是嵌套字典。 12345678910In [20]: dic2 = {'a':[1,2,3,4],'b':[5,6,7,8], ...: 'c':[9,10,11,12],'d':[13,14,15,16]}In [21]: dic2Out[21]:{'a': [1, 2, 3, 4],'b': [5, 6, 7, 8],'c': [9, 10, 11, 12],'d': [13, 14, 15, 16]}In [22]: type(dic2)Out[22]: dict 返回的是字典类型。 12345678910In [23]: df2 = pd.DataFrame(dic2)In [24]: df2Out[24]: a b c d0 1 5 9 131 2 6 10 142 3 7 11 153 4 8 12 16In [25]: type(df2)Out[25]: pandas.core.frame.DataFrame 返回的是数据框类型。 12345678910In [26]: dic3 = {'one':{'a':1,'b':2,'c':3,'d':4}, ...: 'two':{'a':5,'b':6,'c':7,'d':8}, ...: 'three':{'a':9,'b':10,'c':11,'d':12}}In [27]: dic3Out[27]:{'one': {'a': 1, 'b': 2, 'c': 3, 'd': 4},'three': {'a': 9, 'b': 10, 'c': 11, 'd': 12},'two': {'a': 5, 'b': 6, 'c': 7, 'd': 8}}In [28]: type(dic3)Out[28]: dict 返回的是字典类型。 12345678910In [29]: df3 = pd.DataFrame(dic3)In [30]: df3Out[30]: one three twoa 1 9 5b 2 10 6c 3 11 7d 4 12 8In [31]: type(df3)Out[31]: pandas.core.frame.DataFrame 返回的是数据框类型。这里需要说明的是，如果使用嵌套字典创建数据框的话，嵌套字典的最外层键会形成数据框的列变量(columns)，而内层键则会形成数据框的行索引(index)。 同样补充一下，用数组来创建数据框的话也是可以通过columns关键字和index关键字分别指定列名和行索引的： 12345678910&gt;&gt;&gt; a = np.arange(1,7).reshape(3,2)&gt;&gt;&gt; aarray([[1, 2], [3, 4], [5, 6]])&gt;&gt;&gt; pd.DataFrame(a, columns=['c1','c2'], index=['r1','r2','r3']) c1 c2r1 1 2r2 3 4r3 5 6 3）通过数据框的方式创建数据框 可以取数据框的一部分构成新的数据框，索引的用法和numpy是一致的。 12345678910In [32]: df4 = df3[['one','three']]In [33]: df4Out[33]: one threea 1 9b 2 10c 3 11d 4 12In [34]: type(df4)Out[34]: pandas.core.frame.DataFrame 返回的是数据框类型。 12345678910In [35]: s3 = df3['one']In [36]: s3Out[36]:a 1b 2c 3d 4Name: one, dtype: int64In [37]: type(s3)Out[37]: pandas.core.series.Series 如果只选择数据框中的某一列/某一行，返回的就会是一个序列对象。 数据索引index 细致的朋友可能会发现一个现象，不论是序列也好，还是数据框也好，对象的最左边总有一个非原始数据对象，这个是什么呢？不错，就是我们接下来要介绍的索引。 在我看来，序列或数据框的索引有两大用处，一个是通过索引值或索引标签获取目标数据，另一个是通过索引，可以使序列或数据框的计算、操作实现自动化对齐，下面我们就来看看这两个功能的应用。 1、通过索引值或索引标签获取数据 12345678910In [38]: s4 = pd.Series(np.array([1,1,2,3,5,8]))In [39]: s4Out[39]:0 11 12 23 34 55 8dtype: int32 如果不给序列一个指定的索引值，则序列自动生成一个从0开始的自增索引。可以通过index查看序列的索引： 12In [40]: s4.indexOut[40]: RangeIndex(start=0, stop=6, step=1) 现在我们为序列设定一个自定义的索引值： 12345678910In [41]: s4.index = ['a','b','c','d','e','f']In [42]: s4Out[42]:a 1b 1c 2d 3e 5f 8dtype: int32 序列有了索引，就可以通过索引值或索引标签进行数据的获取： 1234567891011121314151617181920212223242526272829303132333435363738In [43]: s4[3] # 获取序列的第4个元素Out[43]: 3In [44]: s4['e'] # 获取序列中索引为'e'的元素Out[44]: 5In [45]: s4[[1,3,5]] # 获取序列的第2，4，6个元素Out[45]:b 1d 3f 8dtype: int32In [46]: s4[['a','b','d','f']]Out[46]:a 1b 1d 3f 8dtype: int32In [47]: s4[:4]Out[47]:a 1b 1c 2d 3dtype: int32In [48]: s4['c':]Out[48]:c 2d 3e 5f 8dtype: int32In [49]: s4['b':'e']Out[49]:b 1c 2d 3e 5dtype: int32 千万注意：如果通过索引标签获取数据的话，末端标签所对应的值是可以返回的（区间左右都是闭合的）！在一维数组中，就无法通过索引标签获取数据，这也是序列不同于一维数组的一个方面。 2、自动化对齐 如果有两个序列，需要对这两个序列进行算术运算，这时索引的存在就体现的它的价值了--自动化对齐。 123456789101112131415161718192021222324252627282930313233343536373839404142434445In [50]: s5 = pd.Series(np.array([10,15,20,30,55,80]), ...: index = ['a','b','c','d','e','f'])In [51]: s5Out[51]:a 10b 15c 20d 30e 55f 80dtype: int32In [52]: s6 = pd.Series(np.array([12,11,13,15,14,16]), ...: index = ['a','c','g','b','d','f'])In [53]: s6Out[53]:a 12c 11g 13b 15d 14f 16dtype: int32In [54]: s5 + s6 # 这两个序列在创建时索引顺序是不同的，但计算时按照索引进行了对齐Out[54]:a 22.0b 30.0c 31.0d 44.0e NaNf 96.0g NaNdtype: float64In [55]: s5/s6Out[55]:a 0.833333b 1.000000c 1.818182d 2.142857e NaNf 5.000000g NaNdtype: float64 由于s5中没有对应的g索引，s6中没有对应的e索引，所以数据的运算会产生两个缺失值NaN。注意，这里的算术结果就实现了两个序列索引的自动对齐，而非简单的将两个序列加总或相除。对于数据框的对齐，不仅仅是行索引的自动对齐，同时也会自动对齐列索引（变量名）。 数据框中同样有索引，而且数据框是二维数组的推广，所以数据框不仅有行索引，而且还存在列索引，关于数据框中的索引相比于序列的应用要强大的多，这部分内容将放在下面的数据查询中讲解。 利用pandas查询数据 这里的查询数据相当于R语言里的subset功能，可以通过布尔索引有针对的选取原数据的子集、指定行、指定列等。我们先导入一个student数据集： 1In [56]: student = pd.io.parsers.read_csv('C:\\\\Users\\\\admin\\\\Desktop\\\\student.csv') 查询数据的前5行或末尾5行： 1234567891011121314151617In [57]: student.head()Out[57]: Name Sex Age Height Weight0 Alfred M 14 69.0 112.51 Alice F 13 56.5 84.02 Barbara F 13 65.3 98.03 Carol F 14 62.8 102.54 Henry M 14 63.5 102.5In [58]: student.tail()Out[58]: Name Sex Age Height Weight14 Philip M 16 72.0 150.015 Robert M 12 64.8 128.016 Ronald M 15 67.0 133.017 Thomas M 11 57.5 85.018 William M 15 66.5 112.0 查询指定的行： 12345678In [59]: student.ix[[0,2,4,5,7]] #这里的ix索引标签函数必须是中括号[]Out[59]: Name Sex Age Height Weight0 Alfred M 14 69.0 112.52 Barbara F 13 65.3 98.04 Henry M 14 63.5 102.55 James M 12 57.3 83.07 Janet F 15 62.5 112.5 查询指定的列： 12345678In [60]: student[['Name','Height','Weight']].head() #如果多个列的话，必须使用双重中括号Out[60]: Name Height Weight0 Alfred 69.0 112.51 Alice 56.5 84.02 Barbara 65.3 98.03 Carol 62.8 102.54 Henry 63.5 102.5 也可以通过ix索引标签查询指定的列： 12345678In [61]: student.ix[:,['Name','Height','Weight']].head()Out[61]: Name Height Weight0 Alfred 69.0 112.51 Alice 56.5 84.02 Barbara 65.3 98.03 Carol 62.8 102.54 Henry 63.5 102.5 查询指定的行和列： 12345678In [62]: student.ix[[0,2,4,5,7],['Name','Height','Weight']].head()Out[62]: Name Height Weight0 Alfred 69.0 112.52 Barbara 65.3 98.04 Henry 63.5 102.55 James 57.3 83.07 Janet 62.5 112.5 这里简单说明一下ix的用法：df.ix[行索引,列索引] ix后面必须是中括号 多个行索引或列索引必须用中括号括起来 如果选择所有行索引或列索引，则用英文状态下的冒号:表示 以上是从行或列的角度查询数据的子集，现在我们来看看如何通过布尔索引实现数据的子集查询。 查询所有女生的信息： 123456789101112In [63]: student[student['Sex']=='F']Out[63]: Name Sex Age Height Weight1 Alice F 13 56.5 84.02 Barbara F 13 65.3 98.03 Carol F 14 62.8 102.56 Jane F 12 59.8 84.57 Janet F 15 62.5 112.510 Joyce F 11 51.3 50.511 Judy F 14 64.3 90.012 Louise F 12 56.3 77.013 Mary F 15 66.5 112.0 查询出所有12岁以上的女生信息： 123456789In [64]: student[(student['Sex']=='F') &amp; (student['Age']&gt;12)]Out[64]: Name Sex Age Height Weight1 Alice F 13 56.5 84.02 Barbara F 13 65.3 98.03 Carol F 14 62.8 102.57 Janet F 15 62.5 112.511 Judy F 14 64.3 90.013 Mary F 15 66.5 112.0 查询出所有12岁以上的女生姓名、身高和体重： 123456789In [66]: student[(student['Sex']=='F') &amp; (student['Age']&gt;12)][['Name','Height','Weight']]Out[66]: Name Height Weight1 Alice 56.5 84.02 Barbara 65.3 98.03 Carol 62.8 102.57 Janet 62.5 112.511 Judy 64.3 90.013 Mary 66.5 112.0 上面的查询逻辑其实非常的简单，需要注意的是，如果是多个条件的查询，必须在&amp;（且）或者|（或）的两端条件用括号括起来。 补充一下，除了使用在方括号索引内，布尔索引同样也能用于ix中作为行索引的部分： 1234567891011121314151617&gt;&gt;&gt; df 0 1a 1 2b 3 4c 0 0&gt;&gt;&gt; df[(df[0]==1) | (df[0]==3)] 0 1a 1 2b 3 4&gt;&gt;&gt; df.ix[(df[0]==1) | (df[0]==3)] 0 1a 1 2b 3 4&gt;&gt;&gt; df.ix[(df[0]==1) | (df[0]==3), 1::-1] 1 0a 2 1b 4 3 统计分析 pandas模块为我们提供了非常多的描述性统计分析的指标函数，如总和、均值、最小值、最大值等，我们来具体看看这些函数： 首先随机生成三组数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546In [67]: np.random.seed(1234)In [68]: d1 = pd.Series(2*np.random.normal(size = 100)+3)In [69]: d2 = np.random.f(2,4,size = 100)In [70]: d3 = np.random.randint(1,100,size = 100)In [71]: d1.count() #非空元素计算Out[71]: 100In [72]: d1.min() #最小值Out[72]: -4.1270333212494705In [73]: d1.max() #最大值Out[73]: 7.7819210309260658In [74]: d1.idxmin() #最小值的位置，类似于R中的which.min函数Out[74]: 81In [75]: d1.idxmax() #最大值的位置，类似于R中的which.max函数Out[75]: 39In [76]: d1.quantile(0.1) #10%分位数Out[76]: 0.68701846440699277In [77]: d1.sum() #求和Out[77]: 307.0224566250874In [78]: d1.mean() #均值Out[78]: 3.070224566250874In [79]: d1.median() #中位数Out[79]: 3.204555266776845In [80]: d1.mode() #众数Out[80]: Series([], dtype: float64)In [81]: d1.var() #方差Out[81]: 4.005609378535085In [82]: d1.std() #标准差Out[82]: 2.0014018533355777In [83]: d1.mad() #平均绝对偏差Out[83]: 1.5112880411556109In [84]: d1.skew() #偏度Out[84]: -0.64947807604842933In [85]: d1.kurt() #峰度Out[85]: 1.2201094052398012In [86]: d1.describe() #一次性输出多个描述性统计指标Out[86]:count 100.000000mean 3.070225std 2.001402min -4.12703325% 2.04010150% 3.20455575% 4.434788max 7.781921dtype: float64 必须注意的是，describe方法只能针对序列或数据框，一维数组（numpy.ndarray）是没有这个方法的。 这里自定义一个函数，将这些统计描述指标全部汇总到一起： 1234567891011121314151617181920212223242526272829In [87]: def stats(x): ...: return pd.Series([x.count(),x.min(),x.idxmin(), ...: x.quantile(.25),x.median(), ...: x.quantile(.75),x.mean(), ...: x.max(),x.idxmax(), ...: x.mad(),x.var(), ...: x.std(),x.skew(),x.kurt()], ...: index = ['Count','Min','Whicn_Min', ...: 'Q1','Median','Q3','Mean', ...: 'Max','Which_Max','Mad', ...: 'Var','Std','Skew','Kurt'])In [88]: stats(d1)Out[88]:Count 100.000000Min -4.127033Whicn_Min 81.000000Q1 2.040101Median 3.204555Q3 4.434788Mean 3.070225Max 7.781921Which_Max 39.000000Mad 1.511288Var 4.005609Std 2.001402Skew -0.649478Kurt 1.220109dtype: float64 在实际的工作中，我们可能需要处理的是一系列的数值型数据框，如何将这个函数应用到数据框中的每一列呢？可以使用apply函数，参数axis默认为0，将函数应用到数据框的每一列；设置为1时，则将函数应用到每一行。 将之前创建的d1,d2,d3数据构建数据框： 123456789101112131415161718192021222324252627In [89]: df = pd.DataFrame(np.array([d1,d2,d3]).T,columns=['x1','x2','x3'])In [90]: df.head()Out[90]: x1 x2 x30 3.942870 1.369531 55.01 0.618049 0.943264 68.02 5.865414 0.590663 73.03 2.374696 0.206548 59.04 1.558823 0.223204 60.0In [91]: df.apply(stats)Out[91]: x1 x2 x3Count 100.000000 100.000000 100.000000Min -4.127033 0.014330 3.000000Whicn_Min 81.000000 72.000000 76.000000Q1 2.040101 0.249580 25.000000Median 3.204555 1.000613 54.500000Q3 4.434788 2.101581 73.000000Mean 3.070225 2.028608 51.490000Max 7.781921 18.791565 98.000000Which_Max 39.000000 53.000000 96.000000Mad 1.511288 1.922669 24.010800Var 4.005609 10.206447 780.090808Std 2.001402 3.194753 27.930106Skew -0.649478 3.326246 -0.118917Kurt 1.220109 12.636286 -1.211579 非常完美，就这样很简单的创建了数值型数据的统计性描述。如果是离散型数据呢？就不能用这个统计口径了，我们需要统计离散变量的观测数、唯一值个数、众数水平及个数。你只需要使用describe方法就可以实现这样的统计了。 1234567In [92]: student['Sex'].describe()Out[92]:count 19 # 有多少个样本unique 2 # 唯一值个数top M # 众数freq 10 # 众数出现的次数Name: Sex, dtype: object 除以上的简单描述性统计之外，还提供了连续变量的相关系数（corr）和协方差矩阵（cov）的求解，这个跟R语言是一致的用法。 123456In [93]: df.corr() # 返回pair-wise相关系数，所以是一个对称矩阵Out[93]: x1 x2 x3x1 1.000000 0.136085 0.037185x2 0.136085 1.000000 -0.005688x3 0.037185 -0.005688 1.000000 关于相关系数的计算可以调用pearson方法或kendell方法或spearman方法，默认使用pearson方法。 123456In [94]: df.corr('spearman')Out[94]: x1 x2 x3x1 1.00000 0.178950 0.006590x2 0.17895 1.000000 -0.033874x3 0.00659 -0.033874 1.000000 如果只想关注某一个变量与其余变量的相关系数的话，可以使用corrwith,如下方只关心x1与其余变量的相关系数： 123456In [95]: df.corrwith(df['x1'])Out[95]:x1 1.000000x2 0.136085x3 0.037185dtype: float64 数值型数据的协方差矩阵： 123456In [96]: df.cov()Out[96]: x1 x2 x3x1 4.005609 0.870124 2.078596x2 0.870124 10.206447 -0.507512x3 2.078596 -0.507512 780.090808 类似于SQL的操作 在SQL中常见的操作主要是增、删、改、查几个动作，那么pandas能否实现对数据的这几项操作呢？答案是Of Course! 增：添加新行或增加新列 1234567891011In [99]: dic = {'Name':['LiuShunxiang','Zhangshan'], ...: 'Sex':['M','F'],'Age':[27,23], ...: 'Height':[165.7,167.2],'Weight':[61,63]}In [100]: student2 = pd.DataFrame(dic)In [101]: student2Out[101]: Age Height Name Sex Weight0 27 165.7 LiuShunxiang M 611 23 167.2 Zhangshan F 63 现在将student2中的数据新增到student中，可以通过concat函数实现： g1 注意到了吗？在数据库中union必须要求两张表的列顺序一致，而这里concat函数可以自动对齐两个数据框的变量（前面student数据框列的顺序是'Name Sex Age Height Weight'，和student2定义的列顺序不同，但是合并时按列名进行了对齐）！ 新增列的话，其实在pandas中就更简单了，例如在student2中新增一列学生成绩： g2 对于新增的列没有赋值，就会出现空NaN的形式。 补充一下，也可以这样初始化并新增一列： 12345678910&gt;&gt;&gt; df = pd.DataFrame({'a':[1,2],'b':[3,4]})&gt;&gt;&gt; df a b0 1 31 2 4&gt;&gt;&gt; df['c'] = 0&gt;&gt;&gt; df a b c0 1 3 01 2 4 0 删：删除表、观测行或变量列 删除数据框student2,通过del命令实现，该命令可以删除Python的所有对象。 g3 删除指定的行 g4 原数据中的第1,2,4,7行的数据已经被删除了。 根据布尔索引删除行数据，其实这个删除就是保留删除条件的反面数据，例如删除所有14岁以下的学生： g5 删除指定的列 g6 我们发现，不论是删除行还是删除列，都可以通过drop方法实现，只需要设定好删除的轴即可，即调整drop方法中的axis参数。默认该参数为0，表示删除行观测，如果需要删除列变量，则需设置为1。 改：修改原始记录的值 如果发现表中的某些数据错误了，如何更改原来的值呢？我们试试结合布尔索引和赋值的方法： 例如发现student3中姓名为Liushunxiang的学生身高错了，应该是173，如何改呢？ g7 这样就可以把原来的身高修改为现在的170了。 看，关于索引的操作非常灵活、方便吧，就这样轻松搞定数据的更改。 查 有关数据查询部分，上面已经介绍过，下面重点讲讲聚合、排序和多表连接操作。 聚合 pandas模块中可以通过groupby()函数实现数据的聚合操作 根据性别分组，计算各组别中学生身高和体重的平均值： g8 如果不对原始数据作限制的话，聚合函数会自动选择数值型数据进行聚合计算。如果不想对年龄计算平均值的话，就需要剔除改变量： g9 groupby还可以使用多个分组变量，例如根本年龄和性别分组，计算身高与体重的平均值： g10 当然，还可以对每个分组计算多个统计量： g11 是不是很简单，只需一句就能完成SQL中的SELECT...FROM...GROUP BY...功能，何乐而不为呢？ 排序 排序在日常的统计分析中还是比较常见的操作，我们可以使用order、sort_index和sort_values实现序列和数据框的排序工作： g12 我们再试试降序排序的设置： g13 上面两个结果其实都是按值排序，并且结果中都给出了警告信息，即建议使用sort_values()函数进行按值排序。 在数据框中一般都是按值排序，例如： g14 多表连接 多表之间的连接也是非常常见的数据库操作，连接分内连接和外连接，在数据库语言中通过join关键字实现，pandas我比较建议使用merge函数实现数据的各种连接操作。 如下是构造一张学生的成绩表： g15 现在想把学生表student与学生成绩表score做一个关联，该如何操作呢？ g16 注意，默认情况下，merge函数实现的是两个表之间的内连接，即返回两张表中共同部分的数据。可以通过how参数设置连接的方式，left为左连接；right为右连接；outer为外连接。 g17 左连接实现的是保留student表中的所有信息，同时将score表的信息与之配对，能配多少配多少，对于没有配对上的Name，将会显示成绩为NaN。 缺失值处理 现实生活中的数据是非常杂乱的，其中缺失值也是非常常见的，对于缺失值的存在可能会影响到后期的数据分析或挖掘工作，那么我们该如何处理这些缺失值呢？常用的有三大类方法，即删除法、填补法和插值法。 删除法 当数据中的某个变量大部分值都是缺失值，可以考虑删除改变量（删除列）当缺失值是随机分布的，且缺失的数量并不是很多是，也可以删除这些缺失的观测（删除行）。 替补法 对于连续型变量，如果变量的分布近似或就是正态分布的话，可以用均值替代那些缺失值；如果变量是有偏的，可以使用中位数来代替那些缺失值；对于离散型变量，我们一般用众数去替换那些存在缺失的观测。 插补法 插补法是基于蒙特卡洛模拟法，结合线性模型、广义线性模型、决策树等方法计算出来的预测值替换缺失值。 我们这里就介绍简单的删除法和替补法： g18 这是一组含有缺失值的序列，我们可以结合sum函数和isnull函数来检测数据中含有多少缺失值： 12In [130]: sum(pd.isnull(s))Out[130]: 9 直接删除缺失值 g19 默认情况下，dropna会删除任何含有缺失值的行，我们再构造一个数据框试试： 返回结果表明，数据中只要含有缺失值NaN,该数据行就会被删除，如果使用参数 how='all'，则表明只删除所有行为缺失值的观测。 g20 补充一个对比例子： 1234567891011121314151617181920&gt;&gt;&gt; df = pd.DataFrame({'x1':[0,1,None,3,None],'x2':[None,1,None,None,4],'x3':[0,None,None,3,4]})&gt;&gt;&gt; df x1 x2 x30 0.0 NaN 0.01 1.0 1.0 NaN2 NaN NaN NaN3 3.0 NaN 3.04 NaN 4.0 4.0&gt;&gt;&gt; df.dropna(how='all') # 只有全部列都为NaN的行被删掉 x1 x2 x30 0.0 NaN 0.01 1.0 1.0 NaN3 3.0 NaN 3.04 NaN 4.0 4.0&gt;&gt;&gt; df.dropna() # 只要有一列包含NaN就会删除掉Empty DataFrameColumns: [x1, x2, x3]Index: [] 再补充一下，如果是想删除列，直接用 DataFrame.drop(['列名1'，'列名2']， axis=1) 这个语句就可以了。 再补充一下，被别人问问题的时候发现了一个新手容易犯的错误，在初始化时，如果某一个值为空，也即想初始化为NaN，注意不是填'NaN'，这样初始化的话，数据框中对应的元素就是一个字符串而不是空值，自然也就没法使用pandas库提供的缺失值处理函数了，正确的方法是初始化为None。 使用一个常量来填补缺失值 可以使用fillna函数实现简单的填补工作： 1）用0填补所有缺失值 g21 2)采用前项填充或后向填充 g22 补充，使用这种填充方法，如果第一行/最后一行出现缺失值，它们将不被填充。 3)使用常量填充不同的列 g23 4)用均值或中位数填充各自的列 g24 很显然，在使用填充法时，相对于常数填充或前项、后项填充，使用各列的众数、均值或中位数填充要更加合理一点，这也是工作中常用的一个快捷手段。 数据透视表 在Excel中有一个非常强大的功能就是数据透视表，通过托拉拽的方式可以迅速的查看数据的聚合情况，这里的聚合可以是计数、求和、均值、标准差等。 pandas为我们提供了非常强大的函数pivot_table()，该函数就是实现数据透视表功能的。对于上面所说的一些聚合函数，可以通过参数aggfunc设定。我们先看看这个函数的语法和参数吧： 12345678pivot_table(data,values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All') data：需要进行数据透视表操作的数据框 values：指定需要聚合的字段 index: 指定某些原始变量作为行索引 columns: 指定哪些离散的分组变量 aggfunc: 指定相应的聚合函数，默认为numpy.mean() fill_value：使用一个常数替代缺失值，默认不替换 margins: 是否进行行或列的汇总，默认不汇总 dropna: 默认所有观测为缺失的列 margins_name：默认行汇总或列汇总的名称为'All' 我们仍然以student表为例，来认识一下数据透视表pivot_table函数的用法： 对一个分组变量（Sex），一个数值变量（Height）作统计汇总： g25 对一个分组变量（Sex），两个数值变量（Height,Weight）作统计汇总： g26 对两个分组变量（Sex，Age)，两个数值变量（Height,Weight）作统计汇总： g27 很显然这样的结果并不像Excel中预期的那样，该如何变成列联表的形式的？很简单，只需将结果进行非堆叠操作（unstack）即可： g29 看，这样的结果是不是比上面那种看起来更舒服一点？ 使用多个聚合函数 g30 有关更多数据透视表的操作，可参考《Pandas透视表（pivot_table）详解》一文。 多层索引的使用 最后我们再来讲讲pandas中的一个重要功能，那就是多层索引。在序列中它可以实现在一个轴上拥有多个索引，就类似于Excel中常见的这种形式： g31 对于这样的数据格式有什么好处呢？pandas可以帮我们实现用低维度形式处理高维数数据，这里举个例子也许你就能明白了： g32 对于这种多层次索引的序列，取数据就显得非常简单了： g33 对于这种多层次索引的序列，我们还可以非常方便的将其转换为数据框的形式： g34 以上针对的是序列的多层次索引，数据框也同样有多层次的索引，而且每条轴上都可以有这样的索引，就类似于Excel中常见的这种形式： g35 我们不妨构造一个类似的高维数据框： g36 同样，数据框中的多层索引也可以非常便捷的取出大块数据： g37 在数据框中使用多层索引，可以将整个数据集控制在二维表结构中，这对于数据重塑和基于分组的操作（如数据透视表的生成）比较有帮助。 就拿student二维数据框为例，我们构造一个多层索引数据集： g38 讲到这里，我们关于pandas模块的学习基本完成，其实在掌握了pandas这8个主要的应用方法就可以灵活的解决很多工作中的数据处理、统计分析等任务。有关更多的pandas介绍，可参考pandas官方文档。 此外，在Efficient_Pandas_Skills中收录了Pandas库使用的一些进阶技巧，也可以参考一下。","link":"/posts/19036.html"},{"title":"Transformer模型及源代码","text":"12345678910import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport math, copy, timefrom torch.autograd import Variableimport matplotlib.pyplot as pltimport seabornseaborn.set_context(context='talk')%matplotlib inline 模型架构 通用的 编码器解码器 架构： 12345678910111213141516171819202122232425262728293031class EncoderDecoder(nn.Module): def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): &quot;&quot;&quot; 编码器、解码器、输入嵌入层、目标嵌入层、输出层 &quot;&quot;&quot; super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): &quot;&quot;&quot; src --&gt; memory memory + tgt --&gt; output &quot;&quot;&quot; memory = self.encode(src, src_mask) return self.decode(memory, src_mask, tgt, tgt_mask) def encode(self, src, src_mask): &quot;&quot;&quot; src --&gt; memory &quot;&quot;&quot; return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): &quot;&quot;&quot; memory + tgt --&gt; output &quot;&quot;&quot; return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) 123456789class Generator(nn.Module): &quot;Define standard linear + softmax generation step.&quot; def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return F.log_softmax(self.proj(x), dim=-1) 编码器 编码器由多层 N=6 完全相同的层堆叠而成 其层次结构如上图中所示： 123456789101112131. Encoder 2. EncoderLayer 3. SublayerConnection 4. sublayer --&gt; self_attn 3. SublayerConnection 4. sublayer --&gt; feed_forward 2. EncoderLayer . . . . 12def clone(module, N): return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) 12345678910111213class Encoder(nn.Module): def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clone(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): &quot;&quot;&quot; 需要自主生成 mask &quot;&quot;&quot; for layer in self.layers: x = layer(x, mask) return self.norm(x) 12345678910111213141516171819class LayerNorm(nn.Module): &quot;&quot;&quot; inputs: batch, seq_len, features 沿输入数据的特征维度归一化 &quot;&quot;&quot; def __init__(self, features, eps=1e-6): # 需要指定特征数量 features super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.ones(features)) self.eps = eps def forward(self, x): &quot;&quot;&quot; x --&gt; (x - x.mean) / x.std &quot;&quot;&quot; mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 1234567891011def test_layernorm(): x = np.array([[[1, 2, 3], [2, 4, 5]],], dtype=np.float) print(&quot;Before Norm: \\n&quot;, x) x = torch.from_numpy(x) # batch, seq_len, features norm = LayerNorm(x.shape[-1]) x = norm(x) print(&quot;After Norm: \\n&quot;, x.detach().numpy())test_layernorm() Before Norm: [[[1. 2. 3.] [2. 4. 5.]]] After Norm: [[[ 9.99999000e-07 1.00000000e+00 1.99999900e+00] [-9.10887369e-02 1.21821775e+00 1.87287099e+00]]] 1 1234567891011class SublayerConnection(nn.Module): def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): &quot;&quot;&quot; 指定内部的结构 sublayer，是 attention 层，还是 feed_forward 层 &quot;&quot;&quot; return x + self.dropout(sublayer(self.norm(x))) 123456789101112class EncoderLayer(nn.Module): &quot;&quot;&quot;size: d_model&quot;&quot;&quot; def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clone(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) 1 解码器 12345678910class Decoder(nn.Module): def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clone(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) 1234567891011121314class DecoderLayer(nn.Module): def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size # 作为参数用于 layernorm 层 self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clone(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) 1234567891011# 解码器一次输入序列中向量，当前步后面的序列需要被遮盖# 需要被遮盖的单词被标记为 False def subsequent_mask(size): attn_shape = (1, size, size) subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8') return torch.from_numpy(subsequent_mask) == 0plt.figure(figsize=(5, 5))plt.imshow(subsequent_mask(20)[0]) &lt;matplotlib.image.AxesImage at 0x7f83a0e5ec90&gt; np.triu(m,k=0)：第 k 对角线以下的元素归零，中心对角线索引为 0 ，索引向右上角增加 1、2、3 ，向左下角-1、-2、-3 12345&gt;&gt;&gt; np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)array([[ 1, 2, 3], [ 4, 5, 6], [ 0, 8, 9], [ 0, 0, 12]]) 注意力 点积注意力 \\[\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\] 123456789101112131415161718def attention(query, key, value, mask=None, dropout=None): &quot;&quot;&quot; query : batch, target_len, feats key : batch, seq_len, feats value : batch, seq_len, val_feats return: batch, target_len, val_feats &quot;&quot;&quot; d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn 1234567891011def test_attention(): query = torch.randn(3, 5, 4) # batch, target_len, feats key = torch.randn(3, 6, 4) # batch, seq_len, feats value = torch.randn(3, 6, 8) # batch, seq_len, val_feats attn, _ = attention(query, key, value) print(attn.shape) assert attn.shape == (3, 5, 8) print(&quot;Test passed&quot;)test_attention() torch.Size([3, 5, 8]) Test passed 1 多头注意力 12345678910111213141516171819202122232425262728293031323334353637class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): &quot;&quot;&quot; h, num_heads d_model, features &quot;&quot;&quot; super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 self.d_k = d_model // h self.h = h self.linears = clone(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(dropout) def forward(self, query, key, value, mask=None): # query,key,value: batch,seq_len,d_model if mask is not None: mask = mask.unsqueeze(1) nbatches = query.size(0) query, key, value = [ l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value)) ] x, self.attn = attention( query, # batch,num_head,seq_len,feats key, value, mask=mask, dropout=self.dropout) x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) # batch,seq_len,num_head*feats return self.linears[-1](x) 123456789def test_multi_head(): x = torch.randn(2, 4, 12) d_model = x.shape[-1] model = MultiHeadedAttention(2, d_model) attn = model(x, x, x) assert attn.shape == (2, 4, 12) print(&quot;Test passed!&quot;)test_multi_head() Test passed! 1 前向层 123456789class PositionwiseFeedForward(nn.Module): def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) 嵌入层 12345678class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) 1 位置编码 \\[ \\text{PE}(i,\\delta) = \\begin{cases} \\sin(\\frac{i}{10000^{2\\delta'/d}}) &amp; \\text{if } \\delta = 2\\delta'\\\\ \\cos(\\frac{i}{10000^{2\\delta'/d}}) &amp; \\text{if } \\delta = 2\\delta' + 1\\\\ \\end{cases} \\] 1234567891011121314151617181920class PositionalEncoding(nn.Module): &quot;Implement the PE function.&quot; def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 12345plt.figure(figsize=(15, 5))pe = PositionalEncoding(20, 0)y = pe.forward(Variable(torch.zeros(1, 100, 20)))plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())plt.legend([&quot;dim %d&quot;%p for p in [4,5,6,7]]) &lt;matplotlib.legend.Legend at 0x7f83a03f5f10&gt; 1 完整的模型 1234567891011121314151617181920212223242526def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1): &quot;Helper: Construct a model from hyperparameters.&quot; c = copy.deepcopy attn = MultiHeadedAttention(h, d_model) ff = PositionwiseFeedForward(d_model, d_ff, dropout) position = PositionalEncoding(d_model, dropout) model = EncoderDecoder( Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), Generator(d_model, tgt_vocab), ) # This was important from their code. # Initialize parameters with Glorot / fan_avg. for p in model.parameters(): if p.dim() &gt; 1: nn.init.xavier_uniform_(p) return model 12tmp_model = make_model(10, 10, 2)tmp_model EncoderDecoder( (encoder): Encoder( (layers): ModuleList( (0): EncoderLayer( (self_attn): MultiHeadedAttention( (linears): ModuleList( (0): Linear(in_features=512, out_features=512, bias=True) (1): Linear(in_features=512, out_features=512, bias=True) (2): Linear(in_features=512, out_features=512, bias=True) (3): Linear(in_features=512, out_features=512, bias=True) ) (dropout): Dropout(p=0.1, inplace=False) ) (feed_forward): PositionwiseFeedForward( (w_1): Linear(in_features=512, out_features=2048, bias=True) (w_2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (sublayer): ModuleList( (0): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (1): EncoderLayer( (self_attn): MultiHeadedAttention( (linears): ModuleList( (0): Linear(in_features=512, out_features=512, bias=True) (1): Linear(in_features=512, out_features=512, bias=True) (2): Linear(in_features=512, out_features=512, bias=True) (3): Linear(in_features=512, out_features=512, bias=True) ) (dropout): Dropout(p=0.1, inplace=False) ) (feed_forward): PositionwiseFeedForward( (w_1): Linear(in_features=512, out_features=2048, bias=True) (w_2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (sublayer): ModuleList( (0): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (norm): LayerNorm() ) (decoder): Decoder( (layers): ModuleList( (0): DecoderLayer( (self_attn): MultiHeadedAttention( (linears): ModuleList( (0): Linear(in_features=512, out_features=512, bias=True) (1): Linear(in_features=512, out_features=512, bias=True) (2): Linear(in_features=512, out_features=512, bias=True) (3): Linear(in_features=512, out_features=512, bias=True) ) (dropout): Dropout(p=0.1, inplace=False) ) (src_attn): MultiHeadedAttention( (linears): ModuleList( (0): Linear(in_features=512, out_features=512, bias=True) (1): Linear(in_features=512, out_features=512, bias=True) (2): Linear(in_features=512, out_features=512, bias=True) (3): Linear(in_features=512, out_features=512, bias=True) ) (dropout): Dropout(p=0.1, inplace=False) ) (feed_forward): PositionwiseFeedForward( (w_1): Linear(in_features=512, out_features=2048, bias=True) (w_2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (sublayer): ModuleList( (0): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (2): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (1): DecoderLayer( (self_attn): MultiHeadedAttention( (linears): ModuleList( (0): Linear(in_features=512, out_features=512, bias=True) (1): Linear(in_features=512, out_features=512, bias=True) (2): Linear(in_features=512, out_features=512, bias=True) (3): Linear(in_features=512, out_features=512, bias=True) ) (dropout): Dropout(p=0.1, inplace=False) ) (src_attn): MultiHeadedAttention( (linears): ModuleList( (0): Linear(in_features=512, out_features=512, bias=True) (1): Linear(in_features=512, out_features=512, bias=True) (2): Linear(in_features=512, out_features=512, bias=True) (3): Linear(in_features=512, out_features=512, bias=True) ) (dropout): Dropout(p=0.1, inplace=False) ) (feed_forward): PositionwiseFeedForward( (w_1): Linear(in_features=512, out_features=2048, bias=True) (w_2): Linear(in_features=2048, out_features=512, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (sublayer): ModuleList( (0): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (1): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) (2): SublayerConnection( (norm): LayerNorm() (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (norm): LayerNorm() ) (src_embed): Sequential( (0): Embeddings( (lut): Embedding(10, 512) ) (1): PositionalEncoding( (dropout): Dropout(p=0.1, inplace=False) ) ) (tgt_embed): Sequential( (0): Embeddings( (lut): Embedding(10, 512) ) (1): PositionalEncoding( (dropout): Dropout(p=0.1, inplace=False) ) ) (generator): Generator( (proj): Linear(in_features=512, out_features=10, bias=True) ) ) 1tmp_model.src_embed Sequential( (0): Embeddings( (lut): Embedding(10, 512) ) (1): PositionalEncoding( (dropout): Dropout(p=0.1, inplace=False) ) ) 训练 数据批次，同时创建 mask 1234567891011121314151617181920212223class Batch: def __init__(self, src, trg=None, pad=0): &quot;&quot;&quot; src: 输入序列 trg: 目标序列 &quot;&quot;&quot; self.src = src self.src_mask = (src != pad).unsqueeze(-2) if trg is not None: self.trg = trg[:, :-1] self.trg_y = trg[:, 1:] self.trg_mask = self.make_std_mask(self.trg, pad) self.ntokens = (self.trg_y != pad).data.sum() @staticmethod def make_std_mask(tgt, pad): &quot;&quot;&quot; 将 pad 产生的 mask，和序列一次预测下一个单词产生的 mask 结合起来 &quot;&quot;&quot; tgt_mask = (tgt != pad).unsqueeze(-2) tgt_mask = tgt_mask &amp; Variable( subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)) return tgt_mask 123456src = torch.tensor([[3, 5, 7, 0, 0], [2, 4, 6, 8, 0]]) # batch=2,seq_len=5trg = torch.tensor([[2, 3, 4, 5, 0, 0], [3, 5, 6, 0, 0, 0]]) # batch=2,seq_len=6sample = Batch(src, trg)sample.src_mask tensor([[[ True, True, True, False, False]], [[ True, True, True, True, False]]]) 1sample.trg_mask, sample.ntokens (tensor([[[ True, False, False, False, False], [ True, True, False, False, False], [ True, True, True, False, False], [ True, True, True, True, False], [ True, True, True, True, False]], [[ True, False, False, False, False], [ True, True, False, False, False], [ True, True, True, False, False], [ True, True, True, False, False], [ True, True, True, False, False]]]), tensor(5)) 训练过程 12345678910111213141516171819def run_epoch(data_iter, model, loss_compute): start = time.time() total_tokens = 0 total_loss = 0 tokens = 0 for i, batch in enumerate(data_iter): out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask) loss = loss_compute(out, batch.trg_y, batch.ntokens) total_loss += loss total_tokens += batch.ntokens # 总 tokens 数 tokens += batch.ntokens # 50 批训练时的总 tokens 数 if i % 50 == 1: elapsed = time.time() - start print(&quot;Epoch Step: %d Loss: %f Tokens per Sec: %f&quot; % (i, loss / batch.ntokens, tokens / elapsed)) start = time.time() tokens = 0 return total_loss / total_tokens 训练数据 standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs 1234567891011global max_src_in_batch, max_tgt_in_batchdef batch_size_fn(mew, count, sofar): global max_src_in_batch, max_tgt_in_batch if count == 1: max_src_in_batch = 0 max_tgt_in_batch = 0 max_src_in_batch = max(max_src_in_batch, len(new.src)) max_tgt_in_batch = max(max_tgt_in_batch, len(new.trg) + 2) src_elements = count * max_src_in_batch tgt_elements = count * max_tgt_in_batch return max(src_elements, tgt_elements) 优化器 Adam 优化器，参数\\(\\beta_1=0.9\\)，\\(\\beta_2=0.98\\)，\\(\\epsilon=10^{-9}\\)，变学习率： \\[lrate = d_{\\text{model}}^{-0.5} \\cdot \\min({step\\_num}^{-0.5}, {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\\] 其中： \\(warmup_{steps}=4000\\) 1234567891011121314151617181920212223242526272829class NoamOpt: def __init__(self, model_size, factor, warmup, optimizer): self.optimizer = optimizer self._step = 0 self.warmup = warmup self.factor = factor self.model_size = model_size self._rate = 0 def step(self): self._step += 1 rate = self.rate() for p in self.optimizer.param_groups: p['lr'] = rate self._rate = rate self.optimizer.step() def rate(self, step=None): if step is None: step = self._step return self.factor * (self.model_size**(-0.5) * min(step**(-0.5), step * self.warmup**(-1.5)))def get_std_opt(model): return NoamOpt( model.src_embed[0].d_model, 2, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)) 12345678opts = [ NoamOpt(512, 1, 4000, None), NoamOpt(512, 1, 8000, None), NoamOpt(256, 1, 4000, None),]plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])plt.legend([&quot;512:4000&quot;, &quot;512:8000&quot;, &quot;256:4000&quot;]) &lt;matplotlib.legend.Legend at 0x7ffb17580990&gt; 正则化 标签平滑Label Smoothing：\\(\\epsilon_{ls}=0.1\\)，会降低 perplexity，因为模型将更不确定，但增加精度和BLEU分数 123456789101112131415161718192021222324class LabelSmoothing(nn.Module): def __init__(self, size, padding_idx, smoothing=0.0): super(LabelSmoothing, self).__init__() self.criterion = nn.KLDivLoss(size_average=False) self.padding_idx = padding_idx self.confidence = 1.0 - smoothing self.smoothing = smoothing self.size = size self.true_dist = None def forward(self, x, target): assert x.size(1) == self.size true_dist = x.data.clone() true_dist.fill_(self.smoothing / (self.size - 2)) true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) true_dist[:, self.padding_idx] = 0 mask = torch.nonzero(target.data == self.padding_idx) if mask.dim() &gt; 0: true_dist.index_fill_(0, mask.squeeze(), 0.0) self.true_dist = true_dist return self.criterion(x, Variable(true_dist, requires_grad=False)) nn.KLDivLoss: 输入为log概率分布，目标为概率分布；\\(l(x,y) = L = \\{ l_1,\\dots,l_N \\}, \\quad l_n = y_n \\cdot \\left( \\log y_n - x_n \\right)\\) 指定reduction参数时： \\[ \\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';} \\\\ \\operatorname{sum}(L), &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}\\] Tensor.scatter_(dim, index, src)： 1234567891011x = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333], [0.1333, 0.1333, 0.1333, 0.1333, 0.1333], [0.1333, 0.1333, 0.1333, 0.1333, 0.1333]]) index = tensor([[2], [1], [0]])x.scatter(1, index, 0.6) --&gt;tensor([[0.1333, 0.1333, 0.6, 0.1333, 0.1333], [0.1333, 0.6, 0.1333, 0.1333, 0.1333], [0.6, 0.1333, 0.1333, 0.1333, 0.1333]]) 例如上述的五分类中，目标序列 [2，1，0] 表示类别 2，1，0。将明确的类别转换成概率分布，使概率分布更均匀些，然后与预测概率分布求损失 123456789crit = LabelSmoothing(size=5, padding_idx=0, smoothing=0.4)predict = torch.FloatTensor([ [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0],])v = crit(Variable(predict.log()), Variable(torch.LongTensor([2, 1, 0])))plt.imshow(crit.true_dist)crit.true_dist tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333], [0.0000, 0.6000, 0.1333, 0.1333, 0.1333], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]) 12345678910111213141516crit = LabelSmoothing(5, 0, 0.1)def loss(x): d = x + 3 * 1 predict = torch.FloatTensor([ [0, x / d, 1 / d, 1 / d, 1 / d], # 概率分布，x 的值越大，标签 1 的概率越大 ]) #print(predict) return crit( Variable(predict.log()), Variable(torch.LongTensor([1])), # 真实标签为 1 ).item()plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)]) [&lt;matplotlib.lines.Line2D at 0x7ffb16742c90&gt;] 测试模型 12345678# 生成随机数据def data_gen(V, batch, nbatches): for i in range(nbatches): data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10))) data[:, 0] = 1 src = Variable(data, requires_grad=False) tgt = Variable(data, requires_grad=False) yield Batch(src, tgt, 0) 123456789101112131415class SimpleLossCompute: def __init__(self, generator, criterion, opt=None): self.generator = generator # 模型最后的输出层 self.criterion = criterion self.opt = opt def __call__(self, x, y, norm): x = self.generator(x) loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm loss.backward() if self.opt is not None: self.opt.step() self.opt.optimizer.zero_grad() return loss.data.item() * norm 123456789101112131415V = 11criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)model = make_model(V, V, N=2)model_opt = NoamOpt( model.src_embed[0].d_model, 1, 400, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))for epoch in range(10): model.train() run_epoch(data_gen(V, 30, 20), model, SimpleLossCompute(model.generator, criterion, model_opt)) model.eval() print( run_epoch(data_gen(V, 30, 5), model, SimpleLossCompute(model.generator, criterion, None))) Epoch Step: 1 Loss: 3.041517 Tokens per Sec: 2320.010498 Epoch Step: 1 Loss: 2.116642 Tokens per Sec: 3337.003174 tensor(2.0963) Epoch Step: 1 Loss: 2.251899 Tokens per Sec: 2198.680176 Epoch Step: 1 Loss: 1.861713 Tokens per Sec: 3322.143066 tensor(1.9056) Epoch Step: 1 Loss: 2.097054 Tokens per Sec: 2199.775635 Epoch Step: 1 Loss: 1.919798 Tokens per Sec: 3104.268799 tensor(1.9386) Epoch Step: 1 Loss: 1.955538 Tokens per Sec: 2096.228027 Epoch Step: 1 Loss: 1.854445 Tokens per Sec: 3121.148926 tensor(1.8574) Epoch Step: 1 Loss: 2.210659 Tokens per Sec: 2088.976074 Epoch Step: 1 Loss: 1.806028 Tokens per Sec: 3116.300537 tensor(1.7957) Epoch Step: 1 Loss: 2.442501 Tokens per Sec: 2085.300537 Epoch Step: 1 Loss: 1.931611 Tokens per Sec: 3112.506348 tensor(1.9629) Epoch Step: 1 Loss: 1.962478 Tokens per Sec: 2092.424561 Epoch Step: 1 Loss: 1.450812 Tokens per Sec: 3114.582275 tensor(1.5110) Epoch Step: 1 Loss: 1.954074 Tokens per Sec: 2087.149170 Epoch Step: 1 Loss: 1.519574 Tokens per Sec: 3094.708252 tensor(1.4481) Epoch Step: 1 Loss: 1.859929 Tokens per Sec: 2039.030151 Epoch Step: 1 Loss: 1.412592 Tokens per Sec: 3088.327881 tensor(1.3660) Epoch Step: 1 Loss: 1.943988 Tokens per Sec: 2065.341797 Epoch Step: 1 Loss: 1.302344 Tokens per Sec: 2979.704590 tensor(1.3880) 解码算法 12345678910111213141516171819def greedy_decode(model, src, src_mask, max_len, start_symbol): memory = model.encode(src, src_mask) ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data) for i in range(max_len - 1): out = model.decode( memory, src_mask, Variable(ys), Variable(subsequent_mask(ys.size(1)).type_as(src.data))) prob = model.generator(out[:, -1]) _, next_word = torch.max(prob, dim=1) next_word = next_word.item() ys = torch.cat( [ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1) return ysmodel.eval()src = Variable(torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]))src_mask = Variable(torch.ones(1, 1, 10))print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1)) tensor([[ 1, 2, 4, 3, 7, 9, 10, 8, 9, 8]]) 1 实战 123456789101112131415161718192021222324252627282930313233343536from torchtext import data, datasetsif True: import spacy spacy_de = spacy.load(&quot;de_core_news_sm&quot;) spacy_en = spacy.load(&quot;en_core_web_sm&quot;) def tokenize_de(text): return [tok.text for tok in spacy_de.tokenizer(text)] def tokenize_en(text): return [tok.text for tok in spacy_en.tokenizer(text)] BOS_WORD = '&lt;s&gt;' EOS_WORD = '&lt;/s&gt;' BLANK_WORD = &quot;&lt;blank&gt;&quot; SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD) # 定义预处理流程，分词、填充、 TGT = data.Field(tokenize=tokenize_en, init_token=BOS_WORD, eos_token=EOS_WORD, pad_token=BLANK_WORD) MAX_LEN = 100 # 数据集 train, val, test = datasets.IWSLT.splits( exts=('.de', '.en'), fields=(SRC, TGT), filter_pred=lambda x: len(vars(x)['src']) &lt;= MAX_LEN and len( vars(x)['trg']) &lt;= MAX_LEN) MIN_FREQ = 2 # 创建词汇表 SRC.build_vocab(train.src, min_freq=MIN_FREQ) TGT.build_vocab(train.trg, min_freq=MIN_FREQ) 1234567891011121314151617181920212223242526# 数据分批对训练速度很重要：需要拆分成均匀的批次，最小的填充class MyIterator(data.Iterator): def create_batches(self): if self.train: # 训练模式，数据分批，然后打乱顺序 def pool(d, random_shuffler): for p in data.batch(d, self.batch_size * 100): p_batch = data.batch(sorted(p, key=self.sort_key), self.batch_size, self.batch_size_fn) for b in random_shuffler(list(p_batch)): yield b self.batches = pool(self.data(), self.random_shuffler) else: self.batches = [] for b in data.batch(self.data(), self.batch_size, self.batch_size_fn): self.batches.append(sorted(b, key=self.sort_key))def rebatch(pad_idx, batch): # batch first --&gt; True &quot;Fix order in torchtext to match ours&quot; src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1) return Batch(src, trg, pad_idx) 并行计算 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 使用 multi-gpu 加速训练速度：将单词生成拆分成块，便于并行处理class MultiGPULossCompute: def __init__(self, generator, criterion, devices, opt=None, chunk_size=5): self.generator = generator self.criterion = nn.parallel.replicate(criterion, devices=devices) self.opt = opt self.devices = devices self.chunk_size = chunk_size def __call__(self, out, targets, normalize): total = 0.0 # 将最终的线性输出层 并行 到多个 gpu中 generator = nn.parallel.replicate(self.generator, devices=devices) # 将 transformer 的输出张量 并行 多个 gpu 中 out_scatter = nn.parallel.scatter(out, target_gpus=self.devices) out_grad = [[] for _ in out_scatter] # 将目标 并行 到多个 gpu 中 targets = nn.parallel.scatter(targets, target_gpus=self.devices) # 将生成拆分成块？？ chunk_size = self.chunk_size for i in range(0, out_scatter[0].size(1), chunk_size): # 预测分布 out_column = [[ Variable(o[:, i:i + chunk_size].data, requires_grad=self.opt is not None) ] for o in out_scatter] gen = nn.parallel.parallel_apply(generator, out_column) # 计算损失 y = [(g.contiguous().view(-1, g.size(-1)), t[:, i:i + chunk_size].contiguous().view(-1)) for g, t in zip(gen, targets)] loss = nn.parallel.parallel_apply(self.criterion, y) # 损失求和并归一化 l = nn.parallel.gather(loss, target_device=self.devices[0]) l = l.sum()[0] / normalize total += l.data[0] # 反向传播 if self.opt is not None: l.backward() for j, l in enumerate(loss): out_grad[j].append(out_column[j][0].grad.data.clone()) # 反向传播整个模型 if self.opt is not None: out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad] o1 = out o2 = nn.parallel.gather(out_grad, target_device=self.devices[0]) o1.backward(gradient=o2) self.opt.step() self.opt.optimizer.zero_grad() return total * normalize 1 12345678910111213141516171819202122232425devices = [0, 1, 2, 3]if True: pad_idx = TGT.vocab.stoi[&quot;&lt;blank&gt;&quot;] model = make_model(len(SRC.vocab), len(TGT.vocab), N=6) model.cuda() criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1) criterion.cuda() BATCH_SIZE = 12000 train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0, repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)), batch_size_fn=batch_size_fn, train=True) valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0, repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)), batch_size_fn=batch_size_fn, train=False) model_par = nn.DataParallel(model, device_ids=devices) 123456789101112131415161718192021if False: model_opt = NoamOpt( model.src_embed[0].d_model, 1, 2000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)) for epoch in range(10): model_par.train() run_epoch((rebatch(pad_idx, b) for b in train_iter), model_par, MultiGPULossCompute(model.generator, criterion, devices=devices, opt=model_opt)) model_par.eval() loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), model_par, MultiGPULossCompute(model.generator, criterion, devices=devices, opt=None)) print(loss)else: model = torch.load(&quot;iwslt.pt&quot;) 123456789101112131415161718192021for i, batch in enumerate(valid_iter): src = batch.src.transpose(0, 1)[:1] src_mask = (src != SRC.vocab.stoi[&quot;&lt;blank&gt;&quot;]).unsqueeze(-2) out = greedy_decode(model, src, src_mask, max_len=60, start_symbol=TGT.vocab.stoi[&quot;&lt;s&gt;&quot;]) print(&quot;Translation:&quot;, end=&quot;\\t&quot;) for i in range(1, out.size(1)): sym = TGT.vocab.itos[out[0, i]] if sym == &quot;&lt;/s&gt;&quot;: break print(sym, end=&quot; &quot;) print() print(&quot;Target:&quot;, end=&quot;\\t&quot;) for i in range(1, batch.trg.size(0)): sym = TGT.vocab.itos[batch.trg.data[i, 0]] if sym == &quot;&lt;/s&gt;&quot;: break print(sym, end=&quot; &quot;) print() break 1 额外的组件 BPE/ Word-piece：将单词拆分成 子词 1 共享权重：输入与目标的嵌入矩阵相同 123if False: model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight model.generator.lut.weight = model.tgt_embed[0].lut.weight Beam Search https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam_search.py 1 Model Averaging：将最后 k 个 checkpoint 平均，创造组合模型 1234def average(model, models): &quot;Average models into model&quot; for ps in zip(*[m.params() for m in [model] + models]): p[0].copy_(torch.sum(*ps[1:]) / len(ps[1:])) 1 1 1 注意力可视化 12345678910111213141516171819202122232425262728293031323334353637383940414243tgt_sent = trans.split()def draw(data, x, y, ax): seaborn.heatmap(data, xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, cbar=False, ax=ax)for layer in range(1, 6, 2): fig, axs = plt.subplots(1, 4, figsize=(20, 10)) print(&quot;Encoder Layer&quot;, layer + 1) for h in range(4): draw(model.encoder.layers[layer].self_attn.attn[0, h].data, sent, sent if h == 0 else [], ax=axs[h]) plt.show()for layer in range(1, 6, 2): fig, axs = plt.subplots(1, 4, figsize=(20, 10)) print(&quot;Decoder Self Layer&quot;, layer + 1) for h in range(4): draw(model.decoder.layers[layer].self_attn.attn[0, h]. data[:len(tgt_sent), :len(tgt_sent)], tgt_sent, tgt_sent if h == 0 else [], ax=axs[h]) plt.show() print(&quot;Decoder Src Layer&quot;, layer + 1) fig, axs = plt.subplots(1, 4, figsize=(20, 10)) for h in range(4): draw(model.decoder.layers[layer].self_attn.attn[0, h]. data[:len(tgt_sent), :len(sent)], sent, tgt_sent if h == 0 else [], ax=axs[h]) plt.show()","link":"/posts/52952.html"},{"title":"随机森林","text":"Bagging Bagging采用自助采样法(bootstrap sampling)采样数据。给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，样本仍可能被选中， 这样，经过m次随机采样操作，我们得到包含m个样本的采样集。 按照此方式，我们可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基本学习器，再将这些基本学习器进行结合。这就是Bagging的一般流程。在对预测输出进行结合时，Bagging通常使用简单投票法， 对回归问题使用简单平均法。若分类预测时，出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可以进一步考察学习器投票的置信度来确定最终胜者。 Bagging的算法描述如下图所示。 随机森林 随机森林是Bagging的一个扩展变体。随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来讲，传统决策树在选择划分属性时， 在当前节点的属性集合（假设有d个属性）中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。 这里的参数k控制了随机性的引入程度。若令k=d，则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性用于划分。在MLlib中，有两种选择用于分类，即k=log2(d)、k=sqrt(d)； 一种选择用于回归，即k=1/3d。在源码分析中会详细介绍。 可以看出，随机森林对Bagging只做了小改动，但是与Bagging中基学习器的“多样性”仅仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动。 这使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。 随机森林在分布式环境下的优化策略 随机森林算法在单机环境下很容易实现，但在分布式环境下特别是在Spark平台上，传统单机形式的迭代方式必须要进行相应改进才能适用于分布式环境 ，这是因为在分布式环境下，数据也是分布式的，算法设计不得当会生成大量的IO操作，例如频繁的网络数据传输，从而影响算法效率。 因此，在Spark上进行随机森林算法的实现，需要进行一定的优化，Spark中的随机森林算法主要实现了三个优化策略： 切分点抽样统计，如下图所示。在单机环境下的决策树对连续变量进行切分点选择时，一般是通过对特征点进行排序，然后取相邻两个数之间的点作为切分点，这在单机环境下是可行的，但如果在分布式环境下如此操作的话， 会带来大量的网络传输操作，特别是当数据量达到PB级时，算法效率将极为低下。为避免该问题，Spark中的随机森林在构建决策树时，会对各分区采用一定的子特征策略进行抽样，然后生成各个分区的统计数据，并最终得到切分点。 (从源代码里面看，是先对样本进行抽样，然后根据抽样样本值出现的次数进行排序，然后再进行切分)。 特征装箱（Binning），如下图所示。决策树的构建过程就是对特征的取值不断进行划分的过程，对于离散的特征，如果有M个值，最多有2^(M-1) - 1个划分。如果值是有序的，那么就最多M-1个划分。 比如年龄特征，有老，中，少3个值，如果无序有2^2-1=3个划分，即老|中，少；老，中|少；老，少|中。；如果是有序的，即按老，中，少的序，那么只有m-1个，即2种划分，老|中，少；老，中|少。 对于连续的特征，其实就是进行范围划分，而划分的点就是split（切分点），划分出的区间就是bin。对于连续特征，理论上split是无数的，在分布环境下不可能取出所有的值，因此它采用的是切点抽样统计方法。 逐层训练（level-wise training），如下图所示。单机版本的决策树生成过程是通过递归调用（本质上是深度优先）的方式构造树，在构造树的同时，需要移动数据，将同一个子节点的数据移动到一起。 此方法在分布式数据结构上无法有效的执行，而且也无法执行，因为数据太大，无法放在一起，所以在分布式环境下采用的策略是逐层构建树节点（本质上是广度优先），这样遍历所有数据的次数等于所有树中的最大层数。 每次遍历时，只需要计算每个节点所有切分点统计参数，遍历完后，根据节点的特征划分，决定是否切分，以及如何切分。 使用实例 下面的例子用于分类。 123456789101112131415161718192021222324252627import org.apache.spark.mllib.tree.RandomForestimport org.apache.spark.mllib.tree.model.RandomForestModelimport org.apache.spark.mllib.util.MLUtils// Load and parse the data file.val data = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)// Split the data into training and test sets (30% held out for testing)val splits = data.randomSplit(Array(0.7, 0.3))val (trainingData, testData) = (splits(0), splits(1))// Train a RandomForest model.// 空的类别特征信息表示所有的特征都是连续的.val numClasses = 2val categoricalFeaturesInfo = Map[Int, Int]()val numTrees = 3 // Use more in practice.val featureSubsetStrategy = &quot;auto&quot; // Let the algorithm choose.val impurity = &quot;gini&quot;val maxDepth = 4val maxBins = 32val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)// Evaluate model on test instances and compute test errorval labelAndPreds = testData.map { point =&gt; val prediction = model.predict(point.features) (point.label, prediction)}val testErr = labelAndPreds.filter(r =&gt; r._1 != r._2).count.toDouble / testData.count()println(&quot;Test Error = &quot; + testErr)println(&quot;Learned classification forest model:\\n&quot; + model.toDebugString) 下面的例子用于回归。 123456789101112131415161718192021222324252627import org.apache.spark.mllib.tree.RandomForestimport org.apache.spark.mllib.tree.model.RandomForestModelimport org.apache.spark.mllib.util.MLUtils// Load and parse the data file.val data = MLUtils.loadLibSVMFile(sc, &quot;data/mllib/sample_libsvm_data.txt&quot;)// Split the data into training and test sets (30% held out for testing)val splits = data.randomSplit(Array(0.7, 0.3))val (trainingData, testData) = (splits(0), splits(1))// Train a RandomForest model.// 空的类别特征信息表示所有的特征都是连续的val numClasses = 2val categoricalFeaturesInfo = Map[Int, Int]()val numTrees = 3 // Use more in practice.val featureSubsetStrategy = &quot;auto&quot; // Let the algorithm choose.val impurity = &quot;variance&quot;val maxDepth = 4val maxBins = 32val model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)// Evaluate model on test instances and compute test errorval labelsAndPredictions = testData.map { point =&gt; val prediction = model.predict(point.features) (point.label, prediction)}val testMSE = labelsAndPredictions.map{ case(v, p) =&gt; math.pow((v - p), 2)}.mean()println(&quot;Test Mean Squared Error = &quot; + testMSE)println(&quot;Learned regression forest model:\\n&quot; + model.toDebugString) 源码分析 训练分析 训练过程简单可以分为两步，第一步是初始化，第二步是迭代构建随机森林。这两大步还分为若干小步，下面会分别介绍这些内容。 初始化 12345678910111213141516171819202122232425262728293031val retaggedInput = input.retag(classOf[LabeledPoint])//建立决策树的元数据信息（分裂点位置、箱子数及各箱子包含特征属性的值等）val metadata = DecisionTreeMetadata.buildMetadata(retaggedInput, strategy, numTrees, featureSubsetStrategy)//找到切分点（splits）及箱子信息（Bins）//对于连续型特征，利用切分点抽样统计简化计算//对于离散型特征，如果是无序的，则最多有个 splits=2^(numBins-1)-1 划分//如果是有序的，则最多有 splits=numBins-1 个划分val (splits, bins) = DecisionTree.findSplitsBins(retaggedInput, metadata)//转换成树形的 RDD 类型，转换后，所有样本点已经按分裂点条件分到了各自的箱子中val treeInput = TreePoint.convertToTreeRDD(retaggedInput, bins, metadata)val withReplacement = if (numTrees &gt; 1) true else false// convertToBaggedRDD 方法使得每棵树就是样本的一个子集val baggedInput = BaggedPoint.convertToBaggedRDD(treeInput, strategy.subsamplingRate, numTrees, withReplacement, seed).persist(StorageLevel.MEMORY_AND_DISK)//决策树的深度，最大为30val maxDepth = strategy.maxDepth//聚合的最大内存val maxMemoryUsage: Long = strategy.maxMemoryInMB * 1024L * 1024Lval maxMemoryPerNode = { val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) { // Find numFeaturesPerNode largest bins to get an upper bound on memory usage. Some(metadata.numBins.zipWithIndex.sortBy(- _._1) .take(metadata.numFeaturesPerNode).map(_._2)) } else { None } //计算聚合操作时节点的内存 RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L} 初始化的第一步就是决策树元数据信息的构建。它的代码如下所示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192def buildMetadata( input: RDD[LabeledPoint], strategy: Strategy, numTrees: Int, featureSubsetStrategy: String): DecisionTreeMetadata = { //特征数 val numFeatures = input.map(_.features.size).take(1).headOption.getOrElse { throw new IllegalArgumentException(s&quot;DecisionTree requires size of input RDD &gt; 0, &quot; + s&quot;but was given by empty one.&quot;) } val numExamples = input.count() val numClasses = strategy.algo match { case Classification =&gt; strategy.numClasses case Regression =&gt; 0 } //最大可能的装箱数 val maxPossibleBins = math.min(strategy.maxBins, numExamples).toInt if (maxPossibleBins &lt; strategy.maxBins) { logWarning(s&quot;DecisionTree reducing maxBins from ${strategy.maxBins} to $maxPossibleBins&quot; + s&quot; (= number of training instances)&quot;) } // We check the number of bins here against maxPossibleBins. // This needs to be checked here instead of in Strategy since maxPossibleBins can be modified // based on the number of training examples. //最大分类数要小于最大可能装箱数 //这里categoricalFeaturesInfo是传入的信息，这个map保存特征的类别信息。 //例如，(n-&gt;k)表示特征k包含的类别有（0,1,...,k-1） if (strategy.categoricalFeaturesInfo.nonEmpty) { val maxCategoriesPerFeature = strategy.categoricalFeaturesInfo.values.max val maxCategory = strategy.categoricalFeaturesInfo.find(_._2 == maxCategoriesPerFeature).get._1 require(maxCategoriesPerFeature &lt;= maxPossibleBins, s&quot;DecisionTree requires maxBins (= $maxPossibleBins) to be at least as large as the &quot; + s&quot;number of values in each categorical feature, but categorical feature $maxCategory &quot; + s&quot;has $maxCategoriesPerFeature values. Considering remove this and other categorical &quot; + &quot;features with a large number of values, or add more training examples.&quot;) } val unorderedFeatures = new mutable.HashSet[Int]() val numBins = Array.fill[Int](numFeatures)(maxPossibleBins) if (numClasses &gt; 2) { // 多分类 val maxCategoriesForUnorderedFeature = ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =&gt; //如果类别特征只有1个类，我们把它看成连续的特征 if (numCategories &gt; 1) { // Decide if some categorical features should be treated as unordered features, // which require 2 * ((1 &lt;&lt; numCategories - 1) - 1) bins. // We do this check with log values to prevent overflows in case numCategories is large. // The next check is equivalent to: 2 * ((1 &lt;&lt; numCategories - 1) - 1) &lt;= maxBins if (numCategories &lt;= maxCategoriesForUnorderedFeature) { unorderedFeatures.add(featureIndex) numBins(featureIndex) = numUnorderedBins(numCategories) } else { numBins(featureIndex) = numCategories } } } } else { // 二分类或者回归 strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =&gt; //如果类别特征只有1个类，我们把它看成连续的特征 if (numCategories &gt; 1) { numBins(featureIndex) = numCategories } } } // 设置每个节点的特征数 (对随机森林而言). val _featureSubsetStrategy = featureSubsetStrategy match { case &quot;auto&quot; =&gt; if (numTrees == 1) {//决策树时，使用所有特征 &quot;all&quot; } else { if (strategy.algo == Classification) {//分类时，使用开平方 &quot;sqrt&quot; } else { //回归时，使用1/3的特征 &quot;onethird&quot; } } case _ =&gt; featureSubsetStrategy } val numFeaturesPerNode: Int = _featureSubsetStrategy match { case &quot;all&quot; =&gt; numFeatures case &quot;sqrt&quot; =&gt; math.sqrt(numFeatures).ceil.toInt case &quot;log2&quot; =&gt; math.max(1, (math.log(numFeatures) / math.log(2)).ceil.toInt) case &quot;onethird&quot; =&gt; (numFeatures / 3.0).ceil.toInt } new DecisionTreeMetadata(numFeatures, numExamples, numClasses, numBins.max, strategy.categoricalFeaturesInfo, unorderedFeatures.toSet, numBins, strategy.impurity, strategy.quantileCalculationStrategy, strategy.maxDepth, strategy.minInstancesPerNode, strategy.minInfoGain, numTrees, numFeaturesPerNode) } 初始化的第二步就是找到切分点（splits）及箱子信息（Bins）。这时，调用了DecisionTree.findSplitsBins方法，进入该方法了解详细信息。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Returns splits and bins for decision tree calculation. * Continuous and categorical features are handled differently. * * Continuous features: * For each feature, there are numBins - 1 possible splits representing the possible binary * decisions at each node in the tree. * This finds locations (feature values) for splits using a subsample of the data. * * Categorical features: * For each feature, there is 1 bin per split. * Splits and bins are handled in 2 ways: * (a) &quot;unordered features&quot; * For multiclass classification with a low-arity feature * (i.e., if isMulticlass &amp;&amp; isSpaceSufficientForAllCategoricalSplits), * the feature is split based on subsets of categories. * (b) &quot;ordered features&quot; * For regression and binary classification, * and for multiclass classification with a high-arity feature, * there is one bin per category. * * @param input Training data: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]] * @param metadata Learning and dataset metadata * @return A tuple of (splits, bins). * Splits is an Array of [[org.apache.spark.mllib.tree.model.Split]] * of size (numFeatures, numSplits). * Bins is an Array of [[org.apache.spark.mllib.tree.model.Bin]] * of size (numFeatures, numBins). */ protected[tree] def findSplitsBins( input: RDD[LabeledPoint], metadata: DecisionTreeMetadata): (Array[Array[Split]], Array[Array[Bin]]) = { //特征数 val numFeatures = metadata.numFeatures // Sample the input only if there are continuous features. // 判断特征中是否存在连续特征 val continuousFeatures = Range(0, numFeatures).filter(metadata.isContinuous) val sampledInput = if (continuousFeatures.nonEmpty) { // Calculate the number of samples for approximate quantile calculation. //采样样本数量，最少有 10000 个 val requiredSamples = math.max(metadata.maxBins * metadata.maxBins, 10000) //计算采样比例 val fraction = if (requiredSamples &lt; metadata.numExamples) { requiredSamples.toDouble / metadata.numExamples } else { 1.0 } //采样数据，有放回采样 input.sample(withReplacement = false, fraction, new XORShiftRandom().nextInt()) } else { input.sparkContext.emptyRDD[LabeledPoint] } //分裂点策略，目前 Spark 中只实现了一种策略：排序 Sort metadata.quantileStrategy match { case Sort =&gt; findSplitsBinsBySorting(sampledInput, metadata, continuousFeatures) case MinMax =&gt; throw new UnsupportedOperationException(&quot;minmax not supported yet.&quot;) case ApproxHist =&gt; throw new UnsupportedOperationException(&quot;approximate histogram not supported yet.&quot;) } } 我们进入findSplitsBinsBySorting方法了解Sort分裂策略的实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172private def findSplitsBinsBySorting( input: RDD[LabeledPoint], metadata: DecisionTreeMetadata, continuousFeatures: IndexedSeq[Int]): (Array[Array[Split]], Array[Array[Bin]]) = { def findSplits( featureIndex: Int, featureSamples: Iterable[Double]): (Int, (Array[Split], Array[Bin])) = { //每个特征分别对应一组切分点位置，这里splits是有序的 val splits = { // findSplitsForContinuousFeature 返回连续特征的所有切分位置 val featureSplits = findSplitsForContinuousFeature( featureSamples.toArray, metadata, featureIndex) featureSplits.map(threshold =&gt; new Split(featureIndex, threshold, Continuous, Nil)) } //存放切分点位置对应的箱子信息 val bins = { //采用最小阈值 Double.MinValue 作为最左边的分裂位置并进行装箱 val lowSplit = new DummyLowSplit(featureIndex, Continuous) //最后一个箱子的计算采用最大阈值 Double.MaxValue 作为最右边的切分位置 val highSplit = new DummyHighSplit(featureIndex, Continuous) // tack the dummy splits on either side of the computed splits val allSplits = lowSplit +: splits.toSeq :+ highSplit //将切分点两两结合成一个箱子 allSplits.sliding(2).map { case Seq(left, right) =&gt; new Bin(left, right, Continuous, Double.MinValue) }.toArray } (featureIndex, (splits, bins)) } val continuousSplits = { // reduce the parallelism for split computations when there are less // continuous features than input partitions. this prevents tasks from // being spun up that will definitely do no work. val numPartitions = math.min(continuousFeatures.length, input.partitions.length) input .flatMap(point =&gt; continuousFeatures.map(idx =&gt; (idx, point.features(idx)))) .groupByKey(numPartitions) .map { case (k, v) =&gt; findSplits(k, v) } .collectAsMap() } val numFeatures = metadata.numFeatures //遍历所有特征 val (splits, bins) = Range(0, numFeatures).unzip { //处理连续特征的情况 case i if metadata.isContinuous(i) =&gt; val (split, bin) = continuousSplits(i) metadata.setNumSplits(i, split.length) (split, bin) //处理离散特征且无序的情况 case i if metadata.isCategorical(i) &amp;&amp; metadata.isUnordered(i) =&gt; // Unordered features // 2^(maxFeatureValue - 1) - 1 combinations val featureArity = metadata.featureArity(i) val split = Range(0, metadata.numSplits(i)).map { splitIndex =&gt; val categories = extractMultiClassCategories(splitIndex + 1, featureArity) new Split(i, Double.MinValue, Categorical, categories) } // For unordered categorical features, there is no need to construct the bins. // since there is a one-to-one correspondence between the splits and the bins. (split.toArray, Array.empty[Bin]) //处理离散特征且有序的情况 case i if metadata.isCategorical(i) =&gt; //有序特征无需处理，箱子与特征值对应 // Ordered features // Bins correspond to feature values, so we do not need to compute splits or bins // beforehand. Splits are constructed as needed during training. (Array.empty[Split], Array.empty[Bin]) } (splits.toArray, bins.toArray) } 计算连续特征的所有切分位置需要调用方法findSplitsForContinuousFeature方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private[tree] def findSplitsForContinuousFeature( featureSamples: Array[Double], metadata: DecisionTreeMetadata, featureIndex: Int): Array[Double] = { val splits = { //切分数是bin的数量减1，即m-1 val numSplits = metadata.numSplits(featureIndex) // （特征，特征出现的次数） val valueCountMap = featureSamples.foldLeft(Map.empty[Double, Int]) { (m, x) =&gt; m + ((x, m.getOrElse(x, 0) + 1)) } // 根据特征进行排序 val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray // if possible splits is not enough or just enough, just return all possible splits val possibleSplits = valueCounts.length //如果特征数小于切分数，所有特征均作为切分点 if (possibleSplits &lt;= numSplits) { valueCounts.map(_._1) } else { // 等频切分 // 切分点之间的步长 val stride: Double = featureSamples.length.toDouble / (numSplits + 1) val splitsBuilder = Array.newBuilder[Double] var index = 1 // currentCount: sum of counts of values that have been visited //第一个特征的出现次数 var currentCount = valueCounts(0)._2 // targetCount: target value for `currentCount`. // If `currentCount` is closest value to `targetCount`, // then current value is a split threshold. // After finding a split threshold, `targetCount` is added by stride. // 如果currentCount离targetCount最近，那么当前值是切分点 var targetCount = stride while (index &lt; valueCounts.length) { val previousCount = currentCount currentCount += valueCounts(index)._2 val previousGap = math.abs(previousCount - targetCount) val currentGap = math.abs(currentCount - targetCount) // If adding count of current value to currentCount // makes the gap between currentCount and targetCount smaller, // previous value is a split threshold. if (previousGap &lt; currentGap) { splitsBuilder += valueCounts(index - 1)._1 targetCount += stride } index += 1 } splitsBuilder.result() } } splits } 在if判断里每步前进stride个样本，累加在targetCount中。while循环逐次把每个特征值的个数加到currentCount里，计算前一次previousCount和这次currentCount到targetCount的距离，有3种情况，一种是pre和cur都在target左边，肯定是cur小，继续循环，进入第二种情况；第二种一左一右，如果pre小，肯定是pre是最好的分割点，如果cur还是小，继续循环步进，进入第三种情况；第三种就是都在右边，显然是pre小。因此if的判断条件pre&lt;cur，只要满足肯定就是split。整体下来的效果就能找到离target最近的一个特征值。 迭代构建随机森林 1234567891011121314151617181920212223242526272829//节点是否使用缓存，节点 ID 从 1 开始，1 即为这颗树的根节点，左节点为 2，右节点为 3，依次递增下去val nodeIdCache = if (strategy.useNodeIdCache) { Some(NodeIdCache.init( data = baggedInput, numTrees = numTrees, checkpointInterval = strategy.checkpointInterval, initVal = 1))} else { None}// FIFO queue of nodes to train: (treeIndex, node)val nodeQueue = new mutable.Queue[(Int, Node)]()val rng = new scala.util.Random()rng.setSeed(seed)// Allocate and queue root nodes.//创建树的根节点val topNodes: Array[Node] = Array.fill[Node](numTrees)(Node.emptyNode(nodeIndex = 1))//将（树的索引，树的根节点）入队，树索引从 0 开始，根节点从 1 开始Range(0, numTrees).foreach(treeIndex =&gt; nodeQueue.enqueue((treeIndex, topNodes(treeIndex))))while (nodeQueue.nonEmpty) { // Collect some nodes to split, and choose features for each node (if subsampling). // Each group of nodes may come from one or multiple trees, and at multiple levels. // 取得每个树所有需要切分的节点,nodesForGroup表示需要切分的节点 val (nodesForGroup, treeToNodeToIndexInfo) = RandomForest.selectNodesToSplit(nodeQueue, maxMemoryUsage, metadata, rng) //找出最优切点 DecisionTree.findBestSplits(baggedInput, metadata, topNodes, nodesForGroup, treeToNodeToIndexInfo, splits, bins, nodeQueue, timer, nodeIdCache = nodeIdCache)} 这里有两点需要重点介绍，第一点是取得每个树所有需要切分的节点，通过RandomForest.selectNodesToSplit方法实现；第二点是找出最优的切分，通过DecisionTree.findBestSplits方法实现。下面分别介绍这两点。 取得每个树所有需要切分的节点 123456789101112131415161718192021222324252627282930313233343536373839404142private[tree] def selectNodesToSplit( nodeQueue: mutable.Queue[(Int, Node)], maxMemoryUsage: Long, metadata: DecisionTreeMetadata, rng: scala.util.Random): (Map[Int, Array[Node]], Map[Int, Map[Int, NodeIndexInfo]]) = { // nodesForGroup保存需要切分的节点，treeIndex --&gt; nodes val mutableNodesForGroup = new mutable.HashMap[Int, mutable.ArrayBuffer[Node]]() // mutableTreeToNodeToIndexInfo保存每个节点中选中特征的索引 // treeIndex --&gt; (global) node index --&gt; (node index in group, feature indices) //(global) node index是树中的索引，组中节点索引的范围是[0, numNodesInGroup) val mutableTreeToNodeToIndexInfo = new mutable.HashMap[Int, mutable.HashMap[Int, NodeIndexInfo]]() var memUsage: Long = 0L var numNodesInGroup = 0 while (nodeQueue.nonEmpty &amp;&amp; memUsage &lt; maxMemoryUsage) { val (treeIndex, node) = nodeQueue.head // Choose subset of features for node (if subsampling). // 选中特征子集 val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) { Some(SamplingUtils.reservoirSampleAndCount(Range(0, metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong)._1) } else { None } // Check if enough memory remains to add this node to the group. // 检查是否有足够的内存 val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L if (memUsage + nodeMemUsage &lt;= maxMemoryUsage) { nodeQueue.dequeue() mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[Node]()) += node mutableTreeToNodeToIndexInfo .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id) = new NodeIndexInfo(numNodesInGroup, featureSubset) } numNodesInGroup += 1 memUsage += nodeMemUsage } // 将可变map转换为不可变map val nodesForGroup: Map[Int, Array[Node]] = mutableNodesForGroup.mapValues(_.toArray).toMap val treeToNodeToIndexInfo = mutableTreeToNodeToIndexInfo.mapValues(_.toMap).toMap (nodesForGroup, treeToNodeToIndexInfo) } 选中最优切分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//所有可切分的节点val nodes = new Array[Node](numNodes)nodesForGroup.foreach { case (treeIndex, nodesForTree) =&gt; nodesForTree.foreach { node =&gt; nodes(treeToNodeToIndexInfo(treeIndex)(node.id).nodeIndexInGroup) = node }}// In each partition, iterate all instances and compute aggregate stats for each node,// yield an (nodeIndex, nodeAggregateStats) pair for each node.// After a `reduceByKey` operation,// stats of a node will be shuffled to a particular partition and be combined together,// then best splits for nodes are found there.// Finally, only best Splits for nodes are collected to driver to construct decision tree.//获取节点对应的特征val nodeToFeatures = getNodeToFeatures(treeToNodeToIndexInfo)val nodeToFeaturesBc = input.sparkContext.broadcast(nodeToFeatures)val partitionAggregates : RDD[(Int, DTStatsAggregator)] = if (nodeIdCache.nonEmpty) { input.zip(nodeIdCache.get.nodeIdsForInstances).mapPartitions { points =&gt; // Construct a nodeStatsAggregators array to hold node aggregate stats, // each node will have a nodeStatsAggregator val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =&gt; //节点对应的特征集 val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =&gt; Some(nodeToFeatures(nodeIndex)) } // DTStatsAggregator，其中引用了 ImpurityAggregator，给出计算不纯度 impurity 的逻辑 new DTStatsAggregator(metadata, featuresForNode) } // 迭代当前分区的所有对象，更新聚合统计信息，统计信息即采样数据的权重值 points.foreach(binSeqOpWithNodeIdCache(nodeStatsAggregators, _)) // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs, // which can be combined with other partition using `reduceByKey` nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator }} else { input.mapPartitions { points =&gt; // Construct a nodeStatsAggregators array to hold node aggregate stats, // each node will have a nodeStatsAggregator val nodeStatsAggregators = Array.tabulate(numNodes) { nodeIndex =&gt; //节点对应的特征集 val featuresForNode = nodeToFeaturesBc.value.flatMap { nodeToFeatures =&gt; Some(nodeToFeatures(nodeIndex)) } // DTStatsAggregator，其中引用了 ImpurityAggregator，给出计算不纯度 impurity 的逻辑 new DTStatsAggregator(metadata, featuresForNode) } // 迭代当前分区的所有对象，更新聚合统计信息 points.foreach(binSeqOp(nodeStatsAggregators, _)) // transform nodeStatsAggregators array to (nodeIndex, nodeAggregateStats) pairs, // which can be combined with other partition using `reduceByKey` nodeStatsAggregators.view.zipWithIndex.map(_.swap).iterator }}val nodeToBestSplits = partitionAggregates.reduceByKey((a, b) =&gt; a.merge(b)) .map { case (nodeIndex, aggStats) =&gt; val featuresForNode = nodeToFeaturesBc.value.map { nodeToFeatures =&gt; nodeToFeatures(nodeIndex) } // find best split for each node val (split: Split, stats: InformationGainStats, predict: Predict) = binsToBestSplit(aggStats, splits, featuresForNode, nodes(nodeIndex)) (nodeIndex, (split, stats, predict))}.collectAsMap() 该方法中的关键是对binsToBestSplit方法的调用，binsToBestSplit方法代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143private def binsToBestSplit( binAggregates: DTStatsAggregator, splits: Array[Array[Split]], featuresForNode: Option[Array[Int]], node: Node): (Split, InformationGainStats, Predict) = { // 如果当前节点是根节点，计算预测和不纯度 val level = Node.indexToLevel(node.id) var predictWithImpurity: Option[(Predict, Double)] = if (level == 0) { None } else { Some((node.predict, node.impurity)) } // 对各特征及切分点，计算其信息增益并从中选择最优 (feature, split) val (bestSplit, bestSplitStats) = Range(0, binAggregates.metadata.numFeaturesPerNode).map { featureIndexIdx =&gt; val featureIndex = if (featuresForNode.nonEmpty) { featuresForNode.get.apply(featureIndexIdx) } else { featureIndexIdx } val numSplits = binAggregates.metadata.numSplits(featureIndex) //特征为连续值的情况 if (binAggregates.metadata.isContinuous(featureIndex)) { // Cumulative sum (scanLeft) of bin statistics. // Afterwards, binAggregates for a bin is the sum of aggregates for // that bin + all preceding bins. val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx) var splitIndex = 0 while (splitIndex &lt; numSplits) { binAggregates.mergeForFeature(nodeFeatureOffset, splitIndex + 1, splitIndex) splitIndex += 1 } // Find best split. val (bestFeatureSplitIndex, bestFeatureGainStats) = Range(0, numSplits).map { case splitIdx =&gt; //计算 leftChild 及 rightChild 子节点的 impurity val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, splitIdx) val rightChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, numSplits) rightChildStats.subtract(leftChildStats) //求 impurity 的预测值，采用的是平均值计算 predictWithImpurity = Some(predictWithImpurity.getOrElse( calculatePredictImpurity(leftChildStats, rightChildStats))) //求信息增益 information gain 值，用于评估切分点是否最优,请参考决策树中1.4.4章节的介绍 val gainStats = calculateGainForSplit(leftChildStats, rightChildStats, binAggregates.metadata, predictWithImpurity.get._2) (splitIdx, gainStats) }.maxBy(_._2.gain) (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats) } //无序离散特征时的情况 else if (binAggregates.metadata.isUnordered(featureIndex)) { // Unordered categorical feature val (leftChildOffset, rightChildOffset) = binAggregates.getLeftRightFeatureOffsets(featureIndexIdx) val (bestFeatureSplitIndex, bestFeatureGainStats) = Range(0, numSplits).map { splitIndex =&gt; val leftChildStats = binAggregates.getImpurityCalculator(leftChildOffset, splitIndex) val rightChildStats = binAggregates.getImpurityCalculator(rightChildOffset, splitIndex) predictWithImpurity = Some(predictWithImpurity.getOrElse( calculatePredictImpurity(leftChildStats, rightChildStats))) val gainStats = calculateGainForSplit(leftChildStats, rightChildStats, binAggregates.metadata, predictWithImpurity.get._2) (splitIndex, gainStats) }.maxBy(_._2.gain) (splits(featureIndex)(bestFeatureSplitIndex), bestFeatureGainStats) } else {//有序离散特征时的情况 // Ordered categorical feature val nodeFeatureOffset = binAggregates.getFeatureOffset(featureIndexIdx) val numBins = binAggregates.metadata.numBins(featureIndex) /* Each bin is one category (feature value). * The bins are ordered based on centroidForCategories, and this ordering determines which * splits are considered. (With K categories, we consider K - 1 possible splits.) * * centroidForCategories is a list: (category, centroid) */ //多元分类时的情况 val centroidForCategories = if (binAggregates.metadata.isMulticlass) { // For categorical variables in multiclass classification, // the bins are ordered by the impurity of their corresponding labels. Range(0, numBins).map { case featureValue =&gt; val categoryStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue) val centroid = if (categoryStats.count != 0) { // impurity 求的就是均方差 categoryStats.calculate() } else { Double.MaxValue } (featureValue, centroid) } } else { // 回归或二元分类时的情况 // For categorical variables in regression and binary classification, // the bins are ordered by the centroid of their corresponding labels. Range(0, numBins).map { case featureValue =&gt; val categoryStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue) val centroid = if (categoryStats.count != 0) { //求的就是平均值作为 impurity categoryStats.predict } else { Double.MaxValue } (featureValue, centroid) } } // bins sorted by centroids val categoriesSortedByCentroid = centroidForCategories.toList.sortBy(_._2) // Cumulative sum (scanLeft) of bin statistics. // Afterwards, binAggregates for a bin is the sum of aggregates for // that bin + all preceding bins. var splitIndex = 0 while (splitIndex &lt; numSplits) { val currentCategory = categoriesSortedByCentroid(splitIndex)._1 val nextCategory = categoriesSortedByCentroid(splitIndex + 1)._1 //将两个箱子的状态信息进行合并 binAggregates.mergeForFeature(nodeFeatureOffset, nextCategory, currentCategory) splitIndex += 1 } // lastCategory = index of bin with total aggregates for this (node, feature) val lastCategory = categoriesSortedByCentroid.last._1 // Find best split. //通过信息增益值选择最优切分点 val (bestFeatureSplitIndex, bestFeatureGainStats) = Range(0, numSplits).map { splitIndex =&gt; val featureValue = categoriesSortedByCentroid(splitIndex)._1 val leftChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, featureValue) val rightChildStats = binAggregates.getImpurityCalculator(nodeFeatureOffset, lastCategory) rightChildStats.subtract(leftChildStats) predictWithImpurity = Some(predictWithImpurity.getOrElse( calculatePredictImpurity(leftChildStats, rightChildStats))) val gainStats = calculateGainForSplit(leftChildStats, rightChildStats, binAggregates.metadata, predictWithImpurity.get._2) (splitIndex, gainStats) }.maxBy(_._2.gain) val categoriesForSplit = categoriesSortedByCentroid.map(_._1.toDouble).slice(0, bestFeatureSplitIndex + 1) val bestFeatureSplit = new Split(featureIndex, Double.MinValue, Categorical, categoriesForSplit) (bestFeatureSplit, bestFeatureGainStats) } }.maxBy(_._2.gain) (bestSplit, bestSplitStats, predictWithImpurity.get._1) } 预测分析 在利用随机森林进行预测时，调用的predict方法扩展自TreeEnsembleModel，它是树结构组合模型的表示，其核心代码如下所示： 12345678910111213141516171819202122232425262728293031//不同的策略采用不同的预测方法def predict(features: Vector): Double = { (algo, combiningStrategy) match { case (Regression, Sum) =&gt; predictBySumming(features) case (Regression, Average) =&gt; predictBySumming(features) / sumWeights case (Classification, Sum) =&gt; // binary classification val prediction = predictBySumming(features) // TODO: predicted labels are +1 or -1 for GBT. Need a better way to store this info. if (prediction &gt; 0.0) 1.0 else 0.0 case (Classification, Vote) =&gt; predictByVoting(features) case _ =&gt; throw new IllegalArgumentException() } }private def predictBySumming(features: Vector): Double = { val treePredictions = trees.map(_.predict(features)) //两个向量的内集 blas.ddot(numTrees, treePredictions, 1, treeWeights, 1)}//通过投票选举private def predictByVoting(features: Vector): Double = { val votes = mutable.Map.empty[Int, Double] trees.view.zip(treeWeights).foreach { case (tree, weight) =&gt; val prediction = tree.predict(features).toInt votes(prediction) = votes.getOrElse(prediction, 0.0) + weight } votes.maxBy(_._2)._1} 参考文献 【1】机器学习.周志华 【2】Spark 随机森林算法原理、源码分析及案例实战 【3】Scalable Distributed Decision Trees in Spark MLlib","link":"/posts/1102132214.html"},{"title":"优化算法","text":"如何解决训练样本少的问题 目前大部分的深度学习模型仍然需要海量的数据支持。例如 ImageNet 数据就拥有1400多万的图片。而现实生产环境中，数据集通常较小，只有几万甚至几百个样本。这时候，如何在这种情况下应用深度学习呢? （1）利用预训练模型进行迁移微调（fine-tuning），预训练模型通常在特征上拥有很好的语义表达。此时，只需将模型在小数据集上进行微调就能取得不错的效果。这也是目前大部分小数据集常用的训练方式。视觉领域内，通常会ImageNet上训练完成的模型。自然语言处理领域，也有BERT模型等预训练模型可以使用。 （2）单样本或者少样本学习（one-shot，few-shot learning），这种方式适用于样本类别远远大于样本数量的情况等极端数据集。例如有1000个类别，每个类别只提供1-5个样本。少样本学习同样也需要借助预训练模型，但有别于微调的在于，微调通常仍然在学习不同类别的语义，而少样本学习通常需要学习样本之间的距离度量。例如孪生网络（Siamese Neural Networks）就是通过训练两个同种结构的网络来判别输入的两张图片是否属于同一类。 ​ 上述两种是常用训练小样本数据集的方式。此外，也有些常用的手段，例如数据集增强、正则或者半监督学习等方式来解决小样本数据集的训练问题。 深度学习是否能胜任所有数据集? 深度学习并不能胜任目前所有的数据环境，以下列举两种情况： （1）深度学习能取得目前的成果，很大一部分原因依赖于海量的数据集以及高性能密集计算硬件。因此，当数据集过小时，需要考虑与传统机器学习相比，是否在性能和硬件资源效率更具有优势。 （2）深度学习目前在视觉，自然语言处理等领域都有取得不错的成果。这些领域最大的特点就是具有局部相关性。例如图像中，人的耳朵位于两侧，鼻子位于两眼之间，文本中单词组成句子。这些都是具有局部相关性的，一旦被打乱则会破坏语义或者有不同的语义。所以当数据不具备这种相关性的时候，深度学习就很难取得效果。 有没有可能找到比已知算法更好的算法? 在最优化理论发展中，有个没有免费午餐的定律，其主要含义在于，在不考虑具体背景和细节的情况下，任何算法和随机猜的效果期望是一样的。即，没有任何一种算法能优于其他一切算法，甚至不比随机猜好。深度学习作为机器学习领域的一个分支同样符合这个定律。所以，虽然目前深度学习取得了非常不错的成果，但是我们同样不能盲目崇拜。 优化算法本质上是在寻找和探索更符合数据集和问题的算法，这里数据集是算法的驱动力，而需要通过数据集解决的问题就是算法的核心，任何算法脱离了数据都会没有实际价值，任何算法的假设都不能脱离实际问题。因此，实际应用中，面对不同的场景和不同的问题，可以从多个角度针对问题进行分析，寻找更优的算法。 什么是共线性，如何判断和解决共线性问题? 对于回归算法，无论是一般回归还是逻辑回归，在使用多个变量进行预测分析时，都可能存在多变量相关的情况，这就是多重共线性。共线性的存在，使得特征之间存在冗余，导致过拟合。 常用判断是否存在共线性的方法有： （1）相关性分析。当相关性系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性； （2）方差膨胀因子VIF。当VIF大于5或10时，代表模型存在严重的共线性问题； （3）条件系数检验。 当条件数大于100、1000时，代表模型存在严重的共线性问题。 通常可通过PCA降维、逐步回归法和LASSO回归等方法消除共线性。 权值初始化方法有哪些？ 在深度学习的模型中，从零开始训练时，权重的初始化有时候会对模型训练产生较大的影响。良好的初始化能让模型快速、有效的收敛，而糟糕的初始化会使得模型无法训练。 目前，大部分深度学习框架都提供了各类初始化方式，其中一般常用的会有如下几种： 1. 常数初始化(constant) ​ 把权值或者偏置初始化为一个常数。例如设置为0，偏置初始化为0较为常见，权重很少会初始化为0。TensorFlow中也有zeros_initializer、ones_initializer等特殊常数初始化函数。 2. 高斯初始化(gaussian) ​ 给定一组均值和标准差，随机初始化的参数会满足给定均值和标准差的高斯分布。高斯初始化是很常用的初始化方式。特殊地，在TensorFlow中还有一种截断高斯分布初始化（truncated_normal_initializer），其主要为了将超过两个标准差的随机数重新随机，使得随机数更稳定。 3. 均匀分布初始化(uniform) ​ 给定最大最小的上下限，参数会在该范围内以均匀分布方式进行初始化，常用上下限为（0，1）。 4. xavier 初始化(uniform) ​ 在batchnorm还未出现之前，要训练较深的网络，防止梯度弥散，需要依赖非常好的初始化方式。xavier 就是一种比较优秀的初始化方式，也是目前最常用的初始化方式之一。其目的是为了使得模型各层的激活值和梯度在传播过程中的方差保持一致。本质上xavier 还是属于均匀分布初始化，但与上述的均匀分布初始化有所不同，xavier 的上下限将在如下范围内进行均匀分布采样： \\[ [-\\sqrt{\\frac{6}{n+m}},\\sqrt{\\frac{6}{n+m}}] \\] ​ 其中，n为所在层的输入维度，m为所在层的输出维度。 6. kaiming初始化（msra 初始化） ​ kaiming初始化，在caffe中也叫msra 初始化。kaiming初始化和xavier 一样都是为了防止梯度弥散而使用的初始化方式。kaiming初始化的出现是因为xavier存在一个不成立的假设。xavier在推导中假设激活函数都是线性的，而在深度学习中常用的ReLu等都是非线性的激活函数。而kaiming初始化本质上是高斯分布初始化，与上述高斯分布初始化有所不同，其是个满足均值为0，方差为2/n的高斯分布： \\[ [0,\\sqrt{\\frac{2}{n}}] \\] ​ 其中，n为所在层的输入维度。 除上述常见的初始化方式以外，不同深度学习框架下也会有不同的初始化方式，读者可自行查阅官方文档。 如何防止梯度下降陷入局部最优解? 梯度下降法(GD)及其一些变种算法是目前深度学习里最常用于求解凸优化问题的优化算法。神经网络很可能存在很多局部最优解，而非全局最优解。 为了防止陷入局部最优，通常会采用如下一些方法，当然，这并不能保证一定能找到全局最优解，或许能得到一个比目前更优的局部最优解也是不错的： （1）stochastic GD /Mini-Batch GD ​ 在GD算法中，每次的梯度都是从所有样本中累计获取的，这种情况最容易导致梯度方向过于稳定一致，且更新次数过少，容易陷入局部最优。而stochastic GD是GD的另一种极端更新方式，其每次都只使用一个样本进行参数更新，这样更新次数大大增加也就不容易陷入局部最优。但引出的一个问题的在于其更新方向过多，导致不易于进一步优化。Mini-Batch GD便是两种极端的折中，即每次更新使用一小批样本进行参数更新。Mini-Batch GD是目前最常用的优化算法，严格意义上Mini-Batch GD也叫做stochastic GD，所以很多深度学习框架上都叫做SGD。 （2）动量 ​ 动量也是GD中常用的方式之一，SGD的更新方式虽然有效，但每次只依赖于当前批样本的梯度方向，这样的梯度方向依然很可能很随机。动量就是用来减少随机，增加稳定性。其思想是模仿物理学的动量方式，每次更新前加入部分上一次的梯度量，这样整个梯度方向就不容易过于随机。一些常见情况时，如上次梯度过大，导致进入局部最小点时，下一次更新能很容易借助上次的大梯度跳出局部最小点。 （3）自适应学习率 ​ 无论是GD还是动量重点优化角度是梯度方向。而学习率则是用来直接控制梯度更新幅度的超参数。自适应学习率的优化方法有很多，例如Adagrad和RMSprop。两种自适应学习率的方式稍有差异，但主要思想都是基于历史的累计梯度去计算一个当前较优的学习率。 为什么需要激活函数？ （1）非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。 （2）几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。 （3）计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。 （4）非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。 （5）单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。 （6）输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。 （7）接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。 （8）参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。 （9）归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。 常见的损失函数有哪些? 机器学习通过对算法中的目标函数进行不断求解优化，得到最终想要的结果。分类和回归问题中，通常使用损失函数或代价函数作为目标函数。 损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。 损失函数可分为经验风险损失和结构风险损失。经验风险损失是根据已知数据得到的损失。结构风险损失是为了防止模型被过度拟合已知数据而加入的惩罚项。 下面介绍常用的损失函数: （1）0-1 损失函数 如果预测值和目标值相等，值为 0，如果不相等，值为 1： \\[ L(Y,f(x))= \\left\\{ \\begin{array}{} 1\\;\\;\\;,\\;\\;Y\\ne f(x), \\\\ 0\\;\\;\\;,\\;\\;Y=f(x). \\end{array} \\right. \\] 一般的在实际使用中，相等的条件过于严格，可适当放宽条件： \\[ L(Y,f(x))= \\left\\{ \\begin{array}{} 1\\;\\;\\;,\\;\\;|Y - f(x)| \\ge T, \\\\ 0\\;\\;\\;,\\;\\;|Y-f(x)| &lt; T. \\end{array} \\right. \\] （2）绝对值损失函数 和 0-1 损失函数相似，绝对值损失函数表示为： \\[ L(Y,f(x))=|Y-f(x)|. \\] （3）平方损失函数 \\[ L(Y|f(x))=\\sum_{N}(Y-f(x))^2. \\] 这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该 使所有点到回归直线的距离和最小。 （4）log 对数损失函数 \\[ L(Y,P(Y|X))=-logP(Y|X). \\] 常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数式平方损失， 其实不然。逻辑回归它假设样本服从伯努利分布，进而求得满足该分布的似然函数，接着取对 数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看， 就是 log 损失函数。 （5）指数损失函数 指数损失函数的标准形式为： \\[ L(Y|f(x))=exp[-yf(x)]. \\] 例如 AdaBoost 就是以指数损失函数为损失函数。 （6）Hinge 损失函数 Hinge 损失函数的标准形式如下： \\[ L(y)=max(0, 1-ty). \\] 其中 y 是预测值，范围为(-1,1), t 为目标值，其为-1 或 1。 在线性支持向量机中，最优化问题可等价于： \\[ \\underset{w,b}{min}\\sum_{i=1}^{N}(1-y_i(wx_i+b))+\\lambda \\lVert w^2 \\rVert \\] \\[ \\frac{1}{m}\\sum_{i=1}^{N}l(wx_i+by_i))+\\lVert w^2 \\rVert \\] 其中\\(l(wx_i+by_i))\\)是Hinge损失函数，\\(\\lVert w^2 \\rVert\\)可看做为正则化项。 如何进行特征选择(feature selection)? 特征类型有哪些？ 对象本身会有许多属性。所谓特征，即能在某方面最能表征对象的一个或者一组属性。一般地，我们可以把特征分为如下三个类型： （1）相关特征：对于特定的任务和场景具有一定帮助的属性，这些属性通常能有效提升算法性能； （2）无关特征：在特定的任务和场景下完全无用的属性，这些属性对对象在本目标环境下完全无用； （3）冗余特征：同样是在特定的任务和场景下具有一定帮助的属性，但这类属性已过多的存在，不具有产生任何新的信息的能力。 如何考虑特征选择 当完成数据预处理之后，对特定的场景和目标而言很多维度上的特征都是不具有任何判别或者表征能力的，所以需要对数据在维度上进行筛选。一般地，可以从以下两个方面考虑来选择特征: （1）特征是否具有发散性：某个特征若在所有样本上的都是一样的或者接近一致，即方差非常小。 也就是说所有样本的都具有一致的表现，那这些就不具有任何信息。 （2）特征与目标的相关性：与目标相关性高的特征，应当优选选择。 特征选择方法分类 根据特征选择的形式又可以将特征选择方法分为 3 种: （1）过滤法：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 （2）包装法：根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。 （3）嵌入法：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。 特征选择目的 （1）减少特征维度，使模型泛化能力更强，减少过拟合; （2）降低任务目标的学习难度； （3）一组优秀的特征通常能有效的降低模型复杂度，提升模型效率 梯度消失/梯度爆炸原因，以及解决方法 为什么要使用梯度更新规则? 目前深度学习的火热，其最大的功臣之一就是反向传播。反向传播，即根据损失评价函数计算的误差，计算得到梯度，通过梯度反向传播的方式，指导深度网络权值的更新优化。这样做的原因在于，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数，因此整个深度网络可以视为是一个复合的非线性多元函数： \\[ F(x)=f_n(\\cdots f_3(f_2(f_1(x)*\\theta_1+b)*\\theta_2+b)\\cdots) \\] 我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，假设不同的输入，输出的最优解是g(x) ，那么，优化深度网络就是为了寻找到合适的权值，满足 Loss=L(g(x),F(x))取得极小值点，比如最简单的损失函数： \\[ Loss = \\lVert g(x)-f(x) \\rVert^2_2. \\] 假设损失函数的数据空间是下图这样的，我们最优的权值就是为了寻找下图中的最小值点， 对于这种数学寻找最小值问题，采用梯度下降的方法再适合不过了。 图 13.8.1 梯度消失/爆炸产生的原因? 本质上，梯度消失和爆炸是一种情况。在深层网络中，由于网络过深，如果初始得到的梯度过小，或者传播途中在某一层上过小，则在之后的层上得到的梯度会越来越小，即产生了梯度消失。梯度爆炸也是同样的。一般地，不合理的初始化以及激活函数，如sigmoid等，都会导致梯度过大或者过小，从而引起消失/爆炸。 下面分别从网络深度角度以及激活函数角度进行解释： （1）网络深度 若在网络很深时，若权重初始化较小，各层上的相乘得到的数值都会0-1之间的小数，而激活函数梯度也是0-1之间的数。那么连乘后，结果数值就会变得非常小，导致梯度消失。若权重初始化较大，大到乘以激活函数的导数都大于1，那么连乘后，可能会导致求导的结果很大，形成梯度爆炸。 （2）激活函数 如果激活函数选择不合适，比如使用 sigmoid，梯度消失就会很明显了，原因看下图，左图是sigmoid的函数图，右边是其导数的图像，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。 图 13.8.2 sigmod函数与其导数 梯度消失、爆炸的解决方案 1、预训练加微调 此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。 2、梯度剪切、正则 梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。 另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是L1和L2正则。 3、ReLu、leakReLu等激活函数 （1）ReLu：其函数的导数在正数部分是恒等于1，这样在深层网络中，在激活函数部分就不存在导致梯度过大或者过小的问题，缓解了梯度消失或者爆炸。同时也方便计算。当然，其也存在存在一些缺点，例如过滤到了负数部分，导致部分信息的丢失，输出的数据分布不在以0为中心，改变了数据分布。 （2）leakrelu：就是为了解决relu的0区间带来的影响，其数学表达为：leakrelu=max(k*x,0)其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。 4、batchnorm Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。Batchnorm全名是Batch Normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。 5、残差结构 残差的方式，能使得深层的网络梯度通过跳级连接路径直接返回到浅层部分，使得网络无论多深都能将梯度进行有效的回传。 6、LSTM LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)。在计算时，将过程中的梯度进行了抵消。 深度学习为什么不用二阶优化？ 目前深度学习中，反向传播主要是依靠一阶梯度。二阶梯度在理论和实际上都是可以应用都网络中的，但相比于一阶梯度，二阶优化会存在以下一些主要问题： （1）计算量大，训练非常慢。 （2）二阶方法能够更快地求得更高精度的解，这在浅层模型是有益的。而在神经网络这类深层模型中对参数的精度要求不高，甚至不高的精度对模型还有益处，能够提高模型的泛化能力。 （3）稳定性。二阶方法能更快求高精度的解，同样对数据本身要的精度也会相应的变高，这就会导致稳定性上的问题。 为什么要设置单一数字评估指标，设置指标的意义？ 在训练模型时，无论是调整超参数，还是调整不同的模型算法，我们都需要一个有效的评价指标，这个评价标准能帮助我们快速了解新的尝试后模型的性能是否更优。例如在分类时，我们通常会选择选择准确率，当样本不平衡时，查准率和查全率又会是更好的评价指标。所以在训练模型时，如果设置了单一数字的评估指标通常能很快的反应出我们模型的改进是否直接产生了收益，从而加速我们的算法改进过程。若在训练过程中，发现优化目标进一步深入，现有指标无法完全反应进一步的目标时，就需要重新选择评估指标了。 训练/验证/测试集的定义及划分 训练、验证、测试集在机器学习领域是非常重要的三个内容。三者共同组成了整个项目的性能的上限和走向。 训练集：用于模型训练的样本集合，样本占用量是最大的； 验证集：用于训练过程中的模型性能评价，跟着性能评价才能更好的调参； 测试集：用于最终模型的一次最终评价，直接反应了模型的性能。 在划分上，可以分两种情况： 1、在样本量有限的情况下，有时候会把验证集和测试集合并。实际中，若划分为三类，那么训练集：验证集：测试集=6:2:2；若是两类，则训练集：验证集=7:3。这里需要主要在数据量不够多的情况，验证集和测试集需要占的数据比例比较多，以充分了解模型的泛化性。 2、在海量样本的情况下，这种情况在目前深度学习中会比较常见。此时由于数据量巨大，我们不需要将过多的数据用于验证和测试集。例如拥有1百万样本时，我们按训练集：验证集：测试集=98:1:1的比例划分，1%的验证和1%的测试集都已经拥有了1万个样本。这已足够验证模型性能了。 此外，三个数据集的划分不是一次就可以的，若调试过程中发现，三者得到的性能评价差异很大时，可以重新划分以确定是数据集划分的问题导致还是由模型本身导致的。其次，若评价指标发生变化，而导致模型性能差异在三者上很大时，同样可重新划分确认排除数据问题，以方便进一步的优化。 什么是TOP5错误率？ 通常对于分类系统而言，系统会对某个未知样本进行所有已知样本的匹配，并给出该未知样本在每个已知类别上的概率。其中最大的概率就是系统系统判定最可能的一个类别。TOP5则就是在前五个最大概率的类别。TOP5错误率，即预测最可能的五类都不是该样本类别的错误率。 TOP5错误率通常会用于在类别数量很多或者细粒度类别的模型系统。典型地，例如著名的ImageNet ，其包含了1000个类别。通常就会采用TOP5错误率。 什么是泛化误差，如何理解方差和偏差？ 一般情况下，我们评价模型性能时都会使用泛化误差。泛化误差越低，模型性能越好。泛化误差可分解为方差、偏差和噪声三部分。这三部分中，噪声是个不可控因素，它的存在是算法一直无法解决的问题，很难约减，所以我们更多考虑的是方差和偏差。 方差和偏差在泛化误差上可做如下分解，假设我们的预测值为g(x)，真实值为f(x)，则均方误差为 \\[ E((g(x)−f(x))2) \\] 这里假设不考虑噪声，g来代表预测值，f代表真实值，g¯=E(g)代表算法的期望预测，则有如下表达： \\[ \\begin{align} E(g-f)^2&amp;=E(g^2-2gf+f^2) \\\\&amp;=E(g^2)-\\bar g^2+(\\bar g-f)^2 \\\\&amp;=E(g^2)-2\\bar g^2+\\bar g^2+(\\bar g-f)^2 \\\\&amp;=E(g^2-2g\\bar g^2+\\bar g^2)+(\\bar g-f)^2 \\\\&amp;=\\underbrace{E(g-\\bar g)^2}_{var(x)}+\\underbrace{(\\bar g-f)^2}_{bias^2(x)} \\end{align} \\] 有上述公式可知，方差描述是理论期望和预测值之间的关系，这里的理论期望通常是指所有适用于模型的各种不同分布类型的数据集；偏差描述为真实值和预测值之间的关系，这里的真实值通常指某一个特定分布的数据集合。 所以综上方差表现为模型在各类分布数据的适应能力，方差越大，说明数据分布越分散，而偏差则表现为在特定分布上的适应能力，偏差越大越偏离真实值。 如何提升模型的稳定性？ 评价模型不仅要从模型的主要指标上的性能，也要注重模型的稳定性。模型的稳定性体现在对不同样本之间的体现的差异。如模型的方差很大，那可以从如下几个方面进行考虑： （1）正则化（L2, L1, dropout）：模型方差大，很可能来自于过拟合。正则化能有效的降低模型的复杂度，增加对更多分布的适应性。 （2）提前停止训练：提前停止是指模型在验证集上取得不错的性能时停止训练。这种方式本质和正则化是一个道理，能减少方差的同时增加的偏差。目的为了平衡训练集和未知数据之间在模型的表现差异。 （3）扩充训练集：正则化通过控制模型复杂度，来增加更多样本的适应性。那增加训练集让模型适应不同类型的数据本身就是一种最简单直接的方式提升模型稳定的方法，也是最可靠的一种方式。 与正则有所不同的是，扩充数据集既可以减小偏差又能减小方差。 （4）特征选择：过高的特征维度会使模型过拟合，减少特征维度和正则一样可能会处理好方差问题，但是同时会增大偏差。但需要注意的是若过度删减特征，很可能会删除很多有用的特征，降低模型的性能。所以需要多注意删减的特征对模型的性能的影响。 有哪些改善模型的思路 改善模型本质是如何优化模型，这本身是个很宽泛的问题。也是目前学界一直探索的目的，而从目前常规的手段上来说，一般可取如下几点。 数据角度 增强数据集。无论是有监督还是无监督学习，数据永远是最重要的驱动力。更多的类型数据对良好的模型能带来更好的稳定性和对未知数据的可预见性。对模型来说，“看到过的总比没看到的更具有判别的信心”。但增大数据并不是盲目的，模型容限能力不高的情况下即使增大数据也对模型毫无意义。而从数据获取的成本角度，对现有数据进行有效的扩充也是个非常有效且实际的方式。良好的数据处理，常见的处理方式如数据缩放、归一化和标准化等。 模型角度 模型的容限能力决定着模型可优化的空间。在数据量充足的前提下，对同类型的模型，增大模型规模来提升容限无疑是最直接和有效的手段。但越大的参数模型优化也会越难，所以需要在合理的范围内对模型进行参数规模的修改。而不同类型的模型，在不同数据上的优化成本都可能不一样，所以在探索模型时需要尽可能挑选优化简单，训练效率更高的模型进行训练。 调参优化角度 如果你知道模型的性能为什么不再提高了，那已经向提升性能跨出了一大步。 超参数调整本身是一个比较大的问题。一般可以包含模型初始化的配置，优化算法的选取、学习率的策略以及如何配置正则和损失函数等等。这里需要提出的是对于同一优化算法，相近参数规模的前提下，不同类型的模型总能表现出不同的性能。这实际上就是模型优化成本。从这个角度的反方向来考虑，同一模型也总能找到一种比较适合的优化算法。所以确定了模型后选择一个适合模型的优化算法也是非常重要的手段。 训练角度 很多时候我们会把优化和训练放一起。但这里我们分开来讲，主要是为了强调充分的训练。在越大规模的数据集或者模型上，诚然一个好的优化算法总能加速收敛。但你在未探索到模型的上限之前，永远不知道训练多久算训练完成。所以在改善模型上充分训练永远是最必要的过程。充分训练的含义不仅仅只是增大训练轮数。有效的学习率衰减和正则同样是充分训练中非常必要的手段。 如何快速构建有效初始模型？ ​ 构建一个有效的初始模型能帮助我们快速了解数据的质量和确定模型构建的方向。构建一个良好的初始模型，一般需要注意如下几点： ​ 1、了解\"对手\"。这里的“对手”通常是指数据，我们在得到数据时，第一步是需要了解数据特点和使用场合。了解数据特点能帮助我们快速定位如何进行建模。确定使用场合能帮助我们进一步确定模型需要优化的方向。数据特点一般需要了解例如数据集规模、训练集和验证集是否匹配、样本的分布是否均匀、数据是否存在缺失值等等。 ​ 2、站在巨人肩膀上。根据数据特点，我们通常能匹配到一个现有比较优秀的模型。这类模型都通常能在类似数据上表现出一个比较不错的性能。 ​ 3、一切从简。初始模型的作用在于迅速了解数据质量和特点，所以模型的性能通常不需要达到很高，模型复杂度也不需要很高。例如，做图像分类时，我们在使用预训练模型时，不需要一开始就使用例如ResNet152这类模型巨大，复杂度过高的模型。这在数据量较小时，很容易造成过拟合而导致出现我们对数据产生一些误导性的判断，此外也增加了额外训练构建时间。所以使用更小更简单的模型以及损失函数来试探数据是相比更明智的选择。 ​ 4、总比瞎猜强。构建模型的意义在于建立一个高效的模型，虽然初始模型我们不对性能做过高的要求。但前提在于必须要比随机猜测好，不然构建模型的意义就不存在了。 ​ 5、解剖模型。一旦确定了一个初始模型时，无论你对该模型多熟悉，当其面对一批新数据时，你永远需要重新去认识这个模型，因为你永远不确定模型内部到底发生了些什么。解剖模型一般需要在训练时注意误差变化、注意训练和验证集的差异；出现一些NAN或者INf等情况时，需要打印观察内部输出，确定问题出现的时间和位置；在完成训练后，需要测试模型的输出是否正确合理，以确认评价指标是否符合该数据场景。无论使用任何一种模型，我们都不能把它当做黑盒去看待。 如何通过模型重新观察数据？ ​ 对于这个问题，与其说如何做，倒不如说这个问题是用来强调这样做的重要性。如何重新观察数据其实不难，而是很多读者，会忽略这一项过程的重要性。 ​ 通过模型重新观察数据，不仅能让我们了解模型情况，也能让我们对数据质量产生进一步的理解。目前深度学习在监督学习领域成就是非常显著的。监督学习需要依赖大量的人为标注，人为标注很难确定是否使用的数据中是否存在错误标注或者漏标注等问题。这无论是哪种情况都会影响我们对模型的判断。所以通过模型重新验证数据质量是非常重要的一步。很多初学者，通常会忽略这一点，而导致出现对模型的一些误判，严重时甚至会影响整个建模方向。此外，对于若出现一些过拟合的情况，我们也可以通过观察来了解模型。例如分类任务，样本严重不平衡时，模型全预测到了一边时，其正确率仍然很高，但显然模型已经出现了问题。 如何解决数据不匹配问题？ 如何定位数据不匹配? ​ 数据不匹配问题是个不容易定位和解决的问题。这个问题出现总会和模型过拟合表现很相似,即在训练集上能体现非常不错的性能,但在测试集上表现总是差强人意但区别在于如果遇到是数据不匹配的问题,通常在用一批和训 练集有看相同或者相似分布的数据上仍然能取得不错的结果。但很多时候,当测试集上结果表现很差时,很多初学 者可能会直接将问题定位在模型过拟合上,最后对模型尝试各种方法后,性能却始终不能得到有效提升。当遇到这 种情况时,建议先定位出是否存在数据不匹配的问题。最简单的验证方式就是可以从训练集中挑选出一部分数据作 为验证集,重新划分后训练和验证模型表现。 举例常见几个数据不匹配的场景? ​ 例如设计款识别物体的app时,实际场景的图片均来自于手机拍摄,而训练集确是来自于网上各类抓取下来的图 片。例如在图像去噪、去模糊、去雾、超分辨率等图像处理场景时,由于大量数据的难以获取,因此都会采用人为 假设合成的图像进行训练,这时候应用到实际场景中也容易出现不匹配的问题 如何解决数据不匹配问题? ​ 数据不匹配是个很难有固定方法来解决的问题。这里提供几条供参考的途径： ​ 1、收集更多符合实际场最需要的数据。这似乎是最简单但也最难方式 ​ 2、对结果做错误分析。找出数据集中出错的数据和正确数据之间的特点和区别,这对你无论是进行后续模型的分析或者是数据的处理提供非常有效的思路。注意,这里的数据集包括训练集和测试集 ​ 3、数据集增强。数据集增强并不意味看数据集越大越好,其目的是丰富数据的分布以适应更多的变化当遇到数 据不匹配时,对数据处理般可以有两种方式。其一,合成或处理更多接近需要的数据特点。其二,对所有数据包 括实际场景数据都进行处理,将所有数据都统一到另一个分布上,统一出一种新的特点。 如何提高深度学习系统的性能 ​ 当我们要试图提高深度学习系统的性能时，目前我们大致可以从三方面考虑： ​ 1、提高模型的结构，比如增加神经网络的层数，或者将简单的神经元单位换成复杂的 LSTM 神经元，比如在自然语言处理领域内，利用 LSTM 模型挖掘语法分析的优势。 ​ 2、改进模型的初始化方式，保证早期梯度具有某些有益的性质，或者具备大量的稀疏性，或者利用线性代数原理的优势。 ​ 3、选择更强大的学习算法，比如对度梯度更新的方式，也可以是采用除以先前梯度 L2 范数来更新所有参数，甚至还可以选用计算代价较大的二阶算法。 参考文献 [1] 冯宇旭, 李裕梅. 深度学习优化器方法及学习率衰减方式综述[J]. 数据挖掘, 2018, 8(4): 186-200.","link":"/posts/1235560676.html"},{"title":"动态规划专题","text":"DP 问题的一般思路 DP 定义 ——有时 DP 的更新很难严格遵循定义，需要额外变量保存全局最优结果 初始化 ——初始值可以通过一个简单的特例来确定 递推公式 + 边界条件 DP 优化 （可选） Reference 常见的动态规划问题分析与求解 - 五岳 - 博客园 什么是动态规划？动态规划的意义是什么？ - 知乎 ## 背包问题 【注】关于“恰好装满” 如果要求恰好装满背包，可以在初始化时将 dp[0] / dp[i][0] 初始化 0，其他初始化为 -INF。这样即可保证最终得到的 dp[N] / dp[N][M] 是一种恰好装满背包的解； 如果不要求恰好装满，则全部初始化为 0 即可。 可以这样理解：初始化的 dp 数组实际上就是在没有任何物品可以放入背包时的合法状态。 如果要求背包恰好装满，那么此时只有容量为 0 的背包可能被价值为 0 的物品“恰好装满”，其它容量的背包均没有合法的解，属于未定义的状态，它们的值就都应该是 -INF 。 如果背包并非必须被装满，那么任何容量的背包都有一个合法解，即“什么都不装”，这个解的价值为0，所以初始时状态的值也全部为 0 。 01 背包 HDOJ - 2602 问题描述 12345678910有 n 个重量个价值分别为 w_i, v_i 的物品。从这些物品中选出总重量不超过 W 的物品，使其总价值最大。示例1 // 用例数5 10 // 物品数 背包容量 N &lt;= 1000 , V &lt;= 10001 2 3 4 55 4 3 2 114 二维 DP（无优化） 定义：dp[i][j] := 从前 i 个物品中选取总重量不超过 j 的物品时总价值的最大值 &gt; i 从 1 开始计，包括第 i 个物品 初始化 1dp[0][j] = 0 状态转移 12345dp[i][j] = dp[i-1][j] if j &lt; w[i] （当前剩余容量不够放下第 i 个物品） = max{ else （取以下两种情况的最大值） dp[i-1][j], // 不拿第 i 个物品 dp[i-1][j-w[i]] + w[j] // 拿第 i 个物品 } 123456789101112131415161718192021222324252627282930313233343536// HDOJ 地址：http://acm.hdu.edu.cn/showproblem.php?pid=2602int solve(int N, int V, vector&lt;int&gt;&amp; v, vector&lt;int&gt;&amp; w) { vector&lt;vector&lt;int&gt; &gt; dp(N + 1, vector&lt;int&gt;(V + 1, 0)); // 不要求装满，初始化为 0 即可 // 核心代码 for (int i = 1; i &lt;= N; i++) { for (int j = 0; j &lt;= V; j++) { // 可能存在重量为 0，但有价值的物品 if (w[i] &gt; j) // 如果当前物品的重量大于剩余容量 dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - w[i]] + v[i]); } } return dp[N][V];}int main() { int T; // 用例数 scanf(&quot;%d&quot;, &amp;T); while (T--) { int N, V; // N: 物品数量；V: 背包容量 scanf(&quot;%d%d&quot;, &amp;N, &amp;V); vector&lt;int&gt; v(N + 1, 0); // 保存每个物品的价值 vector&lt;int&gt; w(N + 1, 0); // 保存每个物品的重量 for (int i = 1; i &lt;= N; i++) scanf(&quot;%d&quot;, &amp;v[i]); for (int i = 1; i &lt;= N; i++) scanf(&quot;%d&quot;, &amp;w[i]); int ans = solve(N, V, v, w); printf(&quot;%d\\n&quot;, ans); } return 0;} 二维 DP（滚动数组） 在上述递推式中，dp[i+1] 的计算实际只用到了 dp[i+1] 和 dp[i]； 因此可以结合奇偶，通过两个数组滚动使用来实现重复利用。 12345678910111213141516171819// HDOJ 地址：http://acm.hdu.edu.cn/showproblem.php?pid=2602int solve(int N, int V, vector&lt;int&gt;&amp; v, vector&lt;int&gt;&amp; w) { //vector&lt;vector&lt;int&gt; &gt; dp(N + 1, vector&lt;int&gt;(V + 1, 0)); // 不要求装满，初始化为 0 即可 vector&lt;vector&lt;int&gt; &gt; dp(2, vector&lt;int&gt;(V + 1, 0)); // N+1 -&gt; 2 // 核心代码 for (int i = 1; i &lt;= N; i++) { for (int j = 0; j &lt;= V; j++) { // 可能存在重量为 0，但有价值的物品 if (w[i] &gt; j) // 如果当前物品的重量大于剩余容量 dp[i &amp; 1][j] = dp[(i - 1) &amp; 1][j]; else dp[i &amp; 1][j] = max(dp[(i - 1) &amp; 1][j], dp[(i - 1) &amp; 1][j - w[i]] + v[i]); } } return dp[N &amp; 1][V]; // 这里别忘了 N &amp; 1}// main 函数略 一维 DP 定义：dp[j] := 重量不超过 j 公斤的最大价值 递推公式 1dp[j] = max{dp[j], dp[j-w[i]] + v[i]} 若 j &gt; w[i] 1234567891011121314151617// HDOJ 地址：http://acm.hdu.edu.cn/showproblem.php?pid=2602// 一维 DP（滚动数组）int solve(int N, int V, vector&lt;int&gt;&amp; v, vector&lt;int&gt;&amp; w) { vector&lt;int&gt; dp(V + 1, 0); // 核心代码 for (int i = 1; i &lt;= N; i++) { for (int j = V; j &gt;= w[i]; j--) { // 递推方向发生了改变 dp[j] = max(dp[j], dp[j - w[i]] + v[i]); } } return dp[V];}// main 函数略 完全背包 NYOJ - 311 问题描述 123401 背包中每个物品只有一个，所以只存在选或不选；完全背包中每个物品可以选取任意件。注意：本题要求是背包恰好装满背包时，求出最大价值总和是多少。如果不能恰好装满背包，输出 NO 二维 DP（无优化） 直观思路：在 01 背包的基础上在加一层循环 递推关系： 12dp[0][j] = 0dp[i][j] = max{dp[i - 1][j - k * w[i]] + k * v[i] | 0 &lt;= k} 123456for (int i = 1; i &lt;= N; i++) { for (int j = 0; j &lt;= V; j++) { // 可能存在重量为 0 的物品 for (int k = 0; k * w[i] &lt;= j; k++) dp[i][j] = max(dp[i][j], dp[i-1][j - k*w[i]] + k*v[i]); }} 关于 k 的循环最坏可能从 0 到 V，因此时间复杂度为 O(N*V^2) 注意到： 123456789dp[i][j] = max{dp[i - 1][j - k*w[i]] + k*v[i] | 0 &lt;= k} ------ = max{dp[i - 1][j], max{dp[i - 1][j - k*w[i]] + k*v[i]} | 1 &lt;= k} ------ = max{dp[i - 1][j], max{dp[i - 1][(j-w[i]) - k*w[i]] + k*v[i] | 0 &lt;= k} + v[i]} -------- ------ ------ --------------------------------------------------- = max{dp[i - 1][j], dp[i][j - w[i]] + v[i]} --------------- 12345678910for (int i = 1; i &lt;= N; i++) { for (int j = 0; j &lt;= V; j++) { if (w[i] &gt; j) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i][j - w[i]] + v[i]); // dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - w[i]] + v[i]); // 对比 01 背包 // ---------（唯一区别） }} 完整代码 注意，这里要求的是恰好装满时的情况，所以需要将 dp[i][0] 全部初始化为 0，其他初始化为 -INF &gt; 以下代码因超内存无法通过 NYOJ 311； &gt; &gt; 可以 AC 的代码，请参考 完全背包（一维 DP） 和 完全背包（滚动数组） 123456789101112131415161718192021222324252627282930313233343536373839404142// NYOJ 311 会报超内存，所以无法测试#include &lt;cstdio&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;const int inf = 0x80000000;void solve() { int T; scanf(&quot;%d&quot;, &amp;T); while (T--) { int N, V; // N 表示物品种类的数目，V 表示背包的总容量 scanf(&quot;%d%d&quot;, &amp;N, &amp;V); vector&lt;int&gt; w(N + 1), v(N + 1); // w 表示重量，v 表示价值 for (int i = 1; i &lt;= N; i++) scanf(&quot;%d%d&quot;, &amp;w[i], &amp;v[i]); vector&lt;vector&lt;int&gt; &gt; dp(N + 1, vector&lt;int&gt;(V + 1, inf)); for (int i = 0; i &lt;= N; i++) dp[i][0] = 0; for (int i = 1; i &lt;= N; i++) { for (int j = 0; j &lt;= V; j++) { if (j &lt; w[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i][j - w[i]] + v[i]); } } if (dp[N][V] &gt; 0) printf(&quot;%d\\n&quot;, dp[N][V]); else puts(&quot;NO&quot;); }}int main() { solve(); return 0;} 二维 DP（滚动数组） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// NYOJ 311-完全背包: http://nyoj.top/problem/311 （未通过测试，报运行时错误）#include &lt;cstdio&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;void solve3() { const int MAX_V = 50000 + 10; const int inf = 0x3f3f3f3f; int T; scanf(&quot;%d&quot;, &amp;T); while (T--) { int N, V; // M 表示物品种类的数目，V 表示背包的总容量 scanf(&quot;%d%d&quot;, &amp;N, &amp;V); //vector&lt;int&gt; w(N + 1), v(N + 1); // w 表示重量，v 表示价值 //for (int i = 1; i &lt;= N; i++) // scanf(&quot;%d%d&quot;, &amp;w[i], &amp;v[i]); //vector&lt;vector&lt;int&gt; &gt; dp(2, vector&lt;int&gt;(V + 1, -inf)); int dp[2][MAX_V]; for (int i = 0; i &lt; 2; i++) { fill(dp[i], dp[i] + MAX_V, -inf); dp[i][0] = 0; } for (int i = 1; i &lt;= N; i++) { int w, v; scanf(&quot;%d%d&quot;, &amp;w, &amp;v); for (int j = 0; j &lt;= V; j++) { if (j &lt; w) dp[i &amp; 1][j] = dp[(i - 1) &amp; 1][j]; else dp[i &amp; 1][j] = max(dp[(i - 1) &amp; 1][j], dp[i &amp; 1][j - w] + v); } } if (dp[N][V] &gt; 0) printf(&quot;%d\\n&quot;, dp[N &amp; 1][V]); else puts(&quot;NO&quot;); }}int main() { solve3(); return 0;} 一维 DP 核心代码与 01 背包一致，只有第二层循环的递推方向不同 完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344// NYOJ 311-完全背包: http://nyoj.top/problem/311#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;const int MAX_V = 50000 + 10;const int inf = 0x80000000;void solve2() { int T; scanf(&quot;%d&quot;, &amp;T); while (T--) { int N, V; // M 表示物品种类的数目，V 表示背包的总容量 scanf(&quot;%d%d&quot;, &amp;N, &amp;V); //vector&lt;int&gt; w(N + 1), v(N + 1); // w 表示重量，v 表示价值 //for (int i = 1; i &lt;= N; i++) // scanf(&quot;%d%d&quot;, &amp;w[i], &amp;v[i]); //vector&lt;int&gt; dp(V + 1, inf); // 注意 NYOJ 的系统开辟稍大的 vector 就会导致超时 int dp[MAX_V]; fill(dp, dp + MAX_V, inf); dp[0] = 0; for (int i = 1; i &lt;= N; i++) { int w, v; scanf(&quot;%d%d&quot;, &amp;w, &amp;v); // 避免开辟新的内存 for (int j = w; j &lt;= V; j++) { dp[j] = max(dp[j], dp[j - w] + v); } } if (dp[V] &gt; 0) printf(&quot;%d\\n&quot;, dp[V]); else puts(&quot;NO&quot;); }}int main() { solve2(); return 0;} 多重背包 TODO 硬币问题 硬币找零 LeetCode - 322. 零钱兑换 问题描述 1234567891011121314151617给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。示例 1: 输入: coins = [1, 2, 5], amount = 11 输出: 3 解释: 11 = 5 + 5 + 1示例 2: 输入: coins = [2], amount = 3 输出: -1 说明: 你可以认为每种硬币的数量是无限的。 思路 - 定义：dp[i] := 组成总金额 i 时的最少硬币数 - 初始化： 12dp[i] = 0 若 i=0 = INF 其他 - 状态转移 123dp[j] = min{ dp[j-coins[i]] + 1 | i=0,..,n-1 } 其中 coins[i] 表示硬币的币值，共 n 种硬币 C++ 1234567891011121314151617class Solution {public: int coinChange(vector&lt;int&gt;&amp; coins, int n) { int INF = n + 1; vector&lt;int&gt; dp(n+1, INF); dp[0] = 0; for (auto c: coins) { for (int i=c; i&lt;=n; i++) { // i &gt;= c dp[i] = min(dp[i], dp[i-c] + 1); } } return dp[n] &lt; INF ? dp[n] : -1; }}; 硬币组合 LeetCode - 518. 零钱兑换 II C++ 1234567891011121314151617class Solution {public: int change(int n, vector&lt;int&gt;&amp; coins) { int m = coins.size(); vector&lt;int&gt; dp(n+1, 0); dp[0] = 1; for (auto c: coins) { for (int i = c; i &lt;= n; i++) { dp[i] += dp[i - c]; } } return dp[n]; }}; 最长公共子序列（LCS） 最长公共子序列_牛客网 - 求两个序列的最长公共字序列 - 示例：s1: \"BDCABA\" 与 s2：\"ABCBDAB\" 的一个最长公共字序列为 \"BCBA\" - 最长公共子序列不唯一，但是它们的长度是一致的 - 子序列不要求连续 思路 - DP 定义 - 记 s[0:i] := s 长度为 i 的**前缀** - 定义 dp[i][j] := s1[0:i] 和 s2[0:j] 最长公共子序列的长度 - DP 初始化 1dp[i][j] = 0 当 i=0 或 j=0 时 - DP 更新 - 当 s1[i] == s2[j] 时 1dp[i][j] = dp[i-1][j-1] + 1 - 当 s1[i] != s2[j] 时 1dp[i][j] = max(dp[i-1][j], dp[i][j-1]) - 完整递推公式 123dp[i][j] = 0 当 i=0 或 j=0 时 = dp[i-1][j-1] + 1 当 `s1[i-1] == s2[j-1]` 时 = max(dp[i-1][j], dp[i][j-1]) 当 `s1[i-1] != s2[j-1]` 时 - Code - C++ 12345678910111213141516class LCS {public: int findLCS(string A, int n, string B, int m) { vector&lt;vector&lt;int&gt; &gt; dp(n+1, vector&lt;int&gt;(m+1, 0)); // 已经初始化为全 0，就不必再手动初始化 DP 了 for (int i=1; i&lt;=n; i++) for (int j=1; j&lt;=m; j++) if (A[i-1] == B[j-1]) // 注意下标问题 dp[i][j] = dp[i-1][j-1] + 1; else dp[i][j] = max(dp[i][j-1], dp[i-1][j]); return dp[n][m]; }}; 最长公共子串 最长公共子串_牛客网 题目描述 12对于两个字符串，请设计一个时间复杂度为`O(m*n)`的算法，求出两串的最长公共子串的长度。（这里的 m 和 n 为两串的长度） 思路 - 暴力求解 Longest common substring problem - Wikipedia 暴力求解思路：每当找到一对元素相同时就斜向比较 123456789101112131415161718192021class LongestSubstring {public: int findLongest(string A, int n, string B, int m) { int ret = 0; for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; m; j++) { int tmp_ret = 0; if (A[i] == B[j]) { // 每当找到一对元素相同 tmp_ret += 1; // 斜向比较 int tmp_i = i + 1; int tmp_j = j + 1; while (tmp_i &lt; n &amp;&amp; tmp_j &lt; m &amp;&amp; A[tmp_i++] == B[tmp_j++]) // 注意边界 tmp_ret++; } ret = max(ret, tmp_ret); // 记录最大 } } return ret; }}; - 注意：如果两个串完全相同的话，时间复杂度将退化为 O(N^3) 思路 - DP - DP 定义 - 记 s[0:i] := s 长度为 i 的**前缀** - 定义 dp[i][j] := s1[0:i] 和 s2[0:j] 最长公共子串的长度 - dp[i][j] 只有当 s1[i] == s2[j] 的情况下才是 s1[0:i] 和 s2[0:j] 最长公共子串的长度 - DP 初始化 1dp[i][j] = 0 当 i=0 或 j=0 时 - DP 更新 12dp[i][j] = dp[i-1][j-1] + 1 if s[i] == s[j] = ; else pass - Code 12345678910111213141516171819class LongestSubstring {public: int findLongest(string A, int n, string B, int m) { vector&lt;vector&lt;int&gt; &gt; dp(n + 1, vector&lt;int&gt;(m + 1, 0)); // 已经初始化为全 0，就不必再手动初始化 DP 了 int ret = 0; for (int i = 1; i &lt;= n; i++) for (int j = 1; j &lt;= m; j++) if (A[i - 1] == B[j - 1]) { dp[i][j] = dp[i - 1][j - 1] + 1; ret = max(ret, dp[i][j]); // 相比最长公共子序列，增加了这行 } else ; // 去掉了这行 return ret; }}; - DP 优化：空间复杂度 O(N) - 好不容易找到的优化为 O(N) 的代码；多数优化直接优化到了 O(1) - 因为内层循环是逆序的，所以有点不好理解，可以画一个矩阵手推 DP 的更新过程，很巧妙 12345678910111213141516171819202122232425262728293031class LongestSubstring {public: int findLongest(string A, int n, string B, int m) { if (n &lt; m) { swap(n, m); swap(A, B); } vector&lt;int&gt; dp(m, 0); int ret = 0; for (int i = 0; i &lt; n; i++) { for (int j = m - 1; j &gt;= 0; j--) { if (A[i] != B[j]) { dp[j] = 0; } else { if (i != 0) { dp[j] = dp[j - 1] + 1; } else { dp[j] = 1; } } ret = max(ret, dp[j]); } } return ret; }}; - DP 优化：空间复杂度 O(1) - 两个字符串的比较总是按一行一行或一列一列来比较，因此至少要保存一行的数据 - 而如果是按照斜向遍历，其实只要保存一个数据即可 斜向遍历的策略很多，下面的代码是从右上角（row=0, col=m-1）开始遍历 123456789101112131415161718192021222324252627class LongestSubstring {public: int findLongest(string A, int n, string B, int m) { int ret = 0; for (int row = 0, col = m - 1; row &lt; n;) { int i = row; int j = col; int dp = 0; while (i &lt; n &amp;&amp; j &lt; m) { if (A[i++] == B[j++]) // 注意：无论走哪个分支，i 和 j 都会 ++ 一次 dp += 1; else dp = 0; ret = max(ret, dp); } if (col &gt; 0) col--; else row++; } return ret; }}; 上述代码其实就是把下面的两段循环合并了 123456789101112131415161718192021222324252627282930313233class LongestSubstring {public: int findLongest(string A, int n, string B, int m) { int ret = 0; int dp; for (int col = m-1; col &gt;= 0; col--) { dp = 0; for (int i = 0, j = col; i &lt; n &amp;&amp; j &lt; m; i++, j++) { if (A[i] == B[j]) dp += 1; else dp = 0; ret = max(ret, dp); } } for (int row = 0; row &lt; n; row++) { dp = 0; for (int i = row, j = 0; i &lt; n &amp;&amp; j &lt; m; i++, j++) { if (A[i] == B[j]) dp += 1; else dp = 0; ret = max(ret, dp); } } return ret; }}; 最长递增子序列（LIS） 最长递增子序列_牛客网 最长上升子序列 - LeetCode &gt; 牛客假设给定的数组中不存在重复元素，LeetCode 可能存在重复元素 问题描述 12345678对于一个数字序列，请设计一个复杂度为O(nlogn)的算法，返回该序列的最长上升子序列的长度测试样例： [2,1,4,3,1,5,6],7返回： 4说明： [1,3,5,6] 是其中一个最长递增子序列 思路0 - O(N^2) - LIS 可以转化成 LCS (最长公共子序列) 问题 - 用另一个序列保存给定序列的排序结果 - O(NlogN) - 则问题转化为求这两个序列的 LCS 问题 - O(N^2) 思路1 - O(N^2)解法 - DP 定义 - 记 nums[0:i] := 序列 nums 的前 i 个元素构成的子序列 - 定义 dp[i] := nums[0:i] 中 LIS 的长度 - 实际并没有严格按照这个定义，中间使用一个变量记录当前全局最长的 LIS - DP 初始化 1dp[:] = 1 // 最长上升子序列的长度最短为 1 - DP 更新 - O(N^2)的解法 123dp[i] = max{dp[j]} + 1, if nums[i] &gt; nums[j] = max{dp[j]}, elsewhere 0 &lt;= j &lt; i 如果只看这个递推公式，很可能会写出如下的错误代码 错误代码（点击展开） 1234567891011121314151617// 牛客网class AscentSequence {public: int findLongest(vector&lt;int&gt; nums, int n) { vector&lt;int&gt; dp(n, 1); for (int i = 1; i &lt; n; i++) { for (int j = 0; j &lt; i; j++) if (nums[i] &gt; nums[j]) dp[i] = max(dp[i], dp[j] + 1); else dp[i] = max(dp[i], dp[j]); } return dp[n-1]; }}; - 这段代码的问题在于 dp[i] 应该等于 max{dp[j]} 对应的那个 dp[j]+1，且只增加一次 - 这么写可能会导致 dp[i] 被增加多次 &gt; 动态规划求解最长递增子序列的长度 - hapjin - 博客园 下面是网上比较流行的一种递推公式 123dp[i] = dp[j] + 1, if nums[i] &gt; nums[j] &amp;&amp; dp[i] &lt; dp[j] + 1 = pass, elsewhere 0 &lt;= j &lt; i 注意：此时并没有严格按照定义处理 dp，它只记录了当 nums[i] &gt; nums[j] &amp;&amp; dp[i] &lt; dp[j] + 1 时的 LIS；不满足该条件的情况跳过了；所以需要额外一个变量记录当前已知全局的 LIS Code 123456789101112131415161718// 牛客网class AscentSequence {public: int findLongest(vector&lt;int&gt; nums, int n) { vector&lt;int&gt; dp(n, 1); int ret = 1; for (int i = 1; i &lt; n; i++) { for (int j = 0; j &lt; i; j++) if (nums[i] &gt; nums[j] &amp;&amp; dp[i] &lt; dp[j] + 1) dp[i] = dp[j] + 1; ret = max(ret, dp[i]); } return ret; }}; 思路2 - O(NlogN) - 该解法的思想是：长度为 i 的 LIS 的尾元素应该大于长度为 i-1 的尾元素 - DP 定义 - 定义 dp[i] := 长度为 i 的 LIS 的最小尾元素 - DP 更新 - 二分查找 nums[j] 在 dp 中的 upper_bound 位置 lower_bound 位置 - upper_bound 位置指的是序列中第一个大于 nums[j] 的元素所在的位置 - lower_bound 位置指的是序列中第一个大于等于 nums[j] 的元素所在的位置 - C++ 中分别实现了 upper_bound 和 lower_bound，定义在 &lt;algorithm&gt; 中 - 如果在末尾，则插入；反之则替换 - upper_bound 只能用于不存在重复元素的情况；而 lower_bound 可以兼容两种情况 Code 12345678910111213141516171819202122232425262728293031323334353637383940// 牛客网class AscentSequence {public: int findLongest(const vector&lt;int&gt;&amp; nums, int n) { vector&lt;int&gt; dp; for (int j = 0; j &lt; n; j++) { // 这里用 upper_bound 也可以 auto it = lower_bound(dp.begin(), dp.end(), nums[j]); if (it == dp.end()) dp.push_back(nums[j]); else *it = nums[j]; } return dp.size(); }};// LeetCodeclass Solution {public: int lengthOfLIS(vector&lt;int&gt;&amp; nums) { int n = nums.size(); vector&lt;int&gt; dp; for (int j = 0; j &lt; n; j++) { // 这里只能使用 lower_bound auto it_l = lower_bound(dp.begin(), dp.end(), nums[j]); // auto it_u = upper_bound(dp.begin(), dp.end(), nums[j]); if (it_l == dp.end()) dp.push_back(nums[j]); else *it_l = nums[j]; } return dp.size(); }}; 最长回文子序列 最长回文子序列 - LeetCode 问题描述 12345678给定一个字符串s，找到其中最长的回文子序列。可以假设s的最大长度为1000。示例 1: 输入: &quot;bbbab&quot; 输出: 4 一个可能的最长回文子序列为 &quot;bbbb&quot;。 思路 - 相比最长回文子串，最长回文子序列更像最长公共子序列，只是改变了循环方向 - DP 定义 - 记 s[i:j] := 字符串 s 在区间 [i:j] 上的子串 - 定义 dp[i][j] := s[i:j] 上回文序列的长度 - DP 初始化 1dp[i][i] = 1 // 单个字符也是一个回文序列 - DP 更新 1234567dp[i][j] = dp[i+1][j-1] + 2, if s[i] == s[j] = max(dp[i+1][j], dp[i][j-1]), else比较一下 LCS 的递推公式dp[i][j] = 0 当 i=0 或 j=0 时 = dp[i-1][j-1] + 1 当 `s1[i-1] == s2[j-1]` 时 = max(dp[i-1][j], dp[i][j-1]) 当 `s1[i-1] != s2[j-1]` 时 - Code 123456789101112131415161718192021class Solution {public: int longestPalindromeSubseq(string s) { int n = s.length(); vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(n, 0)); for (int i = 0; i &lt; n; i++) dp[i][i] = 1; for (int j = 1; j &lt; n; j++) // 子串结束位置 for (int i = j-1; i &gt;=0; i--) { // 子串开始位置 if (s[i] == s[j]) dp[i][j] = dp[i + 1][j - 1] + 2; else dp[i][j] = max(dp[i + 1][j], dp[i][j - 1]); } return dp[0][n - 1]; }}; 最长回文子串 最长回文子串_牛客网 最长回文子串 - LeetCode &gt; 牛客网只需要输出长度；LeetCode 还需要输出一个具体的回文串 问题描述 123456给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。示例 1： 输入: &quot;babad&quot; 输出: &quot;bab&quot; 注意: &quot;aba&quot;也是一个有效答案。 思路 - O(N^2) - DP 定义 - 记 s[i:j] := 字符串 s 在区间 [i:j] 上的子串 - 定义 dp[i][j] := s[i:j] 是否是一个回文串 - DP 初始化 1dp[i][i] = 1 // 单个字符也是一个回文串 - DP 更新 12345678910dp[i][j] = dp[i+1][j-1], if s[i] == s[j] = 0, else注意到：如果 j - i &lt; 2 的话（比如 j=2, i=1），dp[i+1][j-1]=dp[2][1] 会出现不符合 DP 定义的情况所以需要添加边界条件 dp[i][i+1] = 1, if s[i] == s[i+1] = 0, else 该边界条件可以放在初始化部分完成；但是建议放在递推过程中完成过更好（为了兼容牛客和LeetCode） - Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 牛客网 ACclass Palindrome {public: int getLongestPalindrome(const string&amp; s, int n) { vector&lt;vector&lt;int&gt; &gt; dp(n, vector&lt;int&gt;(n, 0)); // 初始化 for (int i=0; i&lt;n-1; i++) dp[i][i] = 1; int len = 1; for (int j=1; j&lt;n; j++) { // 子串结束位置 for (int i=j-1; i&gt;=0; i--) { // 子串开始位置 if (j-i &lt; 2) dp[i][j] = (s[i]==s[j]) ? 1 : 0; else if (s[i]==s[j]) dp[i][j] = dp[i+1][j-1]; else dp[i][j] = 0; // 因为 dp 全局初始化就是 0，这里其实可以不写 if (dp[i][j] &amp;&amp; j-i+1 &gt; len) len = j-i+1; } } return len; }};// LeetCode - 只要添加一个记录开始位置的变量即可class Solution {public: string longestPalindrome(string s) { int n = s.length(); vector&lt;vector&lt;int&gt; &gt; dp(n, vector&lt;int&gt;(n, 0)); // 初始化 for (int i=0; i&lt;n-1; i++) dp[i][i] = 1; int len = 1; int beg = 0; // 记录开始位置 for (int j=1; j&lt;n; j++) { // 子串结束位置 for (int i=j-1; i&gt;=0; i--) { // 子串开始位置 if (j-i &lt; 2) dp[i][j] = (s[i]==s[j]) ? 1 : 0; else if (s[i]==s[j]) dp[i][j] = dp[i+1][j-1]; else dp[i][j] = 0; // 因为 dp 全局初始化就是 0，这里其实可以不写 if (dp[i][j] &amp;&amp; j-i+1 &gt; len) { beg = i; // 保存开始位置 len = j-i+1; } } } return s.substr(beg, len); // 截取子串 }}; Manacher 算法 - O(N) &gt; 算法-最长回文子串(Manacher算法) - 琼珶和予 - 博客园 最大连续子序列和 最大连续子序列_牛客网 &gt; 牛客网要求同时输出最大子序列的首尾元素 思路 - 基本问题：只输出最大连续子序列和 - DP 定义 - 记 a[0:i] := 序列 a 在区间 [0:i] 上的子序列 - 定义 dp[i] := a[0:i] 上的最大子序列和 - 实际并没有严格按照上面的定义，中间使用一个变量记录当前全局的最大连续子序列和 - DP 初始化 1dp[0] = a[0] - DP 更新 123456// 只要 dp[i] &gt; 0 就一直累加下去，一旦小于 0 就重新开始dp[i] = dp[i-1] + a[i], if dp[i-1] &gt; 0 = a[i], elseret = max{ret, dp[i]} // 只要大于 0 就累加会导致 dp[i] 保存的并不是 a[0:i] 中的最大连续子序列和 // 所以需要一个变量保存当前全局的最大连续子序列和 直观实现-无优化-空间复杂度O(N)（点击展开） 123456789101112131415161718192021222324252627282930313233343536void foo() { int n; while (cin &gt;&gt; n) { vector&lt;int&gt; a(n); for (int i = 0; i&lt;n; i++) cin &gt;&gt; a[i]; vector&lt;int&gt; dp(n); dp[0] = a[0]; int ret = a[0]; for (int i = 1; i &lt; n; i++) { if (dp[i - 1] &gt; 0) dp[i] = dp[i - 1] + a[i]; else dp[i] = a[i]; ret = max(ret, dp[i]); } cout &lt;&lt; ret &lt;&lt; endl; }}/*输入51 5 -3 2 461 -2 3 4 -10 64-3 -1 -2 -5输出97-1*/ DP 优化 注意到每次递归实际只用到了 dp[i-1]，实际只要用到一个变量，空间复杂度 O(1) 1234567891011121314151617181920212223242526void foo2() { int n; while (cin &gt;&gt; n) { vector&lt;int&gt; a(n); for (int i = 0; i&lt;n; i++) cin &gt;&gt; a[i]; int ret = INT_MIN; int max_cur = 0; for (int i = 0; i &lt; n; i++) { if (max_cur &gt; 0) // 如果大于 0 就一直累加 max_cur += a[i]; else // 一旦小于 0 就重新开始 max_cur = a[i]; if (max_cur &gt; ret) // 保存找到的最大结果 ret = max_cur; // 以上可以简写成下面两行代码 //max_cur = max(max_cur + a[i], a[i]); //ret = max(ret, max_cur); } cout &lt;&lt; ret &lt;&lt; endl; }} 思路 - 输出区间/首尾 - 增加两个变量即可 - 注意：题目要求，如果序列中全是负数，则输出 0，以及整个序列的首尾元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 牛客网 AC#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;vector&gt;#include &lt;climits&gt;using namespace std;void foo3() { int n; while (cin &gt;&gt; n &amp;&amp; n) { vector&lt;int&gt; a(n); for (int i = 0; i&lt;n; i++) cin &gt;&gt; a[i]; int ret = INT_MIN; int max_cur = 0; int beg = a[0], end = a[n-1]; // 输出首尾 // int beg = 0, end = n-1; // 输出区间 int tmp_beg; // 保存临时 beg for (int i = 0; i &lt; n; i++) { if (max_cur &gt; 0) { max_cur += a[i]; } else { max_cur = a[i]; tmp_beg = a[i]; // tmp_beg = i; } if (max_cur &gt; ret) { // &gt; 表明保存的是第一次出现的最大和，&gt;= 则为最后一次（未验证） ret = max_cur; beg = tmp_beg; end = a[i]; // 输出首尾 // end = i; // 输出区间 } } if (ret &lt; 0) printf(&quot;%d %d %d\\n&quot;, 0, a[0], a[n-1]); // printf(&quot;%d %d %d\\n&quot;, 0, 0, n-1); else printf(&quot;%d %d %d\\n&quot;, ret, beg, end); }}int main() { foo3(); return 0;} 编辑距离 LeetCode-编辑距离 问题描述 1234567891011121314给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数。你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符示例: 输入: word1 = &quot;horse&quot;, word2 = &quot;ros&quot; 输出: 3 解释: horse -&gt; rorse (将 'h' 替换为 'r') rorse -&gt; rose (删除 'r') rose -&gt; ros (删除 'e') - 注意：编辑距离指的是将 word1 转换成 word2 思路 - 用一个 dp 数组维护两个字符串的前缀编辑距离 - DP 定义 - 记 word[0:i] := word 长度为 i 的**前缀子串** - 定义 dp[i][j] := 将 word1[0:i] 转换为 word2[0:j] 的操作数 - 初始化 12dp[i][0] = i // 每次从 word1 删除一个字符dp[0][j] = j // 每次向 word1 插入一个字符 - 递推公式 - word1[i] == word1[j] 时 1dp[i][j] = dp[i-1][j-1] - word1[i] != word1[j] 时，有三种更新方式，取最小 1234// word[1:i] 表示 word 长度为 i 的前缀子串dp[i][j] = min({ dp[i-1][j] + 1 , // 将 word1[1:i-1] 转换为 word2[1:j] 的操作数 + 删除 word1[i] 的操作数(1) dp[i][j-1] + 1 , // 将 word1[0:i] 转换为 word2[0:j-1] 的操作数 + 将 word2[j] 插入到 word1[0:i] 之后的操作数(1) dp[i-1][j-1] + 1 }) // 将 word1[0:i-1] 转换为 word2[0:j-1] 的操作数 + 将 word1[i] 替换为 word2[j] 的操作数(1)","link":"/posts/2819424305.html"},{"title":"卷积神经网络（CNN）","text":"​ 卷积神经网络是一种用来处理局部和整体相关性的计算网络结构，被应用在图像识别、自然语言处理甚至是语音识别领域，因为图像数据具有显著的局部与整体关系，其在图像识别领域的应用获得了巨大的成功。 ## 卷积神经网络的组成层 ​ 以图像分类任务为例，在表5.1所示卷积神经网络中，一般包含5种类型的网络层次结构： ​ 表5.1 卷积神经网络的组成 CNN层次结构 输出尺寸 作用 输入层 \\(W_1\\times H_1\\times 3\\) 卷积网络的原始输入，可以是原始或预处理后的像素矩阵 卷积层 \\(W_1\\times H_1\\times K\\) 参数共享、局部连接，利用平移不变性从全局特征图提取局部特征 激活层 \\(W_1\\times H_1\\times K\\) 将卷积层的输出结果进行非线性映射 池化层 \\(W_2\\times H_2\\times K\\) 进一步筛选特征，可以有效减少后续网络层次所需的参数量 全连接层 \\((W_2 \\cdot H_2 \\cdot K)\\times C\\) 将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值） \\(W_1\\times H_1\\times 3\\)对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;\\(K\\)表示卷积层中卷积核（滤波器）的个数;\\(W_2\\times H_2\\) 为池化后特征图的尺度，在全局池化中尺度对应\\(1\\times 1\\);\\((W_2 \\cdot H_2 \\cdot K)\\)是将多维特征压缩到1维之后的大小，\\(C\\)对应的则是图像类别个数。 输入层 ​ 输入层(Input Layer)通常是输入卷积神经网络的原始数据或经过预处理的数据，可以是图像识别领域中原始三维的多彩图像，也可以是音频识别领域中经过傅利叶变换的二维波形数据，甚至是自然语言处理中一维表示的句子向量。以图像分类任务为例，输入层输入的图像一般包含RGB三个通道，是一个由长宽分别为\\(H\\)和\\(W\\)组成的3维像素值矩阵\\(H\\times W \\times 3\\)，卷积网络会将输入层的数据传递到一系列卷积、池化等操作进行特征提取和转化，最终由全连接层对特征进行汇总和结果输出。根据计算能力、存储大小和模型结构的不同，卷积神经网络每次可以批量处理的图像个数不尽相同，若指定输入层接收到的图像个数为\\(N\\)，则输入层的输出数据为\\(N\\times H\\times W\\times 3\\)。 卷积层 ​ 卷积层(Convolution Layer)通常用作对输入层输入数据进行特征提取，通过卷积核矩阵对原始数据中隐含关联性的一种抽象。卷积操作原理上其实是对两张像素矩阵进行点乘求和的数学操作，其中一个矩阵为输入的数据矩阵，另一个矩阵则为卷积核（滤波器或特征矩阵），求得的结果表示为原始图像中提取的特定局部特征。图5.1表示卷积操作过程中的不同填充策略，上半部分采用零填充，下半部分采用有效卷积（舍弃不能完整运算的边缘部分）。 ​ ! ​ 图5.1 卷积操作示意图 激活层 ​ 激活层(Activation Layer)负责对卷积层抽取的特征进行激活，由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。激活层主要由激活函数组成，即在卷积层输出结果的基础上嵌套一个非线性函数，让输出的特征图具有非线性关系。卷积网络中通常采用ReLU来充当激活函数（还包括tanh和sigmoid等）ReLU的函数形式如公式（5-1）所示，能够限制小于0的值为0,同时大于等于0的值保持不变。 \\[ f(x)=\\begin{cases} 0 &amp;\\text{if } x&lt;0 \\\\ x &amp;\\text{if } x\\ge 0 \\end{cases} \\tag{5-1} \\] 池化层 ​ 池化层又称为降采样层(Downsampling Layer)，作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低输出特征尺度，进而减少模型所需要的参数量。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受域内最大、平均与总和的特征值作为输出，最常用的是最大池化。 全连接层 ​ 全连接层(Full Connected Layer)负责对卷积神经网络学习提取到的特征进行汇总，将多维的特征输入映射为二维的特征输出，高维表示样本批次，低位常常对应任务目标。 卷积在图像中有什么直观作用 ​ 在卷积神经网络中，卷积常用来提取图像的特征，但不同层次的卷积操作提取到的特征类型是不相同的，特征类型粗分如表5.2所示。 ​ 表5.2 卷积提取的特征类型 卷积层次 特征类型 浅层卷积 边缘特征 中层卷积 局部特征 深层卷积 全局特征 图像与不同卷积核的卷积可以用来执行边缘检测、锐化和模糊等操作。表5.3显示了应用不同类型的卷积核（滤波器）后的各种卷积图像。 ​ 表5.3 一些常见卷积核的作用 卷积作用 卷积核 卷积后图像 输出原图 \\(\\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\) 边缘检测（突出边缘差异） \\(\\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}\\) 边缘检测（突出中间值） \\(\\begin{bmatrix} -1 &amp; -1 &amp; -1 \\\\ -1 &amp; 8 &amp; -1 \\\\ -1 &amp; -1 &amp; -1 \\end{bmatrix}\\) 图像锐化 \\(\\begin{bmatrix} 0 &amp; -1 &amp; 0 \\\\ -1 &amp; 5 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}\\) 方块模糊 \\(\\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} \\times \\frac{1}{9}\\) 高斯模糊 \\(\\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 4 &amp; 2 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} \\times \\frac{1}{16}\\) 卷积层有哪些基本参数？ ​ 卷积层中需要用到卷积核（滤波器或特征检测器）与图像特征矩阵进行点乘运算，利用卷积核与对应的特征感受域进行划窗式运算时，需要设定卷积核对应的大小、步长、个数以及填充的方式，如表5.4所示。 ​ 表5.4 卷积层的基本参数 参数名 作用 常见设置 卷积核大小 (Kernel Size) 卷积核的大小定义了卷积的感受野 在过去常设为5，如LeNet-5；现在多设为3，通过堆叠\\(3\\times3\\)的卷积核来达到更大的感受域 卷积核步长 (Stride) 定义了卷积核在卷积过程中的步长 常见设置为1，表示滑窗距离为1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样 填充方式 (Padding) 在卷积核尺寸不能完美匹配输入的图像矩阵时需要进行一定的填充策略 设置为'SAME'表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；当设置为'VALID'时则对不足卷积尺寸的部分进行舍弃，输出维度就无法保证与输入维度一致 输入通道数 (In Channels) 指定卷积操作时卷积核的深度 默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式 输出通道数 (Out Channels) 指定卷积核的个数 若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量 卷积操作维度变换公式： \\(O_d =\\begin{cases} \\lceil \\frac{(I_d - k_{size})+ 1)}{s}\\rceil ,&amp; \\text{padding=VALID}\\\\ \\lceil \\frac{I_d}{s}\\rceil,&amp;\\text{padding=SAME} \\end{cases}\\) 其中，\\(I_d\\)为输入维度，\\(O_d\\)为输出维度，\\(k_{size}\\)为卷积核大小，\\(s\\)为步长 卷积核有什么类型？ ​ 常见的卷积主要是由连续紧密的卷积核对输入的图像特征进行滑窗式点乘求和操作，除此之外还有其他类型的卷积核在不同的任务中会用到，具体分类如表5.5所示。 ​ 表5.5 卷积核分类 卷积类别 示意图 作用 标准卷积 最常用的卷积核，连续紧密的矩阵形式可以提取图像区域中的相邻像素之间的关联关系，\\(3\\times3\\)的卷积核可以获得\\(3\\times3\\)像素范围的感受视野 扩张卷积（带孔卷积或空洞卷积） 引入一个称作扩张率（Dilation Rate）的参数，使同样尺寸的卷积核可以获得更大的感受视野，相应的在相同感受视野的前提下比普通卷积采用更少的参数。同样是\\(3\\times3\\)的卷积核尺寸，扩张卷积可以提取\\(5\\times5\\)范围的区域特征，在实时图像分割领域广泛应用 转置卷积 先对原始特征矩阵进行填充使其维度扩大到适配卷积目标输出维度，然后进行普通的卷积操作的一个过程，其输入到输出的维度变换关系恰好与普通卷积的变换关系相反，但这个变换并不是真正的逆变换操作，通常称为转置卷积(Transpose Convolution)而不是反卷积(Deconvolution)。转置卷积常见于目标检测领域中对小目标的检测和图像分割领域还原输入图像尺度。 可分离卷积 标准的卷积操作是同时对原始图像\\(H\\times W\\times C\\)三个方向的卷积运算，假设有\\(K\\)个相同尺寸的卷积核，这样的卷积操作需要用到的参数为\\(H\\times W\\times C\\times K\\)个；若将长宽与深度方向的卷积操作分离出变为\\(H\\times W\\)与\\(C\\)的两步卷积操作，则同样的卷积核个数\\(K\\)，只需要\\((H\\times W + C)\\times K\\)个参数，便可得到同样的输出尺度。可分离卷积(Seperable Convolution)通常应用在模型压缩或一些轻量的卷积神经网络中，如MobileNet\\(^{[1]}\\)、Xception\\(^{[2]}\\)等 二维卷积与三维卷积有什么区别？ 二维卷积 二维卷积操作如图5.3所示，为了更直观的说明，分别展示在单通道和多通道输入中，对单个通道输出的卷积操作。在单通道输入的情况下，若输入卷积核尺寸为 \\((k_h, k_w, 1)​\\)，卷积核在输入图像的空间维度上进行滑窗操作，每次滑窗和 \\((k_h, k_w)​\\)窗口内的值进行卷积操作，得到输出图像中的一个值。在多通道输入的情况下，假定输入图像特征通道数为3，卷积核尺寸则为\\((k_h, k_w, 3)​\\)，每次滑窗与3个通道上的\\((k_h, k_w)​\\)窗口内的所有值进行卷积操作，得到输出图像中的一个值。 三维卷积 3D卷积操作如图所示，同样分为单通道和多通道，且假定只使用1个卷积核，即输出图像仅有一个通道。对于单通道输入，与2D卷积不同之处在于，输入图像多了一个深度(depth)维度，卷积核也多了一个\\(k_d​\\)维度，因此3D卷积核的尺寸为\\((k_h, k_w, k_d)​\\)，每次滑窗与\\((k_h, k_w, k_d)​\\)窗口内的值进行相关操作，得到输出3D图像中的一个值。对于多通道输入，则与2D卷积的操作一样，每次滑窗与3个channels上的\\((k_h, k_w, k_d)​\\)窗口内的所有值进行相关操作，得到输出3D图像中的一个值。 有哪些池化方法？ ​ 池化操作通常也叫做子采样(Subsampling)或降采样(Downsampling)，在构建卷积神经网络时，往往会用在卷积层之后，通过池化来降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象。池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。因此，为了描述一个大的图像，很直观的想法就是对不同位置的特征进行聚合统计。例如，可以计算图像在固定区域上特征的平均值 (或最大值)来代表这个区域的特征。 ​ 表5.6 池化分类 池化类型 示意图 作用 一般池化(General Pooling) 通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。以最大池化为例，池化范围\\((2\\times2)\\)和滑窗步长\\((stride=2)\\) 相同，仅提取一次相同区域的范化特征。 重叠池化(Overlapping Pooling) 与一般池化操作相同，但是池化范围\\(P_{size}\\)与滑窗步长\\(stride\\)关系为\\(P_{size}&gt;stride\\)，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。 空间金字塔池化\\(^*\\)(Spatial Pyramid Pooling) 在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。 SPPNet\\(^{[3]}\\)就引入了空间池化的组合，对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同\\((win_{size}=\\lceil \\frac{in}{out}\\rceil; stride=\\lfloor \\frac{in}{out}\\rfloor; )\\)，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取 \\(1\\times1\\)卷积作用？ ​ NIN(Network in Network)\\(^{[4]}​\\)是第一篇探索\\(1\\times1​\\)卷积核的论文，这篇论文通过在卷积层中使用MLP替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力，也因其网络结构中嵌套MLP子网络而得名NIN。NIN对不同通道的特征整合到MLP自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，其中的MLP子网络恰恰可以用\\(1\\times1​\\)的卷积进行代替。 ​ GoogLeNet\\(^{[5]}​\\)则采用\\(1\\times1​\\)卷积核来减少模型的参数量。在原始版本的Inception模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。此时在每一个较大卷积核的卷积层前引入\\(1\\times1​\\)卷积，可以通过分离通道与宽高卷积来减少模型参数量。以图5.2为例，在不考虑参数偏置项的情况下，若输入和输出的通道数为\\(C_1=16​\\)，则左半边网络模块所需的参数为\\((1\\times1+3\\times3+5\\times5+0)\\times C_1\\times C_1=8960​\\)；假定右半边网络模块采用的\\(1\\times1​\\)卷积通道数为\\(C_2=8​\\)\\((满足C_1&gt;C_2)​\\)，则右半部分的网络结构所需参数量为\\((1\\times1\\times (3C_1+C_2)+3\\times3\\times C_2 +5\\times5\\times C_2)\\times C_1=5248​\\) ，可以在不改变模型表达能力的前提下大大减少所使用的参数量。 ​ 图5.2 Inception模块 综上所述，\\(1\\times 1​\\)卷积的作用主要为以下两点： 实现信息的跨通道交互和整合。 对卷积核通道数进行降维和升维，减小参数量。 卷积层和池化层有什么区别？ ​ 卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据步长设置获取到不同维度的输出，但是其内在操作是有本质区别的，如表5.7所示。 卷积层 池化层 结构 零填充时输出维度不变，而通道数改变 通常特征维度会降低，通道数不变 稳定性 输入特征发生细微改变时，输出结果会改变 感受域内的细微变化不影响输出结果 作用 感受域内提取局部关联特征 感受域内提取泛化特征，降低维度 参数量 与卷积核尺寸、卷积核个数相关 不引入额外参数 卷积核是否一定越大越好？ ​ 在早期的卷积神经网络中（如LeNet-5、AlexNet），用到了一些较大的卷积核（\\(11\\times11\\)和\\(5\\times 5\\)），受限于当时的计算能力和模型结构的设计，无法将网络叠加得很深，因此卷积网络中的卷积层需要设置较大的卷积核以获取更大的感受域。但是这种大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算性能也会降低。后来的卷积神经网络（VGG、GoogLeNet等），发现通过堆叠2个\\(3\\times 3\\)卷积核可以获得与\\(5\\times 5\\)卷积核相同的感受视野，同时参数量会更少（\\(3×3×2+1\\) &lt; $ 5×5×1+1$），\\(3\\times 3\\)卷积核被广泛应用在许多卷积神经网络中。因此可以认为，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。 ​ 但是，这并不是表示更大的卷积核就没有作用，在某些领域应用卷积神经网络时仍然可以采用较大的卷积核。譬如在自然语言处理领域，由于文本内容不像图像数据可以对特征进行很深层的抽象，往往在该领域的特征提取只需要较浅层的神经网络即可。在将卷积神经网络应用在自然语言处理领域时，通常都是较为浅层的卷积层组成，但是文本特征有时又需要有较广的感受域让模型能够组合更多的特征（如词组和字符），此时直接采用较大的卷积核将是更好的选择。 ​ 综上所述，卷积核的大小并没有绝对的优劣，需要视具体的应用场景而定，但是极大和极小的卷积核都是不合适的，单独的\\(1\\times 1\\)极小卷积核只能用作分离卷积而不能对输入的原始特征进行有效的组合，极大的卷积核通常会组合过多的无意义特征从而浪费了大量的计算资源。 每层卷积是否只能用一种尺寸的卷积核？ ​ 经典的神经网络一般都属于层叠式网络，每层仅用一个尺寸的卷积核，如VGG结构中使用了大量的\\(3×3\\)卷积层。事实上，同一层特征图可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好，如GoogLeNet、Inception系列的网络，均是每层使用了多个卷积核结构。如图5.3所示，输入的特征在同一层分别经过\\(1×1\\)、\\(3×3\\)和\\(5×5\\)三种不同尺寸的卷积核，再将分别得到的特征进行整合，得到的新特征可以看作不同感受域提取的特征组合，相比于单一卷积核会有更强的表达能力。 ​ 图5.3 Inception模块结构 怎样才能减少卷积层参数量？ 减少卷积层参数量的方法可以简要地归为以下几点： 使用堆叠小卷积核代替大卷积核：VGG网络中2个\\(3\\times 3\\)的卷积核可以代替1个\\(5\\times 5\\)的卷积核 使用分离卷积操作：将原本\\(K\\times K\\times C\\)的卷积操作分离为\\(K\\times K\\times 1\\)和\\(1\\times1\\times C\\)的两部分操作 添加\\(1\\times 1\\)的卷积操作：与分离卷积类似，但是通道数可变，在\\(K\\times K\\times C_1\\)卷积前添加\\(1\\times1\\times C_2\\)的卷积核（满足\\(C_2 &lt;C_1\\)） 在卷积层前使用池化操作：池化可以降低卷积层的输入特征维度 在进行卷积操作时，必须同时考虑通道和区域吗？ ​ 标准卷积中，采用区域与通道同时处理的操作，如下图所示： ​ 这样做可以简化卷积层内部的结构，每一个输出的特征像素都由所有通道的同一个区域提取而来。 ​ 但是这种方式缺乏灵活性，并且在深层的网络结构中使得运算变得相对低效，更为灵活的方式是使区域和通道的卷积分离开来，通道分离（深度分离）卷积网络由此诞生。如下图所示，Xception网络可解决上述问题。 ​ 我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道特征矩阵之后，再对这批新通道特征进行标准的\\(1×1​\\)跨通道卷积操作。 采用宽卷积的好处有什么？ ​ 宽卷积对应的是窄卷积，实际上并不是卷积操作的类型，指的是卷积过程中的填充方法，对应的是'SAME'填充和'VALID'填充。'SAME'填充通常采用零填充的方式对卷积核不满足整除条件的输入特征进行补全，以使卷积层的输出维度保持与输入特征维度一致；'VALID'填充的方式则相反，实际并不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃，因此在步长为1的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。此外，由于前一种方式通过补零来进行完整的卷积操作，可以有效地保留原始的输入特征信息。 ​ 比如下图左部分为窄卷积。注意到越在边缘的位置被卷积的次数越少。宽卷积可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，如下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一一部分0值，使得输出和输入的维度一致。 理解转置卷积与棋盘效应 标准卷积 在理解转置卷积之前，需要先理解标准卷积的运算方式。 首先给出一个输入输出结果 那是怎样计算的呢？ 卷积的时候需要对卷积核进行180的旋转，同时卷积核中心与需计算的图像像素对齐，输出结构为中心对齐像素的一个新的像素值，计算例子如下： 这样计算出左上角(即第一行第一列)像素的卷积后像素值。 给出一个更直观的例子，从左到右看，原像素经过卷积由1变成-8。 通过滑动卷积核，就可以得到整张图片的卷积结果。 转置卷积 图像的deconvolution过程如下： 输入：2x2， 卷积核：4x4， 滑动步长：3， 输出：7x7 过程如下： 输入图片每个像素进行一次full卷积，根据full卷积大小计算可以知道每个像素的卷积后大小为 1+4-1=4， 即4x4大小的特征图，输入有4个像素所以4个4x4的特征图 将4个特征图进行步长为3的相加； 输出的位置和输入的位置相同。步长为3是指每隔3个像素进行相加，重叠部分进行相加，即输出的第1行第4列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。 可以看出翻卷积的大小是由卷积核大小与滑动步长决定， in是输入大小， k是卷积核大小， s是滑动步长， out是输出大小 得到 out = (in - 1) * s + k 上图过程就是， (2 - 1) * 3 + 4 = 7。 棋盘效应 卷积神经网络的参数设置 ​ 卷积神经网络中常见的参数在其他类型的神经网络中也是类似的，但是参数的设置还得结合具体的任务才能设置在合理的范围，具体的参数列表如表XX所示。 ​ 表XX 卷积神经网络常见参数 参数名 常见设置 参数说明 学习率(Learning Rate) \\(0-1\\) 反向传播网络中更新权值矩阵的步长，在一些常见的网络中会在固定迭代次数或模型不再收敛后对学习率进行指数下降(如\\(lr=lr\\times 0.1\\))。当学习率越大计算误差对权值矩阵的影响越大，容易在某个局部最优解附近震荡；越小的学习率对网络权值的更新越精细，但是需要花费更多的时间去迭代 批次大小(Batch Size) \\(1-N\\) 批次大小指定一次性流入模型的数据样本个数，根据任务和计算性能限制判断实际取值，在一些图像任务中往往由于计算性能和存储容量限制只能选取较小的值。在相同迭代次数的前提下，数值越大模型越稳定，泛化能力越强，损失值曲线越平滑，模型也更快地收敛，但是每次迭代需要花费更多的时间 数据轮次(Epoch) \\(1-N\\) 数据轮次指定所有训练数据在模型中训练的次数，根据数据集规模和分布情况会设置为不同的值。当模型较为简单或训练数据规模较小时，通常轮次不宜过高，否则模型容易过拟合；模型较为复杂或训练数据规模足够大时，可适当提高数据的训练轮次。 权重衰减系数(Weight Decay) \\(0-0.001\\) 模型训练过程中反向传播权值更新的权重衰减值 提高卷积神经网络的泛化能力 ​ 卷积神经网络与其他类型的神经网络类似，在采用反向传播进行训练的过程中比较依赖输入的数据分布，当数据分布较为极端的情况下容易导致模型欠拟合或过拟合，表XX记录了提高卷积网络泛化能力的方法。 ​ 表XX 提高卷积网络化能力的方法 方法 说明 使用更多数据 在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力 使用更大批次 在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定 调整数据分布 大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力 调整目标函数 在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力 调整网络结构 在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用 数据增强 数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。 权值正则化 权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如)。 屏蔽网络节点 该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。 对大多数神经网络模型同样通用 卷积神经网络在不同领域的应用 ​ 卷积神经网络中的卷积操作是其关键组成，而卷积操作只是一种数学运算方式，实际上对不同类型的数值表示数据都是通用的，尽管这些数值可能表示的是图像像素值、文本序列中单个字符或是语音片段中单字的音频。只要使原始数据能够得到有效地数值化表示，卷积神经网络能够在不同的领域中得到应用，要关注的是如何将卷积的特性更好地在不同领域中应用，如表XX所示。 ​ 表XX 卷积神经网络不同领域的应用 | 应用领域 | 输入数据图示 | 说明 | | :-----: | :----------: | :-- | | 图像处理 | | 卷积神经网络在图像处理领域有非常广泛的应用，这是因为图像数据本身具有的局部完整性非常 | | 自然语言处理 | | | | 语音处理 | | | 联系 ​ 自然语言处理是对一维信号（词序列）做操作。 ​ 计算机视觉是对二维（图像）或三维（视频流）信号做操作。 区别 ​ 自然语言处理的输入数据通常是离散取值（例如表示一个单词或字母通常表示为词典中的one hot向量），计算机视觉则是连续取值（比如归一化到0，1之间的灰度值）。CNN有两个主要特点，区域不变性(location invariance)和组合性(Compositionality)。 区域不变性：滤波器在每层的输入向量(图像)上滑动，检测的是局部信息，然后通过pooling取最大值或均值。pooling这步综合了局部特征，失去了每个特征的位置信息。这很适合基于图像的任务，比如要判断一幅图里有没有猫这种生物，你可能不会去关心这只猫出现在图像的哪个区域。但是在NLP里，词语在句子或是段落里出现的位置，顺序，都是很重要的信息。 局部组合性：CNN中，每个滤波器都把较低层的局部特征组合生成较高层的更全局化的特征。这在CV里很好理解，像素组合成边缘，边缘生成形状，最后把各种形状组合起来得到复杂的物体表达。在语言里，当然也有类似的组合关系，但是远不如图像来的直接。而且在图像里，相邻像素必须是相关的，相邻的词语却未必相关。 卷积神经网络凸显共性的方法？ 局部连接 ​ 我们首先了解一个概念，感受野，即每个神经元仅与输入神经元相连接的一块区域。 在图像卷积操作中，神经元在空间维度上是局部连接，但在深度上是全连接。局部连接的思想，是受启发于生物学里的视觉系统结构，视觉皮层的神经元就是仅用局部接受信息。对于二维图像，局部像素关联性较强。这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征； 下图是一个很经典的图示，左边是全连接，右边是局部连接。 对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此巨大的参数量几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。 权值共享 ​ 权值共享，即计算同一深度的神经元时采用的卷积核参数是共享的。权值共享在一定程度上讲是有意义的，是由于在神经网络中，提取的底层边缘特征与其在图中的位置无关。但是在另一些场景中是无意的，如在人脸识别任务，我们期望在不同的位置学到不同的特征。 需要注意的是，权重只是对于同一深度切片的神经元是共享的。在卷积层中，通常采用多组卷积核提取不同的特征，即对应的是不同深度切片的特征，而不同深度切片的神经元权重是不共享。相反，偏置这一权值对于同一深度切片的所有神经元都是共享的。 权值共享带来的好处是大大降低了网络的训练难度。如下图，假设在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核的大小）。 这里就体现了卷积神经网络的奇妙之处，使用少量的参数，却依然能有非常出色的性能。上述仅仅是提取图像一种特征的过程。如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像不同尺度下的特征，称之为特征图（feature map）。 池化操作 池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图： 全连接、局部连接、全卷积与局部卷积 ​ 大多数神经网络中高层网络通常会采用全连接层(Global Connected Layer)，通过多对多的连接方式对特征进行全局汇总，可以有效地提取全局信息。但是全连接的方式需要大量的参数，是神经网络中最占资源的部分之一，因此就需要由局部连接(Local Connected Layer)，仅在局部区域范围内产生神经元连接，能够有效地减少参数量。根据卷积操作的作用范围可以分为全卷积(Global Convolution)和局部卷积(Local Convolution)。实际上这里所说的全卷积就是标准卷积，即在整个输入特征维度范围内采用相同的卷积核参数进行运算，全局共享参数的连接方式可以使神经元之间的连接参数大大减少;局部卷积又叫平铺卷积(Tiled Convolution)或非共享卷积(Unshared Convolution)，是局部连接与全卷积的折衷。四者的比较如表XX所示。 ​ 表XX 卷积网络中连接方式的对比 连接方式 示意图 说明 全连接 层间神经元完全连接，每个输出神经元可以获取到所有输入神经元的信息，有利于信息汇总，常置于网络末层；连接与连接之间独立参数，大量的连接大大增加模型的参数规模。 局部连接 层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于全连接减少了感受域外的连接，有效减少参数规模 全卷积 层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，连接所采用的参数在不同感受域之间共享，有利于提取特定模式的特征；相比于局部连接，共用感受域之间的参数可以进一步减少参数量。 局部卷积 层间神经元只有局部范围内的连接，感受域内采用全连接的方式，而感受域之间间隔采用局部连接与全卷积的连接方式；相比与全卷积成倍引入额外参数，但有更强的灵活性和表达能力；相比于局部连接，可以有效控制参数量 局部卷积的应用 并不是所有的卷积都会进行权重共享，在某些特定任务中，会使用不权重共享的卷积。下面通过人脸这一任务来进行讲解。在读人脸方向的一些paper时，会发现很多都会在最后加入一个Local Connected Conv，也就是不进行权重共享的卷积层。总的来说，这一步的作用就是使用3D模型来将人脸对齐，从而使CNN发挥最大的效果。 截取论文中的一部分图，经过3D对齐以后，形成的图像均是152×152，输入到上述的网络结构中。该结构的参数如下： Conv：32个11×11×3的卷积核， Max-pooling: 3×3，stride=2， Conv: 16个9×9的卷积核， Local-Conv: 16个9×9的卷积核， Local-Conv: 16个7×7的卷积核， Local-Conv: 16个5×5的卷积核， Fully-connected: 4096维， Softmax: 4030维。 前三层的目的在于提取低层次的特征，比如简单的边和纹理。其中Max-pooling层使得卷积的输出对微小的偏移情况更加鲁棒。但不能使用更多的Max-pooling层，因为太多的Max-pooling层会使得网络损失图像信息。全连接层将上一层的每个单元和本层的所有单元相连，用来捕捉人脸图像不同位置特征之间的相关性。最后使用softmax层用于人脸分类。 中间三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因： （1）对齐的人脸图片中，不同的区域会有不同的统计特征，因此并不存在特征的局部稳定性，所以使用相同的卷积核会导致信息的丢失。 （2）不共享的卷积核并不增加inference时特征的计算量，仅会增加训练时的计算量。 使用不共享的卷积核，由于需要训练的参数量大大增加，因此往往需要通过其他方法增加数据量。 NetVLAD池化 （贡献者：熊楚原-中国人民大学） NetVLAD是论文[15]提出的一个局部特征聚合的方法。 在传统的网络里面，例如VGG啊，最后一层卷积层输出的特征都是类似于Batchsize x 3 x 3 x 512的这种东西，然后会经过FC聚合，或者进行一个Global Average Pooling（NIN里的做法），或者怎么样，变成一个向量型的特征，然后进行Softmax or 其他的Loss。 这种方法说简单点也就是输入一个图片或者什么的结构性数据，然后经过特征提取得到一个长度固定的向量，之后可以用度量的方法去进行后续的操作，比如分类啊，检索啊，相似度对比等等。 那么NetVLAD考虑的主要是最后一层卷积层输出的特征这里，我们不想直接进行欠采样或者全局映射得到特征，对于最后一层输出的W x H x D，设计一个新的池化，去聚合一个“局部特征“，这即是NetVLAD的作用。 NetVLAD的一个输入是一个W x H x D的图像特征，例如VGG-Net最后的3 x 3 x 512这样的矩阵，在网络中还需加一个维度为Batchsize。 NetVLAD还需要另输入一个标量K即表示VLAD的聚类中心数量，它主要是来构成一个矩阵C，是通过原数据算出来的每一个\\(W \\times H\\)特征的聚类中心，C的shape即\\(C: K \\times D\\)，然后根据三个输入，VLAD是计算下式的V: \\[V(j, k) = \\sum_{i=1}^{N}{a_k(x_i)(x_i(j) - c_k(j))}\\] 其中j表示维度，从1到D，可以看到V的j是和输入与c对应的，对每个类别k，都对所有的x进行了计算，如果\\(x_i\\)属于当前类别k，\\(a_k=1\\)，否则\\(a_k=0\\)，计算每一个x和它聚类中心的残差，然后把残差加起来，即是每个类别k的结果，最后分别L2正则后拉成一个长向量后再做L2正则，正则非常的重要，因为这样才能统一所有聚类算出来的值，而残差和的目的主要是消减不同聚类上的分布不均，两者共同作用才能得到最后正常的输出。 输入与输出如下图所示： 中间得到的K个D维向量即是对D个x都进行了与聚类中心计算残差和的过程，最终把K个D维向量合起来后进行即得到最终输出的\\(K \\times D\\)长度的一维向量。 而VLAD本身是不可微的，因为上面的a要么是0要么是1，表示要么当前描述x是当前聚类，要么不是，是个离散的，NetVLAD为了能够在深度卷积网络里使用反向传播进行训练，对a进行了修正。 那么问题就是如何重构一个a，使其能够评估当前的这个x和各个聚类的关联程度？用softmax来得到： \\[a_k = \\frac{e^{W_k^T x_i + b_k}}{e^{W_{k'}^T x_i + b_{k'}}}\\] 将这个把上面的a替换后，即是NetVLAD的公式，可以进行反向传播更新参数。 所以一共有三个可训练参数，上式a中的\\(W: K \\times D\\)，上式a中的\\(b: K \\times 1\\)，聚类中心\\(c: K \\times D\\)，而原始VLAD只有一个参数c。 最终池化得到的输出是一个恒定的K x D的一维向量（经过了L2正则），如果带Batchsize，输出即为Batchsize x (K x D)的二维矩阵。 NetVLAD作为池化层嵌入CNN网络即如下图所示， 原论文中采用将传统图像检索方法VLAD进行改进后应用在CNN的池化部分作为一种另类的局部特征池化，在场景检索上取得了很好的效果。 后续相继又提出了ActionVLAD、ghostVLAD等改进。 参考文献 [1] 卷积神经网络研究综述[J]. 计算机学报, 2017, 40(6):1229-1251. [2] 常亮, 邓小明, 周明全,等. 图像理解中的卷积神经网络[J]. 自动化学报, 2016, 42(9):1300-1312. [3] Chua L O. CNN: A Paradigm for Complexity[M]// CNN a paradigm for complexity /. 1998. [4] He K, Gkioxari G, Dollar P, et al. Mask R-CNN[J]. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, PP(99):1-1. [5] Hoochang S, Roth H R, Gao M, et al. Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning[J]. IEEE Transactions on Medical Imaging, 2016, 35(5):1285-1298. [6] 许可. 卷积神经网络在图像识别上的应用的研究[D]. 浙江大学, 2012. [7] 陈先昌. 基于卷积神经网络的深度学习算法与应用研究[D]. 浙江工商大学, 2014. [8] CS231n Convolutional Neural Networks for Visual Recognition, Stanford [9] Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks [10] cs231n 动态卷积图：http://cs231n.github.io/assets/conv-demo/index.html [11] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105. [12] Sun Y, Wang X, Tang X. Deep learning face representation from predicting 10,000 classes[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 1891-1898. [13] 魏秀参.解析深度学习——卷积神经网络原理与视觉实践[M].电子工业出版社, 2018 [14] Jianxin W U , Gao B B , Wei X S , et al. Resource-constrained deep learning: challenges and practices[J]. Scientia Sinica(Informationis), 2018. [15] Arandjelovic R , Gronat P , Torii A , et al. [IEEE 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - Las Vegas, NV, USA (2016.6.27-2016.6.30)] 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - NetVLAD: CNN Architecture for Weakly Supervised Place Recognition[C]// 2016:5297-5307.","link":"/posts/3383909398.html"},{"title":"循环神经网络详解","text":"概述 CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。 RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。在自然语言处理(NLP)领域，RNN已经可以做语音识别、机器翻译、生成手写字符，以及构建强大的语言模型 (Sutskever et al.)，(Graves)，(Mikolov et al.)（字符级别和单词级别的都有。在机器视觉领域，RNN也非常流行。包括帧级别的视频分类，图像描述，视频描述以及基于图像的Q&amp;A等等。 结构 RNN结构如下图所示： 神经网络的模块A正在读取某个输入\\({x_t}\\)，并输出一个值\\({h_t}\\)，循环可以使得信息从当前步传递到下一步，将这个循环展开，如下所示。链式的特征揭示了 RNN 本质上是与序列和列表相关的，它们是对于这类数据的最自然的神经网络架构。 长期依赖问题 RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。 但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France... I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。 在理论上，RNN 绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。然而，幸运的是，LSTM 并没有这个问题！ LSTM网络 Long Short Term 网络—— 一般就叫做 LSTM，是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter &amp; Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！ 所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。 LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。 LSTM网络详解 下面对LSTM网络进行详细说明，首先说明一下图中使用的图标，如下： &gt; 在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表按位 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。 LSTM 的关键就是细胞状态cell state，水平线在图上方贯穿运行，也就是贯穿每个重复结构的上面这条flow。细胞状态类似于传送带，直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。这条flow其实就承载着之前所有状态的信息，每当flow流经一个重复结构A的时候，都会有相应的操作来决定舍弃什么旧的信息以及添加什么新的信息。 LSTM 有通过精心设计对信息增减进行控制的结构，称作为“门”。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个按位的乘法操作。Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！ LSTM 拥有三个门，来保护和控制细胞状态，分别是遗忘门 (forget gate)、输入门 (input gate)、输出门 (output gate)。下面对这三个门进行详细讲解 遗忘门 (forget gate) 遗忘门决定了要从cell state中舍弃什么信息。其通过输入上一状态的输出ht-1、当前状态输入信息Xt到一个Sigmoid函数中，产生一个介于0到1之间的数值，与cell state相乘之后来确定舍弃（保留）多少信息。0 表示“完全舍弃”，1 表示“完全保留”。 上式中， 是遗忘门的权重矩阵，表示把两个向量连接成一个更长的向量，是遗忘门的偏置项，是sigmoid函数。如果输入的维度是，隐藏层的维度是，单元状态的维度是 （通常 ），则遗忘门的权重矩阵维度是。事实上，权重矩阵都是两个矩阵拼接而成的：一个是，它对应着输入项，其维度为；一个是，它对应着输入项，其维度为。可以写为： 输入门 (input gate) 输入门决定了要往cell state中保存什么新的信息。其通过输入上一状态的输出、当前状态输入信息到一个Sigmoid函数中，产生一个介于0到1之间的数值来确定我们需要保留多少的新信息。同时，一个tanh层会通过上一状态的输出、当前状态输入信息来得到一个将要加入到cell state中的“候选新信息”。 现在计算当前时刻的单元状态。它是由上一次的单元状态按元素乘以遗忘门，丢弃掉我们确定需要丢弃的信息；然后把当前输入的单元状态按元素乘以输入门，将两个积加和，这就是新的候选值： 输出门 (output gate) 输出门决定了要从cell state中输出什么信息。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本，会先有一个Sigmoid函数产生一个介于0到1之间的数值来确定我们需要输出多少cell state中的信息。cell state的信息再与相乘时首先会经过一个tanh层进行“激活”（非线性变换）。得到的就是这个LSTM block的输出信息。 代码 采用单层LSTM实现MNIST分类判别，MNIST的输入为影像，影像的行排列需要有一定的顺序，如果胡乱排列，则无法判断数字，因此可以将此问题看作是RNN。设置时间序列长度为28，每次输入影像的一行（28个维度），batch size为128，隐层结点数为128，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145########## load packages ##########import tensorflow as tf##################### load data ##########################from tensorflow.examples.tutorials.mnist import input_datamnist=input_data.read_data_sets(&quot;mnist_sets&quot;,one_hot=True)########## set net hyperparameters ##########learning_rate=0.001epochs=1batch_size=128display_step=20########## set net parameters ##########n_inputs = 28 # 输入向量的维度，每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素n_steps = 28 # 循环层长度，即时序持续长度为28，即每做一次预测，需要先输入28行#### 0-9 digits ####n_classes=10#### neurons in hidden layer 隐含层的结点数 ####n_hidden_units=128########## placeholder ##########x=tf.placeholder(tf.float32,[None, n_steps, n_inputs])y=tf.placeholder(tf.float32,[None, n_classes])######### Define weights and biases ########## in:每个cell输入的全连接层参数# out:定义用于输出的全连接层参数weights = { # (28, 128) 'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])), # (128, 10) 'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))}biases = { # (128, ) 'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])), # (10, ) 'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))}##################### build net model ############################### RNN LSTM 单层LSTM #######def RNN(x, weights, biases): # hidden layer for input to cell # x (128 batch,28 steps,28 inputs) ==&gt; (128 batch * 28 steps, 28 inputs) x=tf.reshape(x,shape=[-1, n_inputs]) # into hidden # x_in =[128 bach*28 steps,28 inputs]*[28 inputs,128 hidden_units]=[128 batch * 28 steps, 128 hidden] x_in = tf.matmul(x, weights['in']) + biases['in'] # x_in ==&gt; (128 batch, 28 steps, 128 hidden) x_in = tf.reshape(x_in, [-1, n_steps, n_hidden_units]) # cell # basic LSTM Cell.初始的forget_bias=1,不希望遗忘任何信息 cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units,forget_bias=1.0,state_is_tuple=True) # lstm cell is divided into two parts (c_state, h_state) init_state = cell.zero_state(batch_size, dtype=tf.float32) # dynamic_rnn receive Tensor (batch, steps, inputs) or (steps, batch, inputs) as x_in. # n_steps位于次要维度 time_major=False outputs shape 128, 28, 128 outputs, final_state = tf.nn.dynamic_rnn(cell, x_in, initial_state=init_state, time_major=False) # hidden layer for output as the final results # unpack to list [(batch, outputs)..] * steps # steps即时间序列长度，此时输出28个ht，由于输入的是 batch steps inputs，因此需要对outputs做调整，从而取到最后一个ht # permute time_step_size and batch_size outputs shape from [128, 28, 128] to [28, 128, 128] outputs = tf.unstack(tf.transpose(outputs, [1,0,2])) #选择最后一个output与输出的全连接weights相乘再加上biases results = tf.matmul(outputs[-1], weights['out']) + biases['out'] # shape = (128, 10) return results########## define model, loss and optimizer ############## model pred 影像判断结果 ####pred=RNN(x,weights,biases)#### loss 损失计算 ####cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))#### optimization 优化 ####optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)#### accuracy 准确率 ####correct_pred=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))##################### train and evaluate model #################################### initialize variables ##########init=tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) step=1 #### epoch 世代循环 #### for epoch in range(epochs+1): #### iteration #### for _ in range(mnist.train.num_examples//batch_size): step += 1 ##### get x,y ##### batch_x, batch_y=mnist.train.next_batch(batch_size) batch_x = batch_x.reshape([batch_size, n_steps, n_inputs]) ##### optimizer #### sess.run(optimizer,feed_dict={x:batch_x, y:batch_y}) ##### show loss and acc ##### if step % display_step==0: loss,acc=sess.run([cost, accuracy],feed_dict={x: batch_x, y: batch_y}) print(&quot;Epoch &quot;+ str(epoch) + &quot;, Minibatch Loss=&quot; + \\ &quot;{:.6f}&quot;.format(loss) + &quot;, Training Accuracy= &quot;+ \\ &quot;{:.5f}&quot;.format(acc)) print(&quot;Optimizer Finished!&quot;) ##### test accuracy ##### for _ in range(mnist.test.num_examples//batch_size): batch_x,batch_y=mnist.test.next_batch(batch_size) batch_x = batch_x.reshape([batch_size, n_steps, n_inputs]) print(&quot;Testing Accuracy:&quot;, sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})) LSTM文本分类 LSTM由于其设计的特点，非常适合用于对时序数据的建模，如文本数据。将词的表示组合成句子的表示，可以采用相加的方法，即将所有词的表示进行加和，或者取平均等方法，但是这些方法没有考虑到词语在句子中前后顺序。如句子“我不觉得他好”。“不”字是对后面“好”的否定，即该句子的情感极性是贬义。使用LSTM模型可以更好的捕捉到较长距离的依赖关系。因为LSTM通过训练过程可以学到记忆哪些信息和遗忘哪些信息。 基于Keras框架，采用LSTM实现文本分类。文本采用imdb影评分类语料，共25,000条影评，label标记为正面/负面两种评价。影评已被预处理为词下标构成的序列。方便起见，单词的下标基于它在数据集中出现的频率标定，例如整数3所编码的词为数据集中第3常出现的词。这样的组织方法使得用户可以快速完成诸如“只考虑最常出现的10,000个词，但不考虑最常出现的20个词”这样的操作。词向量没有采用预训练好的向量，训练中生成，采用的网络结构如图所示： 具体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172###################### load packages ####################from keras.datasets import imdbfrom keras import preprocessingfrom keras.models import Sequentialfrom keras.layers import Dense, Embedding, Dropout, LSTMfrom keras.utils.np_utils import to_categorical###################### load data ############################# 只考虑最常见的1000个词 ########num_words = 1000######### 导入数据 #########(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)print(x_train.shape)print(x_train[0][:5])print(y_train.shape)print(y_train[0])###################### preprocess data ############################ 句子长度最长设置为20 ########max_len = 20######## 对文本进行填充，将文本转成相同长度 ########x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)print(x_train.shape)print(x_train[0])######## 对label做one-hot处理 ########num_class = 2y_train = to_categorical(y_train, num_class)y_test = to_categorical(y_test, num_class)print(y_train.shape)print(y_train[0])###################### build network ############################ word dim 词向量维度 ########word_dim = 8######## network structure ########model = Sequential()#### Embedding层 ####model.add(Embedding(input_dim=1000, output_dim=word_dim, input_length=max_len))#### 两层LSTM，第一层，设置return_sequences参数为True ####model.add(LSTM(256, return_sequences=True))#### dropout ####model.add(Dropout(0.5))#### 两层LSTM，第二层，设置return_sequences参数为False ####model.add(LSTM(256, return_sequences=False))#### dropout ####model.add(Dropout(0.5))#### 输出层 ####model.add(Dense(num_class, activation='softmax'))print(model.summary())######## optimization and train ########model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])model.fit(x_train, y_train, batch_size=512, epochs=20, verbose=1, validation_data=(x_test, y_test)) 运行结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970(25000,)[1, 14, 22, 16, 43](25000,)1(25000, 20)[ 65 16 38 2 88 12 16 283 5 16 2 113 103 32 15 16 2 19 178 32](25000, 2)[0. 1.]_________________________________________________________________Layer (type) Output Shape Param # =================================================================embedding_33 (Embedding) (None, 20, 8) 8000 _________________________________________________________________lstm_27 (LSTM) (None, 20, 256) 271360 _________________________________________________________________dropout_5 (Dropout) (None, 20, 256) 0 _________________________________________________________________lstm_28 (LSTM) (None, 256) 525312 _________________________________________________________________dropout_6 (Dropout) (None, 256) 0 _________________________________________________________________dense_27 (Dense) (None, 2) 514 =================================================================Total params: 805,186Trainable params: 805,186Non-trainable params: 0_________________________________________________________________NoneTrain on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 47s 2ms/step - loss: 0.6618 - acc: 0.5817 - val_loss: 0.5914 - val_acc: 0.6820Epoch 2/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.5493 - acc: 0.7170 - val_loss: 0.5316 - val_acc: 0.7281Epoch 3/2025000/25000 [==============================] - 41s 2ms/step - loss: 0.5085 - acc: 0.7484 - val_loss: 0.5245 - val_acc: 0.7322Epoch 4/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.5012 - acc: 0.7548 - val_loss: 0.5160 - val_acc: 0.7381Epoch 5/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4946 - acc: 0.7559 - val_loss: 0.5165 - val_acc: 0.7384Epoch 6/2025000/25000 [==============================] - 41s 2ms/step - loss: 0.4924 - acc: 0.7577 - val_loss: 0.5166 - val_acc: 0.7388Epoch 7/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4867 - acc: 0.7596 - val_loss: 0.5264 - val_acc: 0.7292Epoch 8/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4851 - acc: 0.7614 - val_loss: 0.5262 - val_acc: 0.7400Epoch 9/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4803 - acc: 0.7643 - val_loss: 0.5250 - val_acc: 0.7392Epoch 10/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4774 - acc: 0.7651 - val_loss: 0.5220 - val_acc: 0.7376Epoch 11/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4729 - acc: 0.7696 - val_loss: 0.5225 - val_acc: 0.7365Epoch 12/2025000/25000 [==============================] - 39s 2ms/step - loss: 0.4704 - acc: 0.7698 - val_loss: 0.5279 - val_acc: 0.7385Epoch 13/2025000/25000 [==============================] - 46s 2ms/step - loss: 0.4682 - acc: 0.7713 - val_loss: 0.5303 - val_acc: 0.7343Epoch 14/2025000/25000 [==============================] - 44s 2ms/step - loss: 0.4683 - acc: 0.7729 - val_loss: 0.5297 - val_acc: 0.7325Epoch 15/2025000/25000 [==============================] - 43s 2ms/step - loss: 0.4659 - acc: 0.7739 - val_loss: 0.5402 - val_acc: 0.7331Epoch 16/2025000/25000 [==============================] - 41s 2ms/step - loss: 0.4587 - acc: 0.7759 - val_loss: 0.5350 - val_acc: 0.7312Epoch 17/2025000/25000 [==============================] - 42s 2ms/step - loss: 0.4577 - acc: 0.7771 - val_loss: 0.5488 - val_acc: 0.7334Epoch 18/2025000/25000 [==============================] - 42s 2ms/step - loss: 0.4524 - acc: 0.7796 - val_loss: 0.5356 - val_acc: 0.7284Epoch 19/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4492 - acc: 0.7829 - val_loss: 0.5357 - val_acc: 0.7332Epoch 20/2025000/25000 [==============================] - 40s 2ms/step - loss: 0.4472 - acc: 0.7840 - val_loss: 0.5532 - val_acc: 0.7216 双向RNN 概念 但是利用LSTM对句子进行建模还存在一个问题：无法编码从后到前的信息。在更细粒度的分类时，如对于强程度的褒义、弱程度的褒义、中性、弱程度的贬义、强程度的贬义的五分类任务需要注意情感词、程度词、否定词之间的交互。举一个例子，“这个餐厅脏得不行，没有隔壁好”，这里的“不行”是对“脏”的程度的一种修饰，通过BiLSTM可以更好的捕捉双向的语义依赖。BiLSTM是Bi-directional Long Short-Term Memory的缩写，是由前向LSTM与后向LSTM组合而成。比如，我们对“我爱中国”这句话进行编码，模型如图所示： 前向依次输入“我”，“爱”，“中国”得到三个向量，后向依次输入“中国”，“爱”，“我”得到三个向量。最后将前向和后向的隐向量进行拼接得到，即。对于情感分类任务来说，采用的句子的表示往往是，因为其包含了前向与后向的所有信息，如图所示： Tensorflow的Bi-RNN实现 tensorflow的Bi-RNN代码 tensorflow的Bi-RNN代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None, initial_state_fw=None, initial_state_bw=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None): if not _like_rnncell(cell_fw): raise TypeError(&quot;cell_fw must be an instance of RNNCell&quot;) if not _like_rnncell(cell_bw): raise TypeError(&quot;cell_bw must be an instance of RNNCell&quot;) with vs.variable_scope(scope or &quot;bidirectional_rnn&quot;): # Forward direction with vs.variable_scope(&quot;fw&quot;) as fw_scope: output_fw, output_state_fw = dynamic_rnn( cell=cell_fw, inputs=inputs, sequence_length=sequence_length, initial_state=initial_state_fw, dtype=dtype, parallel_iterations=parallel_iterations, swap_memory=swap_memory, time_major=time_major, scope=fw_scope) # Backward direction if not time_major: time_dim = 1 batch_dim = 0 else: time_dim = 0 batch_dim = 1 def _reverse(input_, seq_lengths, seq_dim, batch_dim): if seq_lengths is not None: return array_ops.reverse_sequence( input=input_, seq_lengths=seq_lengths, seq_dim=seq_dim, batch_dim=batch_dim) else: return array_ops.reverse(input_, axis=[seq_dim]) with vs.variable_scope(&quot;bw&quot;) as bw_scope: inputs_reverse = _reverse( inputs, seq_lengths=sequence_length, seq_dim=time_dim, batch_dim=batch_dim) tmp, output_state_bw = dynamic_rnn( cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length, initial_state=initial_state_bw, dtype=dtype, parallel_iterations=parallel_iterations, swap_memory=swap_memory, time_major=time_major, scope=bw_scope) output_bw = _reverse( tmp, seq_lengths=sequence_length, seq_dim=time_dim, batch_dim=batch_dim) outputs = (output_fw, output_bw) output_states = (output_state_fw, output_state_bw) return (outputs, output_states) 代码解读 前向输入 首先是对输入数据inputs，调用dynamic_rnn从前往后跑一下，得到output_fw和output_state_fw，其中output_fw是所有inputs的LSTM输出状态，output_state_fw是最终的输出状态， 12345678with vs.variable_scope(scope or &quot;bidirectional_rnn&quot;): # Forward direction with vs.variable_scope(&quot;fw&quot;) as fw_scope: output_fw, output_state_fw = dynamic_rnn( cell=cell_fw, inputs=inputs, sequence_length=sequence_length, initial_state=initial_state_fw, dtype=dtype, parallel_iterations=parallel_iterations, swap_memory=swap_memory, time_major=time_major, scope=fw_scope) 反向输入 定义一个局部函数：把输入的input_ 按照长度为seq_lengths 调用array_ops.rerverse_sequence 做一次转置： 1234567def _reverse(input_, seq_lengths, seq_dim, batch_dim): if seq_lengths is not None: return array_ops.reverse_sequence( input=input_, seq_lengths=seq_lengths, seq_dim=seq_dim, batch_dim=batch_dim) else: return array_ops.reverse(input_, axis=[seq_dim]) 之后把inputs转置成inputs_reverse，然后对这个inputs_reverse跑一下dynamic_rnn得到tmp和output_state_bw： 123456789with vs.variable_scope(&quot;bw&quot;) as bw_scope: inputs_reverse = _reverse( inputs, seq_lengths=sequence_length, seq_dim=time_dim, batch_dim=batch_dim) tmp, output_state_bw = dynamic_rnn( cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length, initial_state=initial_state_bw, dtype=dtype, parallel_iterations=parallel_iterations, swap_memory=swap_memory, time_major=time_major, scope=bw_scope) 再把这个输出tmp反转一下得到Output_bw向量： 123output_bw = _reverse( tmp, seq_lengths=sequence_length, seq_dim=time_dim, batch_dim=batch_dim) 前向和反向的LSTM输出堆叠 output_fw和output_bw堆叠在一起得到bi-rnn的输出，隐藏层状态output_state_fw和output_state_bw堆叠在一起得到bi-rnn的隐藏层状态，最终输出： 1234outputs = (output_fw, output_bw)output_states = (output_state_fw, output_state_bw)return (outputs, output_states) 代码 基于Keras框架，采用双向LSTM实现文本分类，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778###################### load packages ####################from keras.datasets import imdbfrom keras import preprocessingfrom keras.models import Sequentialfrom keras.layers import Dense, Embedding, Dropout, LSTM, Bidirectional, SpatialDropout1Dfrom keras.utils.np_utils import to_categorical###################### load data ############################# 只考虑最常见的1000个词 ########num_words = 1000######### 导入数据 #########(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)print(x_train.shape)print(x_train[0][:5])print(y_train.shape)print(y_train[0])###################### preprocess data ############################ 句子长度最长设置为20 ########max_len = 20######## 对文本进行填充，将文本转成相同长度 ########x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)print(x_train.shape)print(x_train[0])######## 对label做one-hot处理 ########num_class = 2y_train = to_categorical(y_train, num_class)y_test = to_categorical(y_test, num_class)print(y_train.shape)print(y_train[0])###################### build network ############################ word dim 词向量维度 ########word_dim = 8######## network structure ########model = Sequential()#### Embedding层 ####model.add(Embedding(input_dim=1000, output_dim=word_dim, input_length=max_len))#### dropout ####model.add(SpatialDropout1D(0.3))#### bi-RNN ####model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))#### dense ####model.add(Dense(1024, activation='relu'))#### dropout ####model.add(Dropout(0.8))#### dense ####model.add(Dense(1024, activation='relu'))#### dropout ####model.add(Dropout(0.8))#### 输出层 ####model.add(Dense(num_class, activation='softmax'))print(model.summary())######## optimization and train ########model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])model.fit(x_train, y_train, batch_size=512, epochs=20, verbose=1, validation_data=(x_test, y_test)) 结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374(25000,)[1, 14, 22, 16, 43](25000,)1(25000, 20)[ 65 16 38 2 88 12 16 283 5 16 2 113 103 32 15 16 2 19 178 32](25000, 2)[0. 1.]_________________________________________________________________Layer (type) Output Shape Param # =================================================================embedding_1 (Embedding) (None, 20, 8) 8000 _________________________________________________________________spatial_dropout1d_1 (Spatial (None, 20, 8) 0 _________________________________________________________________bidirectional_1 (Bidirection (None, 200) 87200 _________________________________________________________________dense_1 (Dense) (None, 1024) 205824 _________________________________________________________________dropout_1 (Dropout) (None, 1024) 0 _________________________________________________________________dense_2 (Dense) (None, 1024) 1049600 _________________________________________________________________dropout_2 (Dropout) (None, 1024) 0 _________________________________________________________________dense_3 (Dense) (None, 2) 2050 =================================================================Total params: 1,352,674Trainable params: 1,352,674Non-trainable params: 0_________________________________________________________________NoneTrain on 25000 samples, validate on 25000 samplesEpoch 1/2025000/25000 [==============================] - 22s 884us/step - loss: 0.6930 - acc: 0.5094 - val_loss: 0.6887 - val_acc: 0.5946Epoch 2/2025000/25000 [==============================] - 18s 700us/step - loss: 0.6251 - acc: 0.6475 - val_loss: 0.5463 - val_acc: 0.7220Epoch 3/2025000/25000 [==============================] - 18s 703us/step - loss: 0.5466 - acc: 0.7254 - val_loss: 0.5231 - val_acc: 0.7366Epoch 4/2025000/25000 [==============================] - 18s 704us/step - loss: 0.5296 - acc: 0.7377 - val_loss: 0.5179 - val_acc: 0.7367Epoch 5/2025000/25000 [==============================] - 18s 700us/step - loss: 0.5209 - acc: 0.7432 - val_loss: 0.5207 - val_acc: 0.7310Epoch 6/2025000/25000 [==============================] - 18s 702us/step - loss: 0.5151 - acc: 0.7452 - val_loss: 0.5144 - val_acc: 0.7380Epoch 7/2025000/25000 [==============================] - 17s 694us/step - loss: 0.5118 - acc: 0.7488 - val_loss: 0.5123 - val_acc: 0.7390Epoch 8/2025000/25000 [==============================] - 18s 727us/step - loss: 0.5064 - acc: 0.7542 - val_loss: 0.5153 - val_acc: 0.7361Epoch 9/2025000/25000 [==============================] - 18s 708us/step - loss: 0.5060 - acc: 0.7540 - val_loss: 0.5119 - val_acc: 0.7400Epoch 10/2025000/25000 [==============================] - 18s 720us/step - loss: 0.5042 - acc: 0.7518 - val_loss: 0.5110 - val_acc: 0.7401Epoch 11/2025000/25000 [==============================] - 18s 731us/step - loss: 0.5052 - acc: 0.7508 - val_loss: 0.5126 - val_acc: 0.7415Epoch 12/2025000/25000 [==============================] - 18s 736us/step - loss: 0.5003 - acc: 0.7578 - val_loss: 0.5114 - val_acc: 0.7400Epoch 13/2025000/25000 [==============================] - 19s 741us/step - loss: 0.4983 - acc: 0.7554 - val_loss: 0.5164 - val_acc: 0.7362Epoch 14/2025000/25000 [==============================] - 23s 925us/step - loss: 0.4976 - acc: 0.7616 - val_loss: 0.5115 - val_acc: 0.7403Epoch 15/2025000/25000 [==============================] - 23s 926us/step - loss: 0.4949 - acc: 0.7599 - val_loss: 0.5118 - val_acc: 0.7401Epoch 16/2025000/25000 [==============================] - 23s 903us/step - loss: 0.4957 - acc: 0.7608 - val_loss: 0.5110 - val_acc: 0.7403Epoch 17/2025000/25000 [==============================] - 23s 926us/step - loss: 0.4919 - acc: 0.7610 - val_loss: 0.5109 - val_acc: 0.7405Epoch 18/2025000/25000 [==============================] - 23s 927us/step - loss: 0.4909 - acc: 0.7622 - val_loss: 0.5107 - val_acc: 0.7408Epoch 19/2025000/25000 [==============================] - 23s 921us/step - loss: 0.4886 - acc: 0.7608 - val_loss: 0.5108 - val_acc: 0.7397Epoch 20/2025000/25000 [==============================] - 23s 917us/step - loss: 0.4895 - acc: 0.7624 - val_loss: 0.5132 - val_acc: 0.7369 参考： http://colah.github.io/posts/2015-08-Understanding-LSTMs/ https://www.jianshu.com/p/9dc9f41f0b29/ https://blog.csdn.net/jmh1996/article/details/83476061 https://www.jiqizhixin.com/articles/2018-10-24-13","link":"/posts/3917903105.html"},{"title":"生成对抗网络（GAN）","text":"GAN基本概念 如何通俗理解GAN？ ​ 生成对抗网络(GAN, Generative adversarial network)自从2014年被Ian Goodfellow提出以来，掀起来了一股研究热潮。GAN由生成器和判别器组成，生成器负责生成样本，判别器负责判断生成器生成的样本是否为真。生成器要尽可能迷惑判别器，而判别器要尽可能区分生成器生成的样本和真实样本。 ​ 在GAN的原作[1]中，作者将生成器比喻为印假钞票的犯罪分子，判别器则类比为警察。犯罪分子努力让钞票看起来逼真，警察则不断提升对于假钞的辨识能力。二者互相博弈，随着时间的进行，都会越来越强。那么类比于图像生成任务，生成器不断生成尽可能逼真的假图像。判别器则判断图像是否是真实的图像，还是生成的图像，二者不断博弈优化。最终生成器生成的图像使得判别器完全无法判别真假。 GAN的形式化表达 ​ 上述例子只是简要介绍了一下GAN的思想，下面对于GAN做一个形式化的，更加具体的定义。通常情况下，无论是生成器还是判别器，我们都可以用神经网络来实现。那么，我们可以把通俗化的定义用下面这个模型来表示： ​ 上述模型左边是生成器G，其输入是\\(z\\)，对于原始的GAN，\\(z\\)是由高斯分布随机采样得到的噪声。噪声\\(z\\)通过生成器得到了生成的假样本。 ​ 生成的假样本与真实样本放到一起，被随机抽取送入到判别器D，由判别器去区分输入的样本是生成的假样本还是真实的样本。整个过程简单明了，生成对抗网络中的“生成对抗”主要体现在生成器和判别器之间的对抗。 GAN的目标函数是什么？ ​ 对于上述神经网络模型，如果想要学习其参数，首先需要一个目标函数。GAN的目标函数定义如下： \\[ \\mathop {\\min }\\limits_G \\mathop {\\max }\\limits_D V(D,G) = {\\rm E}{x\\sim{p{data}(x)}}[\\log D(x)] + {\\rm E}_{z\\sim{p_z}(z)}[\\log (1 - D(G(z)))] \\] ​ 这个目标函数可以分为两个部分来理解： ​ 第一部分：判别器的优化通过\\(\\mathop {\\max}\\limits_D V(D,G)\\)实现，\\(V(D,G)\\)为判别器的目标函数，其第一项\\({\\rm E}{x\\sim{p{data}(x)}}[\\log D(x)]\\)表示对于从真实数据分布 中采用的样本 ,其被判别器判定为真实样本概率的数学期望。对于真实数据分布 中采样的样本，其预测为正样本的概率当然是越接近1越好。因此希望最大化这一项。第二项\\({\\rm E}_{z\\sim{p_z}(z)}[\\log (1 - D(G(z)))]\\)表示：对于从噪声\\(P_z(z)​\\)分布当中采样得到的样本，经过生成器生成之后得到的生成图片，然后送入判别器，其预测概率的负对数的期望，这个值自然是越大越好，这个值越大， 越接近0，也就代表判别器越好。 ​ 第二部分：生成器的优化通过\\(\\mathop {\\min }\\limits_G({\\mathop {\\max }\\limits_D V(D,G)})\\)来实现。注意，生成器的目标不是\\(\\mathop {\\min }\\limits_GV(D,G)\\)，即生成器不是最小化判别器的目标函数，二是最小化判别器目标函数的最大值，判别器目标函数的最大值代表的是真实数据分布与生成数据分布的JS散度(详情可以参阅附录的推导)，JS散度可以度量分布的相似性，两个分布越接近，JS散度越小。 GAN的目标函数和交叉熵有什么区别？ ​ 判别器目标函数写成离散形式即为: \\[ V(D,G)=-\\frac{1}{m}\\sum_{i=1}^{i=m}logD(x^i)-\\frac{1}{m}\\sum_{i=1}^{i=m}log(1-D(\\tilde{x}^i)) \\] ​ 可以看出，这个目标函数和交叉熵是一致的，即判别器的目标是最小化交叉熵损失，生成器的目标是最小化生成数据分布和真实数据分布的JS散度。 [1]: Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014. GAN的Loss为什么降不下去？ ​ 对于很多GAN的初学者在实践过程中可能会纳闷，为什么GAN的Loss一直降不下去。GAN到底什么时候才算收敛？其实，作为一个训练良好的GAN，其Loss就是降不下去的。衡量GAN是否训练好了，只能由人肉眼去看生成的图片质量是否好。不过，对于没有一个很好的评价是否收敛指标的问题，也有许多学者做了一些研究，后文提及的WGAN就提出了一种新的Loss设计方式，较好的解决了难以判断收敛性的问题。下面我们分析一下GAN的Loss为什么降不下去？ ​ 对于判别器而言，GAN的Loss如下： \\[ \\mathop {\\min }\\limits_G \\mathop {\\max }\\limits_D V(D,G) = {\\rm E}{x\\sim{p{data}(x)}}[\\log D(x)] + {\\rm E}_{z\\sim{p_z}(z)}[\\log (1 - D(G(z)))] \\]​ ​ 从\\(\\mathop {\\min }\\limits_G \\mathop {\\max }\\limits_D V(D,G)​\\)可以看出，生成器和判别器的目的相反，也就是说两个生成器网络和判别器网络互为对抗，此消彼长。不可能Loss一直降到一个收敛的状态。 对于生成器，其Loss下降快，很有可能是判别器太弱，导致生成器很轻易的就\"愚弄\"了判别器。 对于判别器，其Loss下降快，意味着判别器很强，判别器很强则说明生成器生成的图像不够逼真，才使得判别器轻易判别，导致Loss下降很快。 ​ 也就是说，无论是判别器，还是生成器。loss的高低不能代表生成器的好坏。一个好的GAN网络，其GAN Loss往往是不断波动的。 ​ 看到这里可能有点让人绝望，似乎判断模型是否收敛就只能看生成的图像质量了。实际上，后文探讨的WGAN，提出了一种新的loss度量方式，让我们可以通过一定的手段来判断模型是否收敛。 生成式模型、判别式模型的区别？ ​ 对于机器学习模型，我们可以根据模型对数据的建模方式将模型分为两大类，生成式模型和判别式模型。如果我们要训练一个关于猫狗分类的模型，对于判别式模型，只需要学习二者差异即可。比如说猫的体型会比狗小一点。而生成式模型则不一样，需要学习猫张什么样，狗张什么样。有了二者的长相以后，再根据长相去区分。具体而言： 生成式模型：由数据学习联合概率分布P(X,Y), 然后由P(Y|X)=P(X,Y)/P(X)求出概率分布P(Y|X)作为预测的模型。该方法表示了给定输入X与产生输出Y的生成关系 判别式模型：由数据直接学习决策函数Y=f(X)或条件概率分布P(Y|X)作为预测模型，即判别模型。判别方法关心的是对于给定的输入X，应该预测什么样的输出Y。 ​ 对于上述两种模型，从文字上理解起来似乎不太直观。我们举个例子来阐述一下，对于性别分类问题，分别用不同的模型来做： ​ 1）如果用生成式模型：可以训练一个模型，学习输入人的特征X和性别Y的关系。比如现在有下面一批数据： Y（性别） 0 1 X（特征） 0 1/4 3/4 1 3/4 1/4 ​ 这个数据可以统计得到，即统计人的特征X=0,1….的时候，其类别为Y=0,1的概率。统计得到上述联合概率分布P(X, Y)后，可以学习一个模型，比如让二维高斯分布去拟合上述数据，这样就学习到了X，Y的联合分布。在预测时，如果我们希望给一个输入特征X，预测其类别，则需要通过贝叶斯公式得到条件概率分布才能进行推断： \\[ P(Y|X)={\\frac{P(X,Y)}{P(X)}}={\\frac{P(X,Y)}{P(X|Y)P(Y)}} \\] ​ 2）如果用判别式模型：可以训练一个模型，输入人的特征X，这些特征包括人的五官，穿衣风格，发型等。输出则是对于性别的判断概率，这个概率服从一个分布，分布的取值只有两个，要么男，要么女，记这个分布为Y。这个过程学习了一个条件概率分布P(Y|X)，即输入特征X的分布已知条件下，Y的概率分布。 ​ 显然，从上面的分析可以看出。判别式模型似乎要方便很多，因为生成式模型要学习一个X，Y的联合分布往往需要很多数据，而判别式模型需要的数据则相对少，因为判别式模型更关注输入特征的差异性。不过生成式既然使用了更多数据来生成联合分布，自然也能够提供更多的信息，现在有一个样本（X,Y）,其联合概率P（X,Y）经过计算特别小，那么可以认为这个样本是异常样本。这种模型可以用来做outlier detection。 什么是mode collapsing? ​ 某个模式(mode)出现大量重复样本，例如： ​ 上图左侧的蓝色五角星表示真实样本空间，黄色的是生成的。生成样本缺乏多样性，存在大量重复。比如上图右侧中，红框里面人物反复出现。 如何解决mode collapsing？ 方法一：针对目标函数的改进方法 ​ 为了避免前面提到的由于优化maxmin导致mode跳来跳去的问题，UnrolledGAN采用修改生成器loss来解决。具体而言，UnrolledGAN在更新生成器时更新k次生成器，参考的Loss不是某一次的loss，是判别器后面k次迭代的loss。注意，判别器后面k次迭代不更新自己的参数，只计算loss用于更新生成器。这种方式使得生成器考虑到了后面k次判别器的变化情况，避免在不同mode之间切换导致的模式崩溃问题。此处务必和迭代k次生成器，然后迭代1次判别器区分开[8]。DRAGAN则引入博弈论中的无后悔算法，改造其loss以解决mode collapse问题[9]。前文所述的EBGAN则是加入VAE的重构误差以解决mode collapse。 方法二：针对网络结构的改进方法 ​ Multi agent diverse GAN(MAD-GAN)采用多个生成器，一个判别器以保障样本生成的多样性。具体结构如下： ​ 相比于普通GAN，多了几个生成器，且在loss设计的时候，加入一个正则项。正则项使用余弦距离惩罚三个生成器生成样本的一致性。 ​ MRGAN则添加了一个判别器来惩罚生成样本的mode collapse问题。具体结构如下： ​ 输入样本\\(x​\\)通过一个Encoder编码为隐变量\\(E(x)​\\)，然后隐变量被Generator重构，训练时，Loss有三个。\\(D_M​\\)和\\(R​\\)（重构误差）用于指导生成real-like的样本。而\\(D_D​\\)则对\\(E(x)​\\)和\\(z​\\)生成的样本进行判别，显然二者生成样本都是fake samples，所以这个判别器主要用于判断生成的样本是否具有多样性，即是否出现mode collapse。 方法三：Mini-batch Discrimination ​ Mini-batch discrimination在判别器的中间层建立一个mini-batch layer用于计算基于L1距离的样本统计量，通过建立该统计量，实现了一个batch内某个样本与其他样本有多接近。这个信息可以被判别器利用到，从而甄别出哪些缺乏多样性的样本。对生成器而言，则要试图生成具有多样性的样本。 GAN的生成能力评价 如何客观评价GAN的生成能力？ ​ 最常见评价GAN的方法就是主观评价。主观评价需要花费大量人力物力，且存在以下问题： 评价带有主管色彩，有些bad case没看到很容易造成误判 如果一个GAN过拟合了，那么生成的样本会非常真实，人类主观评价得分会非常高，可是这并不是一个好的GAN。 因此，就有许多学者提出了GAN的客观评价方法。 Inception Score ​ 对于一个在ImageNet训练良好的GAN，其生成的样本丢给Inception网络进行测试的时候，得到的判别概率应该具有如下特性： - 对于同一个类别的图片，其输出的概率分布应该趋向于一个脉冲分布。可以保证生成样本的准确性。 - 对于所有类别，其输出的概率分布应该趋向于一个均匀分布，这样才不会出现mode dropping等，可以保证生成样本的多样性。 ​ 因此，可以设计如下指标： \\[ IS(P_g)=e^{E_{x\\sim P_g}[KL(p_M(y|x)\\Vert{p_M(y)})]} \\] ​ 根据前面分析，如果是一个训练良好的GAN，\\(p_M(y|x)\\)趋近于脉冲分布，\\(p_M(y)\\)趋近于均匀分布。二者KL散度会很大。Inception Score自然就高。实际实验表明，Inception Score和人的主观判别趋向一致。IS的计算没有用到真实数据，具体值取决于模型M的选择。 ​ 特点：可以一定程度上衡量生成样本的多样性和准确性，但是无法检测过拟合。Mode Score也是如此。不推荐在和ImageNet数据集差别比较大的数据上使用。 Mode Score ​ Mode Score作为Inception Score的改进版本，添加了关于生成样本和真实样本预测的概率分布相似性度量一项。具体公式如下： \\[ MS(P_g)=e^{E_{x\\sim P_g}[KL(p_M(y|x)\\Vert{p_M(y)})-KL(p_M(y)\\Vert p_M(y^*))]} \\] Kernel MMD (Maximum Mean Discrepancy) 计算公式如下： \\[ MMD^2(P_r,P_g)=E_{x_r\\sim{P_r},x_g\\sim{P_g}}[\\lVert\\Sigma_{i=1}^{n1}k(x_r)-\\Sigma_{i=1}^{n2}k(x_g)\\rVert] \\] ​ 对于Kernel MMD值的计算，首先需要选择一个核函数\\(k\\)，这个核函数把样本映射到再生希尔伯特空间(Reproducing Kernel Hilbert Space, RKHS) ，RKHS相比于欧几里得空间有许多优点，对于函数内积的计算是完备的。将上述公式展开即可得到下面的计算公式： \\[ MMD^2(P_r,P_g)=E_{x_r,x_r{'}\\sim{P_r},x_g,x_g{'}\\sim{P_g}}[k(x_r,x_r{'})-2k(x_r,x_g)+k(x_g,x_g{'})] \\] MMD值越小，两个分布越接近。 特点：可以一定程度上衡量模型生成图像的优劣性，计算代价小。推荐使用。 Wasserstein distance ​ Wasserstein distance在最优传输问题中通常也叫做推土机距离。这个距离的介绍在WGAN中有详细讨论。公式如下： \\[ WD(P_r,P_g)=min_{\\omega\\in\\mathbb{R}^{m\\times n}}\\Sigma_{i=1}^n\\Sigma_{i=1}^m\\omega_{ij}d(x_i^r,x_j^g) \\] \\[ s.t. \\Sigma_{i=1}^mw_{i,j}=p_r(x_i^r), \\forall i;\\Sigma_{j=1}^nw_{i,j}=p_g(x_j^g), \\forall j \\] ​ Wasserstein distance可以衡量两个分布之间的相似性。距离越小，分布越相似。 特点：如果特征空间选择合适，会有一定的效果。但是计算复杂度为\\(O(n^3)​\\)太高 Fréchet Inception Distance (FID) ​ FID距离计算真实样本，生成样本在特征空间之间的距离。首先利用Inception网络来提取特征，然后使用高斯模型对特征空间进行建模。根据高斯模型的均值和协方差来进行距离计算。具体公式如下： \\[ FID(\\mathbb P_r,\\mathbb P_g)=\\lVert\\mu_r-\\mu_g\\rVert+Tr(C_r+C_g-2(C_rC_g)^{1/2}) \\] \\(\\mu,C​\\)分别代表协方差和均值。 ​ 特点：尽管只计算了特征空间的前两阶矩，但是鲁棒，且计算高效。 Nearest Neighbor classifier ​ 使用留一法，结合1-NN分类器（别的也行）计算真实图片，生成图像的精度。如果二者接近，则精度接近50%，否则接近0%。对于GAN的评价问题，作者分别用正样本的分类精度，生成样本的分类精度去衡量生成样本的真实性，多样性。 - 对于真实样本\\(x_r\\)，进行1-NN分类的时候，如果生成的样本越真实。则真实样本空间\\(\\mathbb R\\)将被生成的样本\\(x_g\\)包围。那么\\(x_r\\)的精度会很低。 - 对于生成的样本\\(x_g​\\)，进行1-NN分类的时候，如果生成的样本多样性不足。由于生成的样本聚在几个mode，则\\(x_g​\\)很容易就和\\(x_r​\\)区分，导致精度会很高。 特点：理想的度量指标，且可以检测过拟合。 其他评价方法 ​ AIS，KDE方法也可以用于评价GAN，但这些方法不是model agnostic metrics。也就是说，这些评价指标的计算无法只利用：生成的样本，真实样本来计算。 其他常见的生成式模型有哪些？ 什么是自回归模型：pixelRNN与pixelCNN？ ​ 自回归模型通过对图像数据的概率分布\\(p_{data}(x)\\)进行显式建模，并利用极大似然估计优化模型。具体如下： \\[ p_{data}(x)=\\prod_{i=1}^np(x_i|x_1,x_2,...,x_{i-1}) \\] ​ 上述公式很好理解，给定\\(x_1,x_2,...,x_{i-1}\\)条件下，所有\\(p(x_i)\\)的概率乘起来就是图像数据的分布。如果使用RNN对上述依然关系建模，就是pixelRNN。如果使用CNN，则是pixelCNN。具体如下[5]： ​ 显然，不论是对于pixelCNN还是pixelRNN，由于其像素值是一个个生成的，速度会很慢。语音领域大火的WaveNet就是一个典型的自回归模型。 什么是VAE？ ​ PixelCNN/RNN定义了一个易于处理的密度函数，我们可以直接优化训练数据的似然；对于变分自编码器我们将定义一个不易处理的密度函数，通过附加的隐变量\\(z\\)对密度函数进行建模。 VAE原理图如下[6]： ​ 在VAE中，真实样本\\(X\\)通过神经网络计算出均值方差（假设隐变量服从正太分布），然后通过采样得到采样变量\\(Z\\)并进行重构。VAE和GAN均是学习了隐变量\\(z\\)到真实数据分布的映射。但是和GAN不同的是： GAN的思路比较粗暴，使用一个判别器去度量分布转换模块（即生成器）生成分布与真实数据分布的距离。 VAE则没有那么直观，VAE通过约束隐变量\\(z\\)服从标准正太分布以及重构数据实现了分布转换映射\\(X=G(z)\\) 生成式模型对比 自回归模型通过对概率分布显式建模来生成数据 VAE和GAN均是：假设隐变量\\(z\\)服从某种分布，并学习一个映射\\(X=G(z)\\)，实现隐变量分布\\(z\\)与真实数据分布\\(p_{data}(x)\\)的转换。 GAN使用判别器去度量映射\\(X=G(z)\\)的优劣，而VAE通过隐变量\\(z\\)与标准正太分布的KL散度和重构误差去度量。 GAN的改进与优化 如何生成指定类型的图像——条件GAN ​ 条件生成对抗网络（CGAN, Conditional Generative Adversarial Networks）作为一个GAN的改进，其一定程度上解决了GAN生成结果的不确定性。如果在Mnist数据集上训练原始GAN，GAN生成的图像是完全不确定的，具体生成的是数字1，还是2，还是几，根本不可控。为了让生成的数字可控，我们可以把数据集做一个切分，把数字0~9的数据集分别拆分开训练9个模型，不过这样太麻烦了，也不现实。因为数据集拆分不仅仅是分类麻烦，更主要在于，每一个类别的样本少，拿去训练GAN很有可能导致欠拟合。因此，CGAN就应运而生了。我们先看一下CGAN的网络结构： ​ 从网络结构图可以看到，对于生成器Generator，其输入不仅仅是随机噪声的采样z，还有欲生成图像的标签信息。比如对于mnist数据生成，就是一个one-hot向量，某一维度为1则表示生成某个数字的图片。同样地，判别器的输入也包括样本的标签。这样就使得判别器和生成器可以学习到样本和标签之间的联系。Loss如下： \\[ \\mathop {\\min }\\limits_G \\mathop {\\max }\\limits_D V(D,G) = {\\rm E}{x\\sim{p{data}(x)}}[\\log D(x|y)] + {\\rm E}_{z\\sim{p_z}(z)}[\\log (1 - D(G(z|y)))] \\] ​ Loss设计和原始GAN基本一致，只不过生成器，判别器的输入数据是一个条件分布。在具体编程实现时只需要对随机噪声采样z和输入条件y做一个级联即可。 CNN与GAN——DCGAN ​ 前面我们聊的GAN都是基于简单的神经网络\b构建的。可是对于视觉问题，如果使用原始的基于DNN的GAN，则会出现许多问题。如果输入GAN的随机噪声为100维的随机噪声，输出图像为256x256大小。也就是说，要将100维的信息映射为65536维。如果单纯用DNN来实现，\b那么整个模型参数会非常巨大，而且学习难度很大（低维度映射到高维度需要添加许多信息）。因此，DCGAN就出现了。具体而言，DCGAN将传统GAN的生成器，判别器均采用GAN实现，且使用了一下tricks： 将pooling层convolutions替代，其中，在discriminator上用strided convolutions替代，在generator上用fractional-strided convolutions替代。 在generator和discriminator上都使用batchnorm。 移除全连接层，global pooling增加了模型的稳定性，但伤害了收敛速度。 在generator的除了输出层外的所有层使用ReLU，输出层采用tanh。 在discriminator的所有层上使用LeakyReLU。 网络结构图如下： 如何理解GAN中的输入随机噪声？ ​ 为了了解输入随机噪声每一个维度代表的含义，作者做了一个非常有趣的工作。即在隐空间上，假设知道哪几个变量控制着某个物体，那么僵这几个变量挡住是不是就可以将生成图片中的某个物体消失？论文中的实验是这样的：首先，生成150张图片，包括有窗户的和没有窗户的，然后使用一个逻辑斯底回归函数来进行分类，对于权重不为0的特征，认为它和窗户有关。将其挡住，得到新的生成图片，结果如下： 此外，将几个输入噪声进行算数运算，\b可以得到语义上进行算数运算的非常有趣的结果。类似于word2vec。 GAN为什么容易训练崩溃？ ​ 所谓GAN的训练崩溃，指的是训练过程中，生成器和判别器存在一方压倒另一方的情况。 GAN原始判别器的Loss在判别器达到最优的时候，等价于最小化生成分布与真实分布之间的JS散度，由于随机生成分布很难与真实分布有不可忽略的重叠以及JS散度的突变特性，使得生成器面临梯度消失的问题；\b可是如果不把判别器训练到最优，那么生成器优化的目标就失去了意义。因此需要我们小心的平衡二者，要把判别器训练的不好也不坏才行。否则就会出现训练崩溃，得不到想要的结果 WGAN如何解决训练崩溃问题？ ​ WGAN作者提出了使用Wasserstein距离，以解决GAN网络训练过程难以判断收敛性的问题。Wasserstein距离定义如下: \\[ L={\\rm E}{x\\sim{p{data}}(x)}[f_w(x)] - {\\rm E}_{x\\sim{p_g}(x)}[f_w(x)] \\] 通过最小化Wasserstein距离，得到了WGAN的Loss： WGAN生成器Loss：\\(- {\\rm E}_{x\\sim{p_g}(x)}[f_w(x)]​\\) WGAN判别器Loss：\\(L=-{\\rm E}{x\\sim{p{data}}(x)}[f_w(x)] + {\\rm E}_{x\\sim{p_g}(x)}[f_w(x)]\\) 从公式上GAN似乎总是让人摸不着头脑，在代码实现上来说，其实就以下几点： 判别器最后一层去掉sigmoid 生成器和判别器的loss不取log 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c WGAN-GP：带有梯度正则的WGAN ​ 实际实验过程发现，WGAN没有那么好用，主要原因在于WAGN进行梯度截断。梯度截断将导致判别网络趋向于一个二值网络，造成模型容量的下降。 于是作者提出使用梯度惩罚来替代梯度裁剪。公式如下： 由于上式是对每一个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization。实际训练过程中，就可以通过Wasserstein距离来度量模型收敛程度了： 上图纵坐标是Wasserstein距离，横坐标是迭代次数。可以看出，随着迭代的进行，Wasserstein距离趋于收敛，生成图像也趋于稳定。 ​ 由于上式是对每一个梯度进行惩罚，所以不适合使用BN，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择Layer Normalization。实际训练过程中，就可以通过Wasserstein距离来度量模型收敛程度了： ​ 上图纵坐标是Wasserstein距离，横坐标是迭代次数。可以看出，随着迭代的进行，Wasserstein距离趋于收敛，生成图像也趋于稳定。 LSGAN ​ LSGAN（Least Squares GAN）这篇文章主要针对标准GAN的稳定性和图片生成质量不高做了一个改进。作者将原始GAN的交叉熵损失采用最小二乘损失替代。LSGAN的Loss： \\[ \\mathop{\\min }\\limits_DJ(D)=\\mathop{\\min}\\limits_D[{\\frac{1}{2}}{\\rm E}{x\\sim{p{data}}(x)}[D(x)-a]^2 + {\\frac{1}{2}}{\\rm E}_{z\\sim{p_z}(z)}[D(G(z))-b]^2] \\] \\[ \\mathop{\\min }\\limits_GJ(G)=\\mathop{\\min}\\limits_G{\\frac{1}{2}}{\\rm E}_{z\\sim{p_z}(z)}[D(G(z))-c]^2 \\] ​ 实际实现的时候非常简单，最后一层去掉sigmoid，并且计算Loss的时候用平方误差即可。之所以这么做，作者在原文给出了一张图: ​ 上面是作者给出的基于交叉熵损失以及最小二乘损失的Loss函数。横坐标代表Loss函数的输入，纵坐标代表输出的Loss值。可以看出，随着输入的增大，sigmoid交叉熵损失很快趋于0，容易导致梯度饱和问题。如果使用右边的Loss设计，则只在x=0点处饱和。因此使用LSGAN可以很好的解决交叉熵损失的问题。 如何尽量避免GAN的训练崩溃问题？ 归一化图像输入到（-1，1）之间；Generator最后一层使用tanh激活函数 生成器的Loss采用：min (log 1-D)。因为原始的生成器Loss存在梯度消失问题；训练生成器的时候，考虑反转标签，real=fake, fake=real 不要在均匀分布上采样，应该在高斯分布上采样 一个Mini-batch里面必须只有正样本，或者负样本。不要混在一起；如果用不了Batch Norm，可以用Instance Norm 避免稀疏梯度，即少用ReLU，MaxPool。可以用LeakyReLU替代ReLU，下采样可以用Average Pooling或者Convolution + stride替代。上采样可以用PixelShuffle, ConvTranspose2d + stride 平滑标签或者给标签加噪声；平滑标签，即对于正样本，可以使用0.7-1.2的随机数替代；对于负样本，可以使用0-0.3的随机数替代。 给标签加噪声：即训练判别器的时候，随机翻转部分样本的标签。 如果可以，请用DCGAN或者混合模型：KL+GAN，VAE+GAN。 使用LSGAN，WGAN-GP Generator使用Adam，Discriminator使用SGD 尽快发现错误；比如：判别器Loss为0，说明训练失败了；如果生成器Loss稳步下降，说明判别器没发挥作用 不要试着通过比较生成器，判别器Loss的大小来解决训练过程中的模型坍塌问题。比如： While Loss D &gt; Loss A: Train D While Loss A &gt; Loss D: Train A 如果有标签，请尽量利用标签信息来训练 给判别器的输入加一些噪声，给G的每一层加一些人工噪声。 多训练判别器，尤其是加了噪声的时候 对于生成器，在训练，测试的时候使用Dropout GAN的应用（图像翻译） 什么是图像翻译？ ​ GAN作为一种强有力的生成模型，其应用十分广泛。最为常见的应用就是图像翻译。所谓图像翻译，指从一副图像到另一副图像的转换。可以类比机器翻译，一种语言转换为另一种语言。常见的图像翻译任务有： - 图像去噪 - 图像超分辨 - 图像补全 - 风格迁移 - ... 本节将介绍一个经典的图像翻译网络及其改进。图像翻译可以分为有监督图像翻译和\b无监督图像翻译： 有监督图像翻译：原始\b域与目标域存在一一对应数据 无监督图像翻译：原始\b域与目标域不存在一一对应数据 有监督图像翻译：pix2pix ​ 在这篇paper里面，作者提出的框架十分简洁优雅（好用的算法总是简洁优雅的）。相比以往算法的大量专家知识，手工复杂的loss。这篇paper非常粗暴，使用CGAN处理了一系列的转换问题。下面是一些转换示例： ​ 上面展示了许多有趣的结果，比如分割图\\(\\longrightarrow\\)街景图，边缘图\\(\\longrightarrow\\)真实图。对于第一次看到的时候还是很惊艳的，那么这个是怎么做到的呢？我们可以设想一下，如果是我们，我们自己会如何设计这个网络？ 直观的想法？ ​ 最直接的想法就是，设计一个CNN网络，直接建立输入-输出的映射，就像图像去噪问题一样。可是对于上面的问题，这样做会带来一个问题。生成图像质量不清晰。 ​ 拿左上角的分割图\\(\\longrightarrow\\)街景图为例，语义分割图的每个标签比如“汽车”可能对应不同样式，颜色的汽车。那么模型学习到的会是所有不同汽车的评均，这样会造成模糊。 如何解决生成图像的模糊问题？ ​ 这里作者想了一个办法，即加入GAN的Loss去惩罚模型。GAN相比于传统生成式模型可以较好的生成高分辨率图片。思路也很简单，在上述直观想法的基础上加入一个判别器，判断输入图片是否是真实样本。模型示意图如下： ​ 上图模型和CGAN有所不同，但它是一个CGAN，只不过输入只有一个，这个输入就是条件信息。原始的CGAN需要输入随机噪声，以及条件。这里之所有没有输入噪声信息，是因为在实际实验中，如果输入噪声和条件，噪声往往被淹没在条件C当中，所以这里直接省去了。 其他图像翻译的tricks 从上面两点可以得到最终的Loss由两部分构成： - 输出和标签信息的L1 Loss。 GAN Loss 测试也使用Dropout，以使输出多样化 \\[ G^*=arg\\mathop {\\min }\\limits_G \\mathop {\\max }\\limits_D \\Gamma_{cGAN}(G,D)+\\lambda\\Gamma_{L1}(G) \\] ​ 采用L1 Loss而不是L2 Loss的理由很简单，L1 Loss相比于L2 Loss保边缘（L2 Loss基于高斯先验，L1 Loss基于拉普拉斯先验）。 ​ GAN Loss为LSGAN的最小二乘Loss，并使用PatchGAN(进一步保证生成图像的清晰度)。PatchGAN将图像换分成很多个Patch，并对每一个Patch使用判别器进行判别（实际代码实现有更取巧的办法），将所有Patch的Loss求平均作为最终的Loss。 如何生成高分辨率图像和高分辨率视频？ ​ pix2pix提出了一个通用的图像翻译框架。对于高分辨率的图像生成以及高分辨率的视频生成，则需要利用更好的网络结构以及更多的先验只是。pix2pixHD提出了一种多尺度的生成器以及判别器等方式从而生成高分辨率图像。Vid2Vid则在pix2pixHD的基础上利用光流，时序约束生成了高分辨率视频。 有监督的图像翻译的缺点？ ​ 许多图像翻译算法如前面提及的pix2pix系列，需要一一对应的图像。可是在许多应用场景下，往往没有这种一一对应的强监督信息。比如说以下一些应用场景： 以第一排第一幅图为例，要找到这种一一配对的数据是不现实的。因此，无监督图像翻译算法就被引入了。 无监督图像翻译：CycleGAN 模型结构 ​ 总体思路如下，假设有两个域的数据，记为A，B。对于上图第一排第一幅图A域就是普通的马，B域就是斑马。由于A-&gt;B的转换缺乏监督信息，于是，作者提出采用如下方法进行转换： &gt;a. A-&gt;fake_B-&gt;rec_A b. B-&gt;fake_A-&gt;rec_B ​ 对于A域的所有图像，学习一个网络G_B，该网络可以生成B。对于B域的所有图像，也学习一个网络G_A，该网络可以生成G_B。 ​ 训练过程分成两步，首先对于A域的某张图像，送入G_B生成fake_B，然后对fake_B送入G_A，得到重构后的A图像rec_A。对于B域的某一张图像也是类似。重构后的图像rec_A/rec_B可以和原图A/B做均方误差，实现了有监督的训练。此处值得注意的是A-&gt;fake_B(B-&gt;fake_A)和fake_A-&gt;rec_B(fake_B-&gt;rec_A)的网络是一模一样的。下图是形象化的网络结构图： ​ cycleGAN的生成器采用U-Net，判别器采用LS-GAN。 Loss设计 ​ 总的Loss就是X域和Y域的GAN Loss，以及Cycle consistency loss： \\[ L(G,F,D_X,D_Y)=L_{GAN}(G,D_Y,X,Y)+L_{GAN}(F,D_X,Y,X)+\\lambda L_{cycle}(G,F) \\] 整个过程End to end训练，效果非常惊艳，利用这一框架可以完成非常多有趣的任务 多领域的无监督图像翻译：StarGAN cycleGAN模型较好的解决了无监督图像转换问题，可是这种单一域的图像转换还存在一些问题： 要针对每一个域训练一个模型，效率太低。举例来说，我希望可以将橘子转换为红苹果和青苹果。对于cycleGAN而言，需要针对红苹果，青苹果分别训练一个模型。 对于每一个域都需要搜集大量数据，太麻烦。还是以橘子转换为红苹果和青苹果为例。不管是红苹果还是青苹果，都是苹果，只是颜色不一样而已。这两个任务信息是可以共享的，没必要分别训练两个模型。而且针对红苹果，青苹果分别取搜集大量数据太费事。 starGAN则提出了一个多领域的无监督图像翻译框架，实现了多个领域的图像转换，且对于不同领域的数据可以混合在一起训练，提高了数据利用率 GAN的应用（文本生成） GAN为什么不适合文本任务？ ​ GAN在2014年被提出之后，在图像生成领域取得了广泛的研究应用。然后在文本领域却一直没有很惊艳的效果。主要在于文本数据是离散数据，而GAN在应用于离散数据时存在以下几个问题： GAN的生成器梯度来源于判别器对于正负样本的判别。然而，对于文本生成问题，RNN输出的是一个概率序列，然后取argmax。这会导致生成器Loss不可导。还可以站在另一个角度理解，由于是argmax，所以参数更新一点点并不会改变argmax的结果，这也使得GAN不适合离散数据。 GAN只能评估整个序列的loss，但是无法评估半句话，或者是当前生成单词对后续结果好坏的影响。 如果不加argmax，那么由于生成器生成的都是浮点数值，而ground truth都是one-hot encoding，那么判别器只要判别生成的结果是不是0/1序列组成的就可以了。这容易导致训练崩溃。 seqGAN用于文本生成 ​ seqGAN在GAN的框架下，结合强化学习来做文本生成。 模型示意图如下： 在文本生成任务，seqGAN相比较于普通GAN区别在以下几点： 生成器不取argmax。 每生成一个单词，则根据当前的词语序列进行蒙特卡洛采样生成完成的句子。然后将句子送入判别器计算reward。 根据得到的reward进行策略梯度下降优化模型。 GAN在其他领域的应用 数据增广 ​ GAN的良好生成特性近年来也开始被用于数据增广。以行人重识别为例，有许多GAN用于数据增广的工作[1-4]。行人重识别问题一个难点在于不同摄像头下拍摄的人物环境，角度差别非常大，导致存在较大的Domain gap。因此，可以考虑使用GAN来产生不同摄像头下的数据进行数据增广。以论文[1]为例，本篇paper提出了一个cycleGAN用于数据增广的方法。具体模型结构如下： ​ 对于每一对摄像头都训练一个cycleGAN，这样就可以实现将一个摄像头下的数据转换成另一个摄像头下的数据，但是内容（人物）保持不变。 在CVPR19中，[9]进一步提升了图像的生成质量，进行了“淘宝换衣”式的高质量图像生成（如下图），提供了更高质量的行人训练数据。 图像超分辨与图像补全 ​ 图像超分辨与补全均可以作为图像翻译问题，该类问题的处理办法也大都是训练一个端到端的网络，输入是原始图片，输出是超分辨率后的图片，或者是补全后的图片。文献[5]利用GAN作为判别器，使得超分辨率模型输出的图片更加清晰，更符合人眼主管感受。日本早稻田大学研究人员[6]提出一种全局+局部一致性的GAN实现图像补全，使得修复后的图像不仅细节清晰，且具有整体一致性。 语音领域 ​ 相比于图像领域遍地开花，GAN在语音领域则应用相对少了很多。这里零碎的找一些GAN在语音领域进行应用的例子作为介绍。文献[7]提出了一种音频去噪的SEGAN，缓解了传统方法支持噪声种类稀少，泛化能力不强的问题。Donahue利用GAN进行语音增强，提升了ASR系统的识别率。 参考文献 [1] Zheng Z , Zheng L , Yang Y . Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro[C]// 2017 IEEE International Conference on Computer Vision (ICCV). IEEE Computer Society, 2017. [2] Zhong Z , Zheng L , Zheng Z , et al. Camera Style Adaptation for Person Re-identification[J]. 2017. [3] Deng W , Zheng L , Ye Q , et al. Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification[J]. 2017. [4] Wei L , Zhang S , Gao W , et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification[J]. CVPR, 2017. [5] Ledig C , Theis L , Huszar F , et al. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network[J]. CVPR, 2016. [6] Iizuka S , Simo-Serra E , Ishikawa H . Globally and locally consistent image completion[J]. ACM Transactions on Graphics, 2017, 36(4):1-14. [7] Pascual S , Bonafonte A , Serrà, Joan. SEGAN: Speech Enhancement Generative Adversarial Network[J]. 2017. [8] Donahue C , Li B , Prabhavalkar R . Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition[J]. 2017. [9] Zheng, Z., Yang, X., Yu, Z., Zheng, L., Yang, Y., &amp; Kautz, J. Joint discriminative and generative learning for person re-identification. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)[C]. 2019.","link":"/posts/2588142454.html"},{"title":"网络搭建及训练","text":"TensorFlow TensorFlow是什么？ TensorFlow支持各种异构平台，支持多CPU/GPU、服务器、移动设备，具有良好的跨平台的特性；TensorFlow架构灵活，能够支持各种网络模型，具有良好的通用性；此外，TensorFlow架构具有良好的可扩展性，对OP的扩展支持，Kernel特化方面表现出众。 TensorFlow最初由Google大脑的研究员和工程师开发出来，用于机器学习和神经网络方面的研究，于2015.10宣布开源，在众多深度学习框架中脱颖而出，在Github上获得了最多的Star量。 TensorFlow的设计理念是什么？ TensorFlow的设计理念主要体现在两个方面： （1）将图定义和图运算完全分开。 TensorFlow 被认为是一个“符号主义”的库。我们知道，编程模式通常分为命令式编程（imperative style programming）和符号式编程（symbolic style programming）。命令式编程就是编写我们理解的通常意义上的程序，很容易理解和调试，按照原有逻辑执行。符号式编程涉及很多的嵌入和优化，不容易理解和调试，但运行速度相对有所提升。现有的深度学习框架中，Torch 是典型的命令式的，Caffe、MXNet 采用了两种编程模式混合的方法，而 TensorFlow 完全采用符号式编程。 符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量间的计算关系，最后需要对据流图进行编译，但此时的数据流图还是一个空壳儿，里面没有任何实际数据，只有把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。 例如： 12t = 8 + 9print(t) 在传统的程序操作中，定义了 t 的运算，在运行时就执行了，并输出 17。而在 TensorFlow中，数据流图中的节点，实际上对应的是 TensorFlow API 中的一个操作，并没有真正去运行： 12345import tensorflow as tft = tf.add(8,9)print(t)#输出 Tensor{&quot;Add_1:0&quot;,shape={},dtype=int32} （2）TensorFlow 中涉及的运算都要放在图中，而图的运行只发生在会话（session）中。开启会话后，就可以用数据去填充节点，进行运算；关闭会话后，就不能进行计算了。因此，会话提供了操作运行和 Tensor 求值的环境。 例如： 12345678910import tensorflow as tf#创建图a = tf.constant([4.0,5.0])b = tf.constant([6.0,7.0])c = a * b#创建会话sess = tf.Session()#计算cprint(sess.run(c)) #进行矩阵乘法，输出[24.,35.]sess.close() TensorFlow特点有哪些？ 高度的灵活性 TensorFlow 并不仅仅是一个深度学习库，只要可以把你的计算过程表示称一个数据流图的过程，我们就可以使用 TensorFlow 来进行计算。TensorFlow 允许我们用计算图的方式建立计算网络，同时又可以很方便的对网络进行操作。用户可以基于 TensorFlow 的基础上用 python 编写自己的上层结构和库，如果TensorFlow没有提供我们需要的API的，我们也可以自己编写底层的 C++ 代码，通过自定义操作将新编写的功能添加到 TensorFlow 中。 真正的可移植性 TensorFlow 可以在 CPU 和 GPU 上运行，可以在台式机、服务器、移动设备上运行。你想在你的笔记本上跑一下深度学习的训练，或者又不想修改代码，想把你的模型在多个CPU上运行， 亦或想将训练好的模型放到移动设备上跑一下，这些TensorFlow都可以帮你做到。 多语言支持 TensorFlow采用非常易用的python来构建和执行我们的计算图，同时也支持 C++ 的语言。我们可以直接写python和C++的程序来执行TensorFlow，也可以采用交互式的ipython来方便的尝试我们的想法。当然，这只是一个开始，后续会支持更多流行的语言，比如Lua，JavaScript 或者R语言。 4.丰富的算法库 TensorFlow提供了所有开源的深度学习框架里，最全的算法库，并且在不断的添加新的算法库。这些算法库基本上已经满足了大部分的需求，对于普通的应用，基本上不用自己再去自定义实现基本的算法库了。 完善的文档 TensorFlow的官方网站，提供了非常详细的文档介绍，内容包括各种API的使用介绍和各种基础应用的使用例子，也包括一部分深度学习的基础理论。 自从宣布开源以来，大量人员对TensorFlow做出贡献，其中包括Google员工，外部研究人员和独立程序员，全球各地的工程师对TensorFlow的完善，已经让TensorFlow社区变成了Github上最活跃的深度学习框架。 TensorFlow的系统架构是怎样的？ 整个系统从底层到上层可分为七层： 设备层：硬件计算资源，支持CPU、GPU 网络层：支持两种通信协议 数值计算层：提供最基础的计算，有线性计算、卷积计算 高维计算层：数据的计算都是以数组的形式参与计算 计算图层：用来设计神经网络的结构 工作流层：提供轻量级的框架调用 构造层：最后构造的深度学习网络可以通过TensorBoard服务端可视化 TensorFlow编程模型是怎样的？ TensorFlow的编程模型：让向量数据在计算图里流动。那么在编程时至少有这几个过程：1.构建图，2.启动图，3.给图输入数据并获取结果。 构建图 TensorFlow的图的类型是tf.Graph，它包含着计算节点和tensor的集合。 这里引用了两个新概念：tensor和计算节点。 我们先介绍tensor，一开始我们就介绍了，我们需要把数据输入给启动的图才能获取计算结果。那么问题来了，在构建图时用什么表示中间计算结果？这个时候tensor的概念就需要引入了。 类型是tf.Tensor，代表某个计算节点的输出，一定要看清楚是“代表”。它主要有两个作用： 1.构建不同计算节点之间的数据流 2.在启动图时，可以设置某些tensor的值，然后获取指定tensor的值。这样就完成了计算的输入输出功能。 如下代码所示： 12inImage = tf.placeholder(tf.float32,[32,32,3],&quot;inputImage&quot;)processedImage = tf.image.per_image_standardization(inImage,&quot;processedImage&quot;) 这里inImage和processedImage都是tensor类型。它们代表着计算节点输出的数据，数据的值具体是多少在启动图的时候才知道。上面两个方法调用都传递了一个字符串，它是计算节点的名字，最好给节点命名，这样我们可以在图上调用get_tensor_by_name(name)获取对应的tensor对象，十分方便。（tensor名字为“:”） 创建tensor时，需要指定类型和shape。对不同tensor进行计算时要求类型相同，可以使用 tf.cast 进行类型转换。同时也要求 shape (向量维度)满足运算的条件，我们可以使用 tf.reshape 改变shape。 现在了解计算节点的概念，其功能是对tensor进行计算、创建tensor或进行其他操作，类型是tf.Operation。获取节点对象的方法为get_operation_by_name(name)。 构建图，如下代码： 12345678910111213141516g=tf.Graph()with g.as_default(): input_data=tf.placeholder(tf.float32,[None,2],&quot;input_data&quot;) input_label=tf.placeholder(tf.float32,[None,2],&quot;input_label&quot;) W1=tf.Variable(tf.truncated_normal([2,2]),name=&quot;W1&quot;) B1=tf.Variable(tf.zeros([2]),name=&quot;B1&quot;) output=tf.add(tf.matmul(input_data,W1),B1,name=&quot;output&quot;) cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=input_label) train_step=tf.train.AdamOptimizer().minimize(cross_entropy,name=&quot;train_step&quot;) initer=tf.global_variables_initializer() 上面的代码中我们创建了一个图，并在上面添加了很多节点。我们可以通过调用get_default_graph()获取默认的图。 Input_data，input_label，W1，B1，output，cross_entropy都是tensor类型，train_step，initer，是节点类型。 有几类tensor或节点比较重要，下面介绍一下： placeholder Tensorflow，顾名思义， tensor代表张量数据，flow代表流，其最初的设计理念就是构建一张静态的数据流图。图是有各个计算节点连接而成，计算节点之间流动的便是中间的张量数据。要想让张量数据在我们构建的静态计算图中流动起来，就必须有最初的输入数据流。而placeholder，翻译过来叫做占位符，顾名思义，是给我们的输入数据提供一个接口，也就是说我们的一切输入数据，例如训练样本数据，超参数数据等都可以通过占位符接口输送到数据流图之中。使用实例如下代码： 12345678import tensorflow as tfx = tf.placeholder(dtype=tf.float32,shape=[],name='x')y = tf.placeholder(dtpe=tf.float32,shape=[],nmae='y')z = x*ywith tf.Session() as sess: prod = sess.run(z,feed_dict={x:1.,y:5.2}) print(prod)[out]:5.2 variable 无论是传统的机器学习算法，例如线性支持向量机（Support Vector Machine, SVM)，其数学模型为y = &lt;w,x&gt; + b，还是更先进的深度学习算法，例如卷积神经网络（Convolutional Neural Network， CNN）单个神经元输出的模型y = w*x + b。可以看到，w和b就是我们要求的模型，模型的求解是通过优化算法（对于SVM，使用 SMO[1]算法，对于CNN，一般基于梯度下降法）来一步一步更新w和b的值直到满足停止条件。因此，大多数机器学习的模型中的w和b实际上是以变量的形式出现在代码中的，这就要求我们在代码中定义模型变量。 1234567import tensorflow as tfa = tf.Variable(2.)b = tf.Variable(3.)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) #变量初始化 print(sess.run(a*b))[out]:6. [1] Platt, John. \"Sequential minimal optimization: A fast algorithm for training support vector machines.\" (1998). initializer 由于tensorflow构建的是静态的计算流图，在开启会话之前，所有的操作都不会被执行。因此为了执行在计算图中所构建的赋值初始化计算节点，需要在开启会话之后，在会话环境下运行初始化。如果计算图中定义了变量，而会话环境下为执行初始化命令，则程序报错，代码如下： 1234567import tensorflow as tfa = tf.Variable(2.)b = tf.Variable(3.)with tf.Session() as sess: #sess.run(tf.global_variables_initializer()) #注释掉初始化命令 print(sess.run(a*b))[Error]: Attempting to use uninitialized value Variable 启动图 先了解session的概念，然后才能更好的理解图的启动。 图的每个运行实例都必须在一个session里，session为图的运行提供环境。Session的类型是tf.Session，在实例化session对象时我们需要给它传递一个图对象，如果不显示给出将使用默认的图。Session有一个graph属性，我们可以通过它获取session对应的图。 代码如下： 12345678910111213141516171819numOfBatch=5datas=np.zeros([numOfBatch,2],np.float32)labels=np.zeros([numOfBatch,2],np.float32)sess=tf.Session(graph=g)graph=sess.graphsess.run([graph.get_operation_by_name(&quot;initer&quot;)])dataHolder=graph.get_tensor_by_name(&quot;input_data:0&quot;)labelHolder=graph.get_tensor_by_name(&quot;input_label:0&quot;)train=graph.get_operation_by_name(&quot;train_step&quot;)out=graph.get_tensor_by_name(&quot;output:0&quot;)for i inrange(200): result=sess.run([out,train],feed_dict={dataHolder:datas,labelHolder:labels}) if i%100==0: saver.save(sess,&quot;./moules&quot;)sess.close() 代码都比较简单，就不介绍了。不过要注意2点：1.别忘记运行初始化节点，2.别忘记close掉session对象以释放资源。 给图输入数据并获取结果 代码： 12for i inrange(200): result=sess.run([out,train],feed_dict={dataHolder:datas,labelHolder:labels}) 这里主要用到了session对象的run方法，它用来运行某个节点或tensor并获取对应的值。我们一般会一次传递一小部分数据进行mini-batch梯度下降来优化模型。 我们需要把我们需要运行的节点或tensor放入一个列表，然后作为第一个参数(不考虑self)传递给run方法，run方法会返回一个计算结果的列表，与我们传递的参数一一对应。 如果我们运行的节点依赖某个placeholder，那我们必须给这个placeholder指定值，怎么指定代码里面很清楚，给关键字参数feed_dict传递一个字典即可，字典里的元素的key是placeholder对象，value是我们指定的值。值的数据的类型必须和placeholder一致，包括shape。值本身的类型是numpy数组。 这里再解释一个细节，在定义placeholder时代码如下： 12input_data=tf.placeholder(tf.float32,[None,2],&quot;input_data&quot;)input_label=tf.placeholder(tf.float32,[None,2],&quot;input_label&quot;) shape为[None,2]，说明数据第一个维度是不确定的，然后TensorFlow会根据我们传递的数据动态推断第一个维度，这样我们就可以在运行时改变batch的大小。比如一个数据是2维，一次传递10个数据对应的tensor的shape就是[10,2]。可不可以把多个维度指定为None？理论上不可以！ 如何基于tensorflow搭建VGG16 ​ 介绍完关于tensorflow的基础知识，是时候来一波网络搭建实战了。虽然网上有很多相关教程，但我想从最标准的tensorflow代码和语法出发（而不是调用更高级的API，失去了原来的味道），向大家展示如何搭建其标准的VGG16网络架构。话不多说，上代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import numpy as npimport tensorflow as tfdef get_weight_variable(shape): return tf.get_variable('weight', shape=shape, initializer=tf.truncated_normal_initializer(stddev=0.1))def get_bias_variable(shape): return tf.get_variable('bias', shape=shape, initializer=tf.constant_initializer(0))def conv2d(x, w, padding = 'SAME', s=1): x = tf.nn.conv2d(x, w, strides=[1, s, s, 1], padding = padding) return xdef maxPoolLayer(x): return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')def conv2d_layer(x,in_chs, out_chs, ksize, layer_name): with tf.variable_scope(layer_name): w = get_weight_variable([ksize, ksize, in_chs, out_chs]) b = get_bias_variable([out_chs]) y = tf.nn.relu(tf.bias_add(conv2d(x,w,padding = 'SAME', s=1), b)) return ydef fc_layer(x,in_kernels, out_kernels, layer_name): with tf.variable_scope(layer_name): w = get_weight_variable([in_kernels,out_kernels]) b = get_bias_variable([out_kernels]) y = tf.nn.relu(tf.bias_add(tf.matmul(x,w),b)) return y def VGG16(x): conv1_1 = conv2d_layer(x,tf.get_shape(x).as_list()[-1], 64, 3, 'conv1_1') conv1_2 = conv2d_layer(conv1_1,64, 64, 3, 'conv1_2') pool_1 = maxPoolLayer(conv1_2) conv2_1 = conv2d_layer(pool1,64, 128, 3, 'conv2_1') conv2_2 = conv2d_layer(conv2_1,128, 128, 3, 'conv2_2') pool2 = maxPoolLayer(conv2_2) conv3_1 = conv2d_layer(pool2,128, 256, 3, 'conv3_1') conv3_2 = conv2d_layer(conv3_1,256, 256, 3, 'conv3_2') conv3_3 = conv2d_layer(conv3_2,256, 256, 3, 'conv3_3') pool3 = maxPoolLayer(conv3_3) conv4_1 = conv2d_layer(pool3,256, 512, 3, 'conv4_1') conv4_2 = conv2d_layer(conv4_1,512, 512, 3, 'conv4_2') conv4_3 = conv2d_layer(conv4_2,512, 512, 3, 'conv4_3') pool4 = maxPoolLayer(conv4_3) conv5_1 = conv2d_layer(pool4,512, 512, 3, 'conv5_1') conv5_2 = conv2d_layer(conv5_1,512, 512, 3, 'conv5_2') conv5_3 = conv2d_layer(conv5_1,512, 512, 3, 'conv5_3') pool5 = maxPoolLayer(conv5_3) pool5_flatten_dims = int(np.prod(pool5.get_shape().as_list()[1:])) pool5_flatten = tf.reshape(pool5,[-1,pool5_flatten_dims]) fc_6 = fc_layer(pool5_flatten, pool5_flatten_dims, 4096, 'fc6') fc_7 = fc_layer(fc_6, 4096, 4096, 'fc7') fc_8 = fc_layer(fc_7, 4096, 10, 'fc8') return fc_8 Pytorch Pytorch是什么？ Pytorch是torch的python版本，是由Facebook开源的神经网络框架，专门针对 GPU 加速的深度神经网络（DNN）编程。Torch 是一个经典的对多维矩阵数据进行操作的张量（tensor ）库，在机器学习和其他数学密集型应用有广泛应用。与Tensorflow的静态计算图不同，pytorch的计算图是动态的，可以根据计算需要实时改变计算图。但由于Torch语言采用 Lua，导致在国内一直很小众，并逐渐被支持 Python 的 Tensorflow 抢走用户。作为经典机器学习库 Torch 的端口，PyTorch 为 Python 语言使用者提供了舒适的写代码选择。 为什么选择 Pytorch？ 简洁： PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像 TensorFlow 中充斥着session、graph、operation、name_scope、variable、tensor、layer等全新的概念，PyTorch 的设计遵循tensor→variable(autograd)→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。 简洁的设计带来的另外一个好处就是代码易于理解。PyTorch的源码只有TensorFlow的十分之一左右，更少的抽象、更直观的设计使得PyTorch的源码十分易于阅读。 速度： PyTorch 的灵活性不以速度为代价，在许多评测中，PyTorch 的速度表现胜过 TensorFlow和Keras 等框架。框架的运行速度和程序员的编码水平有极大关系，但同样的算法，使用PyTorch实现的那个更有可能快过用其他框架实现的。 易用： PyTorch 是所有的框架中面向对象设计的最优雅的一个。PyTorch的面向对象的接口设计来源于Torch，而Torch的接口设计以灵活易用而著称，Keras作者最初就是受Torch的启发才开发了Keras。PyTorch继承了Torch的衣钵，尤其是API的设计和模块的接口都与Torch高度一致。PyTorch的设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法，即所思即所得，不需要考虑太多关于框架本身的束缚。 活跃的社区： PyTorch 提供了完整的文档，循序渐进的指南，作者亲自维护的论坛 供用户交流和求教问题。Facebook 人工智能研究院对 PyTorch 提供了强力支持，作为当今排名前三的深度学习研究机构，FAIR的支持足以确保PyTorch获得持续的开发更新，不至于像许多由个人开发的框架那样昙花一现。 PyTorch 的架构是怎样的？ PyTorch(Caffe2) 通过混合前端，分布式训练以及工具和库生态系统实现快速，灵活的实验和高效生产。PyTorch 和 TensorFlow 具有不同计算图实现形式，TensorFlow 采用静态图机制(预定义后再使用)，PyTorch采用动态图机制(运行时动态定义)。PyTorch 具有以下高级特征： 混合前端:新的混合前端在急切模式下提供易用性和灵活性，同时无缝转换到图形模式，以便在C ++运行时环境中实现速度，优化和功能。 分布式训练:通过利用本地支持集合操作的异步执行和可从Python和C ++访问的对等通信，优化了性能。 Python优先: PyTorch为了深入集成到Python中而构建的，因此它可以与流行的库和Cython和Numba等软件包一起使用。 丰富的工具和库:活跃的研究人员和开发人员社区建立了丰富的工具和库生态系统，用于扩展PyTorch并支持从计算机视觉到强化学习等领域的开发。 本机ONNX支持:以标准ONNX（开放式神经网络交换）格式导出模型，以便直接访问与ONNX兼容的平台，运行时，可视化工具等。 C++前端：C++前端是PyTorch的纯C++接口，它遵循已建立的Python前端的设计和体系结构。它旨在实现高性能，低延迟和裸机C++应用程序的研究。 使用GPU和CPU优化的深度学习张量库。 Pytorch 与 tensorflow 之间的差异在哪里？ 上面也将了PyTorch 最大优势是建立的神经网络是动态的, 对比静态的 Tensorflow, 它能更有效地处理一些问题, 比如说 RNN 变化时间长度的输出。各有各的优势和劣势。两者都是大公司发布的, Tensorflow（Google）宣称在分布式训练上下了很大的功夫, 那就默认 Tensorflow 在分布式训练上要超出 Pytorch（Facebook），还有tensorboard可视化工具, 但是 Tensorflow 的静态计算图使得在 RNN 上有一点点被动 (虽然它用其他途径解决了), 不过用 PyTorch 的时候, 会对这种动态的 RNN 有更好的理解。而且 Tensorflow 的高度工业化, 它的底层代码很难看懂， Pytorch 好那么一点点, 如果深入 PytorchAPI, 至少能比看 Tensorflow 多看懂一点点 Pytorch 的底层在干啥。 Pytorch有哪些常用工具包？ torch ：类似 NumPy 的张量库，强 GPU 支持 ； torch.autograd ：基于 tape 的自动区别库，支持 torch 之中的所有可区分张量运行； torch.nn ：为最大化灵活性未涉及、与 autograd 深度整合的神经网络库； torch.optim：与 torch.nn 一起使用的优化包，包含 SGD、RMSProp、LBFGS、Adam 等标准优化方式； torch.multiprocessing： python 多进程并发，进程之间 torch Tensors 的内存共享； torch.utils：数据载入器。具有训练器和其他便利功能； torch.legacy(.nn/.optim) ：处于向后兼容性考虑，从 Torch 移植来的 legacy 代码； Caffe 什么是 Caffe？ Caffe的全称应该是Convolutional Architecture for Fast Feature Embedding，它是一个清晰、高效的深度学习框架，它是开源的，核心语言是C++，它支持命令行、Python和Matlab接口，它既可以在CPU上运行也可以在GPU上运行。它的license是BSD 2-Clause。 Caffe的特点是什么？ (1)、模块化：Caffe从一开始就设计得尽可能模块化，允许对新数据格式、网络层和损失函数进行扩展。 (2)、表示和实现分离：Caffe的模型(model)定义是用Protocol Buffer语言写进配置文件的。以任意有向无环图的形式，Caffe支持网络架构。Caffe会根据网络的需要来正确占用内存。通过一个函数调用，实现CPU和GPU之间的切换。 (3)、测试覆盖：在Caffe中，每一个单一的模块都对应一个测试。 (4)、python和Matlab接口：同时提供Python和Matlab接口。 (5)、预训练参考模型：针对视觉项目，Caffe提供了一些参考模型，这些模型仅应用在学术和非商业领域，它们的license不是BSD。 Caffe的设计思想是怎样的？ 基本上，Caffe 沿用了神经网络的一个简单假设----所有的计算都是以layer的形式表示的，layer做的事情就是take一些数据，然后输出一些计算以后的结果，比如说卷积，就是输入一个图像，然后和这一层的参数（filter）做卷积，然后输出卷积的结果。每一个layer需要做两个计算：forward是从输入计算输出，然后backward是从上面给的gradient来计算相对于输入的gradient，只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label），在training的时候，我们可以根据已有的label来计算loss和gradient，然后用gradient来update网络的参数，这个就是Caffe的一个基本流程。 基本上，最简单地用Caffe上手的方法就是先把数据写成Caffe的格式，然后设计一个网络，然后用Caffe提供的solver来做优化看效果如何，如果你的数据是图像的话，可以从现有的网络，比如说alexnet或者googlenet开始，然后做fine tuning，如果你的数据稍有不同，比如说是直接的float vector，你可能需要做一些custom的configuration，Caffe的logistic regression example兴许会很有帮助。 Fine tune方法：fine tuning的想法就是说，在imagenet那么大的数据集上train好一个很牛的网络了，那别的task上肯定也不错，所以我们可以把pretrain的网络拿过来，然后只重新train最后几层，重新train的意思是说，比如我以前需要classify imagenet的一千类，现在我只想识别是狗还是猫，或者是不是车牌，于是我就可以把最后一层softmax从一个40961000的分类器变成一个40962的分类器，这个strategy在应用中非常好使，所以我们经常会先在imagenet上pretrain一个网络，因为我们知道imagenet上training的大概过程会怎么样。 Caffe架构是怎样的？ Caffe的架构与其它的深度学习框架稍微不同，它没有根据算法实现过程的方式来进行编码，而是以系统级的抽象作为整体架构，逐层的封装实现细节，使得上层的架构变得很清晰。Caffe的整体架构如下： SyncedMem 这个类的主要功能是封装CPU和GPU的数据交互操作。一般来说，数据的流动形式都是：硬盘-&gt;CPU内存-&gt;GPU内存-&gt;CPU内存-&gt;（硬盘），所以在写代码的过程中经常会写CPU/GPU之间数据传输的代码，同时还要维护CPU和GPU两个处理端的内存指针。这些事情处理起来不会很难，但是会很繁琐。因此SyncedMem的出现就是把CPU/GPU的数据传输操作封装起来，只需要调用简单的接口就可以获得两个处理端同步后的数据。 Blob Blob是用于存储数据的对象，在Caffe中各种数据(图像输入、模型参数)都是以Blob的形式在网络中传输的，Blob提供统一的存储操作接口，可用来保存训练数据、模型参数等，同时Blob还能在CPU和GPU之间进行同步以支持CPU/GPU的混合运算。 这个类做了两个封装：一个是操作数据的封装，使用Blob可以操纵高维的数据，快速访问其中的数据，变换数据的维度等；另一个是对原始数据和更新量的封装，每一个Blob中都有data和diff两个数据指针，data用于存储原始数据，diff 用于存储反向传播（Backpropagation）的梯度更新值。Blob使用了SyncedMem，这样便于访问不同的处理端。Blob基本实现了整个Caffe数据结构部分的封装，在Net类中可以看到所有的前后向数据和参数都用Blob来表示就足够了。数据的抽象到这个就可以了，接下来作层级的抽象。神经网络的前后向计算可以做到层与层之间完全独立，只要每个层按照一定的接口规则实现，就可以确保整个网络的正确性。 Layer Layer是网络Net的基本单元，也是Caffe中能在外部进行调整的最小网络结构单元，每个Layer都有输入Blob和输出Blob。Layer（层）是Caffe中最庞大最繁杂的模块，它是神经网络的基本计算单元。由于Caffe强调模块化设计，因此只允许每个layer完成一类特定的计算，例如convolution操作、pooling、非线性变换、内积运算，以及数据加载、归一化和损失计算等。Caffe中layer的种类有很多，具体的种类及功能请看官方文档。在创建一个Caffe模型的时候，也是以Layer为基础进行的。Layer是一个父类，它的下面还有各种实现特定功能的子类，例如data_layer，conv_layer，loss_layer等。Layer是通过LayFactory来创建的。 Net Net是一个完整的深度网络，包含输入层、隐藏层、输出层，在Caffe中一般是一个卷积神经网络(Convolution Neural Networ，CNN)。通过定义不同类型的Layer，并用Blob将不同的Layer连接起来，就能产生一个Net。Net将数据Blob和层Layer组合起来做进一步的封装，对外提供了初始化和前后传播的接口，使得整体看上去和一个层的功能类似，但内部的组合可以是多种多样的。值得一提的是，每一层的输入输出数据统一保存在Net中，同时每个层内的参数指针也保存在Net中，不同的层可以通过WeightShare共享相同的参数，因此可以通过配置来实现多个神经网络层之间共享参数的功能。一个Net由多个Layer组成。一个典型的网络从data layer（从磁盘中载入数据）出发到loss layer结束。 Solver 有了Net就可以进行神经网络的前后向传播计算了，但是还缺少神经网络的训练和预测功能，Solver类进一步封装了训练和预测相关的一些功能。它还提供了两个接口：一个是更新参数的接口，继承Solver可以实现不同的参数更新方法，如Momentum，Nesterov，Adagrad等，因此可以使用不同的优化算法。另一个接口是训练过程中每一轮特定状态下的可注入的一些回调函数，在代码中这个回调点的直接使用者就是多GPU训练算法。Solver定义了针对Net网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义Solver能够实现不同的神经网络求解方式。阅读Solver的代码可以了解网络的求解优化过程。Solver是一个父类，它下面还有实现不同优化方法的子类，例如sgd_solver，adagrad_sovler等，Solver是通过SolverFactory来创建的。 Proto caffe.proto位于…/src/caffe/proto目录下，在这个文件夹下还有一个.pb.cc和一个.pb.h文件，这两个文件都是由caffe.proto编译而来的。 在caffe.proto中定义了很多结构化数据，包括： BlobProto、Datum、FillerParameter、NetParameter、SolverParameter、SolverState、LayerParameter、ConcatParameter、ConvolutionParameter、DataParameter、DropoutParameter、HDF5DataParameter、HDF5OutputParameter、ImageDataParameter、InfogainLossParameter、InnerProductParameter、LRNParameter、MemoryDataParameter、PoolingParameter、PowerParameter、WindowDataParameter、V0LayerParameter。 IO 除了上面的东西之外，还需要输入数据和参数。DataReader和DataTransformer帮助准备输入数据，Filler对参数进行初始化，一些Snapshot方法可以对模型进行持久化。 Caffe的有哪些接口？ Caffe深度学习框架支持多种编程接口，包括命令行、Python和Matlab,下面将介绍如何使用这些接口。 Caffe Python接口 Caffe提供 Python 接口，即Pycaffe，具体实现在caffe、python文件夹内。在Python代码中import caffe，可以load models（导入模型）、forward and backward （前向、反向迭代）、handle IO（数据输入输出）、visualize networks（绘制net）和instrument model solving（自定义优化方法)。所有的模型数据、计算参数都是暴露在外、可供读写的。 (1)caffe.Net 是主要接口，负责导入数据、校验数据、计算模型。 (2)caffe.Classsifier 用于图像分类。 (3)caffe.Detector 用于图像检测。 (4)caffe.SGDSolver 是露在外的 solver 的接口。 (5)caffe.io 处理输入输出，数据预处理。 (6)caffe.draw 可视化 net 的结构。 (7)caffe blobs 以 numpy ndarrys 的形式表示，方便而且高效。 Caffe MATLAB接口 MATLAB接口（Matcaffe）在 caffe/matlab 目录的 caffe 软件包。在 matcaffe 的基础上，可将Caffe整合到MATLAB代码中。 MATLAB接口包括： (1)MATLAB 中创建多个网络结构。 (2)网络的前向传播（Forward）与反向传播（Backward）计算。 (3)网络中的任意一层以及参数的存取。 (4)网络参数保存至文件或从文件夹加载。 (5)blob 和 network 形状调整。 (6)网络参数编辑和调整。 (7)创建多个 solvers 进行训练。 (8)从solver 快照（Snapshots）恢复并继续训练。 (9)访问训练网络（Train nets）和测试网络(Test nets)。 (10)迭代后网络交由 MATLAB 控制。 (11)MATLAB代码融合梯度算法。 Caffe 命令行接口 命令行接口 Cmdcaffe 是 Caffe 中用来训练模型、计算得分以及方法判断的工具。Cmdcaffe 存放在 caffe/build/tools 目录下。 caffe train caffe train 命令用于模型学习，具体包括： (1)caffe train 带 solver.prototxt 参数完成配置。 (2)caffe train 带 snapshot mode_iter_1000.solverstate 参数加载 solver snapshot。 (3)caffe train 带 weights 参数 model.caffemodel 完成 Fine-tuning 模型初始化。 caffe test caffe test 命令用于测试运行模型的得分，并且用百分比表示网络输出的最终结果，比如 accuracyhuoloss 作为其结果。测试过程中，显示每个 batch 的得分，最后输出全部 batch 的平均得分值。 caffe time caffe time 命令用来检测系统性能和测量模型相对执行时间，此命令通过逐层计时与同步，执行模型检测。 参考文献： 1.深度学习：Caffe之经典模型讲解与实战/ 乐毅，王斌 网络搭建有什么原则？ 新手原则。 刚入门的新手不建议直接上来就开始搭建网络模型。比较建议的学习顺序如下： - 1.了解神经网络工作原理，熟悉基本概念及术语。 - 2.阅读经典网络模型论文+实现源码(深度学习框架视自己情况而定)。 - 3.找数据集动手跑一个网络，可以尝试更改已有的网络模型结构。 - 4.根据自己的项目需要设计网络。 深度优先原则。 通常增加网络深度可以提高准确率，但同时会牺牲一些速度和内存。但深度不是盲目堆起来的，一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率，如果浅层都学不到东西，深了也没效果。 卷积核size一般为奇数。 卷积核为奇数有以下好处： - 1 保证锚点刚好在中间，方便以 central pixel为标准进行滑动卷积，避免了位置信息发生偏移 。 - 2 保证在填充（Padding）时，在图像之间添加额外的零层，图像的两边仍然对称。 卷积核不是越大越好。 AlexNet中用到了一些非常大的卷积核，比如11×11、5×5卷积核，之前人们的观念是，卷积核越大，感受野越大，看到的图片信息越多，因此获得的特征越好。但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。于是在VGG、Inception网络中，利用2个3×3卷积核的组合比1个5×5卷积核的效果更佳，同时参数量（3×3×2+1=19&lt;26=5×5×1+1）被降低，因此后来3×3卷积核被广泛应用在各种模型中。 有哪些经典的网络模型值得我们去学习的？ 提起经典的网络模型就不得不提起计算机视觉领域的经典比赛：ILSVRC .其全称是 ImageNet Large Scale Visual Recognition Challenge.正是因为ILSVRC 2012挑战赛上的AlexNet横空出世，使得全球范围内掀起了一波深度学习热潮。这一年也被称作“深度学习元年”。而在历年ILSVRC比赛中每次刷新比赛记录的那些神经网络也成为了人们心中的经典，成为学术界与工业届竞相学习与复现的对象，并在此基础上展开新的研究。 序号 年份 网络名称 获得荣誉 1 2012 AlexNet ILSVRC图像分类冠军 2 2014 VGGNet ILSVRC图像分类亚军 3 2014 GoogLeNet ILSVRC图像分类冠军 4 2015 ResNet ILSVRC图像分类冠军 5 2017 SeNet ILSVRC图像分类冠军 1 AlexNet 论文:ImageNet Classification with Deep Convolutional Neural Networks 代码实现:tensorflow 主要特点： &gt; - 1.第一次使用非线性激活函数ReLU。 &gt; - 2.增加防加过拟合方法：Droupout层,提升了模型鲁棒性。 &gt; - 3.首次使用数据增强。 &gt; - 4.首次使用GPU加速运算。 2 VGGNet 论文:Very Deep Convolutional Networks for Large-Scale Image Recognition 代码实现:tensorflow 主要特点： &gt; - 1.网络结构更深。 &gt; - 2.普遍使用小卷积核。 3 GoogLeNet 论文:Going Deeper with Convolutions 代码实现:tensorflow 主要特点： &gt; - 1.增强卷积模块功能。 &gt;主要的创新在于他的Inception，这是一种网中网（Network In Network）的结构，即原来的结点也是一个网络。Inception一直在不断发展，目前已经V2、V3、V4。其中1*1卷积主要用来降维，用了Inception之后整个网络结构的宽度和深度都可扩大，能够带来2-3倍的性能提升。 &gt; - 2.连续小卷积代替大卷积，保证感受野不变的同时，减少了参数数目。 4 ResNet 论文:Deep Residual Learning for Image Recognition 代码实现:tensorflow 主要特点: &gt; 解决了“退化”问题，即当模型的层次加深时，错误率却提高了。 5 SeNet 论文:Squeeze-and-Excitation Networks 代码实现:tensorflow 主要特点: &gt; 提出了feature recalibration，通过引入 attention 重新加权，可以得到抑制无效特征，提升有效特征的权重，并很容易地和现有网络结合，提升现有网络性能，而计算量不会增加太多。 CV领域网络结构演进历程： ILSVRC挑战赛历年冠军: 此后，ILSVRC挑战赛的名次一直是衡量一个研究机构或企业技术水平的重要标尺。 ILSVRC 2017 已是最后一届举办.2018年起，将由WebVision竞赛（Challenge on Visual Understanding by Learning from Web Data）来接棒。因此，即使ILSVRC挑战赛停办了，但其对深度学习的深远影响和巨大贡献，将永载史册。 网络训练有哪些技巧吗？ 合适的数据集。 1 没有明显脏数据(可以极大避免Loss输出为NaN)。 2 样本数据分布均匀。 合适的预处理方法。 关于数据预处理，在Batch Normalization未出现之前预处理的主要做法是减去均值，然后除去方差。在Batch Normalization出现之后，减均值除方差的做法已经没有必要了。对应的预处理方法主要是数据筛查、数据增强等。 网络的初始化。 网络初始化最粗暴的做法是参数赋值为全0，这是绝对不可取的。因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的，这可能会直接导致模型失效，无法收敛。吴恩达视频中介绍的方法是将网络权重初始化均值为0、方差为1符合的正态分布的随机数据。 小规模数据试练。 在正式开始训练之前，可以先用小规模数据进行试练。原因如下： - 1 可以验证自己的训练流程对否。 - 2 可以观察收敛速度，帮助调整学习速率。 - 3 查看GPU显存占用情况，最大化batch_size(前提是进行了batch normalization，只要显卡不爆，尽量挑大的)。 设置合理Learning Rate。 1 太大。Loss爆炸、输出NaN等。 2 太小。收敛速度过慢，训练时长大大延长。 3 可变的学习速率。比如当输出准确率到达某个阈值后，可以让Learning Rate减半继续训练。 损失函数 损失函数主要分为两大类:分类损失和回归损失 &gt;1.回归损失： &gt; &gt;&gt; - 1 均方误差(MSE 二次损失 L2损失) &gt;它是我们的目标变量与预测值变量差值平方。 &gt;&gt; - 2 平均绝对误差(MAE L1损失) &gt;它是我们的目标变量与预测值变量差值绝对值。 &gt;关于MSE与MAE的比较。MSE更容易解决问题，但是MAE对于异常值更加鲁棒。更多关于MAE和MSE的性能，可以参考L1vs.L2 Loss Function 2.分类损失： &gt; - 1 交叉熵损失函数。 是目前神经网络中最常用的分类目标损失函数。 &gt; - 2 合页损失函数 &gt;合页损失函数广泛在支持向量机中使用，有时也会在损失函数中使用。缺点:合页损失函数是对错误越大的样本施以更严重的惩罚，但是这样会导致损失函数对噪声敏感。","link":"/posts/2865950339.html"},{"title":"面向对象高级编程","text":"前面一章介绍了OOP最基础的数据封装、继承和多态3个概念，还有一些类和实例的操作。而在Python中，OOP还有很多更高级的特性，这一章会讨论多重继承、定制类、元类等概念。 使用 __slots__ 动态绑定属性 正常情况下，当我们定义了一个类，创建了一个类的实例后，我们可以给这个实例绑定任何属性和方法，这就是动态语言的灵活性。先定义类： 12class Student(object): pass 然后，创建实例并给这个实例绑定一个属性： 1234&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name = 'Michael' # 动态给实例绑定一个属性&gt;&gt;&gt; print(s.name)Michael 动态绑定方法 还可以尝试给实例绑定一个方法： 12345678&gt;&gt;&gt; def set_age(self, age): # 定义一个函数... self.age = age...&gt;&gt;&gt; from types import MethodType&gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 把函数绑定到实例上，变为实例的方法&gt;&gt;&gt; s.set_age(25) # 调用实例方法&gt;&gt;&gt; s.age # 测试结果25 注意到这里使用types模块的 MethodType() 函数来给实例绑定方法，为什么要用 MethodType() 而不是直接用 s.set_age = set_age 直接绑定呢？这是因为我们采用后者绑定时，只是绑定了一个外部函数，它与实例本身没有任何关联，没法使用self变量，而使用 MethodType() 就会真正地为实例绑定一个方法，也因此绑定的函数的第一个参数要设置为self变量。做个对比： 12345678910111213&gt;&gt;&gt; s.set_age = set_age # 直接绑定&gt;&gt;&gt; s.set_age(25) # 无法调用self变量Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: set_age() missing 1 required positional argument: 'age'&gt;&gt;&gt; s.set_age(s,25) # 必须显式地传入实例s自身&gt;&gt;&gt; s.age25&gt;&gt;&gt; from types import MethodType&gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 使用MethodType绑定&gt;&gt;&gt; s.set_age(30) # 可以调用self变量，只需传入一个参数&gt;&gt;&gt; s.age30 但是，给一个实例绑定的方法，对另一个实例是不起作用的： 12345&gt;&gt;&gt; s2 = Student() # 创建新的实例&gt;&gt;&gt; s2.set_age(25) # 尝试调用方法Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute 'set_age' 为了给所有实例都绑定方法，可以直接给类绑定方法： 1234&gt;&gt;&gt; def set_score(self, score):... self.score = score...&gt;&gt;&gt; Student.set_score = set_score 给类绑定方法不需要使用 MethodType() 函数，并且所有实例均可调用绑定在类上的方法： 123456&gt;&gt;&gt; s.set_score(100)&gt;&gt;&gt; s.score100&gt;&gt;&gt; s2.set_score(99)&gt;&gt;&gt; s2.score99 通常情况下，上面的 set_score 定义在类中，但动态绑定允许我们在程序运行的过程中动态给类加上功能，这在静态语言中很难实现。 限制可绑定的属性/方法 上面两个小节介绍了怎样绑定属性和方法，但是如果我们想要限制可以绑定到实例的属性/方法怎么办呢？比方说，只允许对Student类的实例绑定 name 和 age 属性。 为了达到限制的目的，Python允许在定义类的时候，定义一个特殊的 __slots__ 变量，来限制该类实例能添加的属性： 12&gt;&gt;&gt; class Student(object):... `__slots__` = ('name', 'age') # 用tuple定义允许绑定的属性名称 然后，我们试试： 1234567&gt;&gt;&gt; s = Student() # 创建新的实例&gt;&gt;&gt; s.name = 'Michael' # 绑定属性'name'&gt;&gt;&gt; s.age = 25 # 绑定属性'age'&gt;&gt;&gt; s.score = 99 # 绑定属性'score'Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute 'score' 由于属性 score 没有被放到 __slots__ 变量中，所以实例不能绑定 score 属性，试图绑定 score 将得到 AttributeError 错误。 使用 __slots__ 要注意，__slots__ 变量的属性限制仅对当前类的实例起作用，对继承的子类是不起作用的： 12345&gt;&gt;&gt; class GraduateStudent(Student):... pass...&gt;&gt;&gt; s1 = GraduateStudent()&gt;&gt;&gt; s1.score = 9999 # 可以绑定任何属性 但是！如果在子类中也定义 __slots__ ，则子类实例允许定义的属性就既包括自身的 __slots__ 也包括父类的 __slots__ ： 1234567891011&gt;&gt;&gt; class GraduateStudent(Student):... __slots__ = ('score')...&gt;&gt;&gt; s2 = GraduateStudent()&gt;&gt;&gt; s2.name = 'Angela'&gt;&gt;&gt; s2.age = 17&gt;&gt;&gt; s2.score = 99&gt;&gt;&gt; s2.sex = 'Female' # 无法绑定父类__slots__和当前类__slots__都没有的属性Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'GraduateStudent' object has no attribute 'sex' 使用@property 为何需要@property 在绑定属性时，如果我们直接把属性暴露出去供使用者修改，虽然写起来很简单，但是没办法检查设置的属性值是否合理，可以把成绩随便改： 12s = Student()s.score = 9999 # 直接通过属性修改 这显然不合逻辑。为了限制 score 的范围，可以通过一个 set_score() 方法来设置成绩，再通过一个 get_score() 来获取成绩，这样，在 set_score() 方法里，就可以检查参数： 123456789class Student(object): def get_score(self): return self._score def set_score(self, value): if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value 现在，对任意的Student实例进行操作，就不能随心所欲地设置score了： 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.set_score(60) # 通过类的方法修改&gt;&gt;&gt; s.get_score()60&gt;&gt;&gt; s.set_score(9999)Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 但是，通过类的方法修改，调用者使用时比较麻烦，没有直接使用属性进行修改简单，而且对调用者是否自觉也有要求，如果调用者依然直接使用属性修改，就没法检查属性值了。 有没有既能检查属性值，又可以直接使用属性修改的办法呢？答案是有的！ 如何实现@property 在第四章-函数式编程中，我们学习到了装饰器（decorator），它可以给函数动态添加功能。事实上，不仅是对函数，装饰器对类的方法一样起作用。Python内置的 @property 装饰器就可以帮助我们实现前面的需求，把一个方法变成属性调用： 1234567891011class Student(object): @property def score(self): # 对应getter方法，也即前面例子的get_score(self) return self._score @score.setter def score(self, value): # 对应setter方法，也即前面例子的set_score(self, value) if not isinstance(value, int): raise ValueError('score must be an integer!') if value &lt; 0 or value &gt; 100: raise ValueError('score must between 0 ~ 100!') self._score = value @property 的实现比较复杂。准确地说，把一个getter方法变成属性，只需要加上 @property 装饰器就可以了，而把一个setter方法变成属性赋值，这要加上一个 @score.setter 装饰器，也即 @属性名.setter。注意！属性名和方法名一定要区分开，否则会出错！这里我们把 score 属性改为 _score 属性，所以对内部来说 _score 是属性，score 是方法，对外部来说 score 是属性，_score 被封装起来了（因为我们使用了装饰器进行转换）。看看实际效果： 12345678&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.score = 60 # 实际转化为s.score(60)&gt;&gt;&gt; s.score # 实际转化为s.score()60&gt;&gt;&gt; s.score = 9999 # 对外部来说可以直接使用属性赋值，同时也能检查属性值Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性，只读属性只能获取属性值，无法设置属性值： 12345678910class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): # 只读属性age，根据birth进行计算 return 2015 - self._birth 上面的birth是可读写属性，而age就是一个只读属性：。 12345678910&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.birth = 2000 # 可读写属性birth可以进行赋值&gt;&gt;&gt; s.birth2000&gt;&gt;&gt; s.age16&gt;&gt;&gt; s.age = 17 # 只读属性age无法进行赋值Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: can't set attribute 注意必须先对属性 birth 进行赋值，然后才可以访问 birth 和 age，否则就会出现： 12345678910&gt;&gt;&gt; s.birthTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 4, in birthAttributeError: 'Student' object has no attribute '_birth'&gt;&gt;&gt; s.ageTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 10, in ageAttributeError: 'Student' object has no attribute '_birth' 练习 请利用 @property 给一个 Screen 对象加上 width 和 height 属性，以及一个只读属性 resolution： 12345678910111213141516class Screen(object): @property def width(self): return self._width @width.setter def width(self, value): self._width = value @property def height(self): return self._height @height.setter def height(self, value): self._height = value @property def resolution(self): return self._width * self._height 测试： 123456&gt;&gt;&gt; s = Screen()&gt;&gt;&gt; s.width = 1024&gt;&gt;&gt; s.height = 768&gt;&gt;&gt; print(s.resolution)786432&gt;&gt;&gt; assert s.resolution == 786432, '1024 * 768 = %d ?' % s.resolution 小结 @property 广泛应用在类的定义中，可以让调用者写出简短的代码，同时又保证了对属性值进行必要的检查，这样，程序运行时就减少了出错的可能性。 多重继承 为何需要多重继承 在第六章-面向对象编程中，我们学习了面向对象编程的一个重要性质——继承。通过继承，子类可以获得父类的所有功能并进行进一步扩展。 假设我们设计了一个 Animal 类，并要为以下4种动物设计四个新的类： Dog - 狗狗； Bat - 蝙蝠； Parrot - 鹦鹉； Ostrich - 鸵鸟。 如果把这些动物按照哺乳动物和鸟类分类，我们可以设计出这样的类的层次： MixIn1 但是如果按照能跑的和能飞的来分类，则设计出这样的类的层次就变为： MixIn2 但是，如果要把上面的两种分类方法都包含进来，我们就得设计更多的层次了。哺乳类要分为能跑的哺乳类和能飞的哺乳类，鸟类也要能跑的鸟类，能飞的鸟类。这么一来，类的层次变得很复杂了： MixIn3 如果再增加更多的分类方式（例如：宠物和非宠物），那么类的数量会呈指数增长，这样设计就显得很不实用了。 使用多重继承 分析一下前面的设计方法，其实之所以会造成类的数量呈指数增长，是因为每个类只能继承一个类，这就造成了很多不必要的重复实现。解决方法是采用多重继承。比方说设计为： MixIn4 因为能跑和能飞这两个类不受限于动物类，它们是独立的。我们单独实现这两个类，即使要再实现其他非动物的类，比如汽车和飞机，也能很轻松地继承它们的功能，而不需要再重复构造功能类似的新的类。而动物分类方面，假设我们加入宠物非宠物的分类，也不需再构造哺乳的能飞的宠物、鸟类的能飞的宠物等等类别，通过多重继承免去了很多麻烦。先进行动物分类的定义： 123456789class Animal(object): pass# 大类:class Mammal(Animal): passclass Bird(Animal): pass 接下来定义好 Runnable 和 Flyable 的类： 1234567class Runnable(object): def run(self): print('Running...')class Flyable(object): def fly(self): print('Flying...') 对于需要 Runnable 功能的动物，只需要多继承一个 Runnable，例如 Dog 12class Dog(Mammal, Runnable): pass 对于需要 Flyable 功能的动物，只需要多继承一个 Flyable，例如 Bat： 12class Bat(Mammal, Flyable): pass 通过多重继承，一个子类可以同时获得多个父类的所有功能。 MixIn 在设计类的继承关系时，通常主线都是单一继承下来的，例如，Ostrich 继承自 Bird。但是，如果需要混入额外的功能，通过多重继承就可以实现，比如，让 Ostrich 除了继承自 Bird 外，再同时继承 Runnable。种（利用多重继承混入额外的功能）这种设计方式通常称之为MixIn。 为了更好地看出继承关系，我们通常把用于添加额外功能的类命名带上一个后缀MixIn，例如把 Runnable 和 Flyable 改为 RunnableMixIn 和 FlyableMixInn 和 植食动物 HerbivoresMixIn，让某个动物同时拥有好几个MixIn： 12class Dog(Mammal, RunnableMixIn, CarnivorousMixIn): pass MixIn的目的就是给一个类增加多个功能，这样，在设计类的时候，我们可以优先考虑通过多重继承来组合多个MixIn的功能，而不是设计多层次的复杂的继承关系。 Python自带的很多库也使用了MixIn。举个例子，Python自带了 TCPServer 和 UDPServer 这两类网络服务，而要同时服务多个用户就必须使用多进程或多线程模型，这两种模型由 ForkingMixIn 和 ThreadingMixIn 提供。通过组合，我们就可以创造出合适的服务来。 比如，编写一个多进程模式的TCP服务，定义如下： 12class MyTCPServer(TCPServer, ForkingMixIn): pass 编写一个多线程模式的UDP服务，定义如下： 12class MyUDPServer(UDPServer, ThreadingMixIn): pass 如果你打算搞一个更先进的协程模型，可以编写一个 CoroutineMixIn： 12class MyTCPServer(TCPServer, CoroutineMixIn): pass 这样一来，我们不需要复杂而庞大的继承链，只要选择组合不同的类的功能，就可以快速构造出所需的子类。 小结 由于Python允许使用多重继承，因此，MixIn就是一种常见的设计。 只允许单一继承的语言（如Java）不能使用MixIn的设计。 定制类 在前面的章节中，我们知道了可以用 __slots__ 变量限制可绑定的属性，我们也知道了在构造类的时候，只要定义了 __len__() 方法，用户就能使用Python内置的 len() 函数获取该类实例的长度。我们知道形如 __xxx__ 的变量/方法都是有特殊用途的，那么Python中还有哪些特殊的变量/方法可以帮助我们更好地定制类呢？ __str__ 我们先定义一个 Student 类，然后打印一个实例： 123456&gt;&gt;&gt; class Student(object):... def __init__(self, name):... self.name = name...&gt;&gt;&gt; print(Student('Michael'))&lt;__main__.Student object at 0x109afb190&gt; 但是这样打印实例，我们只能知道它属于什么类以及在内存的位置，它的其他信息全都无法了解，所以对使用者来说并不友好。怎么才能定制打印的信息，使得打印实例时可以看到更多有用的信息呢？只需要定义好 __str__() 方法就可以了： 12345678&gt;&gt;&gt; class Student(object):... def __init__(self, name):... self.name = name... def __str__(self):... return 'Student object (name: %s)' % self.name...&gt;&gt;&gt; print(Student('Michael'))Student object (name: Michael) 这样打印实例就不但能知道实例所属的类，也能获得这个实例的属性信息了。 但是细心的朋友会发现直接敲变量不用 print 函数，打印出的实例依然是原来的样子： 123&gt;&gt;&gt; s = Student('Michael')&gt;&gt;&gt; s&lt;__main__.Student object at 0x109afb310&gt; 这是因为直接显示变量调用的不是 __str__() 方法，而是 __repr__() 方法，两者的区别是 __str__() 方法返回用户看到的字符串，而 __repr__() 返回程序开发者看到的字符串，也就是说，__repr__() 是为调试服务的。 解决办法是再定义一个 __repr__() 方法。但是通常 __str__() 和 __repr__()代码都是一样的（当然，要写不同的也行），所以，有个偷懒的写法： 123456class Student(object): def __init__(self, name): self.name = name def __str__(self): return 'Student object (name=%s)' % self.name __repr__ = __str__ # 直接令__repr__等于__str__ __iter__ 如果我们希望用 for ... in 循环来遍历一个类的实例，像遍历 list 或 tuple 那样，就必须实现一个 __iter__() 方法，该方法返回一个迭代对象，然后，Python的 for 循环就会不断调用该迭代对象的 __next__() 方法拿到循环的下一个值，直到遇到 StopIteration 错误时退出循环。 我们以斐波那契数列为例，写一个 Fib 类，可以作用于 for 循环： 123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，返回自己即可 def __next__(self): self.a, self.b = self.b, self.a + self.b # 计算下一个值 if self.a &gt; 100000: # 退出循环的条件 raise StopIteration(); return self.a # 返回下一个值 现在，试试把Fib类的实例作用于 for 循环，就能遍历斐波拉契数列了： 1234567891011&gt;&gt;&gt; for n in Fib():... print(n)...11235...4636875025 __getitem__ Fib 类的实例虽然能作用于 for 循环，看起来和 list 有点像了，但是没有办法使用下标访问： 1234&gt;&gt;&gt; Fib()[5]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: 'Fib' object does not support indexing 要能像 list 那样按照下标访问元素，需要实现 __getitem__() 方法： 123456class Fib(object): def __getitem__(self, n): a, b = 1, 1 for x in range(n): a, b = b, a + b return a 现在，就可以按下标访问数列的任意一项了： 12345678910111213&gt;&gt;&gt; f = Fib()&gt;&gt;&gt; f[0]1&gt;&gt;&gt; f[1]1&gt;&gt;&gt; f[2]2&gt;&gt;&gt; f[3]3&gt;&gt;&gt; f[10]89&gt;&gt;&gt; f[100]573147844013817084101 但是 list 有个神奇的切片方法： 12&gt;&gt;&gt; list(range(100))[5:10][5, 6, 7, 8, 9] 对于 Fib 却报错。原因是 __getitem__() 传入的参数可能是一个 int，也可能是一个 slice（切片对象），所以要做判断： 12345678910111213141516171819class Fib(object): def __getitem__(self, n): if isinstance(n, int): # n是索引 a, b = 1, 1 for x in range(n): a, b = b, a + b return a if isinstance(n, slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a, b = 1, 1 L = [] for x in range(stop): if x &gt;= start: L.append(a) a, b = b, a + b return L 现在再试试对 Fib 类的实例使用切片： 12345&gt;&gt;&gt; f = Fib()&gt;&gt;&gt; f[0:5][1, 1, 2, 3, 5]&gt;&gt;&gt; f[:10][1, 1, 2, 3, 5, 8, 13, 21, 34, 55] 但是没有对 step（步长）参数作处理： 12&gt;&gt;&gt; f[:10:2][1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89] 也没有对负数作处理，所以，要正确实现一个完整的 __getitem__() 还是有很多工作要做的。 此外，如果把对象看成 dict，那么 __getitem__() 的参数也可能是一个可以作key的object，例如 str。 与 __getitem__() 方法对应的是 __setitem__() 方法，把对象视作 list 或 dict 来对一个/多个位置进行赋值。除此之外，还有 __delitem__() 方法，用于删除某个位置的元素。 总之，通过实现上面的方法，可以让我们自己定义的类表现得和Python自带的 list、tuple、dict 没什么区别，这完全归功于动态语言的“鸭子类型”特点，不需要强制继承某个接口就能实现该接口的部分功能。 __getattr__ 正常情况下，当我们调用类的方法或属性时，如果不存在，就会报错。比如定义Student类： 1234class Student(object): def __init__(self): self.name = 'Michael' 调用name属性，没问题，但是，调用不存在的score属性，就有问题了： 1234567&gt;&gt;&gt; s = Student()&gt;&gt;&gt; print(s.name)Michael&gt;&gt;&gt; print(s.score)Traceback (most recent call last): ...AttributeError: 'Student' object has no attribute 'score' 错误信息很清楚地告诉我们，没有找到score这个attribute。 要避免这个错误，除了可以加上一个score属性外，Python还有另一个机制，那就是写一个 __getattr__() 方法，动态返回一个属性。修改如下： 12345678class Student(object): def __init__(self): self.name = 'Michael' def __getattr__(self, attr): if attr=='score': return 99 当调用不存在的属性时，Python解释器会试图调用 __getattr__(self, '属性名')来尝试获得属性，依然用 score 属性做例子，进行上述定义后，再次执行就变成了： 12345&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name'Michael'&gt;&gt;&gt; s.score99 动态返回函数也是完全可以的： 12345class Student(object): def __getattr__(self, attr): if attr=='age': return lambda: 25 只是调用方式要变为： 12&gt;&gt;&gt; s.age()25 注意，只有在没有找到属性的情况下，才调用 __getattr__，已有的属性，比如 name，不会在 __getattr__ 中查找。 此外，注意到此时调用其他任意属性，如 s.abc，返回的是 None，这是因为在 __getattr__ 中我们没有为这些属性定义返回值，那么默认返回就是 None。要让类只响应特定的几个属性，我们可以默认抛出 AttributeError 错误： 123456class Student(object): def __getattr__(self, attr): if attr=='age': return lambda: 25 raise AttributeError('\\'Student\\' object has no attribute \\'%s\\'' % attr) 这样就相当于把一个类的属性和方法调用都进行动态化处理了，不需要其他特殊手段。 这种完全动态调用的特性有什么实际作用呢？作用就是，可以针对完全动态的情况作调用。举个例子，现在很多网站都搞 REST API，比如新浪微博、豆瓣啥的，调用API的URL类似： 12http://api.server/user/friendshttp://api.server/user/timeline/list 如果要写SDK，为每个URL对应的API都写一个方法，那得累死，而且，API一旦改动，SDK也要改。 借助完全动态的 __getattr__ 方法，我们可以非常方便地实现链式调用： 12345678class Chain(object): def __init__(self, path=''): self._path = path def __getattr__(self, path): return Chain('%s/%s' % (self._path, path)) def __str__(self): return self._path __repr__ = __str__ 试试： 1234567&gt;&gt;&gt; chain = Chain('http://api.server')&gt;&gt;&gt; API1 = chain.user.friends&gt;&gt;&gt; print(API1)http://api.server/user/friends&gt;&gt;&gt; API2 = chain.user.timeline.list&gt;&gt;&gt; print(API2)http://api.server/user/timeline/list 由于 __getattr__ 返回的也是一个 Chain 类的实例，所以后面继续接着使用点符访问属性也是可以的，这就是链式调用的本质。这样，无论想调用什么API，SDK都可以根据不同的URL进行完全动态的调用，不需要随API的增加而改变！相当方便！！ 还有一些REST API会把参数放在URL中，比如GitHub的API： 1GET /users/:user/repos 调用时，需要把 :user 替换为实际用户名。这时我们希望可以用这样的链式调用来获取API： 1chain().users('michael').repos 尝试一下： 12345678910class Chain(object): def __init__(self, path=''): self._path = path def __getattr__(self, path): return Chain('%s/%s' % (self._path, path)) def users(self, username): return Chain('%s/%s' % (self._path, username)) def __str__(self): return self._path __repr__ = __str__ 运行结果： 123&gt;&gt;&gt; chain = Chain('/users')&gt;&gt;&gt; chain.users('michael').repos/users/michael/repos 当然，除了实现一个 users 方法之外，直接在 getattr 方法里面使用正则也是可以的。 __call__ 一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用 实例名.方法名() 的方式来调用。能不能直接把实例本身当作一个方法调用呢？在Python中，答案是肯定的。 对任何类来说，只需要实现 __call__() 方法，就可以直接对该类的实例进行调用。比如： 123456class Student(object): def __init__(self, name): self.name = name def __call__(self): print('My name is %s.' % self.name) 调用方式如下： 123&gt;&gt;&gt; s = Student('Michael')&gt;&gt;&gt; s() # 调用实例本身，self参数不需要传入My name is Michael. 和普通的函数和方法一样，我们还可以为 __call__() 方法定义其他参数。但有一点很特别，我们注意到类的实例都是运行期间动态创建出来的，而一般来说可调用对象（函数/方法）都是预先定义的，所以说当我们把实例本身变成可调用的方法时，实际上我们是动态创建了可调用对象。 能被调用的对象就是一个 Callable 对象，要判断一个对象是否可调用可以使用Python内置的 callable 函数： 12345678910&gt;&gt;&gt; callable(Student())True&gt;&gt;&gt; callable(max)True&gt;&gt;&gt; callable([1, 2, 3])False&gt;&gt;&gt; callable(None)False&gt;&gt;&gt; callable('str')False 小结 Python中的类允许定义许多定制方法，可以让我们非常方便地生成特定的类。 本节介绍的是最常用的几个定制方法，还有很多可定制的方法，请参考Python的官方文档。 使用枚举类 为何需要枚举类 当我们需要定义常量/枚举值时，一个比较常见的办法是用大写变量通过整数来定义，例如月份： 123456JAN = 1FEB = 2MAR = 3...NOV = 11DEC = 12 这样做的好处是简单，缺点是把数据类型变为了 int 型，并且在Python中仍然是变量，因此可能会在使用者无法意识到的情况下被错误的操作改变值。 如何使用枚举类 更好的方法是使用Python提供的枚举类 Enum，把每一个枚举对象作为枚举类的一个属性： 123from enum import EnumMonth = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) # 创建一个枚举类的实例 这样我们就获得了一个类型为 Month 的枚举类，可以直接使用 Month.Jan 来引用一个常量，或者枚举它的所有成员： 123456789101112131415&gt;&gt;&gt; for name, member in Month.__members__.items():... print(name, '=&gt;', member, ',', member.value)...Jan =&gt; Month.Jan , 1Feb =&gt; Month.Feb , 2Mar =&gt; Month.Mar , 3Apr =&gt; Month.Apr , 4May =&gt; Month.May , 5Jun =&gt; Month.Jun , 6Jul =&gt; Month.Jul , 7Aug =&gt; Month.Aug , 8Sep =&gt; Month.Sep , 9Oct =&gt; Month.Oct , 10Nov =&gt; Month.Nov , 11Dec =&gt; Month.Dec , 12 特别地，枚举类中的每个成员会被分配一个 int 型的 value 属性，默认按初始化顺序从1开始计数。 自定义枚举类 如果需要更精确地控制枚举类型，可以继承 Enum 类然后进行自定义： 1234567891011from enum import Enum, unique@uniqueclass Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique 装饰器可以帮助我们检查枚举值是否存在重复，注意属性名字重复也会报错，但是与 @unique 装饰器无关。 自定义的枚举类使用方法和使用 Enum 构造的类似： 123456789&gt;&gt;&gt; day1 = Weekday.Mon # 按属性访问&gt;&gt;&gt; print(day1) # 打印枚举变量Weekday.Mon&gt;&gt;&gt; day1 # 直接显示枚举变量&lt;Weekday.Mon: 1&gt;&gt;&gt;&gt; day1.name # 获得枚举变量的名称'Mon'&gt;&gt;&gt; day1.value # 获得枚举变量的值1 123456&gt;&gt;&gt; print(Weekday['Tue']) # 使用属性名作下标访问Weekday.Tue&gt;&gt;&gt; print(day1 == Weekday.Mon) # 属性之间可以直接进行比较True&gt;&gt;&gt; print(day1 == Weekday.Tue)False 12345678&gt;&gt;&gt; print(Weekday(1)) # 把类作为一个方法调用，传入枚举值Weekday.Mon&gt;&gt;&gt; print(day1 == Weekday(1))True&gt;&gt;&gt; Weekday(7) # 找不到该枚举值对应的属性Traceback (most recent call last): ...ValueError: 7 is not a valid Weekday 12345678910&gt;&gt;&gt; for name, member in Weekday.__members__.items(): # 遍历枚举类... print(name, '=&gt;', member)...Sun =&gt; Weekday.SunMon =&gt; Weekday.MonTue =&gt; Weekday.TueWed =&gt; Weekday.WedThu =&gt; Weekday.ThuFri =&gt; Weekday.FriSat =&gt; Weekday.Sat 小结 使用枚举类可以把一组相关常量定义在一个类中，转化为该类的不同属性，该类不可变（属性都是只读的）且属性可以直接进行比较。 使用元类 type函数 动态语言和静态语言最大的不同，就是在动态语言中，函数和类的定义，不是编译时定义的，而是运行时动态创建的。 比方说我们要定义一个 Hello 类，首先编写一个 hello.py 模块，里面的代码如下： 123class Hello(object): def hello(self, name='world'): print('Hello, %s.' % name) 当Python解释器导入 hello 模块时，就会依次执行该模块的所有语句（与我们在交互环境下逐个语句输入来定义类一样），从而动态创建出一个类对象（注意这里说的是类对象而不是实例对象），测试如下： 123456789101112&gt;&gt;&gt; from hello import Hello # 这个语句创建了一个名为Hello的类对象&gt;&gt;&gt; h = Hello() # 创建一个Hello类的实例h&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello)) # Hello对象的类型为type&lt;class 'type'&gt;&gt;&gt;&gt; print(type(h)) # 而Hello类实例的类型为hello.Hello&lt;class 'hello.Hello'&gt;&gt;&gt;&gt; type(str) # str是一个类型&lt;class 'type'&gt;&gt;&gt;&gt; type(int) # int也是一个类型&lt;class 'type'&gt; type() 函数可以用来查看一个变量的类型，Hello 是一个类，它的类型就是 type，而 h 是一个实例，它的类型就是它所属的类。 前面说到，在Python中，类的定义是运行时动态创建的。而动态创建类使用的其实是 type()函数。type() 函数既可以返回一个变量的类型，又可以创建出新的类型。依然举 Hello 类为例子，但我们这次使用 type() 函数来创建 Hello 类而不使用显式的 class Hello： 1234567891011&gt;&gt;&gt; def fn(self, name='world'): # 先定义函数... print('Hello, %s.' % name)...&gt;&gt;&gt; Hello = type('Hello', (object,), dict(hello=fn)) # 创建Hello class&gt;&gt;&gt; h = Hello()&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello))&lt;class 'type'&gt;&gt;&gt;&gt; print(type(h))&lt;class '__main__.Hello'&gt; 使用 type() 函数创建一个类对象，需要依次传入以下3个参数： 类名 继承的父类集合：Python支持多重继承，所以这里用一个 tuple 来囊括继承的所有父类。注意只有一个父类时，要采用 tuple 的单元素写法，不要漏掉逗号。 类的方法名与函数的绑定：在上面的例子中，我们把函数 fn 绑定到方法名 hello 上。也即类 Hello 的方法 hello 就是函数 fn，注意这和这章开头所说的动态绑定方法是不同的。 通过 type() 函数创建的类和直接写类是完全一样的。事实上，Python解释器遇到类定义时，在扫描类定义的语法之后，就是调用 type() 函数来创建类的。 正常情况下，我们都用 class 类名(父类1, 父类2, ...) 的方式来定义类，但是，type() 函数也允许我们动态创建类。 动态语言能够支持运行期间动态创建类，这和静态语言有非常大的不同。关于这两者的区别，感兴趣的话可以再查找其他资料。 什么是元类 除了使用 type() 函数动态创建类以外，要控制类的创建行为，还可以使用元类（metaclass）。 怎么理解什么是元类呢？简单地解释一下： 当我们定义了类以后，以类为模版就可以创建出实例了。 但如果我们要创建类呢？那就必须先定义元类，有了元类之后，以元类为模版就可以创建出类了。 连起来就是：以元类为模版创建类，以类为模版创建该类的实例。 也就是说，可以把类看成是元类创建出来的“实例”。 metaclass 图片来源：What is a metaclass in Python? 在上一个小节中，我们了解到可以使用 type() 函数创建类，但 type 的本质是什么呢？ 12345678910111213&gt;&gt;&gt; help(type)Help on class type in module builtins:class type(object) | type(object_or_name, bases, dict) | type(object) -&gt; the object's type | type(name, bases, dict) -&gt; a new type | | Methods defined here: | | __call__(self, /, *args, **kwargs) | Call self as a function. ... 其实呀， type 本身就是一个类，调用 type() 创建类得到的其实就是 type 类的实例。所以所有类对象的类型都是 type。不难分析出，type 是一个元类，并且类都是默认以元类 type 为模版创建的。 怎样使用元类 如果我们想要创建一个元类，并且想以这个元类为模版创建类，那么定义元类的时候，就应当让这个元类继承自 type 类。 按照习惯，元类的类名应总是以Metaclass结尾，以便清楚地表示这是一个元类。下面举一个例子，定义元类 ListMetaclass： 12345# metaclass是类的模板，所以必须从`type`类型派生：class ListMetaclass(type): def __new__(cls, name, bases, attrs): attrs['add'] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs) 元类的 __new__() 方法用于创建一个类对象，它接收四个参数，依次是： 准备创建的类对象； 准备创建的类的名字； 准备创建的类继承的父类集合； 准备创建的类的方法集合。 我们在 ListMetaclass 的 __new__() 方法中加入了一句 attrs['add'] = lambda self, value: self.append(value)，然后调用元类 type 的 __new__() 方法创建类对象。这多出来的一句，实际上我们是给要创建的类提供了一个 add 方法，这个 add 方法接收实例本身和一个变量，并把这个变量拼接到实例的尾部。其实就是一个 append 方法。 定义好元类 ListMetaclass 之后，我们以它为模版创建类，注意传入关键字参数 metaclass： 12class MyList(list, metaclass=ListMetaclass): pass 传入关键字参数 metaclass 后，Python解释器会在创建 MyList 类时，通过元类 ListMetaclass 的 __new__() 方法来创建。因此虽然我们在类定义时没有为 MyList 类定义任何方法，但因为它是以元类 ListMetaclass 为模版创建的，所以拥有了 add 方法。另外，因为它继承了 list 类，所以我们相当于创建了一个拥有 add 方法的新的 list 类，测试一下： 1234567&gt;&gt;&gt; L = MyList()&gt;&gt;&gt; L.add(1) # 使用add方法在列表尾部添加元素&gt;&gt; L[1]&gt;&gt;&gt; L.add(2)&gt;&gt;&gt; L[1, 2] 普通的 list 是没有 add() 方法的： 12345&gt;&gt;&gt; L2 = list()&gt;&gt;&gt; L2.add(1)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: 'list' object has no attribute 'add' 但是，直接在 MyList 类的定义中写上 add() 方法不是更简单吗？是的，正常情况下我们应该直接在类定义中编写方法，而不是通过元类。 但是，也有需要通过元类动态修改类定义的情况，ORM就是一个典型的例子。 编写ORM框架 ORM 全称 Object Relational Mapping（对象-关系映射），简单来说就是把关系型数据库中表格的每一行都映射为一个对象，而每一个表就是一个类。这样写代码更简单，不用直接操作SQL语句。 要编写一个 ORM 框架供不同的使用者使用，框架中的所有类都应该能动态定义，因为每位使用者的需求不同，需要根据具体的表结构来定义出不同的类。 举个例子，假如使用者想使用这个 ORM 框架定义一个 User 类来操作数据库中的表格 User，我们期望使用者可以写出这样简洁的形式： 123456class User(Model): # 定义类的属性到表格中列的映射： id = IntegerField('id') name = StringField('username') email = StringField('email') password = StringField('password') 1234# 创建一个实例：u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd')# 插入到表格中：u.insert() 也即，用户在使用这个 ORM 框架时，每个表格对应一个类，类定义只需要指定表格每列的字段类型即可，每一行数据都是该类的一个实例。而父类 Model 和数据类型 StringField、IntegerField 等都由 ORM 框架负责提供。save() 之类的方法则全部由元类自动完成。虽然这样元类的编写会比较复杂，但 ORM 的使用者用起来却可以异常简单。 想好了希望实现怎样的效果后，我们可以开始编写调用接口。 首先定义 Field 类，它是最底层的类，负责保存字段名（列名）和对应的字段类型： 1234567class Field(object): def __init__(self, name, column_type): self.name = name self.column_type = column_type def __str__(self): return '&lt;%s:%s&gt;' % (self.__class__.__name__, self.name) __repr__ = __str__ 在 Field 的基础上，我们可以进一步定义各种类型的 Field，比如 StringField，IntegerField 等等： 1234567class StringField(Field): def __init__(self, name): super(StringField, self).__init__(name, 'varchar(100)')class IntegerField(Field): def __init__(self, name): super(IntegerField, self).__init__(name, 'bigint') 注意这里使用了 super 函数来获取父类的方法，并进行绑定，先看一看官方的解释： 12345678910super(type[, object-or-type]) Return the superclass of type. If the second argument is omitted the super object returned is unbound. If the second argument is an object, isinstance(obj, type) must be true. If the second argument is a type, issubclass(type2, type) must be true. super() only works for new-style classes. A typical use for calling a cooperative superclass method is: class C(B): def meth(self, arg): super(C, self).meth(arg) New in version 2.2. 所以这里实际上我们实例化 StringField 和 IntegerField 时，是调用它们的父类，也即 Field 类的 __init__ 方法进行的，这两个类封装了 Field 的功能，使用者只需要传入字段名就可以了，不需要关心在数据库中类型的名字。上面的实现比较简单，不需要使用元类。 接下来先理一理整体的实现思路，我们编写 ORM 框架来实现底层的功能，用户使用该框架时，只需要根据自己的需求来为表格定义对应的类，比方说上面举的例子中定义 User 类那样。这个类的实例对应表格中的一行，定义一个新实例 u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd')，我们希望得到这个实例后可以通过 print(u['name']) 的方式读取字段值，通过 u['id']=23456 的方式来修改字段值，这就类似于Python中的 dict 的功能，所以我们实际上最底层的父类采用 dict 即可。 但是，我们除了 dict 的功能之外，肯定还需要实现一些其他功能，比如把新实例插入到数据库的表格中。这些功能我们可以在 Model 类中实现，Model 类继承 dict 类，这样我们就可以像前面说的那样进行读取和修改了。使用者为表格编写类时继承 Model 类即可，这样所有表格都能得到 Model 类中实现的操作表格的功能了。 但是，我们还注意到一点，我们希望用户定义类的时候，写法尽可能简单，只需要关注有哪些字段，然后每个字段作为一个属性，用 id = IntegerField('id') 的方式来定义，也即 属性名 = 字段类型（'字段名'），字段类型的实现前面已经说过了。 这里我们需要关注另外一个很重要的点，在实例化得到表格的一行以后，我们希望使用者可以采用 实例名.属性名 = 值 的方式来修改这一行某个字段的值。但事实上，使用者定义类的时候，类属性表示的是以某个字段名为名的某字段类型的实例，属性的类型是 StringField 或者 IntegerField。而在读取或修改一个实例的属性值时，我们希望实例属性表示的是这一行数据在这个字段的值，属性的类型是 str 或者 int。这里说得比较绕，简单归纳来说就是用户定义类的方式和使用该类实例的方式不相符。 我们希望使用者定义类的方式尽可能简单，同时也能用简单的方式修改字段值（实例的属性值），但由于类属性和实例属性同名时，对实例属性赋值会覆盖类属性，所以我们必须进行一些修改去避免这个问题。怎么实现呢？这时候我们就要用到元类了，虽然作为框架的编写者，我们要做的工作比较多，但这样使用者用起来就很方便了，他们依然可以很简单地定义类，但运行时类定义会被元类动态修改，我们可以把类属性该为其他名字，这样类定义中的类属性信息就可以保留下来了，而且不会被实例属性的赋值所覆盖。 另外，由于使用者不一定明白元类这么复杂的概念，所以我们把元类封装在 Model 类的定义中，指定 Model 类使用 ModelMetaclass 为模版。把前面所说的更换类属性名的操作封装在 ModelMetaclass 中，使用者为表格编写类的时候只需要继承 Model 类，那么运行时就会自动以 ModelMetaclass 为模版，得到 ModelMetaclass 的所有功能。但是要注意，Model 类本身不需要更换类属性名，所以在 ModelMetaclass 中我们要排除掉 Model 类。 接下来，直接上代码。元类 ModelMetaclass： 1234567891011121314151617181920212223class ModelMetaclass(type): # 四个参数，依次为：准备创建的类对象，类的名字，继承的父类集合，属性&amp;方法集合 def __new__(cls, name, bases, attrs): # Model类不需额外操作，先排除掉 if name=='Model': return type.__new__(cls, name, bases, attrs) # 其他类（对应具体的表格）则把类属性使用dict存好，绑定到__mappings__属性上 # 然后删除掉这些类属性 # 这里还动态地要创建的类添加了一个表名属性__table__，直接令表名等于类名 # 当然也可以作一些其他修改 print('Found model: %s' % name) mappings = dict() for k, v in attrs.items(): # 取出每个类属性 # 因为除了用户定义的类属性之外，还有一些继承自父类的属性等等 # 所以这里要先判断一下，属于字段类型的属性才需要考虑 if isinstance(v, Field): print('Found mapping: %s ==&gt; %s' % (k, v)) mappings[k] = v # 使用一个dict保存 类属性名-字段类型实例 的映射 for k in mappings.keys(): attrs.pop(k) attrs['__mappings__'] = mappings # 把映射绑定到__mappings__属性上 attrs['__table__'] = name # 把表名绑定到__table__属性上 return type.__new__(cls, name, bases, attrs) 父类 Model： 1234567891011121314151617181920class Model(dict, metaclass=ModelMetaclass): def __init__(self, **kw): super(Model, self).__init__(**kw) # 创建一个dict def __getattr__(self, key): # 可以采用点符访问实例属性（字段值） try: return self[key] except KeyError: raise AttributeError(r&quot;'Model' object has no attribute '%s'&quot; % key) def __setattr__(self, key, value): # 可以采用点符修改实例属性（字段值） self[key] = value def insert(self): # 将实例插入到数据库的对应表格中 fields = [] values = [] # 取得这一行数据的字段名及对应字段值 for k, v in self.__mappings__.items(): fields.append(v.name) values.append(str(getattr(self, k, None))) # 格式化SQL语句（MySQL语法） sql = 'INSERT INTO %s (%s) values (%s)' % (self.__table__, ','.join(fields), ','.join(values)) print('SQL: %s' % sql) # 输出SQL语句，这里我们没有写真的插入数据库的操作，只是举例子 当用户为 User 表定义 User 类时，Python解释器首先在当前类 User 的定义中查找是否有metaclass关键字，如果没有找到，就继续在父类 Model 中查找metaclass关键字，因为父类 Model 定义了以元类 ModelMetaclass 为模版来创建，所以 User 类也会以元类 ModelMetaclass 为模版来创建。借助元类，我们可以在运行时动态地修改子类的定义，但使用者定义子类时却不需要显式地声明。 使用者定义 User 类之后，会输出： 12345Found model: UserFound mapping: password ==&gt; &lt;StringField:password&gt;Found mapping: name ==&gt; &lt;StringField:username&gt;Found mapping: email ==&gt; &lt;StringField:email&gt;Found mapping: id ==&gt; &lt;IntegerField:id&gt; 运行时，类定义被元类动态地修改了，使用者定义的四个类属性被集成到 __mappings__ 属性中，因此不会被实例属性覆盖，也就不会丢失字段名信息了。 创建实例，然后把这个实例（一行数据）插入到数据库： 12345&gt;&gt;&gt; # 创建一个实例：... u = User(id=12345, name='Michael', email='test@orm.org', password='my-pwd')&gt;&gt;&gt; # 插入到表格中：... u.insert()SQL: INSERT INTO User (email,username,password,id) values (test@orm.org,Michael,my-pwd,12345) 我们也可以看看 __mappings__ 属性怎么样： 1234&gt;&gt;&gt; User.__mappings__{'email': &lt;StringField:email&gt;, 'name': &lt;StringField:username&gt;, 'password': &lt;StringField:password&gt;, 'id': &lt;IntegerField:id&gt;}&gt;&gt;&gt; u.__mappings__{'email': &lt;StringField:email&gt;, 'name': &lt;StringField:username&gt;, 'password': &lt;StringField:password&gt;, 'id': &lt;IntegerField:id&gt;} 正如我们定义那样，它是一个 dict，里面保存着各个字段的名字和它们的数据类型。 虽然我们没有真的实现插入数据库，但可以看到打印出的SQL语句是正确的，要实现完整的功能，只要再使用数据库模块的接口就可以了。通过这样短短的不到100行代码，我们就借助元类实现了一个精简的 ORM 框架。 小结 元类是Python中非常具有魔术性的对象，它可以改变类创建时的行为。这种强大的功能使用起来务必小心。","link":"/posts/6022.html"},{"title":"BERT基本原理及运用","text":"BERT基本原理 BERT:Bidirectional Encoder Representations from Transformers BERT架构 自编码语言模型，模型结构为 Transformer 的编码器；由12层或更多的EncoderLayer组成 BERT预训练 用两个任务来预训练该模型，如下图 (a) 所示 - MaskLM，输入的话中随机选择一些词，用特殊符号 [MASK] 替换，让模型预测这些词 - 训练过程类似“完形填空”。在一句话中随机选择 15% 的单词用于预测，其中 80% 情况下采用特殊符号 [MASK] 替换，10% 情况采用任意词替换，10% 的情况下保持原词汇不变 - 之所以这样做：在后续微调任务中语句中并不会出现 [MASK] 标记 - 预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（10%概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。 - 因此模型可能需要更多的训练步骤来收敛 Next Sentence Prediction，输入的两句话，让模型判断是否是连续的两句话，让模型学习连续的文本片段之间的关系 类似于“连句任务”，给定一篇文章中的两句话，判断第二句话在文本中是否紧跟在第一句话之后 文本语料库中随机选择 50% 的正确语句对和 50% 的错误语句对进行训练 两个任务联合训练，使模型输出的每个字 / 词的向量表示都能尽可能全面、准确地刻画输入文本（单句或语句对）的整体信息，为后续的微调任务提供更好的模型参数初始值。 BERT微调 如上图 (b) 所示，预训练好的模型，对新的句子进行编码，然后取 [CLS]对应的输出向量，进行分类任务；或最后的几个输出向量，进行其它任务 BERT优缺点 相较于 RNN、LSTM 可以实现并发训练，同时提取词在句子中的关系特征，在不同的层次提取关系特征，进而更全面反映句子语义。 相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。 模型参数太多，而且模型太大，少量数据训练时，容易过拟合。 BERT用于分类任务 因为bert模型的词汇在预训练时已经固定，所以必须使用bert模型自带的分词器对文本处理 文本必须处理成满足bert模型输入的格式要求，添加特殊的token:[CLS],[SEP],[PAD] 语料 COLA(The Corpus of Linguistic Acceptability) ,句子在语法上是否可接受(0=unacceptable, 1=acceptable) 12345678import pandas as pddf = pd.read_csv(&quot;../datasets/cola_public/raw/in_domain_train.tsv&quot;, delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])print('number of training sentences: {:,}\\n'.format(df.shape[0]))df.sample(5) number of training sentences: 8,551 1df.loc[df.label == 0].sample(5)[['sentence', 'label']] 12sentences = df.sentence.valueslabels = df.label.values 分词 1234567891011from transformers import BertTokenizerprint(&quot;Loading BERT tokenizer...&quot;)# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', # 按模型名称载入# do_lower_case=True)# with open('../models/bert/vocabulary.txt', 'w') as f:# for token in tokenizer.vocab.keys():# f.write(token + '\\n')tokenizer = BertTokenizer.from_pretrained( '../models/bert/vocabulary.txt', # 从保存有词汇表的本地文件载入 do_lower_case=True) Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated Loading BERT tokenizer... 1234print(&quot;Original: &quot;, sentences[0])print(&quot;Tokenized: &quot;, tokenizer.tokenize(sentences[0]))print(&quot;Token IDS: &quot;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) Original: Our friends won't buy this analysis, let alone the next one we propose. Tokenized: ['our', 'friends', 'won', &quot;'&quot;, 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.'] Token IDS: [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012] 满足BERT模型的输入格式 特殊标记: - 每个句子的结尾，添加上[SEP]标记：用于决定两个句子的关系，如是否是连续的两个句子 - 分类任务，在每个句子的开端添加[CLS]标记；模型输入的矩阵，只有第一个向量用于分类 句子长度和遮档 - 语料不同句子长度相差很大，需要所有句子填充或截断到相同的长度；且BERT模型，最长512个标记 - 填充是添加标记[PAD]，在词汇表中索引为 0 - [CLS] I like to draw [SEP] [PAD] [PAD] 遮挡是 0 和 1 的序列，1 表示单词，0 表示填充；填充的多少对模型的速度和精度会有影响 1234567891011121314# 文本向量化input_ids = []for sent in sentences: encoded_sent = tokenizer.encode( sent, add_special_tokens=True, # 添加特殊符号 ) input_ids.append(encoded_sent)print(&quot;Original: &quot;, sentences[0])print(&quot;Tokenized: &quot;, tokenizer.tokenize(sentences[0]))print(&quot;Token IDs: &quot;, input_ids[0]) Original: Our friends won't buy this analysis, let alone the next one we propose. Tokenized: ['our', 'friends', 'won', &quot;'&quot;, 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.'] Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102] 12MAX_LEN = max([len(sen) for sen in input_ids])print(&quot;Max sentence length: &quot;, MAX_LEN) Mac sentence length: 47 12345678910111213# 填充为相同长度from tensorflow.keras.preprocessing.sequence import pad_sequencesprint(&quot;Padding token: {:}, ID: {:}&quot;.format(tokenizer.pad_token, tokenizer.pad_token_id))input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', value=0, truncating=&quot;post&quot;, padding='post')input_ids.shape Padding token: [PAD], ID: 0 (8551, 47) 123456# 填充对应的maskattention_masks = []for sent in input_ids: att_mask = [int(token_id &gt; 0) for token_id in sent] attention_masks.append(att_mask) 1234567891011# 拆分为测试集和训练集from sklearn.model_selection import train_test_splittrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split( input_ids, labels, random_state=2008, test_size=0.1)train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2008, test_size=0.1) 1234567891011# 转化为 PyTorch 数据格式import torchtrain_inputs = torch.tensor(train_inputs)validation_inputs = torch.tensor(validation_inputs)train_labels = torch.tensor(train_labels)validation_labels = torch.tensor(validation_labels)train_masks = torch.tensor(train_masks)validation_masks = torch.tensor(validation_masks) 1234567891011121314151617# 创建数据管道from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSamplerbatch_size = 32train_data = TensorDataset(train_inputs, train_masks, train_labels)train_sampler = RandomSampler(train_data)train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)validation_sampler = RandomSampler(validation_data)validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size) 1 创建并训练分类模型 huggingface提供的接口： BertModel BertForPreTraining BertForMaskedLM BertForNextSentencePrediction BertForSequenceClassification BertForTokenClassification BertForQuestionAnswering 将模型\"bert-base-uncased\": \"https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin\", 模型词汇表\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",, 模型配置\"bert-base-uncased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",三个文件下载到本地，文件夹命名为\"bert-base-uncased\", 然后更改文件名，删除名字中的前缀\"bert-base-uncased\" 12345678910111213141516from transformers import BertForSequenceClassification, AdamW, BertConfigimport os# 载入 bert 预训练模型path = os.path.abspath(&quot;../../H/models/huggingface/bert-base-uncased/&quot;)model = BertForSequenceClassification.from_pretrained( # &quot;bert-base-uncased&quot;, # 按名称载入模型 path, # 本地文件载入 num_labels=2, output_attentions=False, output_hidden_states=False,)model.cuda() BertForSequenceClassification( (bert): BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (1): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (2): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (3): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (4): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (5): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (6): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (7): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (8): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (9): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (10): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (11): BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) (dropout): Dropout(p=0.1, inplace=False) (classifier): Linear(in_features=768, out_features=2, bias=True) ) 123456789101112131415161718192021# 模型参数详情params = list(model.named_parameters())print('The BERT model has {:} different named parameters.\\n'.format( len(params)))print('==== Embedding Layer ====\\n')for p in params[0:5]: print(&quot;{:&lt;55} {:&gt;12}&quot;.format(p[0], str(tuple(p[1].size()))))print('\\n==== First Transformer ====\\n')for p in params[5:21]: print(&quot;{:&lt;55} {:&gt;12}&quot;.format(p[0], str(tuple(p[1].size()))))print('\\n==== Output Layer ====\\n')for p in params[-4:]: print(&quot;{:&lt;55} {:&gt;12}&quot;.format(p[0], str(tuple(p[1].size())))) The BERT model has 201 different named parameters. ==== Embedding Layer ==== bert.embeddings.word_embeddings.weight (30522, 768) bert.embeddings.position_embeddings.weight (512, 768) bert.embeddings.token_type_embeddings.weight (2, 768) bert.embeddings.LayerNorm.weight (768,) bert.embeddings.LayerNorm.bias (768,) ==== First Transformer ==== bert.encoder.layer.0.attention.self.query.weight (768, 768) bert.encoder.layer.0.attention.self.query.bias (768,) bert.encoder.layer.0.attention.self.key.weight (768, 768) bert.encoder.layer.0.attention.self.key.bias (768,) bert.encoder.layer.0.attention.self.value.weight (768, 768) bert.encoder.layer.0.attention.self.value.bias (768,) bert.encoder.layer.0.attention.output.dense.weight (768, 768) bert.encoder.layer.0.attention.output.dense.bias (768,) bert.encoder.layer.0.attention.output.LayerNorm.weight (768,) bert.encoder.layer.0.attention.output.LayerNorm.bias (768,) bert.encoder.layer.0.intermediate.dense.weight (3072, 768) bert.encoder.layer.0.intermediate.dense.bias (3072,) bert.encoder.layer.0.output.dense.weight (768, 3072) bert.encoder.layer.0.output.dense.bias (768,) bert.encoder.layer.0.output.LayerNorm.weight (768,) bert.encoder.layer.0.output.LayerNorm.bias (768,) ==== Output Layer ==== bert.pooler.dense.weight (768, 768) bert.pooler.dense.bias (768,) classifier.weight (2, 768) classifier.bias (2,) 1234567891011121314# 优化器optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)# 学习率规划from transformers import get_linear_schedule_with_warmupepochs = 4total_steps = len(train_dataloader) * epochsscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps) 1234567891011121314151617181920# 预测精度import numpy as npdef flat_accuracy(preds, labels): pred_flat = np.argmax(preds, axis=1).flatten() labels_flat = labels.flatten() return np.sum(pred_flat == labels_flat) / len(labels_flat)# 格式化时间显示import timeimport datetimedef format_time(elapsed): elapsed_rounded = int(round((elapsed))) return str(datetime.timedelta(seconds=elapsed_rounded)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186# 训练并验证模型import random# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# Set the seed value all over the place to make this reproducible.seed_val = 42random.seed(seed_val)np.random.seed(seed_val)torch.manual_seed(seed_val)torch.cuda.manual_seed_all(seed_val)# Store the average loss after each epoch so we can plot them.loss_values = []# For each epoch...for epoch_i in range(0, epochs): # ======================================== # Training # ======================================== # Perform one full pass over the training set. print(&quot;&quot;) print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs)) print('Training...') # Measure how long the training epoch takes. t0 = time.time() # Reset the total loss for this epoch. total_loss = 0 # Put the model into training mode. Don't be mislead--the call to # `train` just changes the *mode*, it doesn't *perform* the training. # `dropout` and `batchnorm` layers behave differently during training # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch) model.train() # For each batch of training data... for step, batch in enumerate(train_dataloader): # Progress update every 40 batches. if step % 40 == 0 and not step == 0: # Calculate elapsed time in minutes. elapsed = format_time(time.time() - t0) # Report progress. print(' Batch {:&gt;5,} of {:&gt;5,}. Elapsed: {:}.'.format( step, len(train_dataloader), elapsed)) # Unpack this training batch from our dataloader. # # As we unpack the batch, we'll also copy each tensor to the GPU using the # `to` method. # # `batch` contains three pytorch tensors: # [0]: input ids # [1]: attention masks # [2]: labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) # Always clear any previously calculated gradients before performing a # backward pass. PyTorch doesn't do this automatically because # accumulating the gradients is &quot;convenient while training RNNs&quot;. # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch) model.zero_grad() # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple. loss = outputs[0] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss.item() # Perform a backward pass to calculate the gradients. loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the &quot;exploding gradients&quot; problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Update parameters and take a step using the computed gradient. # The optimizer dictates the &quot;update rule&quot;--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over the training data. avg_train_loss = total_loss / len(train_dataloader) # Store the loss value for plotting the learning curve. loss_values.append(avg_train_loss) print(&quot;&quot;) print(&quot; Average training loss: {0:.2f}&quot;.format(avg_train_loss)) print(&quot; Training epcoh took: {:}&quot;.format(format_time(time.time() - t0))) # ======================================== # Validation # ======================================== # After the completion of each training epoch, measure our performance on # our validation set. print(&quot;&quot;) print(&quot;Running Validation...&quot;) t0 = time.time() # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model.eval() # Tracking variables eval_loss, eval_accuracy = 0, 0 nb_eval_steps, nb_eval_examples = 0, 0 # Evaluate data for one epoch for batch in validation_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch.no_grad(): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the &quot;segment ids&quot;, which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) # Get the &quot;logits&quot; output by the model. The &quot;logits&quot; are the output # values prior to applying an activation function like the softmax. logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to('cpu').numpy() # Calculate the accuracy for this batch of test sentences. tmp_eval_accuracy = flat_accuracy(logits, label_ids) # Accumulate the total accuracy. eval_accuracy += tmp_eval_accuracy # Track the number of batches nb_eval_steps += 1 # Report the final accuracy for this validation run. print(&quot; Accuracy: {0:.2f}&quot;.format(eval_accuracy / nb_eval_steps)) print(&quot; Validation took: {:}&quot;.format(format_time(time.time() - t0)))print(&quot;&quot;)print(&quot;Training complete!&quot;) ======== Epoch 1 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:05. Batch 80 of 241. Elapsed: 0:00:09. Batch 120 of 241. Elapsed: 0:00:14. Batch 160 of 241. Elapsed: 0:00:19. Batch 200 of 241. Elapsed: 0:00:23. Batch 240 of 241. Elapsed: 0:00:28. Average training loss: 0.17 Training epcoh took: 0:00:28 Running Validation... Accuracy: 0.80 Validation took: 0:00:01 ======== Epoch 2 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:05. Batch 80 of 241. Elapsed: 0:00:09. Batch 120 of 241. Elapsed: 0:00:14. Batch 160 of 241. Elapsed: 0:00:19. Batch 200 of 241. Elapsed: 0:00:23. Batch 240 of 241. Elapsed: 0:00:28. Average training loss: 0.20 Training epcoh took: 0:00:28 Running Validation... Accuracy: 0.81 Validation took: 0:00:01 ======== Epoch 3 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:05. Batch 80 of 241. Elapsed: 0:00:09. Batch 120 of 241. Elapsed: 0:00:14. Batch 160 of 241. Elapsed: 0:00:19. Batch 200 of 241. Elapsed: 0:00:23. Batch 240 of 241. Elapsed: 0:00:28. Average training loss: 0.13 Training epcoh took: 0:00:28 Running Validation... Accuracy: 0.82 Validation took: 0:00:01 ======== Epoch 4 / 4 ======== Training... Batch 40 of 241. Elapsed: 0:00:05. Batch 80 of 241. Elapsed: 0:00:09. Batch 120 of 241. Elapsed: 0:00:14. Batch 160 of 241. Elapsed: 0:00:19. Batch 200 of 241. Elapsed: 0:00:23. Batch 240 of 241. Elapsed: 0:00:28. Average training loss: 0.12 Training epcoh took: 0:00:28 Running Validation... Accuracy: 0.82 Validation took: 0:00:01 Training complete! 1234567891011121314151617181920212223# 训练过程的损失变化import matplotlib.pyplot as plt%matplotlib inlineimport seaborn as sns# Use plot styling from seaborn.sns.set(style='darkgrid')# Increase the plot size and font size.sns.set(font_scale=1.5)plt.rcParams[&quot;figure.figsize&quot;] = (12,6)# Plot the learning curve.plt.plot(loss_values, 'b-o')# Label the plot.plt.title(&quot;Training loss&quot;)plt.xlabel(&quot;Epoch&quot;)plt.ylabel(&quot;Loss&quot;)plt.show() 评估模型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 测试集数据处理import pandas as pd# Load the dataset into a pandas dataframe.df = pd.read_csv(&quot;../datasets/cola_public//raw/out_of_domain_dev.tsv&quot;, delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])# Report the number of sentences.print('Number of test sentences: {:,}\\n'.format(df.shape[0]))# Create sentence and label listssentences = df.sentence.valueslabels = df.label.values# 文本预处理# Tokenize all of the sentences and map the tokens to thier word IDs.input_ids = []# For every sentence...for sent in sentences: # `encode` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. encoded_sent = tokenizer.encode( sent, # Sentence to encode. add_special_tokens=True, # Add '[CLS]' and '[SEP]' ) input_ids.append(encoded_sent)# Pad our input tokensinput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=&quot;long&quot;, truncating=&quot;post&quot;, padding=&quot;post&quot;)# Create attention masksattention_masks = []# Create a mask of 1s for each token followed by 0s for paddingfor seq in input_ids: seq_mask = [float(i &gt; 0) for i in seq] attention_masks.append(seq_mask)# Convert to tensors.prediction_inputs = torch.tensor(input_ids)prediction_masks = torch.tensor(attention_masks)prediction_labels = torch.tensor(labels)# Set the batch size.batch_size = 32# Create the DataLoader.prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)prediction_sampler = SequentialSampler(prediction_data)prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size) Number of test sentences: 516 1234567891011121314151617181920212223242526272829303132333435363738# 在测试集上进行预测print('Predicting labels for {:,} test sentences...'.format( len(prediction_inputs)))# Put model in evaluation modemodel.eval()# Tracking variablespredictions, true_labels = [], []# Predictfor batch in prediction_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch # Telling the model not to compute or store gradients, saving memory and # speeding up prediction with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to('cpu').numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids)print(' DONE.') Predicting labels for 516 test sentences... DONE. 123print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0))) Positive samples: 354 of 516 (68.60%) 1234567891011121314151617181920212223242526272829303132from sklearn.metrics import matthews_corrcoefmatthews_set = []# Evaluate each test batch using Matthew's correlation coefficientprint('Calculating Matthews Corr. Coef. for each batch...')# For each input batch...for i in range(len(true_labels)): # The predictions for this batch are a 2-column ndarray (one column for &quot;0&quot; # and one column for &quot;1&quot;). Pick the label with the highest value and turn this # in to a list of 0s and 1s. pred_labels_i = np.argmax(predictions[i], axis=1).flatten() # Calculate and store the coef for this batch. matthews = matthews_corrcoef(true_labels[i], pred_labels_i) matthews_set.append(matthews)# Combine the predictions for each batch into a single list of 0s and 1s.flat_predictions = [item for sublist in predictions for item in sublist]flat_predictions = np.argmax(flat_predictions, axis=1).flatten()# Combine the correct labels for each batch into a single list.flat_true_labels = [item for sublist in true_labels for item in sublist]# Calculate the MCCmcc = matthews_corrcoef(flat_true_labels, flat_predictions)print('MCC: %.3f' % mcc) MCC: 0.524 Matthews相关系数，用于度量二分类的质量。它会考虑TP/FP/TN/FP的情况，通常被认为是一个balanced的度量 ，可以用于那些有着不同size的分类中。MCC本质上是一个介于［－1，+1］之间的相关系数值。相关系数为+1，表示是一个完美的预测，0表示是一个平均随机预测（average random prediction），而-1表示是一个逆预测（inverse prediction）。这种统计方法也被称为：phi coefficient。 \\[MCC = \\frac{tp \\times tn - fp \\times fn}{\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\\] 1 BERT用于问答 给定question文本和包含该问题答案的对应的answer_text文本，从中找出question的答案 12345678import torchfrom transformers import BertForQuestionAnswering, BertTokenizermodel_path = &quot;../../H/models/huggingface/bert-large-uncased-whole-word-masking-\\finetuned-squad/&quot;tokenizer = BertTokenizer.from_pretrained(model_path + &quot;vocab.txt&quot;)model = BertForQuestionAnswering.from_pretrained(model_path) Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated 123question = &quot;How many parameters does BERT-large have?&quot;answer_text = &quot;BERT-large is really big...it has 24-layers and an embedding size \\of 1024, for a total of 340M parameters!&quot; 12input_ids = tokenizer.encode(question, answer_text)print(&quot;The input has a total of {:} tokens.&quot;.format(len(input_ids))) The input has a total of 44 tokens. 123tokens = tokenizer.convert_ids_to_tokens(input_ids)for token, id in zip(tokens, input_ids): print(&quot;{:&lt;12} {:&gt;6,}&quot;.format(token, id)) [CLS] 101 how 2,129 many 2,116 parameters 11,709 does 2,515 bert 14,324 - 1,011 large 2,312 have 2,031 ? 1,029 [SEP] 102 bert 14,324 - 1,011 large 2,312 is 2,003 really 2,428 big 2,502 . 1,012 . 1,012 . 1,012 it 2,009 has 2,038 24 2,484 - 1,011 layers 9,014 and 1,998 an 2,019 em 7,861 ##bed 8,270 ##ding 4,667 size 2,946 of 1,997 102 9,402 ##4 2,549 , 1,010 for 2,005 a 1,037 total 2,561 of 1,997 340 16,029 ##m 2,213 parameters 11,709 ! 999 [SEP] 102 12345678910# segment masksep_index = input_ids.index(tokenizer.sep_token_id)num_seg_a = sep_index + 1num_seg_b = len(input_ids) - num_seg_asegment_ids = [0] * num_seg_a + [1] * num_seg_bassert len(segment_ids) == len(input_ids) 12345# 输入为一维，即无批数据维度时，不需要 pad start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))start_scores.shape, end_scores.shape (torch.Size([1, 44]), torch.Size([1, 44])) 12345# 选择策略，不仅仅：是单个向量中最大的answer_start = torch.argmax(start_scores)answer_end = torch.argmax(end_scores)answer = &quot; &quot;.join(tokens[answer_start:answer_end + 1])print(&quot;Answer: &quot; + answer) Answer: 340 ##m 12345678# 将 子词 重新组合成 单词answer = tokens[answer_start]for i in range(answer_start + 1, answer_end + 1): if tokens[i][0:2] == '##': answer += tokens[i][2:] else: anser += &quot; &quot; + tokens[i]print(&quot;Answer: &quot; + answer) Answer: 340m 12345678910111213141516171819202122# 可视化import matplotlib.pyplot as pltimport seaborn as snssns.set(style='darkgrid')plt.rcParams['figure.figsize'] = (16, 8)# 权重分布s_scores = start_scores.detach().numpy().flatten()e_scores = end_scores.detach().numpy().flatten()# 对应的标签token_labels = []for (i, token) in enumerate(tokens): token_labels.append(&quot;{:} - {:}&quot;.format(token, i))# 可视化权重分布ax = sns.barplot(x=token_labels, y=s_scores, ci=None)ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='center')ax.grid(True)plt.title(&quot;Start Word Scores&quot;)plt.show &lt;function matplotlib.pyplot.show(*args, **kw)&gt; 123456ax = sns.barplot(x=token_labels, y=e_scores, ci=None)ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='center')ax.grid(True)plt.title(&quot;Start Word Scores&quot;)plt.show &lt;function matplotlib.pyplot.show(*args, **kw)&gt; 123456789101112131415161718192021222324252627import pandas as pdscores = []for (i, token_label) in enumerate(token_labels): scores.append({ &quot;token_label&quot;: token_label, &quot;score&quot;: s_scores[i], &quot;marker&quot;: 'start' }) scores.append({ &quot;token_label&quot;: token_label, &quot;score&quot;: e_scores[i], &quot;marker&quot;: 'end' })df = pd.DataFrame(scores)g = sns.catplot(x='token_label', y='score', hue='marker', data=df, kind='bar', height=6, aspect=4)g.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha='center')g.ax.grid(True) 1 总结 BERT 是怎么用 Transformer 的？ BERT_BASE:L=12,H=768,A=12:层的数量为 L，隐藏层的维度为 H，自注意头的个数为 A，前馈过滤器的维度为 4H BERT 的输入向量表示，比 Transformer 多了 Segment Embeddings ELMo 和 BERT 从网络结构以及最后的实验效果来看，BERT 比 ELMo 效果好主要集中在以下几点原因： LSTM 抽取特征的能力远弱于 Transformer BERT 的训练数据以及模型参数均多余 ELMo，这也是比较重要的一点 ELMo 和 BERT 的区别是什么？ ELMo 模型是通过语言模型任务得到句子中单词的 embedding 表示，以此作为补充的新特征给下游任务使用。这一类预训练的方法被称为 Feature-based Pre-Training。 BERT 模型是“基于 Fine-tuning 的模式”，这种做法和图像领域基于 Fine-tuning 的方式基本一致，下游任务需要将模型改造成 BERT 模型，才可利用 BERT 模型预训练好的参数。 BERT 的输入和输出 最底层输入是文本中各个字/词(token)的原始词向量，该向量既可以随机初始化，也可以利用 Word2Vector 等算法进行预训练以作为初始值 英文词汇被进一步切割，划分为更细粒度的 WordPiece，而不是常见的单词 中文语料是直接划分成单字 除此之外还包含文本向量(Segment Embeddings)，该向量的取值在模型训练过程中自动学习，用于刻画文本的全局语义信息，并与单字/词的语义信息相融合 位置向量(Position Embeddings)：由于出现在文本不同位置的字/词所携带的语义信息存在差异（比如：“我爱你”和“你爱我”），因此，BERT 模型对不同位置的字/词分别附加一个不同的向量以作区分 三者加和作为模型输入 输出是文本中各个字/词融合了全文语义信息后的向量表示 在做 Next Sentence Prediction 任务时，在第一个句子的首部会加上一个[CLS] token，在两个句子中间以及最后一个句子的尾部会加上一个[SEP] token 如何用 BERT 结构怎么做 fine-tuning 多标签分类（不同于多类别） 输入通过得到 embedding 表示之后，再并行连接 n 个全连接层，再分别接上 softmax 分类；每个全连接层对一个标签分类 TODO Transformers模块中BERT的简单运用 12345678910111213import torchfrom transformers import BertModel, BertTokenizermodel_name = 'bert-base-uncased'tokenizer = BertTokenizer.from_pretrained(model_name)model = BertModel.from_pretrained(model_name)input_text = &quot;Here is some text to encode&quot;input_ids = tokenizer.encode(input_text)input_ids = torch.tensor([input_ids])with torch.no_grad(): last_hidden_states = model(input_ids)[0] BERT模型 BERT代表Bidirectional Encoder Representations from Transformers，BERT模型的主要结构是Transformer的编码器 BERT是基于大量无标签文本(Wikipedia-2500M单词+书籍-800M单词)进行预训练的 预训练步骤是BERT成功的主要原因，在大量语料上进行训练后，模型开始“理解”语言 BERT是“深度双向”模型，训练时从一个token的左右两边学习信息 如上图中的单词bank，基于其左边和右边不同的信息，有不同的涵义 通过在BERT的预训练模型上添加上一些额外的输出层进行优调，可以很好的用于一些列NLP任务 BERT输入 Position Embeddings：两个句子，“武汉到广州的车票”与“广州到武汉的车票”，self-attention是不考虑token在句子中的位置，因此无法区别这两句话；因此引入位置编码，句子中的每个位置都有一个独特的位置编码 Segment Embeddings：预训练任务NSP，输入文本是句子对，需要预测两句话是否是连续的两句话，因此添加句子编码 Token Embeddings：每个token的词表征","link":"/posts/17970.html"},{"title":"腾讯精选50题（Python实现）","text":"前言 代码精炼是 Python 的核心，同时能够反应对于语言的熟练程度，本项目目的在于汇总 leet code 最短最优雅的解法，拒绝长篇大论，缩短学习周期，掌握各种技巧，助您在面试中写出令人眼前一亮的解答，给考官留个好印象。 为什么我们追求最短?1.短代码更pythonic，而且通常能够避免一些冗余过程。2.除了刷题掌握算法思路之外，我们更追求深入理解和掌握python,学会套用技巧，举一反三。3.真正面试的时候不一定要这么短，可以适当展开几行(O_o 除非你就是想秀其他人一脸 😅)，保证思路更清晰，相信就算展开几行也会比其他题解短很多吧。4.刷题很累，找点乐子，送给自己一些成就感吧。5.所有已收录代码都是优中选优，有利于缩短学习周期，除了短代码外，另有常规解法作为补充。 项目持续更新中，优先使用 python3，不支持的题目使用 python2 代替，如果您有更短更优雅的解法希望分享的话欢迎联系更新~ [直接发issue 或 fork，记得留下署名和联系方式 :panda_face:] 鉴于追求的主题，此项目收录 1.在代码量上明显优于现有解的最短代码 2.思路更高效的一般解法（作为补充放在首选解之后） [题目自带的代码不计入代码量] 如果您对当前解析有任何疑问，咱们 issue 见~ 由于CSDN博客更新需要人工审核比较慢，所以迁移到github上，优先更新github内容。 为了快速找到题目可以按 [Ctrl键 + F键] 输入题目序号或名字定位。 # 里程碑 :penguin: 腾讯精选练习（50题: 25简单 21中等 4困难） 代码行数 总计：140行 平均：2.8行 :bookmark_tabs: 题目详情 ## 推荐 与本项目有关联的，是一个C++最清晰题解汇总👻。Python篇注重熟悉语言特性，充分利用高级语言提供的已内置的功能避免冗余编码，最低成本地解决问题。C++篇注重通用思想，分专题逐个击破，深入探究算法流程。俩者同时服用效果更佳，只想学一门也不必担心，俩个项目相辅相成，Python篇会在题解之后添加常规解法作为补充，并聚合C++篇精华总结一套运用python独特技巧的专题，C++篇会利用python题解的思想优化代码，保证代码简洁，可读性高。 🌟 推荐刷题路线：专题剖析 → 腾讯精选50题 → 全题解析 解析 默认已看过题目，🤡 没看过的话点标题可以跳转链接 ## 1. Two Sum 2行 1234class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: d = {target - n: i for i, n in enumerate(nums)} return [[d[n], i] for i, n in enumerate(nums) if n in d and d[n] &gt; i][0] O(N)时间效率的快速解法，用字典记录 需要的值:当前索引 如果字典中存在相同的数字，那么将会记录比较大的那个索引，因此可以用d[n] &gt; i来避免一个元素重复选择 改成 for 循环加 break 再加动态修改字典能更快一点 123456class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: d = {} for i, n in enumerate(nums): if n in d: return [d[n], i] d[target-n] = i 2. Add Two Numbers 5行 12345678910111213# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode, carry=0) -&gt; ListNode: if not (l1 or l2): return ListNode(1) if carry else None l1, l2 = l1 or ListNode(0), l2 or ListNode(0) val = l1.val + l2.val + carry l1.val, l1.next = val % 10, self.addTwoNumbers(l1.next, l2.next, val &gt; 9) return l1 int(True) 等于 1 None or 7 等于 7 用 carry 记录是否应该进位 ## 3. Longest Substring Without Repeating Characters 3行 12345class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: b, m, d = 0, 0, {} for i, l in enumerate(s): b, m, d[l] = max(b, d.get(l, -1) + 1), max(m, i - b), i return max(m, len(s) - b) b代表起始位置，m代表上一步的最大无重复子串，d是一个字典，记录着到当前步骤出现过的字符对应的最大位置 每次迭代过程中，遇到遇见过的字符时，b就会变为那个字符上一次出现位置+1，m记录上一次应该达到的全局最大值，所以最后需要再比较一次 ## 4. Median of Two Sorted Arrays 5行 1234567class Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: a, b, m = *sorted((nums1, nums2), key=len), (len(nums1) + len(nums2) - 1) // 2 self.__class__.__getitem__ = lambda self, i: m-i-1 &lt; 0 or a[i] &gt;= b[m-i-1] i = bisect.bisect_left(self, True, 0, len(a)) r = sorted(a[i:i+2] + b[m-i:m-i+2]) return (r[0] + r[1 - (len(a) + len(b)) % 2]) / 2 本题思路与官方题解类似，时间复杂度O(log(min(m, n)))，没看过的话建议先大体了解一下 python 中 bisect 模块针对的是 list, 如果直接构造 list，时间复杂度为 O(min(m, n))，因此我们修改当前类的魔法方法伪造 list 在一个有序递增数列中，中位数左边的那部分的最大值一定小于或等于右边部分的最小值 如果总数组长度为奇数，m 代表中位数的索引，否则 m 代表用于计算中位数的那两个数字的左边一个。比如输入为[1,2]，[3]，那么m应该为[1,2,3]中位数2的索引1，如果输入为[1,3]，[2,4]，那么m应该为[1,2,3,4]中2的索引1 使用二分搜索找到 m 对应的值在a或b中对应的索引，也就是说，我们要找的中位数或中位数左部应该是 a[i] 或者 b[m-i] bisect.bisect_left 搜索列表中保持列表升序的情况下，True应该插入的位置（从左侧），比如 [F,F,T] 返回 2，[F,F] 返回 2 这里保证 a 是 nums1 和 nums2 中较短的那个，是为了防止二分搜索的时候索引越界 sorted返回一个list，假设返回值是 [nums1, nums2]，那么前面加个 * 号就代表取出列表的所有内容，相当于一个迭代器，结果相当于直接写 nums1, nums2 ## 5. Longest Palindromic Substring 5行 1234567class Solution: def longestPalindrome(self, s: str) -&gt; str: r = '' for i, j in [(i, j) for i in range(len(s)) for j in (0, 1)]: while i &gt; -1 and i + j &lt; len(s) and s[i] == s[i + j]: i, j = i - 1, j + 2 r = max(r, s[i + 1:i + j], key=len) return '' if not s else r 遍历字符串的每个索引 i，判断能否以 s[i] 或 s[i:i+j+1] 为中心向往拓展回文字符串 ## 7. Reverse Integer 2行 1234class Solution: def reverse(self, x): r = x // max(1, abs(x)) * int(str(abs(x))[::-1]) return r if r.bit_length() &lt; 32 or r == -2**31 else 0 x // max(1, abs(x))意味着 0：x为0， 1：x为正， -1：x为负，相当于被废弃的函数cmp [::-1]代表序列反转 2^31 和 -2^31 的比特数为32，其中正负号占用了一位 32位整数范围 [−2^31, 2^31 − 1] 中正数范围小一个是因为0的存在 ## 8. String to Integer (atoi) 1行 123class Solution: def myAtoi(self, s: str) -&gt; int: return max(min(int(*re.findall('^[\\+\\-]?\\d+', s.lstrip())), 2**31 - 1), -2**31) 使用正则表达式 ^：匹配字符串开头，[\\+\\-]：代表一个+字符或-字符，?：前面一个字符可有可无，\\d：一个数字，+：前面一个字符的一个或多个，\\D：一个非数字字符，*：前面一个字符的0个或多个 max(min(数字, 2**31 - 1), -2**31) 用来防止结果越界 ## 9. Palindrome Number 1行 123class Solution: def isPalindrome(self, x: int) -&gt; bool: return str(x) == str(x)[::-1] 不使用字符串的进阶解法： 1234class Solution: def isPalindrome(self, x: int) -&gt; bool: r = list(map(lambda i: int(10**-i * x % 10), range(int(math.log10(x)), -1, -1))) if x &gt; 0 else [0, x] return r == r[::-1] 思路是一样的，这里把整数转成了列表而不是字符串 比如一个整数12321，我想取出百位数可以这么做：12321 * 10^{int(log_{10}12321)} % 10 = 123 % 10 = 3 ## 11. Container With Most Water 3行 12345class Solution: def maxArea(self, height: List[int]) -&gt; int: res, l, r = 0, 0, len(height) - 1 while l &lt; r: res, l, r = (max(res, height[l] * (r - l)), l + 1, r) if height[l] &lt; height[r] else (max(res, height[r] * (r - l)), l, r - 1) return res 双指针 O(N) 解法 res：结果，l：容器左壁索引，r：容器右壁索引 如果 height[l] &lt; height[r] 那么 l += 1 否则 r -= 1，说明：如果 height[0] &lt; height[3] 那么(0, 1), (0, 2)对应的容器体积一定小于(0, 3)的，因为此时计算体积的时候高为 height(0)，容器的宽减少而高不增加，面积必然缩小 ## 13. Roman to Integer 2行 1234class Solution: def romanToInt(self, s: str) -&gt; int: d = {'I':1, 'IV':3, 'V':5, 'IX':8, 'X':10, 'XL':30, 'L':50, 'XC':80, 'C':100, 'CD':300, 'D':500, 'CM':800, 'M':1000} return sum([d.get(s[max(i-1, 0):i+1], d[n]) for i, n in enumerate(s)]) 构建一个字典记录所有罗马数字子串，注意长度为2的子串记录的值是（实际值-子串内左边罗马数字代表的数值） 这样一来，遍历整个s的时候判断当前位置和前一个位置的两个字符组成的字符串是否在字典内，如果在就记录值，不在就说明当前位置不存在小数字在前面的情况，直接记录当前位置字符对应值 举个例子，遍历经过IV的时候先记录I的对应值1再往前移动一步记录IV的值3，加起来正好是IV的真实值4。max函数在这里是为了防止遍历第一个字符的时候出现[-1:0]的情况 ## 14. Longest Common Prefix 2行 1234class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: r = [len(set(c)) == 1 for c in zip(*strs)] + [0] return strs[0][:r.index(0)] if strs else '' 利用好zip和set ## 15. 3Sum 5行 1234567class Solution: def threeSum(self, nums: List[int]) -&gt; List[List[int]]: nums, r = sorted(nums), set() for i in [i for i in range(len(nums)-2) if i &lt; 1 or nums[i] &gt; nums[i-1]]: d = {-nums[i]-n: j for j, n in enumerate(nums[i + 1:])} r.update([(nums[i], n, -nums[i]-n) for j, n in enumerate(nums[i+1:]) if n in d and d[n] &gt; j]) return list(map(list, r)) 时间复杂度：O(N^2) 这里 sort 一是为了避免重复，这一点可以体现在我们输出的结果都是升序的，如果不这么做 set 无法排除一些相同结果，而是为了节省计算，防止超时 for 循环内部的代码思想同第一题 Two Sum，用字典记录｛需要的值:当前索引｝，如果字典中存在相同的数字，那么将会记录比较大的那个索引，因此可以用d[n] &gt; i来避免一个元素重复选择 (nums[i], n, -nums[i]-n)保证了列表升序 ## 16. 3Sum Closest 7行 123456789class Solution: def threeSumClosest(self, nums: List[int], target: int) -&gt; int: nums, r, end = sorted(nums), float('inf'), len(nums) - 1 for c in range(len(nums) - 2): i, j = max(c + 1, bisect.bisect_left(nums, target - nums[end] - nums[c], c + 1, end) - 1), end while r != target and i &lt; j: s = nums[c] + nums[i] + nums[j] r, i, j = min(r, s, key=lambda x: abs(x - target)), i + (s &lt; target), j - (s &gt; target) return r float('inf') = 正无穷 排序，遍历，双指针，O(N^2) 时间复杂度，二分法初始化 排序是为了使用双指针，首先遍历得到索引 c，然后计算 c，左指针 i，右指针 j 对应数字之和，如果大于 target，j 向内移动，否则 i 向内移动 i 的初始值不是 c + 1，是为了减少计算量，用二分法得到一个合理的初始值 ## 20. Valid Parentheses 2行 1234class Solution: def isValid(self, s: str) -&gt; bool: while any(('()' in s, '[]' in s, '{}' in s)): s = s.replace('()', '').replace('[]', '').replace('{}', '') return not s 不断删除有效括号直到不能删除，思路简单效率低。另外，stack的方法也很简单，而且快多了。 123456789class Solution: def isValid(self, s: str) -&gt; bool: stack, d = [], {'{': '}', '[': ']', '(': ')'} for p in s: if p in '{[(': stack += [p]; elif not (stack and d[stack.pop()] == p): return False return not stack 21. Merge Two Sorted Lists 4行 123456789101112# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: if l1 and l2: if l1.val &gt; l2.val: l1, l2 = l2, l1 l1.next = self.mergeTwoLists(l1.next, l2) return l1 or l2 7 or 9 等于 7 None and 7 等于 None sorted用在这里为了保证 l1 的值小于等于 l2 的值 ## 23. Merge k Sorted Lists 4行 123456789101112# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeKLists(self, lists: List[ListNode]) -&gt; ListNode: r, n, p = [], lists and lists.pop(), None while lists or n: r[len(r):], n = ([n], n.next or lists and lists.pop()) if n else ([], lists.pop()) for n in sorted(r, key=lambda x: x.val, reverse=True): n.next, p = p, n return n if r else [] 本题思路： 把题目给的所有链表中的所有节点放进一个列表 r。 对这个列表 r 中的所有节点进行从大到小的排序。O(NlogN) 把每个节点的指针指向前一个节点。（第一个节点，也就是最大的那个，指向None。） 返回最后一个节点，也就是整个新链表的开头。 如何把所有节点放进 r(result link)？ 我们首先初始化 r 为空列表，初始化 n(node) 为题目所给的第一个链表的开头节点，并删除lists中的这个节点，接着进入while循环，如果 n 不为空，那么 r += [n]，这里使用了切片的技巧（r[len(r):]=[n]相当于r=r+[n]），n=n.next，如果n是第一个链表的最后一个节点的话n.next就是None，下一次while的时候如果lists不为空就说明还有别的链表，此时n为None，我们让 r 不变，n=lists.pop()，也就是从lists中再取下一个节点赋值给n，重复以上步骤直到 lists 为空，我们就把所有节点放进 r 了。 怎么对 r 排序？ 用了sorted函数，其中key定义了排序时用来比较的是每个元素的val属性，同时设置reverse为True代表降序排序。 如何修改每个节点的指针？ 我们初始化 p(previous node) 为None。遍历降序排好的列表 r，r中的第一个元素就是值最大的元素，也就是我们应该返回的链表的结尾，我们设置它指向None，然后让p=这个节点，继续for循环。之后每经过一个节点 n 就把这个节点的next属性设置为上一个节点 p，遍历完成之后的 n，也就是我们遍历经过的最后一个元素，拥有最小的值，自然就是整个新链表的起始节点，我们将其作为输出值，函数返回。 26. Remove Duplicates from Sorted Array 3行 12345class Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: for i in range(len(nums)-1, 0, -1): if nums[i] == nums[i-1]: nums.pop(i) return len(nums) 时间效率O(N)空间效率O(1)，逆遍历可以防止删除某个元素后影响下一步索引的定位。 ## 28. Implement strStr() 1行 123class Solution: def strStr(self, haystack: str, needle: str) -&gt; int: return haystack.find(needle) 不用内置函数也可以 123456class Solution: def strStr(self, haystack: 'str', needle: 'str') -&gt; 'int': for i in range(0, len(haystack) - len(needle) + 1): if haystack[i:i+len(needle)] == needle: return i return -1 ## 33. Search in Rotated Sorted Array 3行 12345class Solution: def search(self, nums, target): self.__class__.__getitem__ = lambda self, m: not(target &lt; nums[0] &lt;= nums[m] or nums[0] &lt;= nums[m] &lt; target or nums[m] &lt; target &lt;= nums[-1]) i = bisect.bisect_left(self, True, 0, len(nums)) return i if target in nums[i:i+1] else -1 作出数列的函数图像，可以看作是一个含断点的局部递增函数，形如:zap:，前面一段总是比较高 python 中 bisect 模块针对的是 list, 如果直接构造 list，相当于遍历所有元素，时间复杂度为 O(N) 而不是 O(logN)，因此我们修改当前类的魔法方法伪造 list，然后用当前类代替list 用二分搜索时，m 代表 middle，low 代表 low，hi 代表 high，当满足任一条件｛① targe &lt; middle 且 middle 在前一段上 且 target &lt; nums[0] ② target &gt; middle 且 middle 在第一段上 ③ target &gt; middle 且 middle 在第二段上 且 target &lt;= nums[-1]｝时，应该向右搜索，因此 getitem 返回 False。 另外还有一种简单的思路：二分法找到断点的位置恢复原始数组，然后正常二分法即可 1234567891011121314151617class Solution: def search(self, nums, target): lo, hi, k = 0, len(nums) - 1, -1 while lo &lt;= hi: m = (lo + hi) // 2 if m == len(nums) - 1 or nums[m] &gt; nums[m+1]: k = m + 1 break elif m == 0 or nums[m] &lt; nums[m-1]: k = m break if nums[m] &gt; nums[0]: lo = m + 1 else: hi = m - 1 i = (bisect.bisect_left(nums[k:] + nums[:k], target) + k) % max(len(nums), 1) return i if nums and nums[i] == target else -1 ## 38. Count and Say 1行 123class Solution: def countAndSay(self, n: int) -&gt; str: return '1' * (n is 1) or re.sub(r'(.)\\1*', lambda m: str(len(m.group())) + m.group(1), self.countAndSay(n - 1)) 正则表达式 re.sub(正则，替换字符串或函数，被替换字符串，是否区分大小写) . 可匹配任意一个除了 (.) 匹配任意一个除了 (.)\\1 匹配一个任意字符的二次重复并把那个字符放入数组 (.)\\1* 匹配一个任意字符的多次重复并把那个字符放入数组 group(default=0)可以取匹配文本 group(1)取第一个括号内的文本 ## 43. Multiply Strings 5行 1234567class Solution: def multiply(self, num1: str, num2: str) -&gt; str: d = {} for i, n1 in enumerate(num1[::-1]): for j, n2 in enumerate(num2[::-1]): d[i + j] = d.get(i + j, 0) + int(n1) * int(n2) for k in [*d]: d[k + 1], d[k] = d.get(k + 1, 0) + int(d[k] * 0.1), d[k] % 10 return re.sub('^0*', '', ''.join(map(str, d.values()))[::-1]) or '0' 本题的难点在于计算整数的时候不能超过32bits，因此使用竖式计算 我们遍历num1中的每个数字n1，然后带着这个数字遍历num2中的每个数字n2做乘法，所得乘积放进 d 中相应的位置然后逐位计算结果 i + j 正好对应俩个数字相乘后所在的位置，比如 0 + 0 就应该是个位， 0 + 1 就是十位， 1 + 1 百位。这里所说的位置可以参考这篇博客中的过程图 ## 46. Permutations 1行 123class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: return [[n] + sub for i, n in enumerate(nums) for sub in self.permute(nums[:i] + nums[i+1:])] or [nums] 每次固定第一个数字递归地排列数组剩余部分 python 有内置函数可以直接实现 1234class Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: from itertools import permutations return list(permutations(nums)) ## 53. Maximum Subarray 2行 1234class Solution: def maxSubArray(self, nums): from functools import reduce return reduce(lambda r, x: (max(r[0], r[1]+x), max(r[1]+x,x)), nums, (max(nums), 0))[0] reduce 函数详解 r[0]代表以当前位置为结尾的局部最优解 r[1]代表全局最优解 直接DP的解法更好理解一些 12345class Solution: def maxSubArray(self, nums: List[int]) -&gt; int: for i in range(1, len(nums)): nums[i] = max(nums[i], nums[i] + nums[i-1]) return max(nums) ## 54. Spiral Matrix 1行 123class Solution: def spiralOrder(self, matrix: List[List[int]]) -&gt; List[int]: return matrix and [*matrix.pop(0)] + self.spiralOrder([*zip(*matrix)][::-1]) 为什么是[*matrix.pop(0)]而不是matrix.pop(0)？因为对于后面的递归，传进来的列表中元素是tuple ## 58. Length of Last Word 1行 123class Solution: def lengthOfLastWord(self, s: str) -&gt; int: return len(s.strip(' ').split(' ')[-1]) ## 59. Spiral Matrix II 3行 12345class Solution: def generateMatrix(self, n: int) -&gt; List[List[int]]: r, n = [[n**2]], n**2 while n &gt; 1: n, r = n - len(r), [[*range(n - len(r), n)]] + [*zip(*r[::-1])] return r 流程图 123|| =&gt; |9| =&gt; |8| |6 7| |4 5| |1 2 3| |9| =&gt; |9 8| =&gt; |9 6| =&gt; |8 9 4| |8 7| |7 6 5| ## 61. Rotate List 4行 123456789101112# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def rotateRight(self, head: ListNode, k: int) -&gt; ListNode: l = [] while head: l[len(l):], head = [head], head.next if l: l[-1].next, l[-1 - k % len(l)].next = l[0], None return l[- k % len(l)] if l else None ## 62. Unique Paths 1行 123class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: return int(math.factorial(m+n-2)/math.factorial(m-1)/math.factorial(n-1)) 题目可以转换为排列组合问题，解是C(min(m,n), m+n)，从m+n个中选出m个下移或n个右移。 用DP做也很快，以后自己算 C(a, b) 也可以用算这题的DP法代替 math.factorial 的速度不亚于DP，可能内部有优化 0的阶乘为1 ## 66. Plus One 1行 123class Solution: def plusOne(self, digits: List[int]) -&gt; List[int]: return list(map(int, str(int(''.join(map(str, digits))) + 1))) 69. Sqrt(x) 1行 123class Solution: def mySqrt(self, x: int) -&gt; int: return int(x ** 0.5) 出题者应该是希望看到下面的答案： 123456class Solution: def mySqrt(self, x: int) -&gt; int: r = x while r*r &gt; x: r = (r + x/r) // 2 return int(r) - 基本不等式(a+b)/2 &gt;=√ab 推导自 (a-b)^2 &gt;= 0，注意 a&gt;0 且 b&gt;0 - r 代表 result ## 70. Climbing Stairs 2行 1234class Solution: def climbStairs(self, n: int) -&gt; int: from functools import reduce return reduce(lambda r, _: (r[1], sum(r)), range(n), (1, 1))[0] dp递归方程：到达当前楼梯的路径数 = 到达上个楼梯的路径数 + 到达上上个楼梯的路径数 这里用一个元组 r 来储存（当前楼梯路径数，下一层楼梯路径数） 利用 reduce 来代替for循环。reduce 函数详解 ## 78. Subsets 2行 1234class Solution: def subsets(self, nums: List[int]) -&gt; List[List[int]]: from itertools import combinations return sum([list(combinations(nums, i)) for i in range(len(nums) + 1)], []) ## 88. Merge Sorted Array 1行 123456class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums1 in-place instead. &quot;&quot;&quot; while n &gt; 0: nums1[m+n-1], m, n = (nums1[m-1], m-1, n) if m and nums1[m-1] &gt; nums2[n-1] else (nums2[n-1], m, n-1) 这种题倒着算更容易 上面那行代码其实就相当于： 12345678910class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums1 in-place instead. &quot;&quot;&quot; while n &gt; 0: if m and nums1[m-1] &gt; nums2[n-1]: nums1[m+n-1], m, n = nums1[m-1], m-1, n else: nums1[m+n-1], m, n = nums2[n - 1], m, n-1 ## 89. Gray Code 1行 123class Solution: def grayCode(self, n: int) -&gt; List[int]: return (lambda r: r + [x | 1&lt;&lt;n-1 for x in r[::-1]])(self.grayCode(n-1)) if n else [0] 前4个结果： 1234[0][0 1][00 01 11 10][000 001 011 010 110 111 101 100] 递归方程：这一步结果 = 上一步结果 + 上一步结果的镜像并在每个二进制数字前面加一位1 &lt;&lt; 左移符号，即在二进制表示后加一位 0 ，例子：3&lt;&lt;1 等于 6（011 → 110），相当于 3 * 2的1次方 x | 1&lt;&lt;n-1 就是在十进制数字 x 的二进制前面加一位1之后的十进制结果，比如 x = 1，有 1 | 10 等于 110 循环可以避免一些不必要的操作，会比递归快一些： 123456class Solution: def grayCode(self, n: int) -&gt; List[int]: r = [0] for i in range(n): r.extend([x | 1&lt;&lt;i for x in r[::-1]]) return r 或者直接背格雷码的公式🥶吧： 123class Solution: def grayCode(self, n: int) -&gt; List[int]: return [i ^ i &gt;&gt; 1 for i in range(1 &lt;&lt; n)] ## 94. Binary Tree Inorder Traversal 2行 1234567891011# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def inorderTraversal(self, root: TreeNode) -&gt; List[int]: f = self.inorderTraversal return f(root.left) + [root.val] + f(root.right) if root else [] 递归 12345678910111213class Solution: def inorderTraversal(self, root: TreeNode) -&gt; List[int]: r, stack = [], [] while True: while root: stack.append(root) root = root.left if not stack: return r node = stack.pop() r.append(node.val) root = node.right return r 迭代 ## 104. Maximum Depth of Binary Tree 1行 12345678910# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def maxDepth(self, root: TreeNode) -&gt; int: return max(map(self.maxDepth,(root.left, root.right))) + 1 if root else 0 利用map函数递归左右节点获取最大值，map函数会将参数一所指向的函数应用于参数二里的所有对象并返回所有结果构成的迭代器 ## 121. Best Time to Buy and Sell Stock 2行 1234class Solution: def maxProfit(self, prices: List[int]) -&gt; int: from functools import reduce return reduce(lambda r, p: (max(r[0], p-r[1]), min(r[1], p)), prices, (0, float('inf')))[0] r = (结果，之前遍历过的所有元素中的最小值) reduce 函数详解 ## 122. Best Time to Buy and Sell Stock II 2行 123class Solution: def maxProfit(self, prices: List[int]) -&gt; int: return sum(b - a for a, b in zip(prices, prices[1:]) if b &gt; a) 本题可以在同一天买入和卖出，因此只要当天票价比昨天的高就可以卖出 ## 124. Binary Tree Maximum Path Sum 4行 12345678910111213# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def maxPathSum(self, root: TreeNode, ok=True) -&gt; int: if not root: return 0 l, r = self.maxPathSum(root.left, False), self.maxPathSum(root.right, False) self.max = max(getattr(self, 'max', float('-inf')), l + root.val + r) return self.max if ok else max(root.val + max(l, r), 0) 使用 self.max 记录全局最大值，getattr 返回自身 max 属性的值或预定义的负无穷 本题思路是：递归每一个节点，返回max(以当前节点为结尾的最大路径和,0)。并更新最大值全局最大路径和=max(全局最大路径和，当前节点值+左子树返回结果+右子树返回结果) 用ok判断是不是第一次递归，是就返回全局最大值，否则照常 ## 136. Single Number 2行 1234class Solution: def singleNumber(self, nums: List[int]) -&gt; int: from functools import reduce return reduce(int.__xor__, nums) 这里用到了异或（xor），相同的数字异或后为0，0异或任何数都等于那个数，用reduce在列表所有元素之间使用异或^，那么留下的就是那个单独的数字了。 ## 141. Linked List Cycle 2行 1234567891011121314# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def hasCycle(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: bool &quot;&quot;&quot; while head and head.val != None: head.val, head = None, head.next return head != None 这题不支持python3，所以用pyhton2解法代替，下题记得调回来 :baby_chick: 破坏走过的所有节点，下次再遇到就知道了 不过以上方法会丢失原有信息，一般解法为快慢指针 123456789101112131415# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def hasCycle(self, head): slow = fast = head while fast and fast.next: fast = fast.next.next slow = slow.next if slow == fast: return True return False ## 142. Linked List Cycle II 5行 1234567891011121314151617# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def detectCycle(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; s = {None} while head not in s: s.add(head) head = head.next return head 把见过的节点丢集合里，下次再遇见就是环的开始 还有一个纯数学的快慢指针解法，设环的起始节点为 E，快慢指针从 head 出发，快指针速度为 2，设相交节点为 X，head 到 E 的距离为 H，E 到 X 的距离为 D，环的长度为 L，那么有：快指针走过的距离等于慢指针走过的距离加快指针多走的距离（多走了 n 圈的 L） 2(H + D) = H + D + nL，因此可以推出 H = nL - D，这意味着如果我们让俩个慢指针一个从 head 出发，一个从 X 出发的话，他们一定会在节点 E 相遇 12345 _____ / \\head___________E \\ \\ / X_____/ 1234567891011121314class Solution(object): def detectCycle(self, head): slow = fast = head while fast and fast.next: fast = fast.next.next slow = slow.next if slow == fast: break else: return None while head is not slow: head = head.next slow = slow.next return head ## 146. LRU Cache 7行 1234567891011121314151617181920class LRUCache(object): def __init__(self, capacity): self.od, self.cap = collections.OrderedDict(), capacity def get(self, key): if key not in self.od: return -1 self.od.move_to_end(key) return self.od[key] def put(self, key, value): if key in self.od: del self.od[key] elif len(self.od) == self.cap: self.od.popitem(False) self.od[key] = value# Your LRUCache object will be instantiated and called as such:# obj = LRUCache(capacity)# param_1 = obj.get(key)# obj.put(key,value) ## 148. Sort List 10行 12345678910111213141516171819# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def sortList(self, head: ListNode) -&gt; ListNode: if not (head and head.next): return head pre, slow, fast = None, head, head while fast and fast.next: pre, slow, fast = slow, slow.next, fast.next.next pre.next = None return self.mergeTwoLists(*map(self.sortList, (head, slow))) def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: if l1 and l2: if l1.val &gt; l2.val: l1, l2 = l2, l1 l1.next = self.mergeTwoLists(l1.next, l2) return l1 or l2 使用快慢指针寻找链表中点，并分解链表 递归融合俩个有序链表，详解见 21 题 此处忽略了递归开栈导致的非 常数级空间复杂度（想太多了吧:laughing:），如果一定要抬杠，推荐使用quicksort 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def sortList(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; def partition(start, end): node = start.next.next pivotPrev = start.next pivotPrev.next = end pivotPost = pivotPrev while node != end: temp = node.next if node.val &gt; pivotPrev.val: node.next = pivotPost.next pivotPost.next = node elif node.val &lt; pivotPrev.val: node.next = start.next start.next = node else: node.next = pivotPost.next pivotPost.next = node pivotPost = pivotPost.next node = temp return [pivotPrev, pivotPost] def quicksort(start, end): if start.next != end: prev, post = partition(start, end) quicksort(start, prev) quicksort(post, end) newHead = ListNode(0) newHead.next = head quicksort(newHead, None) return newHead.next ## 155. Min Stack 每个1行 123456789101112131415161718192021222324class MinStack: def __init__(self): self.data = [(None, float('inf'))] def push(self, x: 'int') -&gt; 'None': self.data.append((x, min(x, self.data[-1][1]))) def pop(self) -&gt; 'None': if len(self.data) &gt; 1: self.data.pop() def top(self) -&gt; 'int': return self.data[-1][0] def getMin(self) -&gt; 'int': return self.data[-1][1]# Your MinStack object will be instantiated and called as such:# obj = MinStack()# obj.push(x)# obj.pop()# param_3 = obj.top()# param_4 = obj.getMin() ## 160. Intersection of Two Linked Lists 3行 123456789101112131415# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def getIntersectionNode(self, headA, headB): &quot;&quot;&quot; :type head1, head1: ListNode :rtype: ListNode &quot;&quot;&quot; a, b = (headA, headB) if headA and headB else (None, None) while a != b: a, b = not a and headB or a.next, not b and headA or b.next return a 这题不支持 Python3 所以只能用 Python2 做了 把第一条链表的尾部接到第二条链表的开头，第二条接到第一条的开头，就能消除俩条链表的长度差，并在某一时刻在第一个交叉点相遇，或在走完俩条链表长度的时候同时为 None ## 169. Majority Element 1行 123class Solution: def majorityElement(self, nums: List[int]) -&gt; int: return sorted(nums)[len(nums) // 2] ## 198. House Robber 2行 1234class Solution: def rob(self, nums: List[int]) -&gt; int: from functools import reduce return reduce(lambda r, n: (max(r[0], n + r[1]), r[0]), nums, (0, 0))[0] DP递归方程：一直偷到这家的钱 = max（一直偷到上一家的钱，一直偷到上上家的钱 + 这家的钱）😃有点小绕 以上为下面代码的化简版，reduce 函数详解 123456class Solution: def rob(self, nums: List[int]) -&gt; int: last, now = 0, 0 for i in nums: last, now = now, max(last + i, now) return now DP不一定要数组，这里两个变量就够了，空间复杂度为O(1) ## 206. Reverse Linked List 2行 12345678910# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode, tail=None) -&gt; ListNode: if head: head.next, tail, head = tail, head, head.next return self.reverseList(head, tail) if head else tail 递归解法 此解法为尾递归，即直接以递归返回值作为结果，一般编译器会做优化，避免多余的函数开栈操作，实现效果相当于迭代 1234567891011# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: p = None while head: head.next, p, head = p, head, head.next return p 迭代解法 ## 215. Kth Largest Element in an Array 1行 123class Solution: def findKthLargest(self, nums: List[int], k: int) -&gt; int: return sorted(nums)[-k] O(NlogN)调库 面试官一般不会接受以上答案的，可以参考下面这个2行O(N)的quick-selection，思路借鉴的quick-sort 1234class Solution: def findKthLargest(self, nums: List[int], k: int) -&gt; int: l, m, r = [x for x in nums if x &gt; nums[0]], [x for x in nums if x == nums[0]], [x for x in nums if x &lt; nums[0]] return self.findKthLargest(l, k) if k &lt;= len(l) else nums[0] if k &lt;= len(l) + len(m) else self.findKthLargest(r, k - len(l) - len(m)) ## 217. Contains Duplicate 1行 123class Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: return len(nums) != len(set(nums)) ## 230. Kth Smallest Element in a BST 3行 123456789101112# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def kthSmallest(self, root, k): from itertools import chain, islice def gen(x): yield from chain(gen(x.left), [x.val], gen(x.right)) if x else () return next(islice(gen(root), k - 1, k)) 本题利用迭代器骚了一波:grinning:，不太了解的话看这里 yield 推荐阅读博客 chain 函数可以组合多个迭代器，islice 函数对迭代器做切片操作 此题常规解法 中序遍历 还是需要了解下的 12345678910111213141516171819202122232425# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def kthSmallest(self, root, k): &quot;&quot;&quot; :type root: TreeNode :type k: int :rtype: int &quot;&quot;&quot; res = [] self.visitNode(root, res) return res[k - 1] # 中序遍历 def visitNode(self, root, res): if root is None: return self.visitNode(root.left, res) res.append(root.val) self.visitNode(root.right, res) ## 231. 2的幂 1行 1234567class Solution: def isPowerOfTwo(self, n: int) -&gt; bool: &quot;&quot;&quot; :type n: int :rtype: bool &quot;&quot;&quot; return n &gt; 0 and n &amp; n - 1 == 0 2 的幂的二进制形式最高位一定是1，其余为0 用常规思路也行 123class Solution(object): def isPowerOfTwo(self, n): return n &gt; 0 and 2**int(math.log2(n)) == n ## 235. Lowest Common Ancestor of a Binary Search Tree 2行 1234567891011# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def lowestCommonAncestor(self, root, p, q): while (root.val - p.val) * (root.val - q.val) &gt; 0: root = (root.left, root.right)[p.val &gt; root.val] return root 最近公共祖先的值一定介于p、q值之间(闭区间) ## 236. Lowest Common Ancestor of a Binary Tree 2行 1234567891011# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def lowestCommonAncestor(self, root: 'TreeNode', p: 'TreeNode', q: 'TreeNode') -&gt; 'TreeNode': l, r = map(lambda x: x and self.lowestCommonAncestor(x, p, q), (root.left, root.right)) return (root in (p, q) or l and r) and root or l or r 递归全部节点，p 的祖先节点全部返回 p，q 的祖先节点全部返回 q，除非它同时是俩个节点的最近祖先，也就是 p，q 分别位于左右子树，那么返回自身 这思路用在235也行 ## 237. Delete Node in a Linked List 1行 12345678910111213# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteNode(self, node): &quot;&quot;&quot; :type node: ListNode :rtype: void Do not return anything, modify node in-place instead. &quot;&quot;&quot; node.val, node.next = node.next.val, node.next.next node = node.next是不行的，因为这里只是改了函数参数引用的对象，而原来传进来的 node 没有任何改变 详细说明下：如果Python的函数得到的参数是可变对象（比如list，set，这样的，内部属性可以改变的），那么我们实际得到的是这个对象的浅拷贝。比如这个函数刚刚开始的时候题目传进来一个参数node，我们设这个节点为A，那么实际上得到的参数node是一个对于A的一个浅拷贝，你可以想象node是一把钥匙，它可以打开真正的节点A的门，如果我们现在让node = node.next，那么我们只是换了钥匙，变成了打开 A.next 的门的对应的钥匙，因此链表没有被修改， A没有被修改，只是我们手里的钥匙变了。而如果我们直接写node.val, node.next = node.next.val, node.next.next，就相当于我们先用钥匙找到 A 的门，然后修改了 A 的属性，链表发生变化 此题考查python函数的传参形式为“传对象引用”，相当于浅拷贝（对于可变对象来说） ## 238. Product of Array Except Self 5行 1234567class Solution: def productExceptSelf(self, nums: List[int]) -&gt; List[int]: res, l, r = [1] * len(nums), 1, 1 for i, j in zip(range(len(nums)), reversed(range(len(nums)))): res[i], l = res[i] * l, l * nums[i] res[j], r = res[j] * r, r * nums[j] return res O(N)双指针双向遍历 ## 268. Missing Number 1行 123class Solution: def missingNumber(self, nums: List[int]) -&gt; int: return int(len(nums) * (len(nums) + 1) / 2 - sum(nums)) 等差数列求和公式 ## 283. Move Zeroes 1行 123456class Solution: def moveZeroes(self, nums: List[int]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; nums.sort(key=bool, reverse=True) sort 时间复杂度为O(NlogN), 直接遍历可以达到 O(N) 12345678910class Solution: def moveZeroes(self, nums: List[int]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify nums in-place instead. &quot;&quot;&quot; i = 0 for i, n in enumerate(filter(lambda x: x, nums)): nums[i] = n for i in range(i + 1, len(nums)): nums[i] = 0 直接使用 filter 迭代器可以避免交换操作，思路更简单 ## 292. Nim Game 1行 123class Solution: def canWinNim(self, n: int) -&gt; bool: return bool(n % 4) 只要轮到你的时候剩余石头数量不是 4 的倍数都是完胜，因为你有办法使得每次轮到对方的时候剩余石头数量都为 4 的倍数 ## 328. Odd Even Linked List 6行 1234567891011121314# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def oddEvenList(self, head: ListNode) -&gt; ListNode: if not head or not head.next: return head r, odd, p, head = head, head, head.next, head.next.next while head: odd.next, head.next, p.next = head, odd.next, head.next p, odd, head = p.next, head, p.next and p.next.next return r odd 记录上一个奇数位节点，p 记录前一个节点 从第3个位置开始循环，每次都把当前节点接到 odd 后面，然后跳到下一个奇数位节点继续循环 ## 344. Reverse String 1行 123456class Solution: def reverseString(self, s: List[str]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify s in-place instead. &quot;&quot;&quot; s.reverse() 412. Fizz Buzz 1行 123class Solution: def fizzBuzz(self, n): return ['Fizz' * (not i % 3) + 'Buzz' * (not i % 5) or str(i) for i in range(1, n+1)] 7 or 8 = 7 0 or 8 = 8 ## 414. Third Maximum Number 3行 12345class Solution: def thirdMax(self, nums: List[int]) -&gt; int: nums = set(nums) for _ in range((2, 0)[len(nums) &lt; 3]): nums.remove(max(nums)) return max(nums) ## 557. Reverse Words in a String III 1行 123class Solution: def reverseWords(self, s: str) -&gt; str: return ' '.join(s.split(' ')[::-1])[::-1] ## 771. Jewels and Stones 1行 123class Solution: def numJewelsInStones(self, J: str, S: str) -&gt; int: return sum(S.count(i) for i in J) 时间复杂度O(N^2)，另附O(N)解法（set内部实现为dict，in操作时间复杂度为O(N)） 1234class Solution: def numJewelsInStones(self, J: str, S: str) -&gt; int: j = set(J) return sum(s in j for s in S) ## 938. Range Sum of BST 1行 12345678910# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def rangeSumBST(self, root: TreeNode, L: int, R: int) -&gt; int: return root and root.val * (L &lt;= root.val &lt;= R) + self.rangeSumBST(root.left, L, R) + self.rangeSumBST(root.right, L, R) or 0 ## 953. Verifying an Alien Dictionary 1行 123class Solution(object): def isAlienSorted(self, words, order): return words == sorted(words, key=lambda w: [order.index(x) for x in w]) 充分利用 python 序列比较的特点，sorted 的参数 key 可传入一个函数，sorted 函数会将每个元素作为输入，输入到 key 函数并获得返回值，整个序列将按此值的大小来排序。此处 key 函数为lambda w: [order.index(x) for x in w]，其为words中每个单词 word 返回一个 list，list 中每个元素为单词中字母 x 在 order 中的索引。比如当 order 为 ‘abcde……’ 时，单词 ‘cab’ 将返回 [3, 2, 1]。关于俩个 list 的大小比较，服从 python 序列比较的特性，请参考官方文档教程 5.8 节内容。 另外一个通用的方法是简单的数学计算，给每个单词赋予一个数字然后排序对比和原来的数组是否一致即可，每个字母的价值按字母表顺序，第几个就代表几，每进一位需要*10^-2避免冲突，比如字母表是abcde……，单词 cab 的价值就是 3 * 1 + 1 * 0.01 + 2 * 0.0001，价值越小的单词位置应该越靠前 1234class Solution: def isAlienSorted(self, words: List[str], order: str) -&gt; bool: d = {c: i + 1 for i, c in enumerate(order)} return sorted(words, key=lambda x: sum(d[c] * 10**(-2 * i) for i, c in enumerate(x))) == words # 专题 相比于解析部分追求代码的绝对精简，本专题追求以高可读性呈现各大专题的常规思路。俩部分题目可能重复，但专题部分会有更详细的解析，且可能运用不同解法。为降低学习成本，🛫 每个方向会根据C++篇收录少数优选的经典题目，若觉得不够请转解析部分，若想更改例题，欢迎 issue 提出您的建议或意见。 数组 238. Product of Array Except Self 双指针 12345678910111213141516class Solution: def productExceptSelf(self, nums: List[int]) -&gt; List[int]: n = len(nums) res = [1] * n l = 1 for i in range(n): res[i] *= l l *= nums[i] r = 1 for j in range(n - 1, -1, -1): res[j] *= r r *= nums[j] return res 本题利用双指针，新数组每个位置上的值应该等于数组左边所有数字的乘积 × 数组右边所有数字的乘积 1.初始化一个新的数组res（result），包含n个1 2.初始化变量l（left）代表左边的乘积，从左到右遍历数组，每次都让新数组的值乘以它左边数字的乘积l，然后更新l。此时新数组里的所有数字就代表了nums数组中对应位置左边所有数字的乘积 3.再从右往左做一遍同样的操作，最终res[i] = 1 * nums中i左边所有数字的乘积 * nums中i右边所有数字的乘积 ## 哈希表 ### 1. Two Sum 字典 123456class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: d = {} for i, n in enumerate(nums): if n in d: return [d[n], i] d[target-n] = i 建立一个字典，每次遍历过一个值就记录与其匹配的值（设置d[匹配值]=当前索引），今后遇见匹配值即可直接返回结果 ## 链表 ### 206. Reverse Linked List 迭代遍历 123456789101112# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: p = None while head: head.next, head, p = p, head.next, head return p 此处利用 python 多重赋值表达式的特性（例：a, b = b, a），python 中多变量同时赋值时，右手边的表达式在任何赋值发生之前就被求值了。右手边的表达式是从左到右被求值的 ## 数学 ### 268. Missing Number 等差数列 123456class Solution: def missingNumber(self, nums: List[int]) -&gt; int: n = len(nums) s = sum(nums) n += 1 return n * (n - 1) // 2 - s 缺失数字 = 0 加到 n+1 的总和 - 数组中所有数字的总和 计算 0 加到 n+1 的总和，可利用等差数列求和公式，此题可理解为总和 = (元素个数 / 2) * (首尾两数字之和) ## 双指针 ### 344. Reverse String 双指针 12345678910class Solution: def reverseString(self, s: List[str]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify s in-place instead. &quot;&quot;&quot; i, j = 0, len(s) - 1 while i &lt; j: s[i], s[j] = s[j], s[i] i += 1 j -= 1","link":"/posts/2424027689.html"},{"title":"迁移学习","text":"​ 本章主要简明地介绍了迁移学习的基本概念、迁移学习的必要性、研究领域和基本方法。重点介绍了几大类常用的迁移学习方法：数据分布自适应方法、特征选择方法、子空间学习方法、以及目前最热门的深度迁移学习方法。除此之外，我们也结合最近的一些研究成果对未来迁移学习进行了一些展望。并提供了一些迁移学习领域的常用学习资源，以方便感兴趣的读者快速开始学习。 ## 迁移学习基础知识 什么是迁移学习？ 找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。 为什么需要迁移学习？ 大数据与少标注的矛盾：虽然有大量的数据，但往往都是没有标注的，无法训练机器学习模型。人工进行数据标定太耗时。 大数据与弱计算的矛盾：普通人无法拥有庞大的数据量与计算资源。因此需要借助于模型的迁移。 普适化模型与个性化需求的矛盾：即使是在同一个任务上，一个模型也往往难以满足每个人的个性化需求，比如特定的隐私设置。这就需要在不同人之间做模型的适配。 特定应用（如冷启动）的需求。 迁移学习的基本问题有哪些？ 基本问题主要有3个： How to transfer： 如何进行迁移学习？（设计迁移方法） What to transfer： 给定一个目标领域，如何找到相对应的源领域，然后进行迁移？（源领域选择） When to transfer： 什么时候可以进行迁移，什么时候不可以？（避免负迁移） 迁移学习有哪些常用概念？ 基本定义 域(Domain)：数据特征和特征分布组成，是学习的主体 源域 (Source domain)：已有知识的域 目标域 (Target domain)：要进行学习的域 任务 (Task)：由目标函数和学习结果组成，是学习的结果 按特征空间分类 同构迁移学习（Homogeneous TL）： 源域和目标域的特征空间相同，\\(D_s=D_t\\) 异构迁移学习（Heterogeneous TL）：源域和目标域的特征空间不同，\\(D_s\\ne D_t\\) 按迁移情景分类 归纳式迁移学习（Inductive TL）：源域和目标域的学习任务不同 直推式迁移学习（Transductive TL)：源域和目标域不同，学习任务相同 无监督迁移学习（Unsupervised TL)：源域和目标域均没有标签 按迁移方法分类 基于实例的迁移 (Instance based TL)：通过权重重用源域和目标域的样例进行迁移 基于特征的迁移 (Feature based TL)：将源域和目标域的特征变换到相同空间 基于模型的迁移 (Parameter based TL)：利用源域和目标域的参数共享模型 基于关系的迁移 (Relation based TL)：利用源域中的逻辑网络关系进行迁移 迁移学习与传统机器学习有什么区别？ 迁移学习 传统机器学习 数据分布 训练和测试数据不需要同分布 训练和测试数据同分布 数据标签 不需要足够的数据标注 足够的数据标注 建模 可以重用之前的模型 每个任务分别建模 迁移学习的核心及度量准则？ 迁移学习的总体思路可以概括为：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。 迁移学习的核心是：找到源领域和目标领域之间的相似性，并加以合理利用。这种相似性非常普遍。比如，不同人的身体构造是相似的；自行车和摩托车的骑行方式是相似的；国际象棋和中国象棋是相似的；羽毛球和网球的打球方式是相似的。这种相似性也可以理解为不变量。以不变应万变，才能立于不败之地。 有了这种相似性后，下一步工作就是， 如何度量和利用这种相似性。度量工作的目标有两点：一是很好地度量两个领域的相似性，不仅定性地告诉我们它们是否相似，更定量地给出相似程度。二是以度量为准则，通过我们所要采用的学习手段，增大两个领域之间的相似性，从而完成迁移学习。 一句话总结： 相似性是核心，度量准则是重要手段。 迁移学习与其他概念的区别？ 迁移学习与多任务学习关系： 多任务学习：多个相关任务一起协同学习； 迁移学习：强调信息复用，从一个领域(domain)迁移到另一个领域。 迁移学习与领域自适应：领域自适应：使两个特征分布不一致的domain一致。 迁移学习与协方差漂移：协方差漂移：数据的条件概率分布发生变化。 Reference： 王晋东，迁移学习简明手册 Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175. Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM. 11.1.8 什么是负迁移？产生负迁移的原因有哪些？ 负迁移(Negative Transfer)指的是，在源域上学习到的知识，对于目标域上的学习产生负面作用。 产生负迁移的原因主要有： - 数据问题：源域和目标域压根不相似，谈何迁移？ - 方法问题：源域和目标域是相似的，但是，迁移学习方法不够好，没找到可迁移的成分。 负迁移给迁移学习的研究和应用带来了负面影响。在实际应用中，找到合理的相似性，并且选择或开发合理的迁移学习方法，能够避免负迁移现象。 迁移学习的基本思路？ 迁移学习的总体思路可以概括为：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。 找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。 迁移学习，是指利用数据、任务、或模型之间的相似性，将在旧领域学习过的模型，应用于新领域的一种学习过程。 迁移学习最有用的场合是，如果你尝试优化任务B的性能，通常这个任务数据相对较少。 例如，在放射科中你知道很难收集很多射线扫描图来搭建一个性能良好的放射科诊断系统，所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用 1 百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务在放射科任务上做得更好，尽管任务没有这么多数据。 迁移学习什么时候是有意义的？它确实可以显著提高你的学习任务的性能，但我有时候也见过有些场合使用迁移学习时，任务实际上数据量比任务要少， 这种情况下增益可能不多。 &gt; 什么情况下可以使用迁移学习？ &gt; &gt; 假如两个领域之间的区别特别的大，不可以直接采用迁移学习，因为在这种情况下效果不是很好。在这种情况下，推荐使用[3]的工作，在两个相似度很低的domain之间一步步迁移过去（踩着石头过河）。 迁移学习主要解决方案有哪些？ 除直接看infer的结果的Accurancy以外，如何衡量迁移学习学习效果？ 对抗网络是如何进行迁移的？ Reference： 王晋东，迁移学习简明手册 Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175. Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM. 迁移学习的基本思路有哪些？ ​ 迁移学习的基本方法可以分为四种。这四种基本的方法分别是：基于样本的迁移， 基于模型 的迁移， 基于特征的迁移，及基于关系的迁移。 基于样本迁移 ​ 基于样本的迁移学习方法 (Instance based Transfer Learning) 根据一定的权重生成规则，对数据样本进行重用，来进行迁移学习。图14形象地表示了基于样本迁移方法的思想源域中存在不同种类的动物，如狗、鸟、猫等，目标域只有狗这一种类别。在迁移时，为了最大限度地和目标域相似，我们可以人为地提高源域中属于狗这个类别的样本权重。 图 14: 基于样本的迁移学习方法示意图 ​ 在迁移学习中，对于源域Ds和目标域Dt，通常假定产生它们的概率分布是不同且未知的(P(Xs) =P(Xt))。另外，由于实例的维度和数量通常都非常大，因此，直接对 P(Xs) 和P(Xt) 进行估计是不可行的。因而，大量的研究工作 [Khan and Heisterkamp,2016, Zadrozny, 2004, Cortes et al.,2008, Dai et al., 2007, Tan et al.,2015, Tan et al., 2017] 着眼于对源域和目标域的分布比值进行估计(P(Xt)/P(Xs))。所估计得到的比值即为样本的权重。这些方法通常都假设P(xs) &lt;并且源域和目标域的条件概率分布相同(P(y|xs)=P(y|xt))。特别地，上海交通大学Dai等人[Dai et al.,2007]提出了 TrAdaboost方法，将AdaBoost的思想应用于迁移学习中，提高有利于目标分类任务的实例权重、降低不利于目标分类任务的实例权重，并基于PAC理论推导了模型的泛化误差上界。TrAdaBoost方法是此方面的经典研究之一。文献 [Huang et al., 2007]提出核均值匹配方法 (Kernel Mean atching, KMM)对于概率分布进行估计，目标是使得加权后的源域和目标域的概率分布尽可能相近。在最新的研究成果中，香港科技大学的Tan等人扩展了实例迁移学习方法的应用场景，提出 了传递迁移学习方法(Transitive Transfer Learning, TTL) [Tan etal., 2015] 和远域迁移学习 (Distant Domain Transfer Learning,DDTL) [Tan et al., 2017]，利用联合矩阵分解和深度神经网络，将迁移学习应用于多个不相似的领域之间的知识共享，取得了良好的效果。 ​ 虽然实例权重法具有较好的理论支撑、容易推导泛化误差上界，但这类方法通常只在领域间分布差异较小时有效，因此对自然语言处理、计算机视觉等任务效果并不理想。而基于特征表示的迁移学习方法效果更好,是我们研究的重点。 基于特征迁移 ​ 基于特征的迁移方法 (Feature based Transfer Learning) 是指将通过特征变换的方式互相迁移 [Liu et al., 2011, Zheng et al.,2008, Hu and Yang, 2011],来减少源域和目标域之间的差距；或者将源域和目标域的数据特征变换到统一特征空间中 [Pan et al.,2011, Long et al., 2014b, Duan et al.,2012],然后利用传统的机器学习方法进行分类识别。根据特征的同构和异构性,又可以分为同构和异构迁移学习。图15很形象地表示了两种基于特 征的迁移学习方法。 图 15: 基于特征的迁移学习方法示意图 ​ 基于特征的迁移学习方法是迁移学习领域中最热门的研究方法,这类方法通常假设源域和目标域间有一些交叉的特征。香港科技大学的 Pan 等人 [Pan et al.,2011] 提出的迁移 成分分析方法 (Transfer Component Analysis, TCA)是其中较为典型的一个方法。该方法的 核心内容是以最大均值差异 (Maximum MeanDiscrepancy, MMD) [Borgwardt et al., 2006]作为度量准则,将不同数据领域中的分布差异最小化。加州大学伯克利分校的 Blitzer 等人 [Blitzer et al., 2006] 提出了一种基于结构对应的学习方法(Structural Corresponding Learning,SCL),该算法可以通过映射将一个空间中独有的一些特征变换到其他所有空间中的轴特征上,然后在该特征上使用机器学习的算法进行分类预测。清华大学龙明盛等人[Long et al.,2014b]提出在最小化分布距离的同时，加入实例选择的迁移联合匹配(Tran-fer Joint Matching, TJM) 方法,将实例和特征迁移学习方法进行了有机的结合。澳大利亚卧龙岗大学的 Jing Zhang 等人 [Zhang et al., 2017a]提出对于源域和目标域各自训练不同 的变换矩阵,从而达到迁移学习的目标。 基于模型迁移 ​ 基于模型的迁移方法 (Parameter/Model based Transfer Learning) 是指从源域和目标域中找到他们之间共享的参数信息,以实现迁移的方法。这种迁移方式要求的假设条件是： 源域中的数据与目标域中的数据可以共享一些模型的参数。其中的代表性工作主要有［Zhao et al., 2010, Zhao et al., 2011, Panet al., 2008b, Pan et al., 2008a］。图16形象地 表示了基于模型的迁移学习方法的基本思想。 图 16: 基于模型的迁移学习方法示意图 ​ 其中，中科院计算所的Zhao等人[Zhao et al., 2011]提出了TransEMDT方法。该方法首先针对已有标记的数据，利用决策树构建鲁棒性的行为识别模型，然后针对无标定数据，利用K-Means聚类方法寻找最优化的标定参数。西安邮电大学的Deng等人[Deng et al.,2014] 也用超限学习机做了类似的工作。香港科技大学的Pan等人[Pan etal., 2008a]利用HMM，针对Wifi室内定位在不同设备、不同时间和不同空间下动态变化的特点，进行不同分布下的室内定位研究。另一部分研究人员对支持向量机 SVM 进行了改进研究 [Nater et al.,2011, Li et al., 2012]。这些方法假定 SVM中的权重向量 w 可以分成两个部分： w = wo+v， 其中 w0代表源域和目标域的共享部分， v 代表了对于不同领域的特定处理。在最新的研究成果中，香港科技大学的 Wei 等人 [Wei et al., 2016b]将社交信息加入迁移学习方法的 正则项中，对方法进行了改进。清华大学龙明盛等人[Long et al., 2015a, Long et al., 2016, Long etal., 2017]改进了深度网络结构，通过在网络中加入概率分布适配层，进一步提高了深度迁移学习网络对于大数据的泛化能力。 基于关系迁移 ​ 基于关系的迁移学习方法 (Relation Based Transfer Learning) 与上述三种方法具有截然不同的思路。这种方法比较关注源域和目标域的样本之间的关系。图17形象地表示了不 同领域之间相似的关系。 ​ 就目前来说，基于关系的迁移学习方法的相关研究工作非常少，仅有几篇连贯式的文章讨论： [Mihalkova et al., 2007, Mihalkova and Mooney,2008, Davis and Domingos, 2009]。这些文章都借助于马尔科夫逻辑网络(Markov Logic Net)来挖掘不同领域之间的关系相似性。 ​ 我们将重点讨论基于特征和基于模型的迁移学习方法，这也是目前绝大多数研究工作的热点。 图 17: 基于关系的迁移学习方法示意图 图 18: 基于马尔科夫逻辑网的关系迁移 迁移学习的常用方法 数据分布自适应 ​ 数据分布自适应 (Distribution Adaptation) 是一类最常用的迁移学习方法。这种方法的基本思想是,由于源域和目标域的数据概率分布不同,那么最直接的方式就是通过一些变换,将不同的数据分布的距离拉近。 ​ 图 19形象地表示了几种数据分布的情况。简单来说，数据的边缘分布不同，就是数据整体不相似。数据的条件分布不同，就是数据整体相似，但是具体到每个类里，都不太相似。 图 19: 不同数据分布的目标域数据 ​ 根据数据分布的性质,这类方法又可以分为边缘分布自适应、条件分布自适应、以及联合分布自适应。下面我们分别介绍每类方法的基本原理和代表性研究工作。介绍每类研究工作时,我们首先给出基本思路,然后介绍该类方法的核心,最后结合最近的相关工作介绍该类方法的扩展。 边缘分布自适应 ​ 边缘分布自适应方法 (Marginal Distribution Adaptation) 的目标是减小源域和目标域的边缘概率分布的距离,从而完成迁移学习。从形式上来说,边缘分布自适应方法是用P(Xs)和 P(Xt)之间的距离来近似两个领域之间的差异。即： ​ \\(DISTANCE(D~s~,D~t~)\\approx\\lVert P(X_s)-P(X_t)\\Vert\\) (6.1) ​ 边缘分布自适应对应于图19中由图19(a)迁移到图19(b)的情形。 条件分布自适应 ​ 条件分布自适应方法 (Conditional Distribution Adaptation) 的目标是减小源域和目标域的条件概率分布的距离，从而完成迁移学习。从形式上来说，条件分布自适应方法是用 P(ys|Xs) 和 P (yt|Xt) 之间的距离来近似两个领域之间的差异。即： ​ \\(DISTANCE(D~s~,D~t~)\\approx\\lVert P(y_s|X_s)-P(y_t|X_t)\\Vert\\)(6.8) ​ 条件分布自适应对应于图19中由图19(a)迁移到图19(c)的情形。 ​ 目前单独利用条件分布自适应的工作较少，这些工作主要可以在 [Saito et al.,2017] 中找到。最近，中科院计算所的 Wang 等人提出了 STL 方法(Stratified Transfer Learn­ing) [Wang tal.,2018]。作者提出了类内迁移 (Intra-class Transfer)的思想。指出现有的 绝大多数方法都只是学习一个全局的特征变换(Global DomainShift)，而忽略了类内的相 似性。类内迁移可以利用类内特征，实现更好的迁移效果。 ​ STL 方法的基本思路如图所示。首先利用大多数投票的思想，对无标定的位置行为生成伪标；然后在再生核希尔伯特空间中，利用类内相关性进行自适应地空间降维，使得不同情境中的行为数据之间的相关性增大；最后，通过二次标定，实现对未知标定数据的精准标定。 图 21: STL 方法的示意图 ### 联合分布自适应 ​ 联合分布自适应方法 (Joint Distribution Adaptation) 的目标是减小源域和目标域的联合概率分布的距离，从而完成迁移学习。从形式上来说，联合分布自适应方法是用P(xs) 和P(xt)之间的距离、以及P(ys|xs)和P(yt|xt)之间的距离来近似两个领域之间的差异。即: ​ \\(DISTANCE(D~s~,D~t~)\\approx\\lVert P(X_s)-P(X_t)\\Vert-\\lVert P(y_s|X_s)-P(y_t|X_t)\\Vert​\\)(6.10) ​ 联合分布自适应对应于图19中由图19(a)迁移到图19(b)的情形、以及图19(a)迁移到 图19(c)的情形。 概率分布自适应方法优劣性比较 综合上述三种概率分布自适应方法，我们可以得出如下的结论： 精度比较： BDA &gt;JDA &gt;TCA &gt;条件分布自适应。 将不同的概率分布自适应方法用于神经网络，是一个发展趋势。图23展示的结果表明将概率分布适配加入深度网络中，往往会取得比非深度方法更好的结果。 图 22: BDA 方法的效果第二类方法：特征选择 特征选择 ​ 特征选择法的基本假设是：源域和目标域中均含有一部分公共的特征，在这部分公共的特征，源领域和目标领域的数据分布是一致的。因此，此类方法的目标就是，通过机器学习方法，选择出这部分共享的特征，即可依据这些特征构建模型。 ​ 图 24形象地表示了特征选择法的主要思路。 图 23: 不同分布自适应方法的精度比较 图 24: 特征选择法示意图 ​ 这这个领域比较经典的一个方法是发表在 2006 年的 ECML-PKDD 会议上,作者提出了一个叫做 SCL 的方法 (Structural Correspondence Learning) [Blitzer et al.,2006]。这个方法的目标就是我们说的,找到两个领域公共的那些特征。作者将这些公共的特征叫做Pivot feature。找出来这些Pivot feature,就完成了迁移学习的任务。 图 25: 特征选择法中的 Pivot feature 示意图 ​ 图 25形象地展示了 Pivot feature 的含义。 Pivot feature指的是在文本分类中,在不同领域中出现频次较高的那些词。总结起来： 特征选择法从源域和目标域中选择提取共享的特征,建立统一模型 通常与分布自适应方法进行结合 通常采用稀疏表示 ||A||2,1 实现特征选择 统计特征对齐方法 ​ 统计特征对齐方法主要将数据的统计特征进行变换对齐。对齐后的数据，可以利用传统机器学习方法构建分类器进行学习。SA方法(Subspace Alignment，子空间对齐)[Fernando et al.,2013]是其中的代表性成果。SA方法直接寻求一个线性变换M，将不同的数据实现变换对齐。SA方法的优化目标如下： 则变换 M 的值为： 可以直接获得上述优化问题的闭式解： ​ SA 方法实现简单，计算过程高效，是子空间学习的代表性方法。 流形学习方法 什么是流形学习 ​ 流形学习自从 2000 年在 Science 上被提出来以后,就成为了机器学习和数据挖掘领域的热门问题。它的基本假设是,现有的数据是从一个高维空间中采样出来的,所以,它具有高维空间中的低维流形结构。流形就是是一种几何对象（就是我们能想像能观测到的）。通俗点说就是,我们无法从原始的数据表达形式明显看出数据所具有的结构特征,那我把它想像成是处在一个高维空间,在这个高维空间里它是有个形状的。一个很好的例子就是星座。满天星星怎么描述？我们想像它们在一个更高维的宇宙空间里是有形状的,这就有了各自星座,比如织女座、猎户座。流形学习的经典方法有Isomap、locally linear embedding、 laplacian eigenmap 等。 ​ 流形空间中的距离度量：两点之间什么最短？在二维上是直线（线段）,可在三维呢？地球上的两个点的最短距离可不是直线,它是把地球展开成二维平面后画的那条直线。那条线在三维的地球上就是一条曲线。这条曲线就表示了两个点之间的最短距离,我们叫它测地线。更通俗一点, 两点之间，测地线最短。在流形学习中,我们遇到测量距离的时候更多的时候用的就是这个测地线。在我们要介绍的 GFK 方法中,也是利用了这个测地线距离。比如在下面的图中,从 A 到 C 最短的距离在就是展开后的线段,但是在三维球体上看它却是一条曲线。 图 28: 三维空间中两点之间的距离示意图 ​ 由于在流形空间中的特征通常都有着很好的几何性质,可以避免特征扭曲,因此我们首先将原始空间下的特征变换到流形空间中。在众多已知的流形中, Grassmann 流形G（d） 可以通过将原始的 d 维子空间 （特征向量）看作它基础的元素,从而可以帮助学习分类 器。在 Grassmann流形中,特征变换和分布适配通常都有着有效的数值形式,因此在迁移学习问题中可以被很高效地表示和求解 [Hamm and Lee,2008]。因此,利用 Grassmann流形空间中来进行迁移学习是可行的。现存有很多方法可以将原始特征变换到流形空间 中[Gopalan et al., 2011, Baktashmotlagh et al.,2014]。 ​ 在众多的基于流形变换的迁移学习方法中，GFK(Geodesic Flow Kernel)方法[Gong et al., 2012]是最为代表性的一个。GFK是在2011年发表在ICCV上的SGF方法[Gopalan et al., 2011]发展起来的。我们首先介绍SGF方法。 ​ SGF 方法从增量学习中得到启发：人类从一个点想到达另一个点，需要从这个点一步一步走到那一个点。那么，如果我们把源域和目标域都分别看成是高维空间中的两个点，由源域变换到目标域的过程不就完成了迁移学习吗？也就是说， 路是一步一步走出来的。 ​ 于是 SGF 就做了这个事情。它是怎么做的呢？把源域和目标域分别看成高维空间 (即Grassmann流形)中的两个点，在这两个点的测地线距离上取d个中间点，然后依次连接起来。这样，源域和目标域就构成了一条测地线的路径。我们只需要找到合适的每一步的变换，就能从源域变换到目标域了。图 29是 SGF 方法的示意图。 图 29: SGF 流形迁移学习方法示意图 ​ SGF 方法的主要贡献在于：提出了这种变换的计算及实现了相应的算法。但是它有很明显的缺点：到底需要找几个中间点？ SGF也没能给出答案，就是说这个参数d是没法估计的，没有一个好的方法。这个问题在 GFK 中被回答了。 ​ GFK方法首先解决SGF的问题：如何确定中间点的个数d。它通过提出一种核学习的方法，利用路径上的无穷个点的积分，把这个问题解决了。这是第一个贡献。然后，它又解决了第二个问题：当有多个源域的时候，我们如何决定使用哪个源域跟目标域进行迁移？ GFK通过提出Rank of Domain度量，度量出跟目标域最近的源域，来解决这个问题。图 30是 GFK 方法的示意图。 图 30: GFK 流形迁移学习方法示意图 ​ 用Ss和St分别表示源域和目标域经过主成分分析(PCA)之后的子空间，则G可以视为所有的d维子空间的集合。每一个d维的原始子空间都可以被看作G上的一个点。因此，在两点之间的测地线｛$(t) :0 &lt; t &lt;1｝可以在两个子空间之间构成一条路径。如果我 们令Ss = $(0)，St =$(1)，则寻找一条从$(0)到$(1)的测地线就等同于将原始的特征变换到一个无穷维度的空间中，最终减小域之间的漂移现象。这种方法可以被看作是一种从$(0)到$(1)的増量式“行走”方法。 ​ 特别地，流形空间中的特征可以被表示为z =$(t)Tx。变换后的特征Zi和Zj的内积定义了一个半正定 (positive semidefinite) 的测地线流式核 ​ GFK 方法详细的计算过程可以参考原始的文章，我们在这里不再赘述。 什么是finetune？ ​ 深度网络的finetune也许是最简单的深度网络迁移方法。Finetune,也叫微调、fine-tuning, 是深度学习中的一个重要概念。简而言之，finetune就是利用别人己经训练好的网络，针对自己的任务再进行调整。从这个意思上看，我们不难理解finetune是迁移学习的一部分。 为什么需要已经训练好的网络？ ​ 在实际的应用中,我们通常不会针对一个新任务,就去从头开始训练一个神经网络。这样的操作显然是非常耗时的。尤其是，我们的训练数据不可能像ImageNet那么大，可以训练出泛化能力足够强的深度神经网络。即使有如此之多的训练数据,我们从头开始训练,其代价也是不可承受的。 ​ 那么怎么办呢？迁移学习告诉我们,利用之前己经训练好的模型,将它很好地迁移到自己的任务上即可。 为什么需要 finetune？ ​ 因为别人训练好的模型,可能并不是完全适用于我们自己的任务。可能别人的训练数据和我们的数据之间不服从同一个分布；可能别人的网络能做比我们的任务更多的事情；可能别人的网络比较复杂,我们的任务比较简单。 ​ 举一个例子来说,假如我们想训练一个猫狗图像二分类的神经网络,那么很有参考价值的就是在 CIFAR-100 上训练好的神经网络。但是 CIFAR-100 有 100 个类别,我们只需要 2个类别。此时,就需要针对我们自己的任务,固定原始网络的相关层,修改网络的输出层以使结果更符合我们的需要。 ​ 图36展示了一个简单的finetune过程。从图中我们可以看到，我们采用的预训练好的网络非常复杂,如果直接拿来从头开始训练,则时间成本会非常高昂。我们可以将此网络进行改造,固定前面若干层的参数,只针对我们的任务,微调后面若干层。这样,网络训练速度会极大地加快,而且对提高我们任务的表现也具有很大的促进作用。 图 36: 一个简单的 finetune 示意图 Finetune 的优势 ​ Finetune 的优势是显然的，包括: 不需要针对新任务从头开始训练网络，节省了时间成本； 预训练好的模型通常都是在大数据集上进行的，无形中扩充了我们的训练数据，使得模型更鲁棒、泛化能力更好； Finetune 实现简单，使得我们只关注自己的任务即可。 Finetune 的扩展 ​ 在实际应用中，通常几乎没有人会针对自己的新任务从头开始训练一个神经网络。Fine-tune 是一个理想的选择。 ​ Finetune 并不只是针对深度神经网络有促进作用，对传统的非深度学习也有很好的效果。例如， finetune对传统的人工提取特征方法就进行了很好的替代。我们可以使用深度网络对原始数据进行训练，依赖网络提取出更丰富更有表现力的特征。然后，将这些特征作为传统机器学习方法的输入。这样的好处是显然的: 既避免了繁复的手工特征提取，又能自动地提取出更有表现力的特征。 ​ 比如，图像领域的研究，一直是以 SIFT、SURF 等传统特征为依据的，直到 2014 年，伯克利的研究人员提出了 DeCAF特征提取方法［Donahue et al.,2014］，直接使用深度卷积神经网络进行特征提取。实验结果表明，该特征提取方法对比传统的图像特征，在精度上有着无可匹敌的优势。另外，也有研究人员用卷积神经网络提取的特征作为SVM分类器的输 入［Razavian et al.,014］，显著提升了图像分类的精度。 finetune为什么有效？ ​ 随着 AlexNet [Krizhevsky et al., 2012] 在 2012 年的 ImageNet大赛上获得冠军，深度学习开始在机器学习的研究和应用领域大放异彩。尽管取得了很好的结果，但是神经网络本身就像一个黑箱子，看得见，摸不着，解释性不好。由于神经网络具有良好的层次结构很自然地就有人开始关注，能否通过这些层次结构来很好地解释网络？于是，有了我们熟知的例子：假设一个网络要识别一只猫，那么一开始它只能检测到一些边边角角的东西，和猫根本没有关系；然后可能会检测到一些线条和圆形；慢慢地，可以检测到有猫的区域；接着是猫腿、猫脸等等。图 32是一个简单的示例。 图 32: 深度神经网络进行特征提取到分类的简单示例 ​ 这表达了一个什么事实呢？概括来说就是：前面几层都学习到的是通用的特征（general feature）；随着网络层次的加深，后面的网络更偏重于学习任务特定的特征（specific feature）。 这非常好理解，我们也都很好接受。那么问题来了：如何得知哪些层能够学习到 general feature，哪些层能够学习到specific feature。更进一步：如果应用于迁移学习，如何决定该迁移哪些层、固定哪些层？ ​ 这个问题对于理解神经网络以及深度迁移学习都有着非常重要的意义。 ​ 来自康奈尔大学的 Jason Yosinski 等人 [Yosinski et al., 2014]率先进行了深度神经网络可迁移性的研究，将成果发表在2014年机器学习领域顶级会议NIPS上并做了口头汇报。该论文是一篇实验性质的文章（通篇没有一个公式）。其目的就是要探究上面我们提到的几个关键性问题。因此，文章的全部贡献都来自于实验及其结果。（别说为啥做实验也能发文章：都是高考，我只上了个普通一本，我高中同学就上了清华） ​ 在ImageNet的1000类上，作者把1000类分成两份（A和B），每份500个类别。然后，分别对A和B基于Caffe训练了一个AlexNet网络。一个AlexNet网络一共有8层， 除去第8层是类别相关的网络无法迁移以外，作者在 1 到 7这 7层上逐层进行 finetune 实验，探索网络的可迁移性。 ​ 为了更好地说明 finetune 的结果，作者提出了有趣的概念： AnB 和 BnB。 ​ 迁移A网络的前n层到B （AnB） vs固定B网络的前n层（BnB） ​ 简单说一下什么叫AnB:（所有实验都是针对数据B来说的）将A网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对B进行分类。 ​ 相应地，有BnB:把训练好的B网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对 B 进行分类。 ​ 实验结果 ​ 实验结果如下图（图33） 所示: ​ 这个图说明了什么呢？我们先看蓝色的BnB和BnB+（就是BnB加上finetune）。对 BnB而言，原训练好的 B 模型的前 3 层直接拿来就可以用而不会对模型精度有什么损失到了第4 和第5 层，精度略有下降，不过还是可以接受。然而到了第6 第第7层，精度居然奇迹般地回升了！这是为什么？原因如下:对于一开始精度下降的第4 第 5 层来说，确 图 33: 深度网络迁移实验结果 1 实是到了这一步，feature变得越来越specific,所以下降了。那对于第6第7层为什么精度又不变了？那是因为，整个网络就8层，我们固定了第6第7层，这个网络还能学什么呢？所以很自然地，精度和原来的 B 网络几乎一致！ ​ 对 BnB+ 来说，结果基本上都保持不变。说明 finetune 对模型结果有着很好的促进作用！ ​ 我们重点关注AnB和AnB+。对AnB来说，直接将A网络的前3层迁移到B,貌似不会有什么影响，再一次说明，网络的前3层学到的几乎都是general feature!往后，到了第4第5层的时候，精度开始下降，我们直接说：一定是feature不general 了！然而，到了第6第7层，精度出现了小小的提升后又下降，这又是为什么？作者在这里提出两点co-adaptation和feature representation。就是说，第4第5层精度下降的时候，主要是由于A和B两个数据集的差异比较大，所以会下降；至I」了第6第7层，由于网络几乎不迭代了，学习能力太差，此时 feature 学不到，所以精度下降得更厉害。 ​ 再看AnB+。加入了 finetune以后，AnB+的表现对于所有的n几乎都非常好，甚至 比baseB （最初的B）还要好一些！这说明：finetune对于深度迁移有着非常好的促进作用! ​ 把上面的结果合并就得到了下面一张图 （图34）： ​ 至此， AnB 和 BnB 基本完成。作者又想，是不是我分 A 和 B 数据的时候，里面存在一些比较相似的类使结果好了？比如说A里有猫，B里有狮子，所以结果会好？为了排除这些影响，作者又分了一下数据集，这次使得A和B里几乎没有相似的类别。在这个条件下再做AnB,与原来精度比较（0%为基准）得到了下图（图35）: ​ 这个图说明了什么呢？简单：随着可迁移层数的增加，模型性能下降。但是，前3层仍然还是可以迁移的！同时,与随机初始化所有权重比较,迁移学习的精度是很高的!总之： 深度迁移网络要比随机初始化权重效果好； 网络层数的迁移可以加速网络的学习和优化。 什么是深度网络自适应？ 基本思路 ​ 深度网络的 finetune 可以帮助我们节省训练时间，提高学习精度。但是 finetune 有它的先天不足:它无法处理训练数据和测试数据分布不同的情况。而这一现象在实际应用中比比皆是。因为 finetune 的基本假设也是训练数据和测试数据服从相同的数据分布。这在迁移学习中也是不成立的。因此，我们需要更进一步，针对深度网络开发出更好的方法使之更好地完成迁移学习任务。 ​ 以我们之前介绍过的数据分布自适应方法为参考，许多深度学习方法［Tzeng et al.,2014, Long et al.,2015a］都开发出了自适应层(AdaptationLayer)来完成源域和目标域数据的自适应。自适应能够使得源域和目标域的数据分布更加接近，从而使得网络的效果更好。 ​ 从上述的分析我们可以得出，深度网络的自适应主要完成两部分的工作: ​ 一是哪些层可以自适应，这决定了网络的学习程度； ​ 二是采用什么样的自适应方法 (度量准则)，这决定了网络的泛化能力。 ​ 深度网络中最重要的是网络损失的定义。绝大多数深度迁移学习方法都采用了以下的损失定义方式: ​ 其中，I表示网络的最终损失，lc(Ds,ys)表示网络在有标注的数据(大部分是源域)上的常规分类损失(这与普通的深度网络完全一致)，Ia(Ds,Dt)表示网络的自适应损失。最后一部分是传统的深度网络所不具有的、迁移学习所独有的。此部分的表达与我们先前讨论过的源域和目标域的分布差异，在道理上是相同的。式中的A是权衡两部分的权重参数。 ​ 上述的分析指导我们设计深度迁移网络的基本准则：决定自适应层，然后在这些层加入自适应度量，最后对网络进行 finetune。 GAN在迁移学习中的应用 生成对抗网络 GAN(Generative Adversarial Nets) [Goodfellow et al.,2014] 是目前人工智能领域最炙手可热的概念之一。其也被深度学习领军人物 Yann Lecun 评为近年来最令人欣喜的成就。由此发展而来的对抗网络，也成为了提升网络性能的利器。本小节介绍深度对抗网络用于解决迁移学习问题方面的基本思路以及代表性研究成果。 基本思路 ​ GAN 受到自博弈论中的二人零和博弈 (two-player game) 思想的启发而提出。它一共包括两个部分：一部分为生成网络(Generative Network)，此部分负责生成尽可能地以假乱真的样本，这部分被成为生成器(Generator)；另一部分为判别网络(Discriminative Network), 此部分负责判断样本是真实的，还是由生成器生成的，这部分被成为判别器(Discriminator) 生成器和判别器的互相博弈，就完成了对抗训练。 ​ GAN 的目标很明确：生成训练样本。这似乎与迁移学习的大目标有些许出入。然而，由于在迁移学习中，天然地存在一个源领域，一个目标领域，因此，我们可以免去生成样本的过程，而直接将其中一个领域的数据 (通常是目标域) 当作是生成的样本。此时，生成器的职能发生变化，不再生成新样本，而是扮演了特征提取的功能：不断学习领域数据的特征使得判别器无法对两个领域进行分辨。这样，原来的生成器也可以称为特征提取器 (Feature Extractor)。 ​ 通常用 Gf 来表示特征提取器，用 Gd 来表示判别器。正是基于这样的领域对抗的思想，深度对抗网络可以被很好地运用于迁移学习问题中。与深度网络自适应迁移方法类似，深度对抗网络的损失也由两部分构成：网络训练的损失lc*和领域判别损失Id： DANN Yaroslav Ganin 等人 [Ganin et al., 2016]首先在神经网络的训练中加入了对抗机制，作者将他们的网络称之为DANN(Domain-Adversarial Neural Network)。在此研宄中，网络的学习目标是：生成的特征尽可能帮助区分两个领域的特征，同时使得判别器无法对两个领域的差异进行判别。该方法的领域对抗损失函数表示为： Id = max 其中的 Ld 表示为 参考文献 王晋东，迁移学习简明手册 [Baktashmotlagh et al., 2013] Baktashmotlagh, M., Harandi, M. T., Lovell, B. C.,and Salz- mann, M. (2013). Unsupervised domain adaptation by domain invariant projection. In ICCV, pages 769-776. [Baktashmotlagh et al., 2014] Baktashmotlagh, M., Harandi, M. T., Lovell, B. C., and Salz- mann, M. (2014). Domain adaptation on the statistical manifold. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,pages 2481-2488. [Ben-David et al., 2010] Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2):151-175. [Ben-David et al., 2007] Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2007). Analysis of representations for domain adaptation. In NIPS, pages 137-144. [Blitzer et al., 2008] Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wortman, J. (2008). Learning bounds for domain adaptation. In Advances in neural information processing systems, pages 129-136. [Blitzer et al., 2006] Blitzer, J., McDonald, R., and Pereira, F. (2006). Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empiri­cal methods in natural language processing, pages 120-128. Association for Computational Linguistics. [Borgwardt et al., 2006] Borgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P., Scholkopf, B., and Smola, A. J. (2006). Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49-e57. [Bousmalis et al., 2016] Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D. (2016). Domain separation networks. In Advances in Neural Information Processing Systems, pages 343-351. [Cai et al., 2011] Cai, D., He, X., Han, J., and Huang, T. S. (2011). Graph regularized nonnegative matrix factorization for data representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1548-1560. [Cao et al., 2017] Cao, Z., Long, M., Wang, J., and Jordan, M. I. (2017). Partial transfer learning with selective adversarial networks. arXiv preprint arXiv:1707.07901. [Carlucci et al., 2017] Carlucci, F. M., Porzi, L., Caputo, B., Ricci, E., and Bulo, S. R. (2017). Autodial: Automatic domain alignment layers. In International Conference on* Computer Vision. [Cook et al., 2013] Cook, D., Feuz, K. D., and Krishnan, N. C. (2013). Transfer learning for activity recognition: A survey. Knowledge and information systems, 36(3):537-556. [Cortes et al., 2008] Cortes, C., Mohri, M., Riley, M., and Rostamizadeh, A. (2008). Sample selection bias correction theory. In International Conference on Algorithmic Learning Theory, pages 38-53, Budapest, Hungary. Springer. [Dai et al., 2007] Dai, W., Yang, Q., Xue, G.-R., and Yu, Y. (2007). Boosting for transfer learning. In ICML, pages 193-200. ACM. [Davis and Domingos, 2009] Davis, J. and Domingos, P. (2009). Deep transfer via second- order markov logic. In Proceedings of the 26th annual international conference on machine learning, pages 217-224. ACM. [Denget al., 2014] Deng,W.,Zheng,Q.,andWang,Z.(2014).Cross-personactivityrecog-nition using reduced kernel extreme learning machine. Neural Networks, 53:1-7. [Donahue et al., 2014] Donahue, J., Jia, Y., et al. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647-655. [Dorri and Ghodsi, 2012] Dorri, F. and Ghodsi, A. (2012). Adapting component analysis. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 846-851. IEEE. [Duan et al., 2012] Duan, L., Tsang, I. W., and Xu, D. (2012). Domain transfer multi­ple kernel learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):465-479. [Fernando et al., 2013] Fernando, B., Habrard, A., Sebban, M., and Tuytelaars, T. (2013). Unsupervised visual domain adaptation using subspace alignment. In ICCV*, pages 2960­2967. [Fodor, 2002] Fodor, I. K. (2002). A survey of dimension reduction techniques. Center for Applied Scientific Computing, Lawrence Livermore National Laboratory*, 9:1-18. [Ganin et al., 2016] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Lavi- olette, F., Marchand, M., and Lempitsky, V. (2016).Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1-35. [Gao et al., 2012] Gao, C., Sang, N., and Huang, R. (2012). Online transfer boosting for object tracking. In Pattern Recognition (ICPR), 2012 21st International Conference on, pages 906-909. IEEE. [Ghifary et al., 2017] Ghifary, M., Balduzzi, D., Kleijn, W. B., and Zhang, M. (2017). Scat­ter component analysis: A unified framework for domain adaptation and domain general­ization. IEEE transactions on pattern analysis and machine intelligence, 39(7):1414-1430. [Ghifary et al., 2014] Ghifary, M., Kleijn, W. B., and Zhang, M. (2014). Domain adaptive neural networks for object recognition. In PRICAI, pages 898-904. [Gong et al., 2012] Gong, B., Shi, Y., Sha, F., and Grauman, K. (2012). Geodesic flow kernel for unsupervised domain adaptation. In CVPR, pages 2066-2073. [Goodfellow et al., 2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde- Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680. [Gopalan et al., 2011] Gopalan, R., Li, R., and Chellappa, R. (2011). Domain adaptation for object recognition: An unsupervised approach. In ICCV, pages 999-1006. IEEE. [Gretton et al., 2012] Gretton, A., Sejdinovic, D., Strathmann, H., Balakrishnan, S., Pontil, M., Fukumizu, K., and Sriperumbudur, B. K. (2012). Optimal kernel choice for large- scale two-sample tests. In Advances in neural information processing systems, pages 1205-1213. [Gu et al., 2011] Gu, Q., Li, Z., Han, J., et al. (2011). Joint feature selection and subspace learning. In IJCAI Proceedings-International Joint Conference on Artificial Intel ligence, volume 22, page 1294. [Hamm and Lee, 2008] Hamm, J. and Lee, D. D. (2008). Grassmann discriminant analysis: a unifying view on subspace-based learning. In ICML, pages 376-383. ACM. [Hou et al., 2015] Hou, C.-A., Yeh, Y.-R., and Wang, Y.-C. F. (2015). An unsupervised domain adaptation approach for cross-domain visual classification. In Advanced Video and Signal Based Surveil lance (AVSS), 2015 12th IEEE International Conference on,pages 1-6. IEEE. [Hsiao et al., 2016] Hsiao, P.-H., Chang, F.-J., and Lin, Y.-Y. (2016). Learning discrim­inatively reconstructed source data for object recognition with few examples. IEEETransactions on Image Processing*, 25(8):3518-3532. [Hu and Yang, 2011] Hu, D. H. and Yang, Q. (2011). Transfer learning for activity recog­nition via sensor mapping. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22, page 1962, Barcelona, Catalonia, Spain. IJCAI. [Huang et al., 2007] Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M., Scholkopf, B., et al. (2007). Correcting sample selection bias by unlabeled data. Advances in neural information processing systems, 19:601. [Jaini et al., 2016] Jaini, P., Chen, Z., Carbajal, P., Law, E., Middleton, L., Regan, K., Schaekermann, M., Trimponias, G., Tung, J., and Poupart, P. (2016). Online bayesian transfer learning for sequential data modeling. In ICLR 2017. [Kermany et al., 2018] Kermany, D. S., Goldbaum, M., Cai, W., Valentim, C. C., Liang, H., Baxter, S. L., McKeown, A., Yang, G., Wu, X., Yan, F., et al. (2018). Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell, 172(5):1122-1131. [Khan and Heisterkamp, 2016] Khan, M. N. A. and Heisterkamp, D. R. (2016). Adapting instance weights for unsupervised domain adaptation using quadratic mutual informa­tion and subspace learning. In Pattern Recognition (ICPR), 2016 23rd International Conference on, pages 1560-1565, Mexican City. IEEE. [Krizhevsky et al., 2012] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems*, pages 1097-1105. [Li et al., 2012] Li, H., Shi, Y., Liu, Y., Hauptmann, A. G., and Xiong, Z. (2012). Cross­domain video concept detection: A joint discriminative and generative active learning approach. Expert Systems with Applications, 39(15):12220-12228. [Li et al., 2016] Li, J., Zhao, J., and Lu, K. (2016). Joint feature selection and structure preservation for domain adaptation. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, pages 1697-1703. AAAI Press. [Li et al., 2018] Li, Y., Wang, N., Shi, J., Hou, X., and Liu, J. (2018). Adaptive batch normalization for practical domain adaptation. Pattern Recognition, 80:109-117. [Liu et al., 2011] Liu, J., Shah, M., Kuipers, B., and Savarese, S. (2011). Cross-view action recognition via view knowledge transfer. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 3209-3216, Colorado Springs, CO, USA. IEEE. [Liu and Tuzel, 2016] Liu, M.-Y. and Tuzel, O. (2016). Coupled generative adversarial networks. In Advances in neural information processing systems, pages 469-477. [Liu et al., 2017] Liu, T., Yang, Q., and Tao, D. (2017). Understanding how feature struc­ture transfers in transfer learning. In IJCAI. [Long et al., 2015a] Long, M., Cao, Y., Wang, J., and Jordan, M. (2015a). Learning trans­ferable features with deep adaptation networks. In ICML, pages 97-105. [Long et al., 2016] Long, M., Wang, J., Cao, Y., Sun, J., and Philip, S. Y. (2016). Deep learning of transferable representation for scalable domain adaptation. IEEE Transac­tions on Knowledge and Data Engineering, 28(8):2027-2040. [Long et al., 2014a] Long, M., Wang, J., Ding, G., Pan, S. J., and Yu, P. S. (2014a). Adaptation regularization: A general framework for transfer learning.*IEEE TKDE, 26(5):1076-1089. [Long et al., 2014b] Long, M., Wang, J., Ding, G., Sun, J., and Yu, P. S. (2014b). Transfer joint matching for unsupervised domain adaptation. In *CVPR ,pages 1410-1417. [Long et al., 2013] Long, M., Wang, J., et al. (2013). Transfer feature learning with joint distribution adaptation. In ICCV, pages 2200-2207. [Long et al., 2017] Long, M., Wang, J., and Jordan, M. I. (2017). Deep transfer learning with joint adaptation networks. In ICML, pages 2208-2217. [Long et al., 2015b] Long, M., Wang, J., Sun, J., and Philip, S. Y. (2015b). Domain invari­ant transfer kernel learning. IEEE Transactions on Knowledge and Data Engineering, 27(6):1519-1532. [Luo et al., 2017] Luo, Z., Zou, Y., Hoffman, J., and Fei-Fei, L. F. (2017). Label efficient learning of transferable representations acrosss domains and tasks. In Advances in Neural Information Processing Systems, pages 164-176. [Mihalkova et al., 2007] Mihalkova, L., Huynh, T., and Mooney, R. J. (2007). Mapping and revising markov logic networks for transfer learning. In AAAI, volume 7, pages 608-614. [Mihalkova and Mooney, 2008] Mihalkova, L. and Mooney, R. J. (2008). Transfer learning by mapping with minimal target data. In Proceedings of the AAAI-08 workshop on transfer learning for complex tasks. [Nater et al., 2011] Nater, F., Tommasi, T., Grabner, H., Van Gool, L., and Caputo, B. (2011). Transferring activities: Updating human behavior analysis. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 1737­1744, Barcelona, Spain. IEEE. [Pan et al., 2008a] Pan, S. J., Kwok, J. T., and Yang, Q. (2008a). Transfer learning via dimensionality reduction. In Proceedings of the 23rd AAAI conference on Artificial in­telligence, volume 8, pages 677-682. [Pan et al., 2008b] Pan, S. J., Shen, D., Yang, Q., and Kwok, J. T. (2008b). Transferring localization models across space. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pages 1383-1388. [Pan et al., 2011] Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. (2011). Domain adaptation via transfer component analysis. IEEE TNN, 22(2):199-210. [PanandYang, 2010] Pan,S.J.andYang,Q.(2010). A survey on transfer learning. IEEE TKDE*, 22(10):1345-1359. [Patil and Phursule, 2013] Patil, D. M. and Phursule, R. (2013). Knowledge transfer using cost sensitive online learning classification. International Journal of Science and Research, pages 527-529. [Razavian et al., 2014] Razavian, A. S., Azizpour, H., Sullivan, J., and Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512­519. IEEE. [Saito et al., 2017] Saito, K., Ushiku, Y., and Harada, T. (2017). Asymmetric tri-training for unsupervised domain adaptation. In International Conference on Machine Learning. [Sener et al., 2016] Sener, O., Song, H. O., Saxena, A., and Savarese, S. (2016). Learning transferrable representations for unsupervised domain adaptation. In Advances in Neural Information Processing Systems, pages 2110-2118. [Shen et al., 2018] Shen, J., Qu, Y., Zhang, W., and Yu, Y. (2018). Wasserstein distance guided representation learning for domain adaptation. In AAAI. [Si et al., 2010] Si, S., Tao, D., and Geng, B. (2010). Bregman divergence-based regu­larization for transfer subspace learning. IEEE Transactions on Knowledge and Data Engineering, 22(7):929-942. [Silver et al., 2017] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. Nature, 550(7676):354. [Stewart and Ermon, 2017] Stewart, R. and Ermon, S. (2017). Label-free supervision of neural networks with physics and domain knowledge. In AAAI, pages 2576-2582. [Sun et al., 2016] Sun, B., Feng, J., and Saenko, K. (2016). Return of frustratingly easy domain adaptation. In AAAI, volume 6, page 8. [Sun and Saenko, 2015] Sun, B. and Saenko, K. (2015). Subspace distribution alignment for unsupervised domain adaptation. In BMVC, pages 24-1. [Sun and Saenko, 2016] Sun, B. and Saenko, K. (2016). Deep coral: Correlation alignment for deep domain adaptation. In European Conference on Computer Vision, pages 443-450. Springer. [Tahmoresnezhad and Hashemi, 2016] Tahmoresnezhad, J. and Hashemi, S. (2016). Visual domain adaptation via transfer feature learning. Knowledge and Information Systems, pages 1-21. [Tan et al., 2015] Tan, B., Song, Y., Zhong, E., and Yang, Q. (2015). Transitive trans­fer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1155-1164. ACM. [Tan et al., 2017] Tan, B., Zhang, Y., Pan, S. J., and Yang, Q. (2017). Distant domain transfer learning. In Thirty-First AAAI Conference on Artificial Intelligence. [Taylor and Stone, 2009] Taylor, M. E. and Stone, P. (2009). Transfer learning for reinforce­ment learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633- 1685. [Tzeng et al., 2015] Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. (2015). Simulta­neous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4068-4076, Santiago, Chile. IEEE. [Tzeng et al., 2017] Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017). Adversarial discriminative domain adaptation. In CVPR, pages 2962-2971. [Tzeng et al., 2014] Tzeng, E., Hoffman, J., Zhang, N., et al. (2014). Deep domain confu­sion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474. [Wang et al., 2017] Wang, J., Chen, Y., Hao, S., et al. (2017). Balanced distribution adap­tation for transfer learning. In ICDM, pages 1129-1134. [Wang et al., 2018] Wang, J., Chen, Y., Hu, L., Peng, X., and Yu, P. S. (2018). Strati­fied transfer learning for cross-domain activity recognition. In 2018 IEEE International Conference on Pervasive Computing and Communications (PerCom). [Wang et al., 2014] Wang, J., Zhao, P., Hoi, S. C., and Jin, R. (2014). Online feature selection and its applications. IEEE Transactions on Knowledge and Data Engineering, 26(3):698-710. [Wei et al., 2016a] Wei, P., Ke, Y., and Goh, C. K. (2016a). Deep nonlinearfeature coding for unsupervised domain adaptation. In IJCAI, pages 2189-2195. [Wei et al., 2017] Wei, Y., Zhang, Y., and Yang, Q. (2017). Learning totransfer. arXiv preprint arXiv:1708.05629*. [Wei et al., 2016b] Wei, Y., Zhu, Y., Leung, C. W.-k., Song, Y., and Yang, Q. (2016b). Instilling social to physical: Co-regularized heterogeneous transfer learning. In Thirtieth AAAI Conference on Artificial Intelligence*. [Weiss et al., 2016] Weiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. Journal of Big Data, 3(1):1-40. [Wu et al., 2017] Wu, Q., Zhou, X., Yan, Y., Wu, H., and Min, H. (2017). Online transfer learning by leveraging multiple source domains. Knowledge and Information Systems, 52(3):687-707. [xinhua, 2016] xinhua (2016). http://mp.weixin.qq.com/s?__biz=MjM5ODYzNzAyMQ==&amp; mid=2651933920&amp;idx=1\\&amp;sn=ae2866bd12000f1644eae1094497837e. [Yan et al., 2017] Yan, Y., Wu, Q., Tan, M., Ng, M. K., Min, H., and Tsang, I. W. (2017). Online heterogeneous transfer by hedge ensemble of offline and online decisions. IEEE transactions on neural networks and learning systems. [Yosinski et al., 2014] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320-3328. [Zadrozny, 2004] Zadrozny, B. (2004). Learning and evaluating classifiers under sample selection bias. In Proceedings of the twenty-first international conference on Machine learning, page 114, Alberta, Canada. ACM. [Zellinger et al., 2017] Zellinger, W., Grubinger, T., Lughofer, E., Natschlager, T., and Saminger-Platz, S. (2017). Central moment discrepancy (cmd) for domain-invariant rep­resentation learning. arXiv preprint arXiv:1702.08811. [Zhang et al., 2017a] Zhang, J., Li, W., and Ogunbona, P. (2017a). Joint geometrical and statistical alignment for visual domain adaptation. In CVPR. [Zhang et al., 2017b] Zhang, X., Zhuang, Y., Wang, W., and Pedrycz, W. (2017b). On­line feature transformation learning for cross-domain object category recognition. IEEE transactions on neural networks and learning systems. [Zhao and Hoi, 2010] Zhao, P. and Hoi, S. C. (2010). Otl: A framework of online transfer learning. In Proceedings of the 27th international conference on machine learning (ICML- 10), pages 1231-1238. [Zhao et al., 2010] Zhao, Z., Chen, Y., Liu, J., and Liu, M. (2010). Cross-mobile elm based activity recognition. International Journal of Engineering and Industries, 1(1):30-38. [Zhao et al., 2011] Zhao, Z., Chen, Y., Liu, J., Shen, Z., and Liu, M. (2011). Cross-people mobile-phone based activity recognition. In Proceedings of the Twenty-Second interna­tional joint conference on Artificial Intelligence (IJCAI), volume 11, pages 2545-2550. Citeseer. [Zheng et al., 2009] Zheng, V. W., Hu, D. H., and Yang, Q. (2009). Cross-domain activity recognition. In Proceedings of the 11th international conference on Ubiquitous computing, pages 61-70. ACM. [Zheng et al., 2008] Zheng, V. W., Pan, S. J., Yang, Q., and Pan, J. J. (2008). Transferring multi-device localization models using latent multi-task learning. In AAAI, volume 8, pages 1427-1432, Chicago, Illinois, USA. AAAI. [Zhuang et al., 2015] Zhuang, F., Cheng, X., Luo, P., Pan, S. J., and He, Q. (2015). Su­pervised representation learning: Transfer learning with deep autoencoders. In IJCAI,pages 4119-4125. [Zhuo et al., 2017] Zhuo, J., Wang, S., Zhang, W., and Huang, Q. (2017). Deep unsuper­vised convolutional domain adaptation. In Proceedings of the 2017 ACM on Multimedia Conference, pages 261-269. ACM.","link":"/posts/3343981380.html"},{"title":"Google Go语言 golang 语法详解笔记","text":"包 Package 包的声明 Declare 使用package关键字声明当前源文件所在的包 包声明语句是所有源文件的第一行非注释语句 包名称中不能包含空白字符 包名推荐与源文件所在的目录名称保持一致 每个目录中只能定义一个package 12345package cxy // 声明一个名为“cxy”的包package 我的包 // 声明一个名为“我的包”的包package main // main包, 程序启动执行的入口包 错误的包声明 12345package &quot;mypkg&quot; // 错误package a/b/c // 错误pakcage a.b.c // 错误 包的导入 Import 导入包路径是对应包在$GOROOT/pkg/$GOOS_$GOARCH/、$GOPATH/pkg/$GOOS_$GOARCH/或当前路径中的相对路径 1234567// 导入$GOROOT/$GOOS_$GOARCH/中的相对路径包(官方标准库)import &quot;fmt&quot;import &quot;math/rand&quot;// 导入$GOPATH/$GOOS_$GOARCH/中的相对路径包import &quot;github.com/user/project/pkg&quot;import &quot;code.google.com/p/project/pkg&quot; 导入当前包的相对路径包 例如有Go目录如下： $GOPATH/src ├─x0 │ ├─y0 │ │ └─z0 │ └─y1 │ └─z1 └─x1 └─y2 123import &quot;./y0/z0&quot; // x0包中导入子包 z0包import &quot;../y0/z0&quot; // y1包中导入子包 z0包import &quot;x0/y1/z1&quot; // y2包中导入 z1包 错误的导入包路径 123import a/b/c // 错误import &quot;a.b.c&quot; // 错误import a.b.c // 错误 用圆括号组合导入包路径 123456import (&quot;fmt&quot;; &quot;math&quot;)import ( &quot;fmt&quot; &quot;math&quot;) 导入包可以定义别名，防止同名称的包冲突 123456789import ( &quot;a/b/c&quot; c1 &quot;x/y/c&quot; // 将导入的包c定义别名为 c1 格式化 &quot;fmt&quot; // 将导入的包fmt定义别名为 格式化 m &quot;math&quot; // 将导入的包math定义别名为 m) 引用包名是导入包路径的最后一个目录中定义的唯一包的名称 定义的包名与目录同名时，直接引用即可 12345// 引用普通名称的导入包c.hello()// 引用定义别名的包格式化.Println(m.Pi) 定义的包名与所在目录名称不同时，导入包路径仍为目录所在路径，引用包名为定义的包名称 123// 源文件路径: $GOPATH/src/proj/my-util/util.go// 定义包名: utilpackage util 12345// 导入util包路径import &quot;proj/my-util&quot;// 引用util包util.doSomething() 静态导入，在导入的包路径之前增加一个小数点. 12345// 类似C中的include 或Java中的import staticimport . &quot;fmt&quot;// 然后像使用本包元素一样使用fmt包中可见的元素，不需要通过包名引用Println(&quot;no need package name&quot;) 导入包但不直接使用该包，在导入的包路径之前增加一个下划线_ 123456// 如果当前go源文件中未引用过log包，将会导致编译错误import &quot;log&quot; // 错误import . &quot;log&quot; // 静态导入未使用同样报错// 在包名前面增加下划线表示导入包但是不直接使用它，被导入的包中的init函数会在导入的时候执行import _ &quot;github.com/go-sql-driver/mysql&quot; 包内元素的可见性 Accessability 名称首字符为Unicode包含的大写字母的元素是被导出的，对外部包是可见的 首字为非大写字母的元素只对本包可见(同包跨源文件可以访问，子包不能访问) 1234567891011var In int // In is exportedvar in byte // in is unexportedvar ȸȹ string // ȸȹ is unexportedconst Ȼom bool = false // Ȼom is exportedconst ѧѩ uint8 = 1 // ѧѩ is unexportedtype Ĩnteger int // Ĩnteger is exportedtype ブーリアン *bool // ブーリアン is unexportedfunc Ӭxport() {...} // Ӭxport is exportedfunc įnner() {...} // įnner is unexportedfunc (me *Integer) ⱱalueOf(s string) int {...} // ⱱalueOf is unexportedfunc (i ブーリアン) Ȿtring() string {...} // Ȿtring is exported internal包（内部包）Go 1.4 internal包及其子包中的导出元素只能被与internal同父包的其他包访问 例如有Go目录如下： $GOPATH/src ├─x0 │ ├─internal │ │ └─z0 │ └─y0 │ └─z1 └─x1 └─y1 x0，y0，z1包中可以访问internal，z0包中的可见元素 x1，y1包中不能导入internal，z0包 规范导入包路径Canonical import pathsGo 1.4 包声明语句后面添加标记注释，用于标识这个包的规范导入路径。 1package pdf // import &quot;rsc.io/pdf&quot; 如果使用此包的代码的导入的路径不是规范路径，go命令会拒绝编译。 例如有 rsc.io/pdf 的一个fork路径 github.com/rsc/pdf 如下程序代码导入路径时使用了非规范的路径则会被go拒绝编译 1import &quot;github.com/rsc/pdf&quot; 数据类型 Data Type 基础数据类型 Basic data type 基本类型包含：数值类型，布尔类型，字符串 类型 取值范围 默认零值 类型 取值范围 默认零值 int int32,int64 0 uint uint32,uint64 0 int8 -27 ~ 27-1 0 uint8,byte 0 ~ 28-1 0 int16 -215 ~ 215-1 0 uint16 0 ~ 216-1 0 int32,rune -231 ~ 231-1 0 uint32 0 ~ 232-1 0 int64 -263 ~ 263-1 0 uint64 0 ~ 264-1 0 float32 IEEE-754 32-bit 0.0 float64 IEEE-754 64-bit 0.0 complex64 float32+float32i 0 + 0i complex128 float64+float64i 0 + 0i bool true,false false string \"\" ~ \"∞\" \"\",`` uintptr uint32,uint64 0 error - nil byte 是 uint8 的别名 rune 是 int32 的别名，代表一个Unicode码点 int与int32或int64是不同的类型，只是根据架构对应32/64位值 uint与uint32或uint64是不同的类型，只是根据架构对应32/64位值 变量 Variable 变量声明, 使用var关键字 Go中只能使用var 声明变量，无需显式初始化值 12345var i int // i = 0var s string // s = &quot;&quot; (Go中的string是值类型，默认零值是空串 &quot;&quot; 或 ``，不存在nil(null)值)var e error // e = nil, error是Go的内建接口类型。 关键字的顺序错误或缺少都是编译错误的 123var int a // 编译错误a int // 编译错误int a // 编译错误 var 语句可以声明一个变量列表，类型在变量名之后 12345678910var a,b,c int // a = 0, b = 0, c = 0var ( a int // a = 0 b string // b = &quot;&quot; c uint // c = 0)var ( a,b,c int d string) 变量定义时初始化赋值，每个变量对应一个值 12var a int = 0var a, b int = 0, 1 变量定义并初始化时可以省略类型，Go自动根据初始值推导变量的类型 12var a = 'A' // a int32var a,b = 0, &quot;B&quot; // a int, b string 使用组合符号:=定义并初始化变量，根据符号右边表达式的值的类型声明变量并初始化它的值 := 不能在函数外使用，函数外的每个语法块都必须以关键字开始 12345a := 3 // a inta, b, c := 8, '呴', true // a int, b int32, c boolc := `formatted string` // c stringc := 1 + 2i // c complex128 常量 Constant 常量可以是字符、字符串、布尔或数值类型的值，数值常量是高精度的值 12345678910const x int = 3const y,z int = 1,2const ( a byte = 'A' b string = &quot;B&quot; c bool = true d int = 4 e float32 = 5.1 f complex64 = 6 + 6i) 根据常量值自动推导类型 12345const a = 0 // a intconst ( b = 2.3 // b float64 c = true // c bool) 常量组内定义时复用表达式 常量组内定义的常量只有名称时，其值会根据上一次最后出现的常量表达式计算相同的类型与值 12345678910const ( a = 3 // a = 3 b // b = 3 c // c = 3 d = len(&quot;asdf&quot;) // d = 4 e // e = 4 f // f = 4 g,h,i = 7,8,9 // 复用表达式要一一对应 x,y,z // x = 7, y = 8, z = 9) 自动递增枚举常量 iota iota的枚举值可以赋值给数值兼容类型 每个常量单独声明时，iota不会自动递增 1234const a int = iota // a = 0const b int = iota // b = 0const c byte = iota // c = 0const d uint64 = iota // d = 0 常量组合声明时，iota每次引用会逐步自增，初始值为0，步进值为1 1234567const ( a uint8 = iota // a = 0 b int16 = iota // b = 1 c rune = iota // c = 2 d float64 = iota // d = 3 e uintptr = iota // e = 4) 即使iota不是在常量组内第一个开始引用，也会按组内常量数量递增 1234567const ( a = &quot;A&quot; b = 'B' c = iota // c = 2 d = &quot;D&quot; e = iota // e = 4) 枚举的常量都为同一类型时，可以使用简单序列格式(组内复用表达式). 12345const ( a = iota // a int32 = 0 b // b int32 = 1 c // c int32 = 2) 枚举序列中的未指定类型的常量会跟随序列前面最后一次出现类型定义的类型 12345678const ( a byte = iota // a uint8 = 0 b // b uint8 = 1 c // c uint8 = 2 d rune = iota // d int32 = 3 e // e int32 = 4 f // f int32 = 5) iota自增值只在一个常量定义组合中有效，跳出常量组合定义后iota初始值归0 123456789const ( a = iota // a int32 = 0 b // b int32 = 1 c // c int32 = 2)const ( e = iota // e int32 = 0 (iota重新初始化并自增) f // f int32 = 1) 定制iota序列初始值与步进值 (通过组合内复用表达式实现) 123456const ( a = (iota + 2) * 3 // a int32 = 6 (a=(0+2)*3) 初始值为6,步进值为3 b // b int32 = 9 (b=(1+2)*3) c // c int32 = 12 (c=(2+2)*3) d // d int32 = 15 (d=(3+2)*3)) 数组 Array 数组声明带有长度信息且长度固定，数组是值类型默认零值不是nil，传递参数时会进行复制。 声明定义数组时中括号[ ]在类型名称之前，赋值引用元素时中括号[ ]在数组变量名之后。 1234567var a [3]int = [3]int{0, 1, 2} // a = [0 1 2]var b [3]int = [3]int{} // b = [0 0 0]var c [3]intc = [3]int{}c = [3]int{0,0,0} // c = [0 0 0]d := [3]int{} // d = [0 0 0]fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, d, d, len(d), cap(d)) // [3]int [3]int{0, 0, 0} 3 3 使用...自动计算数组的长度 12345678var a = [...]int{0, 1, 2}// 多维数组只能自动计算最外围数组长度x := [...][3]int{{0, 1, 2}, {3, 4, 5}}y := [...][2][2]int{{{0,1},{2,3}},{{4,5},{6,7}}}// 通过下标访问数组元素println(y[1][1][0]) // 6 初始化指定索引的数组元素，未指定初始化的元素保持默认零值 12var a = [3]int{2:3}var b = [...]string{2:&quot;c&quot;, 3:&quot;d&quot;} 切片 Slice slice 切片是对一个数组上的连续一段的引用，并且同时包含了长度和容量信息 因为是引用类型，所以未初始化时的默认零值是nil，长度与容量都是0 12345678var a []intfmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, a, a, len(a), cap(a)) // []int []int(nil) 0 0// 可用类似数组的方式初始化slicevar d []int = []int{0, 1, 2}fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, d, d, len(d), cap(d)) // []int []int{0, 1, 2} 3 3var e = []string{2:&quot;c&quot;, 3:&quot;d&quot;} 使用内置函数make初始化slice，第一参数是slice类型，第二参数是长度，第三参数是容量(省略时与长度相同) 12345678var b = make([]int, 0)fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, b, b, len(b), cap(b)) // []int []int{} 0 0var c = make([]int, 3, 10)fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, c, c, len(c), cap(c)) // []int []int{} 3 10var a = new([]int)fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, a, a, len(*a), cap(*a)) // *[]int &amp;[]int(nil) 0 0 基于slice或数组重新切片，创建一个新的 slice 值指向相同的数组 重新切片支持两种格式： 2个参数 slice[beginIndex:endIndex] 需要满足条件：0 &lt;= beginIndex &lt;= endIndex &lt;= cap(slice) 截取从开始索引到结束索引-1 之间的片段 新slice的长度：length=(endIndex - beginIndex) 新slice的容量：capacity=(cap(slice) - beginIndex) beginIndex的值可省略，默认为0 endIndex 的值可省略，默认为len(slice) 123456s := []int{0, 1, 2, 3, 4}a := s[1:3] // a: [1 2], len: 2, cap: 4b := s[:4] // b: [0 1 2 3], len: 4, cap: 5c := s[1:] // c: [1 2 3 4], len: 4, cap: 4d := s[1:1] // d: [], len: 0, cap: 4e := s[:] // e: [0 1 2 3 4], len: 5, cap: 5 3个参数 slice[beginIndex:endIndex:capIndex] 需要满足条件：0 &lt;= beginIndex &lt;= endIndex &lt;= capIndex &lt;= cap(slice) 新slice的长度：length=(endIndex - beginIndex) 新slice的容量：capacity=(capIndex - beginIndex) beginIndex的值可省略，默认为0 123s := make([]int, 5, 10)a := s[9:10:10] // a: [0], len: 1, cap: 1b := s[:3:5] // b: [0 0 0], len: 3, cap: 5 向slice中增加/修改元素 123456789s := []string{}s = append(s, &quot;a&quot;) // 添加一个元素s = append(s, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) // 添加一列元素t = []string{&quot;e&quot;, &quot;f&quot;, &quot;g&quot;}s = append(s, t...} // 添加另一个切片t的所有元素s = append(s, t[:2]...} // 添加另一个切片t的部分元素s[0] = &quot;A&quot; // 修改切片s的第一个元素s[len(s)-1] = &quot;G&quot; // 修改切片s的最后一个元素 删除slice中指定的元素 因为slice引用指向底层数组，数组的长度不变元素是不能删除的，所以删除的原理就是排除待删除元素后用其他元素重新构造一个数组 1234567891011121314func deleteByAppend() { i := 3 s := []int{1, 2, 3, 4, 5, 6, 7} //delete the fourth element(index is 3), using append s = append(s[:i], s[i+1:]...)}func deleteByCopy() { i := 3 s := []int{1, 2, 3, 4, 5, 6, 7} //delete the fourth element(index is 3), using copy copy(s[i:], s[i+1:]) s = s[:len(s)-1]} 字典/映射 Map map是引用类型，使用内置函数 make进行初始化，未初始化的map零值为 nil长度为0，并且不能赋值元素 1234567var m map[int]intm[0] = 0 // × runtime error: assignment to entry in nil mapfmt.Printf(&quot;type: %T\\n&quot;, m) // map[int]intfmt.Printf(&quot;value: %#v\\n&quot;, m) // map[int]int(nil)fmt.Printf(&quot;value: %v\\n&quot;, m) // map[]fmt.Println(&quot;is nil: &quot;, nil == m) // truefmt.Println(&quot;length: &quot;, len(m)) // 0，if m is nil, len(m) is zero. 使用内置函数make初始化map 1234567var m map[int]int = make(map[int]int)m[0] = 0 // 插入或修改元素fmt.Printf(&quot;type: %T\\n&quot;, m) // map[int]intfmt.Printf(&quot;value: %#v\\n&quot;, m) // map[int]int(0:0)fmt.Printf(&quot;value: %v\\n&quot;, m) // map[0:0]fmt.Println(&quot;is nil: &quot;, nil == m) // falsefmt.Println(&quot;length: &quot;, len(m)) // 1 直接赋值初始化map 12345678m := map[int]int{0:0,1:1, // 最后的逗号是必须的}n := map[string]S{&quot;a&quot;:S{0,1},&quot;b&quot;:{2,3}, // 类型名称可省略} map的使用：读取、添加、修改、删除元素 1234567m[0] = 3 // 修改m中key为0的值为3m[4] = 8 // 添加到m中key为4值为8a := n[&quot;a&quot;] // 获取n中key为“a“的值b, ok := n[&quot;c&quot;] // 取值, 并通过ok(bool)判断key对应的元素是否存在.delete(n, &quot;a&quot;) // 使用内置函数delete删除key为”a“对应的元素. 结构体 Struct 结构体类型struct是一个字段的集合 1234type S struct { A int B, c string} 结构体初始化通过结构体字段的值作为列表来新分配一个结构体。 1var s S = S{0, &quot;1&quot;, &quot;2&quot;} 使用 Name: 语法可以仅列出部分字段(字段名的顺序无关) 1var s S = S{B: &quot;1&quot;, A: 0} 结构体是值类型，传递时会复制值，其默认零值不是nil 123var a Svar b = S{}fmt.Println(a == b) // true 结构体组合 将一个命名类型作为匿名字段嵌入一个结构体 嵌入匿名字段支持命名类型、命名类型的指针和接口类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package maintype ( A struct { v int } // 定义结构体B，嵌入结构体A作为匿名字段 B struct { A } // 定义结构体C，嵌入结构体A的指针作为匿名字段 C struct { *A })func (a *A) setV(v int) { a.v = v}func (a A) getV() int { return a.v}func (b B) getV() string { return &quot;B&quot;}func (c *C) getV() bool { return true}func main() { a := A{} b := B{} // 初始化结构体B，其内匿名字段A默认零值是A{} c := C{&amp;A{}} // 初始化结构体C，其内匿名指针字段*A默认零值是nil，需要初始化赋值 println(a.v) // 结构体A嵌入B，A内字段自动提升到B println(b.v) // 结构体指针*A嵌入C，*A对应结构体内字段自动提升到C println(c.v) a.setV(3) b.setV(5) c.setV(7) println(a.getV(), b.A.getV(), c.A.getV()) println(a.getV(), b.getV(), c.getV())} 匿名结构体 匿名结构体声明时省略了type关键字，并且没有名称 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mainimport &quot;fmt&quot;type Integer int// 声明变量a为空的匿名结构体类型var a struct{}// 声明变量b为包含一个字段的匿名结构体类型var b struct{ x int }// 声明变量c为包含两个字段的匿名结构体类型var c struct { u int v bool}func main() { printa(a) b.x = 1 fmt.Printf(&quot;bx: %#v\\n&quot;, printb(b)) // bx: struct { y uint8 }{y:0x19} printc(c) // 声明d为包含3个字段的匿名结构体并初始化部分字段 d := struct { x int y complex64 z string }{ z: &quot;asdf&quot;, x: 111, } d.y = 22 + 333i fmt.Printf(&quot;d: %#v\\n&quot;, d) // d: struct { x int; y complex64; z string }{x:111, y:(22+333i), z:&quot;asdf&quot;} // 声明变量e为包含两个字段的匿名结构体类型 // 包含1个匿名结构体类型的命名字段和1个命名类型的匿名字段 e := struct { a struct{ x int } // 结构体组合嵌入匿名字段只支持命名类型 Integer }{} e.Integer = 444 fmt.Printf(&quot;e: %#v\\n&quot;, e) // e: struct { a struct { x int }; main.Integer }{a:struct { x int }{x:0}, Integer:444}}// 函数参数为匿名结构体类型时，传入参数类型声明必须保持一致func printa(s struct{}) { fmt.Printf(&quot;a: %#v\\n&quot;, s) // a: struct {}{}}// 函数入参和返回值都支持匿名结构体类型func printb(s struct{ x int }) (x struct{ y byte }) { fmt.Printf(&quot;b: %#v\\n&quot;, s) // b: struct { x int }{x:1} x.y = 25 return}func printc(s struct {u int; v bool }) { fmt.Printf(&quot;c: %#v\\n&quot;, s) // c: struct { u int; v bool }{u:0, v:false}} 指针 Pointer 通过取地址操作符&amp;获取指向值/引用对象的指针。 1234567var i int = 1pi := &amp;i // 指向数值的指针a := []int{0, 1, 2}pa := &amp;a // 指向引用对象的指针var s *S = &amp;S{0, &quot;1&quot;, &quot;2&quot;} // 指向值对象的指针 内置函数new(T)分配了一个零初始化的 T 值，并返回指向它的指针 12var i = new(int)var s *S = new(S) 使用*读取/修改指针指向的值 12345678func main() { i := new(int) *i = 3 println(i, *i) // 0xc208031f80 3 i = new(int) println(i, *i) // 0xc208031f78 0} 指针使用点号来访问结构体字段 结构体字段/方法可以通过结构体指针来访问，通过指针间接的访问是透明的。 12fmt.Println(s.A)fmt.Println((*s).A) 指针的指针 123456789101112func main() { var i int var p *int var pp **int var ppp ***int var pppp ****int println(i, p, pp, ppp, pppp) // 0 0x0 0x0 0x0 0x0 i, p, pp, ppp, pppp = 123, &amp;i, &amp;p, &amp;pp, &amp;ppp println(i, p, pp, ppp, pppp) // 123 0xc208031f68 0xc208031f88 0xc208031f80 0xc208031f78 println(i, *p, **pp, ***ppp, ****pppp) // 123 123 123 123 123} 跨层指针元素的使用 在指针引用多层对象时，指针是针对引用表达式的最后一位元素。 1234567891011package atype X struct { A Y}type Y struct { B Z}type Z struct { C int} 12345678910111213141516package mainimport ( &quot;a&quot; &quot;fmt&quot;)func main() { var x = a.X{} var p = &amp;x fmt.Println(&quot;x: &quot;, x) // x: {{{0}}} println(&quot;p: &quot;, p) // p: 0xc208055f20 fmt.Println(&quot;*p: &quot;, *p) // *p: {{{0}}} println(&quot;x.A.B.C: &quot;, x.A.B.C) // x.A.B.C: 0 // println(&quot;*p.A.B.C: &quot;, *p.A.B.C) // invalid indirect of p.A.B.C (type int) println(&quot;(*p).A.B.C: &quot;, (*p).A.B.C) // (*p).A.B.C: 0} Go的指针没有指针运算，但是 道高一尺，魔高一丈 Go语言中的指针运算 利用unsafe操作未导出变量 通道 Channel channel用于两个goroutine之间传递指定类型的值来同步运行和通讯。 操作符&lt;-用于指定channel的方向，发送或接收。 如果未指定方向，则为双向channel。 123var c0 chan int // 可用来发送和接收int类型的值var c1 chan&lt;- int // 可用来发送int类型的值var c2 &lt;-chan int // 可用来接收int类型的值 channel是引用类型，使用make函数来初始化。 未初始化的channel零值是nil，且不能用于发送和接收值。 12c0 := make(chan int) // 不带缓冲的int类型channelc1 := make(chan *int, 10) // 带缓冲的*int类型指针channel 无缓冲的channe中有值时发送方会阻塞，直到接收方从channel中取出值。 带缓冲的channel在缓冲区已满时发送方会阻塞，直到接收方从channel中取出值。 接收方在channel中无值会一直阻塞。 通过channel发送一个值时，&lt;-作为二元操作符使用， 1c0 &lt;- 3 通过channel接收一个值时，&lt;-作为一元操作符使用。 1i := &lt;-c1 关闭channel，只能用于双向或只发送类型的channel 只能由 发送方调用close函数来关闭channel 接收方取出已关闭的channel中发送的值后，后续再从channel中取值时会以非阻塞的方式立即返回channel传递类型的零值。 12345678910111213141516171819202122232425ch := make(chan string, 1)// 发送方，发送值后关闭channelch &lt;- &quot;hello&quot;close(ch)// 接收方，取出发送的值fmt.Println(&lt;-ch) // 输出： “hello”// 再次从已关闭的channel中取值，返回channel传递类型的零值fmt.Println(&lt;-ch) // 输出： 零值，空字符串“”// 接收方判断接收到的零值是由发送方发送的还是关闭channel返回的默认值s, ok := &lt;-chif ok { fmt.Println(&quot;Receive value from sender:&quot;, s)} else { fmt.Println(&quot;Get zero value from closed channel&quot;)}// 向已关闭的通道发送值会产生运行时恐慌panicch &lt;- &quot;hi&quot;// 再次关闭已经关闭的通道也会产生运行时恐慌panicclose(ch) 使用for range语句依次读取发送到channel的值，直到channel关闭。 12345678910111213141516171819package mainimport &quot;fmt&quot;func main() { // 无缓冲和有缓冲的channel的range用法相同 var ch = make(chan int) // make(chan int, 2) 或 make(chan int , 100) go func() { for i := 0; i &lt; 5; i++ { ch &lt;- i } close(ch) }() // channel中无发送值且未关闭时会阻塞 for x := range ch { fmt.Println(x) }} 下面方式与for range用法效果相同 12345678910loop: for { select { case x, ok := &lt;-c: if !ok { break loop } fmt.Println(x) } } 接口 Interface 接口类型是由一组方法定义的集合。 接口类型的值可以存放实现这些方法的任何值。 123type Abser interface { Abs() float64} 类型通过实现定义的方法来实现接口， 不需要显式声明实现某接口。 12345678type MyFloat float64func (f MyFloat) Abs() float64 { if f &lt; 0 { return float64(-f) } return float64(f)} 接口组合 12345678910111213141516171819202122232425262728293031323334353637383940type Reader interface { Read(b []byte) (n int)}type Writer interface { Write(b []byte) (n int)}// 接口ReadWriter组合了Reader和Writer两个接口type ReadWriter interface { Reader Writer}type File struct { // ...}func (f *File) Read(b []byte) (n int) { println(&quot;Read&quot;, len(b),&quot;bytes data.&quot;) return len(b)}func (f *File) Write(b []byte) (n int) { println(&quot;Write&quot;, len(b),&quot;bytes data.&quot;) return len(b)}func main() { // *File 实现了Read方法和Write方法，所以实现了Reader接口和Writer接口以及组合接口ReadWriter var f *File = &amp;File{} var r Reader = f var w Writer = f var rw ReadWriter = f bs := []byte(&quot;asdf&quot;) r.Read(bs) rw.Read(bs) w.Write(bs) rw.Write(bs)} 内置接口类型error是一个用于表示错误情况的常规接口，其零值nil表示没有错误 所有实现了Error方法的类型都能表示为一个错误 123type error interface { Error() string} 自定义类型 Go中支持自定义的类型可基于： 基本类型、数组类型、切片类型、字典类型、函数类型、结构体类型、通道类型、接口类型以及自定义类型的类型 12345678910111213141516171819202122232425262728type ( A int B int8 C int16 D rune E int32 F int64 G uint H byte I uint16 J uint32 K uint64 L float32 M float64 N complex64 O complex128 P uintptr Q bool R string S [3]uint8 T []complex128 U map[string]uintptr V func(i int) (b bool) W struct {a, b int} X chan int Y interface {} Z A) 以及支持以上所有支持类型的指针类型 12345678910111213141516171819202122232425262728type ( A *int B *int8 C *int16 D *rune E *int32 F *int64 G *uint H *byte I *uint16 J *uint32 K *uint64 L *float32 M *float64 N *complex64 O *complex128 P *uintptr Q *bool R *string S *[3]uint8 T *[]complex128 U *map[string]uintptr V *func(i int) (b bool) W *struct {a, b int} X *chan int Y *interface {} Z *A) 类型别名Go 1.9 1234567891011121314151617181920type ( A struct{} B struct{} // 定义两个结构相同的类型A，B C = A // 定义类型A的别名)func main() { var ( a A b B c C ) // 因为类型名不同，所以a和b不是相同类型，此处编译错误 fmt.Println(a == b) // invalid operation: a == b (mismatched types A and B) fmt.Println(a == c) // true a = C{} c = A{} fmt.Println(c == a) // true} 语句 Statement 分号/括号 ; { Go是采用语法解析器自动在每行末尾增加分号，所以在写代码的时候可以省略分号。 Go编程中只有几个地方需要手工增加分号： for循环使用分号把初始化、条件和遍历元素分开。 if/switch的条件判断带有初始化语句时使用分号分开初始化语句与判断语句。 在一行中有多条语句时，需要增加分号。 控制语句(if，for，switch，select)、函数、方法 的左大括号不能单独放在一行， 语法解析器会在大括号之前自动插入一个分号，导致编译错误。 条件语句 if if语句 小括号 ( )是可选的，而大括号 { } 是必须的。 123456789101112131415161718192021if (i &lt; 0) // 编译错误. println(i)if i &lt; 0 // 编译错误. println(i)if (i &lt; 0) { // 编译通过. println(i)}if (i &lt; 0 || i &gt; 10) { println(i)}if i &lt; 0 { println(i)} else if i &gt; 5 &amp;&amp; i &lt;= 10 { println(i)} else { println(i)} 可以在条件之前执行一个简单的语句，由这个语句定义的变量的作用域仅在 if / else if / else 范围之内 123456789101112131415if (i := 0; i &lt; 1) { // 编译错误. println(i)}if i := 0; (i &lt; 1) { // 编译通过. println(i)}if i := 0; i &lt; 0 { // 使用gofmt格式化代码会自动移除代码中不必要的小括号( ) println(i)} else if i == 0 { println(i)} else { println(i)} if语句作用域范围内定义的变量会覆盖外部同名变量，与方法函数内局部变量覆盖全局变量同理 12345a, b := 0, 1if a, b := 3, 4; a &gt; 1 &amp;&amp; b &gt; 2 { println(a, b) // 3 4}println(a, b) // 0 1 if判断语句类型断言 12345678910111213141516171819202122232425262728package mainfunc f0() int {return 333}func main() { x := 9 checkType(x) checkType(f0)}func checkType(x interface{}) { // 断言传入的x为int类型，并获取值 if i, ok := x.(int); ok { println(&quot;int: &quot;, i) // int: 0 } if f, ok := x.(func() int); ok { println(&quot;func: &quot;, f()) // func: 333 } // 如果传入x类型为int，则可以直接获取其值 a := x.(int) println(a) // 如果传入x类型不是byte，则会产生恐慌panic b := x.(byte) println(b)} 分支选择 switch switch存在分支选择对象时，case分支支持单个常量、常量列表 12345678switch x {case 0: println(&quot;single const&quot;)case 1, 2, 3: println(&quot;const list&quot;)default: println(&quot;default&quot;)} 分支选择对象之前可以有一个简单语句，case语句的大括号可以省略 1234567891011switch x *= 2; x {case 4: { println(&quot;single const&quot;)}case 5, 6, 7: { println(&quot;const list&quot;)}default: { println(&quot;default&quot;)}} switch只有一个简单语句，没有分支选择对象时，case分支支持逻辑表达式语句 12345678switch x /= 3; {case x == 8: println(&quot;expression&quot;)case x &gt;= 9: println(&quot;expression&quot;)default: println(&quot;default&quot;)} switch没有简单语句，没有分支选择对象时，case分支支持逻辑表达式语句 12345678switch {case x == 10: println(&quot;expression&quot;)case x &gt;= 11: println(&quot;expression&quot;)default: println(&quot;default&quot;)} switch类型分支，只能在switch语句中使用的.(type)获取对象的类型。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package mainimport ( &quot;fmt&quot; &quot;code.google.com/p/go.crypto/openpgp/errors&quot;)func main() { var ( a = 0.1 b = 2+3i c = &quot;asdf&quot; d = [...]byte{1, 2, 3} e = []complex128{1+2i} f = map[string]uintptr{&quot;a&quot;: 0} g = func(int) bool {return true} h = struct { a, b int }{} i = &amp;struct {}{} j chan int k chan &lt;- bool l &lt;-chan string m errors.SignatureError ) values := []interface{}{nil, a, b, &amp;c, d, e, f, g, &amp;g, h, &amp;h, i, j, k, l, m} for _, v := range values { typeswitch(v) }}func typeswitch(x interface{}) { // switch x.(type) { // 不使用类型值时 switch i := x.(type) { case nil: fmt.Println(&quot;x is nil&quot;) case int, int8, int16, rune, int64, uint, byte, uint16, uint32, uint64, float32, float64, complex64, complex128, uintptr, bool, string: fmt.Printf(&quot;basic type : %T\\n&quot;, i) case *int, *int8, *int16, *rune, *int64, *uint, *byte, *uint16, *uint32, *uint64, *float32, *float64, *complex64, *complex128, *uintptr, *bool, *string: fmt.Printf(&quot;basic pointer type : %T\\n&quot;, i) case [3]byte, []complex128, map[string]uintptr: fmt.Printf(&quot;collection type : %T\\n&quot;, i) case func(i int) (b bool), *func(): fmt.Printf(&quot;function type : %T\\n&quot;, i) case struct {a, b int}, *struct {}: fmt.Printf(&quot;struct type : %T\\n&quot;, i) case chan int, chan &lt;- bool, &lt;-chan string: fmt.Printf(&quot;channel type : %T\\n&quot;, i) case error, interface{a(); b()}: fmt.Printf(&quot;interface type : %T\\n&quot;, i) default: fmt.Printf(&quot;other type : %T\\n&quot;, i) }}// output: // x is nil// basic type : float64// basic type : complex128// basic pointer type : *string// collection type : [3]uint8// collection type : []complex128// collection type : map[string]uintptr// function type : func(int) bool// other type : *func(int) bool// struct type : struct { a int; b int }// other type : *struct { a int; b int }// struct type : *struct {}// channel type : chan int// channel type : chan&lt;- bool// channel type : &lt;-chan string// interface type : errors.SignatureError switch中每个case分支默认带有break效果，一个分支执行后就跳出switch，不会自动向下执行其他case。 使用fallthrough强制向下继续执行后面的case代码。 在类型分支中不允许使用fallthrough语句 12345678910111213141516171819switch {case false: println(&quot;case 1&quot;) fallthroughcase true: println(&quot;case 2&quot;) fallthroughcase false: println(&quot;case 3&quot;) fallthroughcase true: println(&quot;case 4&quot;)case false: println(&quot;case 5&quot;) fallthroughdefault: println(&quot;default case&quot;)}// 输出：case 2 case 3 case 4 循环语句 for Go只有一种循环结构：for 循环。 可以让前置(初始化)、中间(条件)、后置(迭代)语句为空，或者全为空。 12345678910for i := 0; i &lt; 10; i++ {...}for i := 0; i &lt; 10; {...} // 省略迭代语句for i := 0; ; i++; {...} // 省略条件语句for ; i &lt; 10; i++ {...} // 省略初始化语句for i := 0; ; {...} // 省略条件和迭代语句, 分号不能省略for ; i &lt; 10; {...} // 省略初始化和迭代语句, 分号可省略for ; ; i++ {...} // 省略初始化和条件语句, 分号不能省略for i &lt; 10 {...}for ; ; {...} // 分号可省略for {...} for语句中小括号 ( )是可选的，而大括号 { } 是必须的。 123for (i := 0; i &lt; 10; i++) {...} // 编译错误.for i := 0; (i &lt; 10); i++ {...} // 编译通过.for (i &lt; 10) {...} // 编译通过. Go的for each循环for range 123456789101112131415161718a := [5]int{2, 3, 4, 5, 6}for k, v := range a { fmt.Println(k, v) // 输出：0 2, 1 3, 2 4, 3 5, 4 6}for k := range a { fmt.Println(k) // 输出：0 1 2 3 4}for _ = range a { fmt.Println(&quot;print without care about the key and value&quot;)}for range a { fmt.Println(&quot;new syntax – print without care about the key and value&quot;)} 循环的继续、中断、跳转 123456789for k, v := range s { if v == 3 { continue // 结束本次循环，进入下一次循环中 } else if v == 5 { break // 结束整个for循环 } else { goto SOMEWHERE // 跳转到标签指定的代码处 }} for range只支持遍历数组、数组指针、slice、string、map、channel类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package mainimport &quot;fmt&quot;func main() { var arr = [...]int{33, 22, 11, 0} // 遍历数组，取一位值时为索引值 for k := range arr { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, } fmt.Println() // 遍历数组，取两位值时，第一位为索引值，第二位为元素值 for k, v := range arr { fmt.Printf(&quot;%d %d, &quot;, k, v) // 0 33, 1 22, 2 11, 3 0, } fmt.Println() // 遍历数组指针，取一位值时为索引值 for k := range &amp;arr { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, } fmt.Println() // 遍历数组指针，取两位值时，第一位为索引值，第二位为元素值 for k, v := range &amp;arr { fmt.Printf(&quot;%d %d, &quot;, k, v) // 0 33, 1 22, 2 11, 3 0, } fmt.Println() var slc = []byte{44, 55, 66, 77} // 遍历切片，取一位值时为索引值 for k := range slc { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, } fmt.Println() // 遍历切片，取两位值时，第一位为索引值，第二位为元素值 for k, v := range slc { fmt.Printf(&quot;%d %d, &quot;, k, v) // 0 44, 1 55, 2 66, 3 77, } fmt.Println() var str = &quot;abc一二3&quot; // 遍历字符串，取一位值时为字节索引值 for k := range str { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, 6, 9, } fmt.Println() // 遍历字符串，取两位值时，第一位为字节索引值，第二位为Unicode字符 for k, v := range str { fmt.Printf(&quot;%d %d %s, &quot;, k, v, string(v)) // 0 97 a, 1 98 b, 2 99 c, 3 19968 一, 6 20108 二, 9 51 3, } fmt.Println() var mp = map[int]string{5:&quot;A&quot;, 9:&quot;B&quot;} // 遍历map，取一位值时为键key for k := range mp { fmt.Printf(&quot;%d, &quot;, k) // 9, 5, } fmt.Println() // 遍历map，取两位值时，第一位为键key，第二位为元素值value for k, v := range mp { fmt.Printf(&quot;%d %s, &quot;, k, v) // 5 A, 9 B, } fmt.Println() var ch = make(chan int) go func() { for i := 0; i &lt; 5; i++ { ch &lt;- i } close(ch) }() // 遍历channel时，只能取一位值，为发送方发送到channel中的值 for x := range ch { fmt.Printf(&quot;%d &quot;, x) // 0 1 2 3 4 }} 通道选择 select select用于当前goroutine从一组可能的通讯中选择一个进一步处理。 如果任意一个通讯都可以进一步处理，则从中随机选择一个，执行对应的语句。否则在没有默认分支(default case)时，select语句则会阻塞，直到其中一个通讯完成。 select 的 case 里的操作语句只能是IO操作 123456789ch1, ch2 := make(chan int), make(chan int)// 因为没有值发送到select中的任一case的channel中，此select将会阻塞select {case &lt;-ch1: println(&quot;channel 1&quot;)case &lt;-ch2: println(&quot;channel 2&quot;)} 1234567891011ch1, ch2 := make(chan int), make(chan int)// 因为没有值发送到select中的任一case的channel中，此select将会执行default分支select {case &lt;-ch1: println(&quot;channel 1&quot;)case &lt;-ch2: println(&quot;channel 2&quot;)default: println(&quot;default&quot;)} select只会执行一次case分支的逻辑，与for组合使用实现多次遍历分支 12345678910111213func main() { for { select { case &lt;-time.Tick(time.Second): println(&quot;Tick&quot;) case &lt;-time.After(5 * time.Second): println(&quot;Finish&quot;) default: println(&quot;default&quot;) time.Sleep(5e8) } }} 延迟执行 defer defer语句调用函数，将调用的函数加入defer栈，栈中函数在defer所在的主函数返回时执行，执行顺序是先进后出/后进先出。 1234567891011121314package mainfunc main() { defer print(0) defer print(1) defer print(2) defer print(3) defer print(4) for i := 5; i &lt;= 9; i++ { defer print(i) } // 输出：9876543210} defer在函数返回后执行，可以修改函数返回值 123456789101112package mainfunc main() { println(f()) // 返回： 15}func f() (i int) { defer func() { i *= 5 }() return 3} defer用于释放资源 释放锁 12mu.Lock()defer mu.Unlock() 关闭channel 12ch &lt;- &quot;hello&quot;defer close(ch) 关闭IO流 12f, err := os.Open(&quot;file.xxx&quot;)defer f.Close() 关闭数据库连接 12345db, err := sql.Open(&quot;mysql&quot;,&quot;user:password@tcp(127.0.0.1:3306)/hello&quot;)if err != nil { log.Fatal(err)}defer db.Close() defer用于恐慌的截获 panic用于产生恐慌，recover用于截获恐慌，recover只能在defer语句中使用, 直接调用recover是无效的。 123456789101112131415161718func main() { f() fmt.Println(&quot;main normal...&quot;)}func f() { defer func() { if r := recover(); r != nil { fmt.Println(&quot;catch:&quot;, r) } }() p() fmt.Println(&quot;normal...&quot;)}func p() { panic(&quot;exception...&quot;)} 跳转语句 goto goto用于在一个函数内部运行跳转到指定标签的代码处，不能跳转到其他函数中定义的标签。 goto模拟循环 1234567891011package mainfunc main() { i := 0loop: i++ if i &lt; 5 { goto loop } println(i)} goto模拟continue，break 12345678910111213141516func main() { i, sum := 0, 0head: for ; i &lt;= 10; i++ { if i &lt; 5 { i++ // 此处必须单独调用一次，因为goto跳转时不会执行for循环的自增语句 goto head // continue } if i &gt; 9 { goto tail // break } sum += i }tail: println(sum) // 输出：35} 注意：任何时候都不建议使用goto 函数 Function 函数声明 Declare 使用关键字func声明函数，函数可以没有参数或接受多个参数 12345func f0() {/*...*/}func f1(a int) {/*...*/}func f2(a int, b byte) {/*...*/} 在函数参数类型之前使用...声明该参数为可变数量的参数 可变参数只能声明为函数的最后一个参数。 123func f3(a ...int) {/*...*/}func f4(a int, b bool, c ...string) {/*...*/} 函数可以返回任意数量的返回值 1234567891011func f0() { return}func f1() int { return 0}func f2() (int, string) { return 0, &quot;A&quot;} 函数返回结果参数，可以像变量那样命名和使用 123456func f() (a int, b string) { a = 1 b = &quot;B&quot; return // 即使return后面没有跟变量，关键字在函数结尾也是必须的 // 或者 return a, b} 当两个或多个连续的函数命名参数是同一类型，则除了最后一个类型之外，其他都可以省略 12345func f0(a,b,c int) {/*...*/}func f1() (a,b,c int) {/*...*/}func f2(a,b int, c,d byte) (x,y int, z,s bool) {/*...*/} 函数闭包 Closure 匿名函数、闭包、函数值 Go中函数作为第一类对象，可以作为值对象赋值给变量 可以在函数体外/内定义匿名函数，命名函数不能嵌套定义到函数体内，只能定义在函数体外 123456789101112131415161718192021222324252627package maintype Myfunc func(i int) intfunc f0(name string){ println(name)}func main() { var a = f0 a(&quot;hello&quot;) // hello var f1 Myfunc = func(i int) int { return i } fmt.Println(f1(3)) // 3 var f2 func() int = func() int { return 0 } fmt.Println(f2()) // 0 // 省略部分关键字 var f3 func() = func() {/*...*/} var f4 = func() {/*...*/} f5 := func() {/*...*/}} 内建函数 Builtin func append 1func append(slice []Type, elems ...Type) []Type 内建函数append将元素追加到切片的末尾。若它有足够的容量，其目标就会重新切片以容纳新的元素。否则，就会分配一个新的基本数组。append返回更新后的切片，因此必须存储追加后的结果。 12slice = append(slice, elem1, elem2)slice = append(slice, anotherSlice...) 作为特例，可以向一个字节切片append字符串，如下： 1slice = append([]byte(&quot;hello &quot;), &quot;world&quot;...) func cap 1func cap(v Type) int 内建函数cap返回 v 的容量，这取决于具体类型： 数组：v中元素的数量，与 len(v) 相同 数组指针：*v中元素的数量，与len(v) 相同 切片：切片的容量（底层数组的长度）；若 v为nil，cap(v) 即为零 信道：按照元素的单元，相应信道缓存的容量；若v为nil，cap(v)即为零 func close 1func close(c chan&lt;- Type) 内建函数close关闭信道，该通道必须为双向的或只发送的。它应当只由发送者执行，而不应由接收者执行，其效果是在最后发送的值被接收后停止该通道。在最后的值从已关闭的信道中被接收后，任何对其的接收操作都会无阻塞的成功。对于已关闭的信道，语句： 1x, ok := &lt;-c // ok值为false func complex 1func complex(r, i FloatType) ComplexType 使用实部r和虚部i生成一个复数。 12c := complex(1, 2)fmt.Println(c) // (1+2i) func copy 1func copy(dst, src []Type) int 内建函数copy将元素从来源切片复制到目标切片中，也能将字节从字符串复制到字节切片中。copy返回被复制的元素数量，它会是 len(src) 和 len(dst) 中较小的那个。来源和目标的底层内存可以重叠。 12345678910111213a, b, c := []byte{1, 2, 3}, make([]byte, 2), 0fmt.Println(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot; c: &quot;, c) // a: [1 2 3] b: [0 0] c: 0c = copy(b, a)fmt.Println(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot; c: &quot;, c) // a: [1 2 3] b: [1 2] c: 2b = make([]byte, 5)c = copy(b, a)fmt.Println(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot; c: &quot;, c) // a: [1 2 3] b: [1 2 3 0 0] c: 3s := &quot;ABCD&quot;c = copy(b, s)fmt.Println(&quot;s:&quot;, s, &quot; b:&quot;, b, &quot; c: &quot;, c) // s: ABCD b: [65 66 67 68 0] c: 4 func delete 1func delete(m map[Type]Type1, key Type) 内建函数delete按照指定的键将元素从映射中删除。若m为nil或无此元素，delete不进行操作。 123456789m := map[int]string{ 0: &quot;A&quot;, 1: &quot;B&quot;, 2: &quot;C&quot;,}delete(m, 1)fmt.Println(m) // map[2:C 0:A]delete(m, 3) // 此行代码执行没有任何操作，也不会报错。 func imag 1func imag(c ComplexType) FloatType 返回复数c的虚部。 12c := 2+5ifmt.Println(imag(c)) // 5 func len 1func len(v Type) int 内建函数len返回 v 的长度，这取决于具体类型： 数组：v中元素的数量 数组指针：*v中元素的数量（v为nil时panic） 切片、映射：v中元素的数量；若v为nil，len(v)即为零 字符串：v中字节的数量，计算字符数量使用utf8.RuneCountInString() 通道：通道缓存中队列（未读取）元素的数量；若v为 nil，len(v)即为零 func make 1func make(Type, size IntegerType) Type 内建函数make分配并初始化一个类型为切片、映射、或通道的对象。其第一个实参为类型，而非值。make的返回类型与其参数相同，而非指向它的指针。其具体结果取决于具体的类型： 切片：size指定了其长度。该切片的容量等于其长度。切片支持第二个整数实参可用来指定不同的容量；它必须不小于其长度，因此 make([]int, 0, 10) 会分配一个长度为0，容量为10的切片。 映射：初始分配的创建取决于size，但产生的映射长度为0。size可以省略，这种情况下就会分配一个小的起始大小。 通道：通道的缓存根据指定的缓存容量初始化。若 size为零或被省略，该信道即为无缓存的。 func new 1func new(Type) *Type 内建函数new分配内存。其第一个实参为类型，而非值。其返回值为指向该类型的新分配的零值的指针。 func panic 1func panic(v interface{}) 内建函数panic停止当前Go程的正常执行。当函数F调用panic时，F的正常执行就会立刻停止。F中defer的所有函数先入后出执行后，F返回给其调用者G。G如同F一样行动，层层返回，直到该Go程中所有函数都按相反的顺序停止执行。之后，程序被终止，而错误情况会被报告，包括引发该恐慌的实参值，此终止序列称为恐慌过程。 func print 1func print(args ...Type) 内建函数print以特有的方法格式化参数并将结果写入标准错误，用于自举和调试。 func println 1func println(args ...Type) println类似print，但会在参数输出之间添加空格，输出结束后换行。 func real 1func real(c ComplexType) FloatType 返回复数c的实部。 123456789 c := 2+5i fmt.Println(real(c)) // 2 ``` - `func recover` ```go func recover() interface{} 内建函数recover允许程序管理恐慌过程中的Go程。在defer的函数中，执行recover调用会取回传至panic调用的错误值，恢复正常执行，停止恐慌过程。若recover在defer的函数之外被调用，它将不会停止恐慌过程序列。在此情况下，或当该Go程不在恐慌过程中时，或提供给panic的实参为nil时，recover就会返回nil。 初始化函数 init init函数是用于程序执行前做包的初始化工作的函数 init函数的声明没有参数和返回值 123func init() { // ...} 一个package或go源文件可以包含零个或多个init函数 1234567891011121314package mainfunc main() {}func init() { println(&quot;init1...&quot;)}func init() { println(&quot;init2...&quot;)}func init() { println(&quot;init3...&quot;)} init函数被自动调用，在main函数之前执行，不能在其他函数中调用，显式调用会报错该函数未定义。 1234567func init() { println(&quot;init...&quot;)}func main() { init() // undefined: init} 所有init函数都会被自动调用，调用顺序如下： 同一个go文件的init函数调用顺序是 从上到下的 同一个package中按go源文件名字符串比较 从小到大顺序调用各文件中的init函数 不同的package，如果不相互依赖的，按照main包中 先import的后调用的顺序调用其包中的init函数 如果package存在依赖，则先调用最早被依赖的package中的init函数 方法 Method 通过指定函数的接收者receiver,将函数绑定到一个类型或类型的指针上,使这个函数成为该类型的方法。 只能对命名类型和命名类型的指针编写方法。 只能在定义命名类型的那个包编写其方法。 不能对接口类型和接口类型的指针编写方法。 方法的接收者receiver是类型的值时，编译器会隐式的生成一个同名方法，其接收者receiver为该类型的指针，反过来却不会。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package maintype A struct { x, y int}// 定义结构体的方法，'_'表示方法内忽略使用结构体、字段及其他方法func (_ A) echo_A() { println(&quot;(_ A)&quot;)}// 同上func (A) echoA(s string) { println(&quot;(A)&quot;, s)}// 定义结构体指针的方法，'_'表示方法内忽略使用结构体指针、字段及其他方法func (_ *A) echo_жA() { println(&quot;(_ *A)&quot;)}// 同上func (*A) echoжA(s string) { println(&quot;(*A)&quot;, s)}// 定义结构体的方法，方法内可以引用结构体、字段及其他方法func (a A) setX(x int) { a.x = x}// 定义结构体指针的方法，方法内可以引用结构体、结构体指针、字段及其他方法func (a *A) setY(y int) { a.y = y}func main() { var a A // a = A{} a.setX(3) a.setY(6) println(a.x, a.y) // 0 6 a.echo_A() // (_ A) a.echoA(&quot;a&quot;) // (A) a a.echo_жA() // (_ *A) a.echoжA(&quot;a&quot;) // (*A) a // 以下是定义在结构体值上的方法原型，通过调用结构体类型上定义的函数，传入结构体的值 A.echo_A(a) // (_ A) A.echoA(a, &quot;a&quot;) // (A) a // A.echo_жA(a) // A.echo_жA未定义 // A.echoжA(a) // A.echoжA未定义 A.setX(a, 4) // A.setY(a, 7) // A.setY未定义 println(a.x) // 0 b := &amp;a b.setX(2) b.setY(5) println(b.x, b.y) // 0 5 b.echo_A() // (_ A) b.echoA(&quot;b&quot;) // (A) b b.echo_жA() // (_ *A) b.echoжA(&quot;b&quot;) // (*A) b // 以下是定义在结构体指针上的方法原型，通过调用结构体类型指针上定义的函数，传入结构体的指针 (*A).echo_A(b) // (_ A) (*A).echoA(b, &quot;b&quot;) // (A) b (*A).echo_жA(b) // (_ *A) (*A).echoжA(b, &quot;b&quot;) // (*A) b (*A).setX(b, 1) (*A).setY(b, 8) println(b.x, b.y) // 0 8 // 调用结构体空指针上的方法，以下注释掉的代码都是空指针错误 var c *A // c = nil // c.setX(2) // c.setY(5) // println(c.x, c.y) // c.echo_A() // c.echoA() c.echo_жA() // (_ *A) c.echoжA(&quot;c&quot;) // (*A) c // (*A).echo_A(c) // (*A).echoA(c) (*A).echo_жA(c) // (_ *A) (*A).echoжA(c, &quot;c&quot;) // (*A) c // (*A).setX(c, 1) // (*A).setY(c, 8) // println(c.x, c.y)} 结构体中组合匿名字段时，匿名字段的方法会向外传递，其规则如下： 匿名字段为值类型时：值的方法会传递给结构体的值，指针的方法会传递给结构体的指针； 匿名字段为指针类型时：指针的方法会传递给值和指针； 匿名字段为接口类型时：方法会传递给值和指针； Go中有匿名函数，但是没有匿名方法 并发 Concurrency 协程goroutine是由Go运行时环境管理的轻量级线程。 使用关键字go调用一个函数/方法，启动一个新的协程goroutine 1234567891011121314151617package mainimport ( &quot;time&quot;)func say(i int) { println(&quot;goroutine:&quot;, i)}func main() { for i := 1; i &lt;= 5; i++ { go say(i) } say(0) time.Sleep(5 * time.Second)} 主协程goroutine输出0，其他由go启动的几个子协程分别输出1～5 goroutine: 0 goroutine: 1 goroutine: 2 goroutine: 3 goroutine: 4 goroutine: 5 goroutine 在相同的地址空间中运行，因此访问共享内存必须进行同步。 12345678910111213141516171819202122232425package mainimport ( &quot;sync&quot; &quot;time&quot;)var mu sync.Mutexvar i intfunc main() { for range [5]byte{} { go Add() } time.Sleep(5*time.Second) println(i)}func Add() { // 使用互斥锁防止多个协程goroutine同时修改共享变量 // 只能限制同时访问此方法修改变量，在方法外修改则限制是无效的 mu.Lock() defer mu.Unlock() i++} 使用通道channel进行同步 12345678910111213141516171819202122package mainimport ( &quot;time&quot;)var i intvar ch = make(chan byte, 1)func main() { for range [5]byte{} { go Add() } time.Sleep(5*time.Second) println(i)}func Add() { ch &lt;- 0 i++ &lt;-ch} 使用channel在不同的goroutine之间通信 1234567891011121314151617181920212223242526// 上一个例子只是将channel用作同步开关，稍做修改即可在不同goroutine间通信package mainimport ( &quot;time&quot;)var i intvar ch = make(chan int, 1)func main() { for range [5]byte{} { go Add() } ch &lt;- i time.Sleep(5*time.Second) i = &lt;-ch println(i)}func Add() { // 从channel中接收的值是来自其他goroutine发送的 x := &lt;-ch x++ ch &lt;- x} 测试 Testing Go中自带轻量级的测试框架testing和自带的go test命令来实现单元测试和基准测试 单元测试 Unit 有如下待测试testgo包，一段简单的求和代码 1234567891011121314package testgoimport &quot;math&quot;func Sum(min, max int) (sum int) { if min &lt; 0 || max &lt; 0 || max &gt; math.MaxInt32 || min &gt; max { return 0 } for ; min &lt;= max; min++ { sum += min } return} 测试源文件名必须是_test.go结尾的，go test的时候才会执行到相应的代码 必须import testing包 所有的测试用例函数必须以Test开头 测试用例按照源码中编写的顺序依次执行 测试函数TestXxx()的参数是*testing.T，可以使用该类型来记录错误或者是测试状态 测试格式：func TestXxx (t *testing.T)，Xxx部分可以为任意的字母数字的组合，首字母不能是小写字母[a-z]，例如Testsum是错误的函数名。 函数中通过调用*testing.T的Error，Errorf，FailNow，Fatal，FatalIf方法标注测试不通过，调用Log方法用来记录测试的信息。 12345678910111213141516package testgoimport &quot;testing&quot;func TestSum(t *testing.T) { s := Sum(1, 0) t.Log(&quot;Sum 1 to 0:&quot;, s) if 0 != s { t.Error(&quot;not equal.&quot;) } s = Sum(1, 10) t.Log(&quot;Sum 1 to 10:&quot;, s) if 55 != s { t.Error(&quot;not equal.&quot;) }} 在当前包中执行测试：go test -v === RUN TestSum --- PASS: TestSum (0.00s) t0_test.go:7: Sum 1 to 0: 0 t0_test.go:12: Sum 1 to 10: 55 PASS ok /home/cxy/go/src/testgo 0.004s 基准测试 Benchmark 基准测试 Benchmark用来检测函数/方法的性能 基准测试用例函数必须以Benchmark开头 go test默认不会执行基准测试的函数，需要加上参数-test.bench，语法:-test.bench=\"test_name_regex\"，例如go test -test.bench=\".*\"表示测试全部的基准测试函数 在基准测试用例中，在循环体内使用testing.B.N，使测试可以正常的运行 1234567package testgoimport &quot;testing&quot;func BenchmarkSum(b *testing.B) { b.Logf(&quot;Sum 1 to %d: %d\\n&quot;, b.N, Sum(1, b.N))} 在当前包中执行测试：go test -v -bench . BenchmarkSum 2000000000 0.91 ns/op --- BENCH: BenchmarkSum t0_test.go:19: Sum 1 to 1: 1 t0_test.go:19: Sum 1 to 100: 5050 t0_test.go:19: Sum 1 to 10000: 50005000 t0_test.go:19: Sum 1 to 1000000: 500000500000 t0_test.go:19: Sum 1 to 100000000: 5000000050000000 t0_test.go:19: Sum 1 to 2000000000: 2000000001000000000 ok /home/cxy/go/src/testgo 1.922s","link":"/posts/2466014920.html"},{"title":"Google Go语言 语法笔记","text":"包 Package 包的声明 Declare 使用package关键字声明当前源文件所在的包 包声明语句是所有源文件的第一行非注释语句 包名称中不能包含空白字符 包名推荐与源文件所在的目录名称保持一致 每个目录中只能定义一个package 12345package cxy // 声明一个名为“cxy”的包package 我的包 // 声明一个名为“我的包”的包package main // main包, 程序启动执行的入口包 错误的包声明 12345package &quot;mypkg&quot; // 错误package a/b/c // 错误pakcage a.b.c // 错误 包的导入 Import 导入包路径是对应包在$GOROOT/pkg/$GOOS_$GOARCH/、$GOPATH/pkg/$GOOS_$GOARCH/或当前路径中的相对路径 1234567// 导入$GOROOT/$GOOS_$GOARCH/中的相对路径包(官方标准库)import &quot;fmt&quot;import &quot;math/rand&quot;// 导入$GOPATH/$GOOS_$GOARCH/中的相对路径包import &quot;github.com/user/project/pkg&quot;import &quot;code.google.com/p/project/pkg&quot; 导入当前包的相对路径包 例如有Go目录如下： $GOPATH/src ├─x0 │ ├─y0 │ │ └─z0 │ └─y1 │ └─z1 └─x1 └─y2 123import &quot;./y0/z0&quot; // x0包中导入子包 z0包import &quot;../y0/z0&quot; // y1包中导入子包 z0包import &quot;x0/y1/z1&quot; // y2包中导入 z1包 错误的导入包路径 123import a/b/c // 错误import &quot;a.b.c&quot; // 错误import a.b.c // 错误 用圆括号组合导入包路径 123456import (&quot;fmt&quot;; &quot;math&quot;)import ( &quot;fmt&quot; &quot;math&quot;) 导入包可以定义别名，防止同名称的包冲突 123456789import ( &quot;a/b/c&quot; c1 &quot;x/y/c&quot; // 将导入的包c定义别名为 c1 格式化 &quot;fmt&quot; // 将导入的包fmt定义别名为 格式化 m &quot;math&quot; // 将导入的包math定义别名为 m) 引用包名是导入包路径的最后一个目录中定义的唯一包的名称 定义的包名与目录同名时，直接引用即可 12345// 引用普通名称的导入包c.hello()// 引用定义别名的包格式化.Println(m.Pi) 定义的包名与所在目录名称不同时，导入包路径仍为目录所在路径，引用包名为定义的包名称 123// 源文件路径: $GOPATH/src/proj/my-util/util.go// 定义包名: utilpackage util 12345// 导入util包路径import &quot;proj/my-util&quot;// 引用util包util.doSomething() 静态导入，在导入的包路径之前增加一个小数点. 12345// 类似C中的include 或Java中的import staticimport . &quot;fmt&quot;// 然后像使用本包元素一样使用fmt包中可见的元素，不需要通过包名引用Println(&quot;no need package name&quot;) 导入包但不直接使用该包，在导入的包路径之前增加一个下划线_ 123456// 如果当前go源文件中未引用过log包，将会导致编译错误import &quot;log&quot; // 错误import . &quot;log&quot; // 静态导入未使用同样报错// 在包名前面增加下划线表示导入包但是不直接使用它，被导入的包中的init函数会在导入的时候执行import _ &quot;github.com/go-sql-driver/mysql&quot; 包内元素的可见性 Accessability 名称首字符为Unicode包含的大写字母的元素是被导出的，对外部包是可见的 首字为非大写字母的元素只对本包可见(同包跨源文件可以访问，子包不能访问) 1234567891011var In int // In is exportedvar in byte // in is unexportedvar ȸȹ string // ȸȹ is unexportedconst Ȼom bool = false // Ȼom is exportedconst ѧѩ uint8 = 1 // ѧѩ is unexportedtype Ĩnteger int // Ĩnteger is exportedtype ブーリアン *bool // ブーリアン is unexportedfunc Ӭxport() {...} // Ӭxport is exportedfunc įnner() {...} // įnner is unexportedfunc (me *Integer) ⱱalueOf(s string) int {...} // ⱱalueOf is unexportedfunc (i ブーリアン) Ȿtring() string {...} // Ȿtring is exported internal包（内部包） Go1.4+ internal包及其子包中的导出元素只能被与internal同父包的其他包访问 例如有Go目录如下： $GOPATH/src ├─x0 │ ├─internal │ │ └─z0 │ └─y0 │ └─z1 └─x1 └─y1 x0，y0，z1包中可以访问internal，z0包中的可见元素 x1，y1包中不能导入internal，z0包 规范导入包路径Canonical import paths Go1.4+ 包声明语句后面添加标记注释，用于标识这个包的规范导入路径。 1package pdf // import &quot;rsc.io/pdf&quot; 如果使用此包的代码的导入的路径不是规范路径，go命令会拒绝编译。 例如有 rsc.io/pdf 的一个fork路径 github.com/rsc/pdf 如下程序代码导入路径时使用了非规范的路径则会被go拒绝编译 1import &quot;github.com/rsc/pdf&quot; 数据类型 Data Type 基础数据类型 Basic data type 基本类型包含：数值类型，布尔类型，字符串 类型 取值范围 默认零值 类型 取值范围 默认零值 int int32,int64 0 uint uint32,uint64 0 int8 -27 ~ 27-1 0 uint8,byte 0 ~ 28-1 0 int16 -215 ~ 215-1 0 uint16 0 ~ 216-1 0 int32,rune -231 ~ 231-1 0 uint32 0 ~ 232-1 0 int64 -263 ~ 263-1 0 uint64 0 ~ 264-1 0 float32 IEEE-754 32-bit 0.0 float64 IEEE-754 64-bit 0.0 complex64 float32+float32i 0 + 0i complex128 float64+float64i 0 + 0i bool true,false false string \"\" ~ \"∞\" \"\",`` uintptr uint32,uint64 0 error - nil byte 是 uint8 的别名 rune 是 int32 的别名，代表一个Unicode码点 int与int32或int64是不同的类型，只是根据架构对应32/64位值 uint与uint32或uint64是不同的类型，只是根据架构对应32/64位值 变量 Variable 变量声明, 使用var关键字 Go中只能使用var 声明变量，无需显式初始化值 12345var i int // i = 0var s string // s = &quot;&quot; (Go中的string是值类型，默认零值是空串 &quot;&quot; 或 ``，不存在nil(null)值)var e error // e = nil, error是Go的内建接口类型。 关键字的顺序错误或缺少都是编译错误的 123var int a // 编译错误a int // 编译错误int a // 编译错误 var 语句可以声明一个变量列表，类型在变量名之后 12345678910var a,b,c int // a = 0, b = 0, c = 0var ( a int // a = 0 b string // b = &quot;&quot; c uint // c = 0)var ( a,b,c int d string) 变量定义时初始化赋值，每个变量对应一个值 12var a int = 0var a, b int = 0, 1 变量定义并初始化时可以省略类型，Go自动根据初始值推导变量的类型 12var a = 'A' // a int32var a,b = 0, &quot;B&quot; // a int, b string 使用组合符号:=定义并初始化变量，根据符号右边表达式的值的类型声明变量并初始化它的值 := 不能在函数外使用，函数外的每个语法块都必须以关键字开始 12345a := 3 // a inta, b, c := 8, '呴', true // a int, b int32, c boolc := `formatted string` // c stringc := 1 + 2i // c complex128 常量 Constant 常量可以是字符、字符串、布尔或数值类型的值，数值常量是高精度的值 12345678910const x int = 3const y,z int = 1,2const ( a byte = 'A' b string = &quot;B&quot; c bool = true d int = 4 e float32 = 5.1 f complex64 = 6 + 6i) 根据常量值自动推导类型 12345const a = 0 // a intconst ( b = 2.3 // b float64 c = true // c bool) 常量组内定义时复用表达式 常量组内定义的常量只有名称时，其值会根据上一次最后出现的常量表达式计算相同的类型与值 12345678910const ( a = 3 // a = 3 b // b = 3 c // c = 3 d = len(&quot;asdf&quot;) // d = 4 e // e = 4 f // f = 4 g,h,i = 7,8,9 // 复用表达式要一一对应 x,y,z // x = 7, y = 8, z = 9) 自动递增枚举常量 iota iota的枚举值可以赋值给数值兼容类型 每个常量单独声明时，iota不会自动递增 1234const a int = iota // a = 0const b int = iota // b = 0const c byte = iota // c = 0const d uint64 = iota // d = 0 常量组合声明时，iota每次引用会逐步自增，初始值为0，步进值为1 1234567const ( a uint8 = iota // a = 0 b int16 = iota // b = 1 c rune = iota // c = 2 d float64 = iota // d = 3 e uintptr = iota // e = 4) 即使iota不是在常量组内第一个开始引用，也会按组内常量数量递增 1234567const ( a = &quot;A&quot; b = 'B' c = iota // c = 2 d = &quot;D&quot; e = iota // e = 4) 枚举的常量都为同一类型时，可以使用简单序列格式(组内复用表达式). 12345const ( a = iota // a int32 = 0 b // b int32 = 1 c // c int32 = 2) 枚举序列中的未指定类型的常量会跟随序列前面最后一次出现类型定义的类型 12345678const ( a byte = iota // a uint8 = 0 b // b uint8 = 1 c // c uint8 = 2 d rune = iota // d int32 = 3 e // e int32 = 4 f // f int32 = 5) iota自增值只在一个常量定义组合中有效，跳出常量组合定义后iota初始值归0 123456789const ( a = iota // a int32 = 0 b // b int32 = 1 c // c int32 = 2)const ( e = iota // e int32 = 0 (iota重新初始化并自增) f // f int32 = 1) 定制iota序列初始值与步进值 (通过组合内复用表达式实现) 123456const ( a = (iota + 2) * 3 // a int32 = 6 (a=(0+2)*3) 初始值为6,步进值为3 b // b int32 = 9 (b=(1+2)*3) c // c int32 = 12 (c=(2+2)*3) d // d int32 = 15 (d=(3+2)*3)) 数组 Array 数组声明带有长度信息且长度固定，数组是值类型默认零值不是nil，传递参数时会进行复制。 声明定义数组时中括号[ ]在类型名称之前，赋值引用元素时中括号[ ]在数组变量名之后。 1234567var a [3]int = [3]int{0, 1, 2} // a = [0 1 2]var b [3]int = [3]int{} // b = [0 0 0]var c [3]intc = [3]int{}c = [3]int{0,0,0} // c = [0 0 0]d := [3]int{} // d = [0 0 0]fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, d, d, len(d), cap(d)) // [3]int [3]int{0, 0, 0} 3 3 使用...自动计算数组的长度 12345678var a = [...]int{0, 1, 2}// 多维数组只能自动计算最外围数组长度x := [...][3]int{{0, 1, 2}, {3, 4, 5}}y := [...][2][2]int{{{0,1},{2,3}},{{4,5},{6,7}}}// 通过下标访问数组元素println(y[1][1][0]) // 6 初始化指定索引的数组元素，未指定初始化的元素保持默认零值 12var a = [3]int{2:3}var b = [...]string{2:&quot;c&quot;, 3:&quot;d&quot;} 切片 Slice slice 切片是对一个数组上的连续一段的引用，并且同时包含了长度和容量信息 因为是引用类型，所以未初始化时的默认零值是nil，长度与容量都是0 12345678var a []intfmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, a, a, len(a), cap(a)) // []int []int(nil) 0 0// 可用类似数组的方式初始化slicevar d []int = []int{0, 1, 2}fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, d, d, len(d), cap(d)) // []int []int{0, 1, 2} 3 3var e = []string{2:&quot;c&quot;, 3:&quot;d&quot;} 使用内置函数make初始化slice，第一参数是slice类型，第二参数是长度，第三参数是容量(省略时与长度相同) 12345678var b = make([]int, 0)fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, b, b, len(b), cap(b)) // []int []int{} 0 0var c = make([]int, 3, 10)fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, c, c, len(c), cap(c)) // []int []int{} 3 10var a = new([]int)fmt.Printf(&quot;%T\\t%#v\\t%d\\t%d\\n&quot;, a, a, len(*a), cap(*a)) // *[]int &amp;[]int(nil) 0 0 基于slice或数组重新切片，创建一个新的 slice 值指向相同的数组 重新切片支持两种格式： 2个参数 slice[beginIndex:endIndex] 需要满足条件：0 &lt;= beginIndex &lt;= endIndex &lt;= cap(slice) 截取从开始索引到结束索引-1 之间的片段 新slice的长度：length=(endIndex - beginIndex) 新slice的容量：capacity=(cap(slice) - beginIndex) beginIndex的值可省略，默认为0 endIndex 的值可省略，默认为len(slice) 123456s := []int{0, 1, 2, 3, 4}a := s[1:3] // a: [1 2], len: 2, cap: 4b := s[:4] // b: [0 1 2 3], len: 4, cap: 5c := s[1:] // c: [1 2 3 4], len: 4, cap: 4d := s[1:1] // d: [], len: 0, cap: 4e := s[:] // e: [0 1 2 3 4], len: 5, cap: 5 3个参数 slice[beginIndex:endIndex:capIndex] Go1.2+ 需要满足条件：0 &lt;= beginIndex &lt;= endIndex &lt;= capIndex &lt;= cap(slice) 新slice的长度：length=(endIndex - beginIndex) 新slice的容量：capacity=(capIndex - beginIndex) beginIndex的值可省略，默认为0 123s := make([]int, 5, 10)a := s[9:10:10] // a: [0], len: 1, cap: 1b := s[:3:5] // b: [0 0 0], len: 3, cap: 5 向slice中追加/修改元素 123456789s := []string{}s = append(s, &quot;a&quot;) // 添加一个元素s = append(s, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) // 添加一列元素t = []string{&quot;e&quot;, &quot;f&quot;, &quot;g&quot;}s = append(s, t...} // 添加另一个切片t的所有元素s = append(s, t[:2]...} // 添加另一个切片t的部分元素s[0] = &quot;A&quot; // 修改切片s的第一个元素s[len(s)-1] = &quot;G&quot; // 修改切片s的最后一个元素 向slice指定位置插入元素，从slice中删除指定的元素 因为slice引用指向底层数组，数组的长度不变元素是不能插入/删除的 插入的原理就是从插入的位置将切片分为两部分依次将首部、新元素、尾部拼接为一个新的切片 删除的原理就是排除待删除元素后用其他元素重新构造一个数组 12345678910111213141516171819202122func insertSlice(s []int, i int, elements ...int) []int { // x := append(s[:i], append(elements, s[i:]...)...) x := s[:i] x = append(x, elements...) x = append(x, s[i:]...) return x}func deleteByAppend() { i := 3 s := []int{1, 2, 3, 4, 5, 6, 7} // delete the fourth element(index is 3), using append s = append(s[:i], s[i+1:]...)}func deleteByCopy() { i := 3 s := []int{1, 2, 3, 4, 5, 6, 7} // delete the fourth element(index is 3), using copy copy(s[i:], s[i+1:]) s = s[:len(s)-1]} 字典/映射 Map map是引用类型，使用内置函数 make进行初始化，未初始化的map零值为 nil长度为0，并且不能赋值元素 1234567var m map[int]intm[0] = 0 // × runtime error: assignment to entry in nil mapfmt.Printf(&quot;type: %T\\n&quot;, m) // map[int]intfmt.Printf(&quot;value: %#v\\n&quot;, m) // map[int]int(nil)fmt.Printf(&quot;value: %v\\n&quot;, m) // map[]fmt.Println(&quot;is nil: &quot;, nil == m) // truefmt.Println(&quot;length: &quot;, len(m)) // 0，if m is nil, len(m) is zero. 使用内置函数make初始化map 1234567var m map[int]int = make(map[int]int)m[0] = 0 // 插入或修改元素fmt.Printf(&quot;type: %T\\n&quot;, m) // map[int]intfmt.Printf(&quot;value: %#v\\n&quot;, m) // map[int]int(0:0)fmt.Printf(&quot;value: %v\\n&quot;, m) // map[0:0]fmt.Println(&quot;is nil: &quot;, nil == m) // falsefmt.Println(&quot;length: &quot;, len(m)) // 1 直接赋值初始化map 12345678m := map[int]int{0:0,1:1, // 最后的逗号是必须的}n := map[string]S{&quot;a&quot;:S{0,1},&quot;b&quot;:{2,3}, // 类型名称可省略} map的使用：读取、添加、修改、删除元素 1234567m[0] = 3 // 修改m中key为0的值为3m[4] = 8 // 添加到m中key为4值为8a := n[&quot;a&quot;] // 获取n中key为“a“的值b, ok := n[&quot;c&quot;] // 取值, 并通过ok(bool)判断key对应的元素是否存在.delete(n, &quot;a&quot;) // 使用内置函数delete删除key为”a“对应的元素. 结构体 Struct 结构体类型struct是一个字段的集合 1234type S struct { A int B, c string} 结构体初始化通过结构体字段的值作为列表来新分配一个结构体。 1var s S = S{0, &quot;1&quot;, &quot;2&quot;} 使用 Name: 语法可以仅列出部分字段(字段名的顺序无关) 1var s S = S{B: &quot;1&quot;, A: 0} 结构体是值类型，传递时会复制值，其默认零值不是nil 123var a Svar b = S{}fmt.Println(a == b) // true 结构体组合 将一个命名类型作为匿名字段嵌入一个结构体 嵌入匿名字段支持命名类型、命名类型的指针和接口类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package maintype ( A struct { v int } // 定义结构体B，嵌入结构体A作为匿名字段 B struct { A } // 定义结构体C，嵌入结构体A的指针作为匿名字段 C struct { *A })func (a *A) setV(v int) { a.v = v}func (a A) getV() int { return a.v}func (b B) getV() string { return &quot;B&quot;}func (c *C) getV() bool { return true}func main() { a := A{} b := B{} // 初始化结构体B，其内匿名字段A默认零值是A{} c := C{&amp;A{}} // 初始化结构体C，其内匿名指针字段*A默认零值是nil，需要初始化赋值 println(a.v) // 结构体A嵌入B，A内字段自动提升到B println(b.v) // 结构体指针*A嵌入C，*A对应结构体内字段自动提升到C println(c.v) a.setV(3) b.setV(5) c.setV(7) println(a.getV(), b.A.getV(), c.A.getV()) println(a.getV(), b.getV(), c.getV())} 匿名结构体 匿名结构体声明时省略了type关键字，并且没有名称 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mainimport &quot;fmt&quot;type Integer int// 声明变量a为空的匿名结构体类型var a struct{}// 声明变量b为包含一个字段的匿名结构体类型var b struct{ x int }// 声明变量c为包含两个字段的匿名结构体类型var c struct { u int v bool}func main() { printa(a) b.x = 1 fmt.Printf(&quot;bx: %#v\\n&quot;, printb(b)) // bx: struct { y uint8 }{y:0x19} printc(c) // 声明d为包含3个字段的匿名结构体并初始化部分字段 d := struct { x int y complex64 z string }{ z: &quot;asdf&quot;, x: 111, } d.y = 22 + 333i fmt.Printf(&quot;d: %#v\\n&quot;, d) // d: struct { x int; y complex64; z string }{x:111, y:(22+333i), z:&quot;asdf&quot;} // 声明变量e为包含两个字段的匿名结构体类型 // 包含1个匿名结构体类型的命名字段和1个命名类型的匿名字段 e := struct { a struct{ x int } // 结构体组合嵌入匿名字段只支持命名类型 Integer }{} e.Integer = 444 fmt.Printf(&quot;e: %#v\\n&quot;, e) // e: struct { a struct { x int }; main.Integer }{a:struct { x int }{x:0}, Integer:444}}// 函数参数为匿名结构体类型时，传入参数类型声明必须保持一致func printa(s struct{}) { fmt.Printf(&quot;a: %#v\\n&quot;, s) // a: struct {}{}}// 函数入参和返回值都支持匿名结构体类型func printb(s struct{ x int }) (x struct{ y byte }) { fmt.Printf(&quot;b: %#v\\n&quot;, s) // b: struct { x int }{x:1} x.y = 25 return}func printc(s struct {u int; v bool }) { fmt.Printf(&quot;c: %#v\\n&quot;, s) // c: struct { u int; v bool }{u:0, v:false}} 指针 Pointer 通过取地址操作符&amp;获取指向值/引用对象的指针。 1234567var i int = 1pi := &amp;i // 指向数值的指针a := []int{0, 1, 2}pa := &amp;a // 指向引用对象的指针var s *S = &amp;S{0, &quot;1&quot;, &quot;2&quot;} // 指向值对象的指针 内置函数new(T)分配了一个零初始化的 T 值，并返回指向它的指针 12var i = new(int)var s *S = new(S) 使用*读取/修改指针指向的值 12345678func main() { i := new(int) *i = 3 println(i, *i) // 0xc208031f80 3 i = new(int) println(i, *i) // 0xc208031f78 0} 指针使用点号来访问结构体字段 结构体字段/方法可以通过结构体指针来访问，通过指针间接的访问是透明的。 12fmt.Println(s.A)fmt.Println((*s).A) 指针的指针 123456789101112func main() { var i int var p *int var pp **int var ppp ***int var pppp ****int println(i, p, pp, ppp, pppp) // 0 0x0 0x0 0x0 0x0 i, p, pp, ppp, pppp = 123, &amp;i, &amp;p, &amp;pp, &amp;ppp println(i, p, pp, ppp, pppp) // 123 0xc208031f68 0xc208031f88 0xc208031f80 0xc208031f78 println(i, *p, **pp, ***ppp, ****pppp) // 123 123 123 123 123} 跨层指针元素的使用 在指针引用多层对象时，指针是针对引用表达式的最后一位元素。 1234567891011package atype X struct { A Y}type Y struct { B Z}type Z struct { C int} 12345678910111213141516package mainimport ( &quot;a&quot; &quot;fmt&quot;)func main() { var x = a.X{} var p = &amp;x fmt.Println(&quot;x: &quot;, x) // x: {{{0}}} println(&quot;p: &quot;, p) // p: 0xc208055f20 fmt.Println(&quot;*p: &quot;, *p) // *p: {{{0}}} println(&quot;x.A.B.C: &quot;, x.A.B.C) // x.A.B.C: 0 // println(&quot;*p.A.B.C: &quot;, *p.A.B.C) // invalid indirect of p.A.B.C (type int) println(&quot;(*p).A.B.C: &quot;, (*p).A.B.C) // (*p).A.B.C: 0} Go的指针没有指针运算，但是 道高一尺，魔高一丈 Go语言中的指针运算 利用unsafe操作未导出变量 通道 Channel channel用于两个goroutine之间传递指定类型的值来同步运行和通讯。 操作符&lt;-用于指定channel的方向，发送或接收。 如果未指定方向，则为双向channel。 123var c0 chan int // 可用来发送和接收int类型的值var c1 chan&lt;- int // 可用来发送int类型的值var c2 &lt;-chan int // 可用来接收int类型的值 channel是引用类型，使用make函数来初始化。 未初始化的channel零值是nil，且不能用于发送和接收值。 12c0 := make(chan int) // 不带缓冲的int类型channelc1 := make(chan *int, 10) // 带缓冲的*int类型指针channel 无缓冲的channe中有值时发送方会阻塞，直到接收方从channel中取出值。 带缓冲的channel在缓冲区已满时发送方会阻塞，直到接收方从channel中取出值。 接收方在channel中无值会一直阻塞。 通过channel发送一个值时，&lt;-作为二元操作符使用， 1c0 &lt;- 3 通过channel接收一个值时，&lt;-作为一元操作符使用。 1i := &lt;-c1 关闭channel，只能用于双向或只发送类型的channel 只能由 发送方调用close函数来关闭channel 接收方取出已关闭的channel中发送的值后，后续再从channel中取值时会以非阻塞的方式立即返回channel传递类型的零值。 12345678910111213141516171819202122232425ch := make(chan string, 1)// 发送方，发送值后关闭channelch &lt;- &quot;hello&quot;close(ch)// 接收方，取出发送的值fmt.Println(&lt;-ch) // 输出： “hello”// 再次从已关闭的channel中取值，返回channel传递类型的零值fmt.Println(&lt;-ch) // 输出： 零值，空字符串“”// 接收方判断接收到的零值是由发送方发送的还是关闭channel返回的默认值s, ok := &lt;-chif ok { fmt.Println(&quot;Receive value from sender:&quot;, s)} else { fmt.Println(&quot;Get zero value from closed channel&quot;)}// 向已关闭的通道发送值会产生运行时恐慌panicch &lt;- &quot;hi&quot;// 再次关闭已经关闭的通道也会产生运行时恐慌panicclose(ch) 使用for range语句依次读取发送到channel的值，直到channel关闭。 12345678910111213141516171819package mainimport &quot;fmt&quot;func main() { // 无缓冲和有缓冲的channel的range用法相同 var ch = make(chan int) // make(chan int, 2) 或 make(chan int , 100) go func() { for i := 0; i &lt; 5; i++ { ch &lt;- i } close(ch) }() // channel中无发送值且未关闭时会阻塞 for x := range ch { fmt.Println(x) }} 下面方式与for range用法效果相同 12345678910loop: for { select { case x, ok := &lt;-c: if !ok { break loop } fmt.Println(x) } } 接口 Interface 接口类型是由一组方法定义的集合。 接口类型的值可以存放实现这些方法的任何值。 123type Abser interface { Abs() float64} 类型通过实现定义的方法来实现接口， 不需要显式声明实现某接口。 12345678type MyFloat float64func (f MyFloat) Abs() float64 { if f &lt; 0 { return float64(-f) } return float64(f)} 接口组合 12345678910111213141516171819202122232425262728293031323334353637383940type Reader interface { Read(b []byte) (n int)}type Writer interface { Write(b []byte) (n int)}// 接口ReadWriter组合了Reader和Writer两个接口type ReadWriter interface { Reader Writer}type File struct { // ...}func (f *File) Read(b []byte) (n int) { println(&quot;Read&quot;, len(b),&quot;bytes data.&quot;) return len(b)}func (f *File) Write(b []byte) (n int) { println(&quot;Write&quot;, len(b),&quot;bytes data.&quot;) return len(b)}func main() { // *File 实现了Read方法和Write方法，所以实现了Reader接口和Writer接口以及组合接口ReadWriter var f *File = &amp;File{} var r Reader = f var w Writer = f var rw ReadWriter = f bs := []byte(&quot;asdf&quot;) r.Read(bs) rw.Read(bs) w.Write(bs) rw.Write(bs)} 内置接口类型error是一个用于表示错误情况的常规接口，其零值nil表示没有错误 所有实现了Error方法的类型都能表示为一个错误 123type error interface { Error() string} 自定义类型 Go中支持自定义的类型可基于： 基本类型、数组类型、切片类型、字典类型、函数类型、结构体类型、通道类型、接口类型以及自定义类型的类型 12345678910111213141516171819202122232425262728type ( A int B int8 C int16 D rune E int32 F int64 G uint H byte I uint16 J uint32 K uint64 L float32 M float64 N complex64 O complex128 P uintptr Q bool R string S [3]uint8 T []complex128 U map[string]uintptr V func(i int) (b bool) W struct {a, b int} X chan int Y interface {} Z A) 以及支持以上所有支持类型的指针类型 12345678910111213141516171819202122232425262728type ( A *int B *int8 C *int16 D *rune E *int32 F *int64 G *uint H *byte I *uint16 J *uint32 K *uint64 L *float32 M *float64 N *complex64 O *complex128 P *uintptr Q *bool R *string S *[3]uint8 T *[]complex128 U *map[string]uintptr V *func(i int) (b bool) W *struct {a, b int} X *chan int Y *interface {} Z *A) 类型别名 Go1.9+ 1234567891011121314151617181920type ( A struct{} B struct{} // 定义两个结构相同的类型A，B C = A // 定义类型A的别名)func main() { var ( a A b B c C ) // 因为类型名不同，所以a和b不是相同类型，此处编译错误 fmt.Println(a == b) // invalid operation: a == b (mismatched types A and B) fmt.Println(a == c) // true a = C{} c = A{} fmt.Println(c == a) // true} 强制类型转换 数据类型转换语法规则 12// T 为新的数据类型newDataTypeVariable = T(oldDataTypeVariable) 数值类型转换 123var i int = 123var f = float64(i)var u = uint(f) 接口类型转换 任意类型的数据都可以转换为其类型已实现的接口类型 1234567type I interface{}var x int = 123var y = I(x) var s struct{a string}var t = I(s) 结构体类型转换 Go1.8+ 如果两个结构体包含的所有字段的名称和类型相同(忽略字段的标签差异)，则可以互相强制转换类型。 12345678910type T1 struct { X int `json:&quot;foo&quot;`}type T2 struct { X int `json:&quot;bar&quot;`}var v1 = T1{X: 123}var v2 = T2(v1) 语句 Statement 分号/括号 ; { Go是采用语法解析器自动在每行末尾增加分号，所以在写代码的时候可以省略分号。 Go编程中只有几个地方需要手工增加分号： for循环使用分号把初始化、条件和遍历元素分开。 if/switch的条件判断带有初始化语句时使用分号分开初始化语句与判断语句。 在一行中有多条语句时，需要增加分号。 控制语句(if，for，switch，select)、函数、方法 的左大括号不能单独放在一行， 语法解析器会在大括号之前自动插入一个分号，导致编译错误。 条件语句 if if语句 小括号 ( )是可选的，而大括号 { } 是必须的。 123456789101112131415161718192021if (i &lt; 0) // 编译错误. println(i)if i &lt; 0 // 编译错误. println(i)if (i &lt; 0) { // 编译通过. println(i)}if (i &lt; 0 || i &gt; 10) { println(i)}if i &lt; 0 { println(i)} else if i &gt; 5 &amp;&amp; i &lt;= 10 { println(i)} else { println(i)} 可以在条件之前执行一个简单的语句，由这个语句定义的变量的作用域仅在 if / else if / else 范围之内 123456789101112131415if (i := 0; i &lt; 1) { // 编译错误. println(i)}if i := 0; (i &lt; 1) { // 编译通过. println(i)}if i := 0; i &lt; 0 { // 使用gofmt格式化代码会自动移除代码中不必要的小括号( ) println(i)} else if i == 0 { println(i)} else { println(i)} if语句作用域范围内定义的变量会覆盖外部同名变量，与方法函数内局部变量覆盖全局变量同理 12345a, b := 0, 1if a, b := 3, 4; a &gt; 1 &amp;&amp; b &gt; 2 { println(a, b) // 3 4}println(a, b) // 0 1 if判断语句类型断言 12345678910111213141516171819202122232425262728package mainfunc f0() int {return 333}func main() { x := 9 checkType(x) checkType(f0)}func checkType(x interface{}) { // 断言传入的x为int类型，并获取值 if i, ok := x.(int); ok { println(&quot;int: &quot;, i) // int: 0 } if f, ok := x.(func() int); ok { println(&quot;func: &quot;, f()) // func: 333 } // 如果传入x类型为int，则可以直接获取其值 a := x.(int) println(a) // 如果传入x类型不是byte，则会产生恐慌panic b := x.(byte) println(b)} 分支选择 switch switch存在分支选择对象时，case分支支持单个常量、常量列表 12345678switch x {case 0: println(&quot;single const&quot;)case 1, 2, 3: println(&quot;const list&quot;)default: println(&quot;default&quot;)} 分支选择对象之前可以有一个简单语句，case语句的大括号可以省略 1234567891011switch x *= 2; x {case 4: { println(&quot;single const&quot;)}case 5, 6, 7: { println(&quot;const list&quot;)}default: { println(&quot;default&quot;)}} switch只有一个简单语句，没有分支选择对象时，case分支支持逻辑表达式语句 12345678switch x /= 3; {case x == 8: println(&quot;expression&quot;)case x &gt;= 9: println(&quot;expression&quot;)default: println(&quot;default&quot;)} switch没有简单语句，没有分支选择对象时，case分支支持逻辑表达式语句 12345678switch {case x == 10: println(&quot;expression&quot;)case x &gt;= 11: println(&quot;expression&quot;)default: println(&quot;default&quot;)} switch类型分支，只能在switch语句中使用的.(type)获取对象的类型。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package mainimport ( &quot;fmt&quot; &quot;code.google.com/p/go.crypto/openpgp/errors&quot;)func main() { var ( a = 0.1 b = 2+3i c = &quot;asdf&quot; d = [...]byte{1, 2, 3} e = []complex128{1+2i} f = map[string]uintptr{&quot;a&quot;: 0} g = func(int) bool {return true} h = struct { a, b int }{} i = &amp;struct {}{} j chan int k chan &lt;- bool l &lt;-chan string m errors.SignatureError ) values := []interface{}{nil, a, b, &amp;c, d, e, f, g, &amp;g, h, &amp;h, i, j, k, l, m} for _, v := range values { typeswitch(v) }}func typeswitch(x interface{}) { // switch x.(type) { // 不使用类型值时 switch i := x.(type) { case nil: fmt.Println(&quot;x is nil&quot;) case int, int8, int16, rune, int64, uint, byte, uint16, uint32, uint64, float32, float64, complex64, complex128, uintptr, bool, string: fmt.Printf(&quot;basic type : %T\\n&quot;, i) case *int, *int8, *int16, *rune, *int64, *uint, *byte, *uint16, *uint32, *uint64, *float32, *float64, *complex64, *complex128, *uintptr, *bool, *string: fmt.Printf(&quot;basic pointer type : %T\\n&quot;, i) case [3]byte, []complex128, map[string]uintptr: fmt.Printf(&quot;collection type : %T\\n&quot;, i) case func(i int) (b bool), *func(): fmt.Printf(&quot;function type : %T\\n&quot;, i) case struct {a, b int}, *struct {}: fmt.Printf(&quot;struct type : %T\\n&quot;, i) case chan int, chan &lt;- bool, &lt;-chan string: fmt.Printf(&quot;channel type : %T\\n&quot;, i) case error, interface{a(); b()}: fmt.Printf(&quot;interface type : %T\\n&quot;, i) default: fmt.Printf(&quot;other type : %T\\n&quot;, i) }}// output: // x is nil// basic type : float64// basic type : complex128// basic pointer type : *string// collection type : [3]uint8// collection type : []complex128// collection type : map[string]uintptr// function type : func(int) bool// other type : *func(int) bool// struct type : struct { a int; b int }// other type : *struct { a int; b int }// struct type : *struct {}// channel type : chan int// channel type : chan&lt;- bool// channel type : &lt;-chan string// interface type : errors.SignatureError switch中每个case分支默认带有break效果，一个分支执行后就跳出switch，不会自动向下执行其他case。 使用fallthrough强制向下继续执行后面的case代码。 在类型分支中不允许使用fallthrough语句 12345678910111213141516171819switch {case false: println(&quot;case 1&quot;) fallthroughcase true: println(&quot;case 2&quot;) fallthroughcase false: println(&quot;case 3&quot;) fallthroughcase true: println(&quot;case 4&quot;)case false: println(&quot;case 5&quot;) fallthroughdefault: println(&quot;default case&quot;)}// 输出：case 2 case 3 case 4 循环语句 for Go只有一种循环结构：for 循环。 可以让前置(初始化)、中间(条件)、后置(迭代)语句为空，或者全为空。 12345678910for i := 0; i &lt; 10; i++ {...}for i := 0; i &lt; 10; {...} // 省略迭代语句for i := 0; ; i++; {...} // 省略条件语句for ; i &lt; 10; i++ {...} // 省略初始化语句for i := 0; ; {...} // 省略条件和迭代语句, 分号不能省略for ; i &lt; 10; {...} // 省略初始化和迭代语句, 分号可省略for ; ; i++ {...} // 省略初始化和条件语句, 分号不能省略for i &lt; 10 {...}for ; ; {...} // 分号可省略for {...} for语句中小括号 ( )是可选的，而大括号 { } 是必须的。 123for (i := 0; i &lt; 10; i++) {...} // 编译错误.for i := 0; (i &lt; 10); i++ {...} // 编译通过.for (i &lt; 10) {...} // 编译通过. Go的for each循环for range 12345678910111213a := [5]int{2, 3, 4, 5, 6}for k, v := range a { fmt.Println(k, v) // 输出：0 2, 1 3, 2 4, 3 5, 4 6}for k := range a { fmt.Println(k) // 输出：0 1 2 3 4}for _ = range a { fmt.Println(&quot;print without care about the key and value&quot;)} Go1.4+ 123for range a { fmt.Println(&quot;new syntax – print without care about the key and value&quot;)} 循环的继续、中断、跳转 123456789for k, v := range s { if v == 3 { continue // 结束本次循环，进入下一次循环中 } else if v == 5 { break // 结束整个for循环 } else { goto SOMEWHERE // 跳转到标签指定的代码处 }} for range只支持遍历数组、数组指针、slice、string、map、channel类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package mainimport &quot;fmt&quot;func main() { var arr = [...]int{33, 22, 11, 0} // 遍历数组，取一位值时为索引值 for k := range arr { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, } fmt.Println() // 遍历数组，取两位值时，第一位为索引值，第二位为元素值 for k, v := range arr { fmt.Printf(&quot;%d %d, &quot;, k, v) // 0 33, 1 22, 2 11, 3 0, } fmt.Println() // 遍历数组指针，取一位值时为索引值 for k := range &amp;arr { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, } fmt.Println() // 遍历数组指针，取两位值时，第一位为索引值，第二位为元素值 for k, v := range &amp;arr { fmt.Printf(&quot;%d %d, &quot;, k, v) // 0 33, 1 22, 2 11, 3 0, } fmt.Println() var slc = []byte{44, 55, 66, 77} // 遍历切片，取一位值时为索引值 for k := range slc { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, } fmt.Println() // 遍历切片，取两位值时，第一位为索引值，第二位为元素值 for k, v := range slc { fmt.Printf(&quot;%d %d, &quot;, k, v) // 0 44, 1 55, 2 66, 3 77, } fmt.Println() var str = &quot;abc一二3&quot; // 遍历字符串，取一位值时为字节索引值 for k := range str { fmt.Printf(&quot;%d, &quot;, k) // 0, 1, 2, 3, 6, 9, } fmt.Println() // 遍历字符串，取两位值时，第一位为字节索引值，第二位为Unicode字符 for k, v := range str { fmt.Printf(&quot;%d %d %s, &quot;, k, v, string(v)) // 0 97 a, 1 98 b, 2 99 c, 3 19968 一, 6 20108 二, 9 51 3, } fmt.Println() var mp = map[int]string{5:&quot;A&quot;, 9:&quot;B&quot;} // 遍历map，取一位值时为键key for k := range mp { fmt.Printf(&quot;%d, &quot;, k) // 9, 5, } fmt.Println() // 遍历map，取两位值时，第一位为键key，第二位为元素值value for k, v := range mp { fmt.Printf(&quot;%d %s, &quot;, k, v) // 5 A, 9 B, } fmt.Println() var ch = make(chan int) go func() { for i := 0; i &lt; 5; i++ { ch &lt;- i } close(ch) }() // 遍历channel时，只能取一位值，为发送方发送到channel中的值 for x := range ch { fmt.Printf(&quot;%d &quot;, x) // 0 1 2 3 4 }} 通道选择 select select用于当前goroutine从一组可能的通讯中选择一个进一步处理。 如果任意一个通讯都可以进一步处理，则从中随机选择一个，执行对应的语句。否则在没有默认分支(default case)时，select语句则会阻塞，直到其中一个通讯完成。 select 的 case 里的操作语句只能是IO操作 123456789ch1, ch2 := make(chan int), make(chan int)// 因为没有值发送到select中的任一case的channel中，此select将会阻塞select {case &lt;-ch1: println(&quot;channel 1&quot;)case &lt;-ch2: println(&quot;channel 2&quot;)} 1234567891011ch1, ch2 := make(chan int), make(chan int)// 因为没有值发送到select中的任一case的channel中，此select将会执行default分支select {case &lt;-ch1: println(&quot;channel 1&quot;)case &lt;-ch2: println(&quot;channel 2&quot;)default: println(&quot;default&quot;)} select只会执行一次case分支的逻辑，与for组合使用实现多次遍历分支 12345678910111213func main() { for { select { case &lt;-time.Tick(time.Second): println(&quot;Tick&quot;) case &lt;-time.After(5 * time.Second): println(&quot;Finish&quot;) default: println(&quot;default&quot;) time.Sleep(5e8) } }} 延迟执行 defer defer语句调用函数，将调用的函数加入defer栈，栈中函数在defer所在的主函数返回时执行，执行顺序是先进后出/后进先出。 1234567891011121314package mainfunc main() { defer print(0) defer print(1) defer print(2) defer print(3) defer print(4) for i := 5; i &lt;= 9; i++ { defer print(i) } // 输出：9876543210} defer在函数返回后执行，可以修改函数返回值 123456789101112package mainfunc main() { println(f()) // 返回： 15}func f() (i int) { defer func() { i *= 5 }() return 3} defer用于释放资源 释放锁 12mu.Lock()defer mu.Unlock() 关闭channel 12ch &lt;- &quot;hello&quot;defer close(ch) 关闭IO流 12f, err := os.Open(&quot;file.xxx&quot;)defer f.Close() 关闭数据库连接 12345db, err := sql.Open(&quot;mysql&quot;,&quot;user:password@tcp(127.0.0.1:3306)/hello&quot;)if err != nil { log.Fatal(err)}defer db.Close() defer用于恐慌的截获 panic用于产生恐慌，recover用于截获恐慌，recover只能在defer语句中使用, 直接调用recover是无效的。 123456789101112131415161718func main() { f() fmt.Println(&quot;main normal...&quot;)}func f() { defer func() { if r := recover(); r != nil { fmt.Println(&quot;catch:&quot;, r) } }() p() fmt.Println(&quot;normal...&quot;)}func p() { panic(&quot;exception...&quot;)} 跳转语句 goto goto用于在一个函数内部运行跳转到指定标签的代码处，不能跳转到其他函数中定义的标签。 goto模拟循环 1234567891011package mainfunc main() { i := 0loop: i++ if i &lt; 5 { goto loop } println(i)} goto模拟continue，break 12345678910111213141516func main() { i, sum := 0, 0head: for ; i &lt;= 10; i++ { if i &lt; 5 { i++ // 此处必须单独调用一次，因为goto跳转时不会执行for循环的自增语句 goto head // continue } if i &gt; 9 { goto tail // break } sum += i }tail: println(sum) // 输出：35} 注意：任何时候都不建议使用goto 阻塞语句 永久阻塞语句 12345678// 向一个未初始化的channel中写入数据会永久阻塞(chan int)(nil) &lt;- 0// 从一个未初始化的channel中读取数据会永久阻塞&lt;-(chan struct{})(nil)for range (chan struct{})(nil){}// select无任何选择分支会永久阻塞select{} 函数 Function 函数声明 Declare 使用关键字func声明函数，函数可以没有参数或接受多个参数 12345func f0() {/*...*/}func f1(a int) {/*...*/}func f2(a int, b byte) {/*...*/} 在函数参数类型之前使用...声明该参数为可变数量的参数 可变参数只能声明为函数的最后一个参数。 123func f3(a ...int) {/*...*/}func f4(a int, b bool, c ...string) {/*...*/} 函数可以返回任意数量的返回值 1234567891011func f0() { return}func f1() int { return 0}func f2() (int, string) { return 0, &quot;A&quot;} 函数返回结果参数，可以像变量那样命名和使用 123456func f() (a int, b string) { a = 1 b = &quot;B&quot; return // 即使return后面没有跟变量，关键字在函数结尾也是必须的 // 或者 return a, b} 当两个或多个连续的函数命名参数是同一类型，则除了最后一个类型之外，其他都可以省略 12345func f0(a,b,c int) {/*...*/}func f1() (a,b,c int) {/*...*/}func f2(a,b int, c,d byte) (x,y int, z,s bool) {/*...*/} 函数闭包 Closure 匿名函数、闭包、函数值 Go中函数作为第一类对象，可以作为值对象赋值给变量 可以在函数体外/内定义匿名函数，命名函数不能嵌套定义到函数体内，只能定义在函数体外 123456789101112131415161718192021222324252627package maintype Myfunc func(i int) intfunc f0(name string){ println(name)}func main() { var a = f0 a(&quot;hello&quot;) // hello var f1 Myfunc = func(i int) int { return i } fmt.Println(f1(3)) // 3 var f2 func() int = func() int { return 0 } fmt.Println(f2()) // 0 // 省略部分关键字 var f3 func() = func() {/*...*/} var f4 = func() {/*...*/} f5 := func() {/*...*/}} 内建函数 Builtin func append 1func append(slice []Type, elems ...Type) []Type 内建函数append将元素追加到切片的末尾。若它有足够的容量，其目标就会重新切片以容纳新的元素。否则，就会分配一个新的基本数组。append返回更新后的切片，因此必须存储追加后的结果。 12slice = append(slice, elem1, elem2)slice = append(slice, anotherSlice...) 作为特例，可以向一个字节切片append字符串，如下： 1slice = append([]byte(&quot;hello &quot;), &quot;world&quot;...) func cap 1func cap(v Type) int 内建函数cap返回 v 的容量，这取决于具体类型： 数组：v中元素的数量，与 len(v) 相同 数组指针：*v中元素的数量，与len(v) 相同 切片：切片的容量（底层数组的长度）；若 v为nil，cap(v) 即为零 信道：按照元素的单元，相应信道缓存的容量；若v为nil，cap(v)即为零 func close 1func close(c chan&lt;- Type) 内建函数close关闭信道，该通道必须为双向的或只发送的。它应当只由发送者执行，而不应由接收者执行，其效果是在最后发送的值被接收后停止该通道。在最后的值从已关闭的信道中被接收后，任何对其的接收操作都会无阻塞的成功。对于已关闭的信道，语句： 1x, ok := &lt;-c // ok值为false func complex 1func complex(r, i FloatType) ComplexType 使用实部r和虚部i生成一个复数。 12c := complex(1, 2)fmt.Println(c) // (1+2i) func copy 1func copy(dst, src []Type) int 内建函数copy将元素从来源切片复制到目标切片中，也能将字节从字符串复制到字节切片中。copy返回被复制的元素数量，它会是 len(src) 和 len(dst) 中较小的那个。来源和目标的底层内存可以重叠。 12345678910111213a, b, c := []byte{1, 2, 3}, make([]byte, 2), 0fmt.Println(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot; c: &quot;, c) // a: [1 2 3] b: [0 0] c: 0c = copy(b, a)fmt.Println(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot; c: &quot;, c) // a: [1 2 3] b: [1 2] c: 2b = make([]byte, 5)c = copy(b, a)fmt.Println(&quot;a:&quot;, a, &quot; b:&quot;, b, &quot; c: &quot;, c) // a: [1 2 3] b: [1 2 3 0 0] c: 3s := &quot;ABCD&quot;c = copy(b, s)fmt.Println(&quot;s:&quot;, s, &quot; b:&quot;, b, &quot; c: &quot;, c) // s: ABCD b: [65 66 67 68 0] c: 4 func delete 1func delete(m map[Type]Type1, key Type) 内建函数delete按照指定的键将元素从映射中删除。若m为nil或无此元素，delete不进行操作。 123456789m := map[int]string{ 0: &quot;A&quot;, 1: &quot;B&quot;, 2: &quot;C&quot;,}delete(m, 1)fmt.Println(m) // map[2:C 0:A]delete(m, 3) // 此行代码执行没有任何操作，也不会报错。 func imag 1func imag(c ComplexType) FloatType 返回复数c的虚部。 12c := 2+5ifmt.Println(imag(c)) // 5 func len 1func len(v Type) int 内建函数len返回 v 的长度，这取决于具体类型： 数组：v中元素的数量 数组指针：*v中元素的数量（v为nil时panic） 切片、映射：v中元素的数量；若v为nil，len(v)即为零 字符串：v中字节的数量，计算字符数量使用utf8.RuneCountInString() 通道：通道缓存中队列（未读取）元素的数量；若v为 nil，len(v)即为零 func make 1func make(Type, size IntegerType) Type 内建函数make分配并初始化一个类型为切片、映射、或通道的对象。其第一个实参为类型，而非值。make的返回类型与其参数相同，而非指向它的指针。其具体结果取决于具体的类型： 切片：size指定了其长度。该切片的容量等于其长度。切片支持第二个整数实参可用来指定不同的容量；它必须不小于其长度，因此 make([]int, 0, 10) 会分配一个长度为0，容量为10的切片。 映射：初始分配的创建取决于size，但产生的映射长度为0。size可以省略，这种情况下就会分配一个小的起始大小。 通道：通道的缓存根据指定的缓存容量初始化。若 size为零或被省略，该信道即为无缓存的。 func new 1func new(Type) *Type 内建函数new分配内存。其第一个实参为类型，而非值。其返回值为指向该类型的新分配的零值的指针。 func panic 1func panic(v interface{}) 内建函数panic停止当前Go程的正常执行。当函数F调用panic时，F的正常执行就会立刻停止。F中defer的所有函数先入后出执行后，F返回给其调用者G。G如同F一样行动，层层返回，直到该Go程中所有函数都按相反的顺序停止执行。之后，程序被终止，而错误情况会被报告，包括引发该恐慌的实参值，此终止序列称为恐慌过程。 func print 1func print(args ...Type) 内建函数print以特有的方法格式化参数并将结果写入标准错误，用于自举和调试。 func println 1func println(args ...Type) println类似print，但会在参数输出之间添加空格，输出结束后换行。 func real 1func real(c ComplexType) FloatType 返回复数c的实部。 123456789 c := 2+5i fmt.Println(real(c)) // 2 ``` - `func recover` ```go func recover() interface{} 内建函数recover允许程序管理恐慌过程中的Go程。在defer的函数中，执行recover调用会取回传至panic调用的错误值，恢复正常执行，停止恐慌过程。若recover在defer的函数之外被调用，它将不会停止恐慌过程序列。在此情况下，或当该Go程不在恐慌过程中时，或提供给panic的实参为nil时，recover就会返回nil。 初始化函数 init init函数是用于程序执行前做包的初始化工作的函数 init函数的声明没有参数和返回值 123func init() { // ...} 一个package或go源文件可以包含零个或多个init函数 1234567891011121314package mainfunc main() {}func init() { println(&quot;init1...&quot;)}func init() { println(&quot;init2...&quot;)}func init() { println(&quot;init3...&quot;)} init函数被自动调用，在main函数之前执行，不能在其他函数中调用，显式调用会报错该函数未定义。 1234567func init() { println(&quot;init...&quot;)}func main() { init() // undefined: init} 所有init函数都会被自动调用，调用顺序如下： 同一个go文件的init函数调用顺序是 从上到下的 同一个package中按go源文件名字符串比较 从小到大顺序调用各文件中的init函数 不同的package，如果不相互依赖的，按照main包中 先import的后调用的顺序调用其包中的init函数 如果package存在依赖，则先调用最早被依赖的package中的init函数 方法 Method 通过指定函数的接收者receiver,将函数绑定到一个类型或类型的指针上,使这个函数成为该类型的方法。 只能对命名类型和命名类型的指针编写方法。 只能在定义命名类型的那个包编写其方法。 不能对接口类型和接口类型的指针编写方法。 方法的接收者receiver是类型的值时，编译器会隐式的生成一个同名方法，其接收者receiver为该类型的指针，反过来却不会。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package maintype A struct { x, y int}// 定义结构体的方法，'_'表示方法内忽略使用结构体、字段及其他方法func (_ A) echo_A() { println(&quot;(_ A)&quot;)}// 同上func (A) echoA(s string) { println(&quot;(A)&quot;, s)}// 定义结构体指针的方法，'_'表示方法内忽略使用结构体指针、字段及其他方法func (_ *A) echo_жA() { println(&quot;(_ *A)&quot;)}// 同上func (*A) echoжA(s string) { println(&quot;(*A)&quot;, s)}// 定义结构体的方法，方法内可以引用结构体、字段及其他方法func (a A) setX(x int) { a.x = x}// 定义结构体指针的方法，方法内可以引用结构体、结构体指针、字段及其他方法func (a *A) setY(y int) { a.y = y}func main() { var a A // a = A{} a.setX(3) a.setY(6) println(a.x, a.y) // 0 6 a.echo_A() // (_ A) a.echoA(&quot;a&quot;) // (A) a a.echo_жA() // (_ *A) a.echoжA(&quot;a&quot;) // (*A) a // 以下是定义在结构体值上的方法原型，通过调用结构体类型上定义的函数，传入结构体的值 A.echo_A(a) // (_ A) A.echoA(a, &quot;a&quot;) // (A) a // A.echo_жA(a) // A.echo_жA未定义 // A.echoжA(a) // A.echoжA未定义 A.setX(a, 4) // A.setY(a, 7) // A.setY未定义 println(a.x) // 0 b := &amp;a b.setX(2) b.setY(5) println(b.x, b.y) // 0 5 b.echo_A() // (_ A) b.echoA(&quot;b&quot;) // (A) b b.echo_жA() // (_ *A) b.echoжA(&quot;b&quot;) // (*A) b // 以下是定义在结构体指针上的方法原型，通过调用结构体类型指针上定义的函数，传入结构体的指针 (*A).echo_A(b) // (_ A) (*A).echoA(b, &quot;b&quot;) // (A) b (*A).echo_жA(b) // (_ *A) (*A).echoжA(b, &quot;b&quot;) // (*A) b (*A).setX(b, 1) (*A).setY(b, 8) println(b.x, b.y) // 0 8 // 调用结构体空指针上的方法，以下注释掉的代码都是空指针错误 var c *A // c = nil // c.setX(2) // c.setY(5) // println(c.x, c.y) // c.echo_A() // c.echoA() c.echo_жA() // (_ *A) c.echoжA(&quot;c&quot;) // (*A) c // (*A).echo_A(c) // (*A).echoA(c) (*A).echo_жA(c) // (_ *A) (*A).echoжA(c, &quot;c&quot;) // (*A) c // (*A).setX(c, 1) // (*A).setY(c, 8) // println(c.x, c.y)} 结构体中组合匿名字段时，匿名字段的方法会向外传递，其规则如下： 匿名字段为值类型时：值的方法会传递给结构体的值，指针的方法会传递给结构体的指针； 匿名字段为指针类型时：指针的方法会传递给值和指针； 匿名字段为接口类型时：方法会传递给值和指针； Go中有匿名函数，但是没有匿名方法 并发 Concurrency 协程goroutine是由Go运行时环境管理的轻量级线程。 使用关键字go调用一个函数/方法，启动一个新的协程goroutine 1234567891011121314151617package mainimport ( &quot;time&quot;)func say(i int) { println(&quot;goroutine:&quot;, i)}func main() { for i := 1; i &lt;= 5; i++ { go say(i) } say(0) time.Sleep(5 * time.Second)} 主协程goroutine输出0，其他由go启动的几个子协程分别输出1～5 goroutine: 0 goroutine: 1 goroutine: 2 goroutine: 3 goroutine: 4 goroutine: 5 goroutine 在相同的地址空间中运行，因此访问共享内存必须进行同步。 12345678910111213141516171819202122232425package mainimport ( &quot;sync&quot; &quot;time&quot;)var mu sync.Mutexvar i intfunc main() { for range [5]byte{} { go Add() } time.Sleep(5*time.Second) println(i)}func Add() { // 使用互斥锁防止多个协程goroutine同时修改共享变量 // 只能限制同时访问此方法修改变量，在方法外修改则限制是无效的 mu.Lock() defer mu.Unlock() i++} 使用通道channel进行同步 12345678910111213141516171819202122package mainimport ( &quot;time&quot;)var i intvar ch = make(chan byte, 1)func main() { for range [5]byte{} { go Add() } time.Sleep(5*time.Second) println(i)}func Add() { ch &lt;- 0 i++ &lt;-ch} 使用channel在不同的goroutine之间通信 1234567891011121314151617181920212223242526// 上一个例子只是将channel用作同步开关，稍做修改即可在不同goroutine间通信package mainimport ( &quot;time&quot;)var i intvar ch = make(chan int, 1)func main() { for range [5]byte{} { go Add() } ch &lt;- i time.Sleep(5*time.Second) i = &lt;-ch println(i)}func Add() { // 从channel中接收的值是来自其他goroutine发送的 x := &lt;-ch x++ ch &lt;- x} 测试 Testing Go中自带轻量级的测试框架testing和自带的go test命令来实现单元测试和基准测试 单元测试 Unit 有如下待测试testgo包，一段简单的求和代码 1234567891011121314package testgoimport &quot;math&quot;func Sum(min, max int) (sum int) { if min &lt; 0 || max &lt; 0 || max &gt; math.MaxInt32 || min &gt; max { return 0 } for ; min &lt;= max; min++ { sum += min } return} 测试源文件名必须是_test.go结尾的，go test的时候才会执行到相应的代码 必须import testing包 所有的测试用例函数必须以Test开头 测试用例按照源码中编写的顺序依次执行 测试函数TestXxx()的参数是*testing.T，可以使用该类型来记录错误或者是测试状态 测试格式：func TestXxx (t *testing.T)，Xxx部分可以为任意的字母数字的组合，首字母不能是小写字母[a-z]，例如Testsum是错误的函数名。 函数中通过调用*testing.T的Error，Errorf，FailNow，Fatal，FatalIf方法标注测试不通过，调用Log方法用来记录测试的信息。 12345678910111213141516package testgoimport &quot;testing&quot;func TestSum(t *testing.T) { s := Sum(1, 0) t.Log(&quot;Sum 1 to 0:&quot;, s) if 0 != s { t.Error(&quot;not equal.&quot;) } s = Sum(1, 10) t.Log(&quot;Sum 1 to 10:&quot;, s) if 55 != s { t.Error(&quot;not equal.&quot;) }} 在当前包中执行测试：go test -v === RUN TestSum --- PASS: TestSum (0.00s) t0_test.go:7: Sum 1 to 0: 0 t0_test.go:12: Sum 1 to 10: 55 PASS ok /home/cxy/go/src/testgo 0.004s 基准测试 Benchmark 基准测试 Benchmark用来检测函数/方法的性能 基准测试用例函数必须以Benchmark开头 go test默认不会执行基准测试的函数，需要加上参数-test.bench，语法:-test.bench=\"test_name_regex\"，例如go test -test.bench=\".*\"表示测试全部的基准测试函数 在基准测试用例中，在循环体内使用testing.B.N，使测试可以正常的运行 1234567package testgoimport &quot;testing&quot;func BenchmarkSum(b *testing.B) { b.Logf(&quot;Sum 1 to %d: %d\\n&quot;, b.N, Sum(1, b.N))} 在当前包中执行测试：go test -v -bench . BenchmarkSum 2000000000 0.91 ns/op --- BENCH: BenchmarkSum t0_test.go:19: Sum 1 to 1: 1 t0_test.go:19: Sum 1 to 100: 5050 t0_test.go:19: Sum 1 to 10000: 50005000 t0_test.go:19: Sum 1 to 1000000: 500000500000 t0_test.go:19: Sum 1 to 100000000: 5000000050000000 t0_test.go:19: Sum 1 to 2000000000: 2000000001000000000 ok /home/cxy/go/src/testgo 1.922s","link":"/posts/92784393.html"},{"title":"word2vec中的数学原理详解","text":"简介 word2vec最初是由Tomas Mikolov 2013年在ICLR发表的一篇文章Efficient Estimation of Word Representations in Vector Space， 并且开源了代码，作用是将所有词语投影到K维的向量空间，每个词语都可以用一个K维向量表示。由于它简洁，高效的特点，引起了人们的广泛关注，并应用在很多NLP任务中，用于训练相应的词向量。 预备知识 本节介绍word2vec中将用到的一些重要知识点，包括sigmoid函数、Beyes公式和Huffman编码等 sigmoid函数 sigmoid函数是神经网络中常用的激活函数之一，其定义为 \\[ \\sigma (x)=\\frac{1}{1+e^{-x}} \\] 该函数的定义域为\\((-\\infty ,+\\infty)\\)，值域为\\((0,1)\\).图1给出了sigmoid函数的图像 sigmoid函数的导函数具有以下形式 \\[ {\\sigma (x)}'=\\sigma (x)[1-\\sigma (x)] \\] 由此易得，函数\\(\\log\\sigma (x)\\)和\\(\\log(1-\\sigma (x))\\)的导函数分别为 \\[ [{\\log\\sigma (x)}']=1-\\sigma (x),[{\\log(1-\\sigma (x))}']=-\\sigma (x) \\] 公式(2.1)在后面的推导将用到。 逻辑回归 生活中经常会碰到二分类问题，例如，某封电子邮件是否为垃圾邮件，某个客户是否为潜在客户，某次在线交易是否存在欺诈行为，等等。设\\(\\left \\{ ({x_i,y_i}) \\right \\}_{i=1}^{m}\\)为一个二分类问题的样本数据，其中\\({x_i}\\in \\mathbb{R}^{n},{y_i}\\in (0,1)\\)，当\\({y_i}=1\\)时称相应的样本为正例，当\\({y_i}=0\\)时称相应的样本为负例。 利用sigmoid函数，对于任意样本\\(x=(x_1,x_2,...,x_n)^{T}\\)，可将二分类问题的hypothesis函数写成 \\[ h_\\theta (x)=\\sigma (\\theta _0+\\theta _1x_1+\\theta _2x_2+...+\\theta _nx_n) \\] 其中\\(\\theta=(\\theta_0,\\theta_1,...,\\theta_n)^{T}\\)为待定参数。为了符号上简化起见，引入\\(x_0=1\\)将\\(x\\)扩展为\\((x_0,x_1,...x_n)^{T}\\)，且在不引起混淆的情况下仍将其记为\\(x\\)。于是\\(h_\\theta\\)可简写为 \\[ h_\\theta (x)=\\sigma (\\theta ^{T}x)=\\frac{1}{1+e^{-\\theta ^{T}x}} \\] 取阈值\\(T=0.5\\)，则二分类的判别公式为 \\[ y(x)=\\left\\{\\begin{matrix} 1,h_\\theta (x)\\geqslant 0.5\\\\ 0,h_\\theta (x)&lt; 0.5 \\end{matrix}\\right. \\] 那参数\\(\\theta\\)如何求呢？通常的做法是，先确定一个形如下式的整体损失函数 \\[ J(\\theta )=\\frac{1}{m}\\sum_{i=1}^{m}cost(X_i,y_i) \\] 然后对其进行优化，从而得到最优的参数\\(\\theta ^{*}\\)。 实际应用中，单个样本的损失函数\\(cost(X_i,y_i)\\)常取为对数似然函数 \\[ cost(X_i,y_i)=\\left\\{\\begin{matrix} -\\log (h_\\theta (X_i)) &amp; y_i=1;\\\\ -\\log (1-h_\\theta (X_i))&amp; y_i=0. \\end{matrix}\\right. \\] 注意，上式是一个分段函数，也可将其写成如下的整体表达式 \\[ cost(X_i,y_i)=-y_i·\\log(h_\\theta(X_i))-(1-y_i)·\\log(1-h_\\theta(X_i)). \\] Bayes公式 贝叶斯公式是英国数学家贝叶斯(Thomas Bayes)提出来的，用来描述两个条件概率之间的关系。若记\\(P(A)\\),\\(P(B)\\)分别表示事件A和事件B发生的概率，\\(P(A|B)\\)表示事件B发生的情况下事件A发生的概率，\\(P(A,B)\\)表示事件A,B同时发生的概率，则有 \\[ P(A|B)=\\frac{P(A,B)}{P(B)},P(B|A)=\\frac{P(A,B)}{P(A)} \\] 这就是Bayes公式 Huffman编码 本节简单介绍Huffman编码，为此，首先介绍Huffman树的定义及其构造算法。 Huffman树 在计算机科学中，树是一种重要的非线性数据结构，它是数据元素（在树中称为结点）按分支关系组织起来的结构。若干棵互不相交的树构成的集合称为森林。下面给出几个与树相关的常用概念。 路径和路径长度 在一棵树中，从一个结点往下可以达到的孩子或孙子结点之间的通路，称为路径。通路中分支的数目称为路径长度。若规定根结点的层号为1，则从根结点到第\\(L\\)层结点的路径长度为\\(L-1\\) 结点的权和带权路径长度 若为树中结点赋予一个具有某种含义的（非负）数值，则这个数值称为该结点的权。结点的带权路径长度是指，从根结点到该结点之间的路径长度与该结点的权的乘积。 树的带权路径长度 树的带权路径长度规定为所有叶子结点的带权路径长度之和。 二叉树是每个结点最多有两个子树的有序树。两个子树通常被称为“左子树”和“右子树”，定义中的“有序”是指两个子树有左右之分，顺序不能颠倒。 给定\\(n\\)个权值作为\\(n\\)个叶子结点，构造一棵二叉树，若它的带权路径长度达到最小，则成这样的二叉树为最优二叉树，也称为Huffman树 Huffman树的构造 给定\\(n\\)个权值\\({w_1,w_2,...,w_n}\\)作为二叉树的\\(n\\)个叶子结点，可通过以下算法来构造一棵Huffman树。 算法2.1(Huffman树构造算法) 接下来给出算法2.1的一个具体实例。 利用算法2.1，易知其构造过程如图2所示，图中第六步给出了最终的Huffman树，由图可见词频越大的词离根结点越近。 构造过程中，通过合并新增的结点被标记为黄色。由于每两个结点都要进行一次合并，因此，若叶子结点的个数为n，则构造的Huffman树新增结点的个数为n-1。本例中n=6，因此新增的结点个数为5. 注意，前面有提到，二叉树的两个子树是分左右的，对于某个非叶子结点来说，就是其两个孩子结点是分左右的，在本例中，统一将词频打的结点作为左孩子结点，词频小的作为右孩子结点。当然，这只是一个约定，你要将词频大的结点作为右孩子结点也没有问题。 Huffman编码 在数据通信中，需要将传送的文字转换成二进制的字符串，用0,1码的不同排列來表示字符.例如，需传送的报文为“AFTER DATA EAR ARE ART AREA”，这里用到的字符集为“A, E, R, T，F, D”，各字母出现的次数为8, 4, 5, 3, 1, 1.现要求为这些字母设计编码. 要区別6个字母，最简单的二进制编码方式是等长编码，固定采用3位二进制（\\(2^{3}=8&gt;6\\)),可分別用000、001、010、011、100、101对“A, E, R, T, F, D”进行编码发送，当对方接收报文时再按照三位一分进行译码. 显然编码的长度取决报文中不同字符的个数.苻报文中可能出现26个不同字符，则固定编码长度为5 (\\(2^{5}=32&gt;26\\)).然而，传送报文时总是希望总长度尽可能短.在实际应用中, 各个字符的出现频度或使用次数是不相同的，如A、B、 C的使用频率远远高于X、Y、Z,自然会想到设计编码时，让使用频率高的用短码，使用频率低的用长码，以优化整个报文编码. 为使不等长编码为前缀编码（即要求一个字符的编码不能足另一个字符编码的前缀)，可用字符集中的毎个字符作为叶子结点生成一棵编码二叉树，为了获得传送报文的最短长度, 可将每个字符的出现频率作为字符结点的权值赋予该结点上，显然字使用频率越小权值越小，权值越小叶子就越靠下,于是频率小编码长，频率高编码短,这作就保证了此树的最小带 权路径长度，效果上就是传送报文的最短长度.因此，求传送报文的最短长度问题转化为求由字符集中的所有字符作为叶子结点，由字符出现频率作为其权值所产生的Huffman树的问题。利用Huffman树设计的二进制前缀编码，称为Huffman编码，它既能满足前缀编码的条件，又能保证报文编码总长最短。 本文将介绍的word2vec工具中也将用到Huffman编码，它把训练语料中的词当成叶子结点，其在训练语料出现的次数当作权值，通过构造相应的Huffman树来对每一个词进行Huffman编码. 图3给出了例2.1中六个词的Huffman编码，其中约定（词频较大的）左孩子结点编码为1,(词频较小的）右孩子编码为0.这样一来，“我”、“喜欢”、“观看”、“巴西”、“足球”、“世界杯”这六个问的Huffman编码分別为0, 111, 110, 101, 1001和1000. 注意，到目前为止，关于Huffman树和Huffman编码，有两个约定：（1）将权值大的结点作为左孩子结点，权值小的作为右孩子结点；（2）左孩子结点编码为1，右孩子结点编码为0.在word2vec源码中将权值较大的孩子结点编码为1，较小的孩子结点编码为0。为与上述约定统一起见，下文中提到的“左孩子结点”都是权值较大的孩子结点。 背景知识 word2vec是用来生成词向量的工具，而词向量与语言模型有着密切的关系，为此，不妨先来了解一些语言模型方面的知识。 统计语言模型 当今的互联网迅猛发展，每天都在产生大量的文本、图片、语音和视频数据，要对这些数据进行处理并从中挖掘出有价值的信息，离不开自然语言处理技术，其中统计语言模型就是很重要的一环，它是所有NLP的基础，被广泛用于语音识别、机器翻译、分词、词性标注和信息检索等任务。 简单地说，统计语言模型是用来计算一个句子的概率模型，它通常基于一个语料库来构建。那什么叫一个句子的概率呢？假设，\\(W=w_1^{T}:=(w_1,w_2,...,w_T)\\)表示由T个词\\(w_1,w_2,...,w_T\\)构成的一个句子，则\\(w_1,w_2,...,w_T\\)的联合概率 \\[ p(W)=p(w_1^{T})=p(w_1,w_2,...,w_T) \\] 就是这个句子的概率。利用Bayes公式，上式可以链式的分解为 \\[ p(w_1^{T})=p(w_1)·p(w_2|w_1)·p(w_3|w_1^{2})···p(w_T|w_1^{T-1}) \\] 其中的（条件）概率\\(p(w_1),p(w_2|w_1),...,p(w_T|w_1^{T-1})\\)就是语言模型的参数，若这些参数已经全部算得，那么给定一个句子\\(w_1^{T}\\)，就可以很快的算出相应的\\(p(w_1^{T})\\)了。 看起来好像很简单，但是，具体实现起来还是有点麻烦，例如，先来看看模型参数的个数。刚才是考虑一个给定长度为T的句子，就需要计算T个参数。不妨假设语料库对应词典D的大小（即词汇量）为N，那么，如果考虑长度为T的任意句子，理论上就有\\(N^{T}\\)种可能，而每种可能都要计算T个参数，总共需要计算\\(TN^{T}\\)个参数，当然，这里只是简单估算，并没有考虑重复参数，但是这个量级还是蛮吓人的。此外，这些概率计算好后，还得保存下来，因此，存储这些信息也需要很大的内存开销。 此外，这些参数如何计算呢？常见的方法有n-gram模型、决策树、最大熵模型、最大熵马尔科夫模型、条件随机场、神经网络等方法。本文只讨论n-gram模型和神经网络两种方法。首先来看n-gram模型。 n-gram模型 考虑\\(p(w_k|w_1^{k-1})(k&gt;1)\\)的近似计算，利用Bayes公式，有 \\[ p(w_k|w_1^{k-1})=\\frac{p(w_1^{k})}{p(w_1^{k-1})} \\] 根据大数定理，当语料库足够大时，\\(p(w_k|w_1^{k-1})\\)可近似的表示为 \\[ p(w_k|w_1^{k-1})\\approx \\frac{count(w_1^{k})}{count(w_1^{k-1})} \\] 其中\\(count(w_1^{k})\\)和\\(count(w_1^{k-1})\\)分别表示词串\\(w_1^{k}\\)和\\(w_1^{k-1}\\)在语料库中出现的次数。可想而知，当\\(k\\)很大时，\\(count(w_1^{k})\\)和\\(count(w_1^{k-1})\\)的统计会多么耗时 从公式(3.1)可以看出：一个词出现的概率与它前面的所有词都相关。如果假定一个词出现的概率只与它前面固定数目的词相关呢？这就是n-gram模型的基本思想，它做了一个n-1阶的Markov假设，认为一个词出现的概率就只与它前面的n-1个词相关，即 \\[ p(w_k|w_1^{k-1})\\approx p(w_k|w_{k-n+1}^{k}) \\] 于是，(3.2)就变成了 \\[ p(w_k|w_1^{k-1})\\approx \\frac{count(w_{k-n+1}^{k})}{count(w_{k-n+1}^{k-1})} \\] 以n=2为例，就有 \\[ p(w_k|w_1^{k-1})\\approx \\frac{count(w_{k-1},w_k)}{count(w_{k-1})} \\] 这样一简化，不仅使得单个参数的统计变得更容易（统计时需要匹配的词串更短），也使得参数的总数变少了。 那么，n-gram中的参数n取多大比较合适呢？一般来说，n的选取需要同时考虑计算复杂度和模型效果两个因素。 在计算复杂度方面，表1给出了n-gram模型中模型参数数量随着n的逐渐增大而变化的情况，其中假定词典大小N=200000（汉语的词汇量大致是这个量级）。事实上，模型参数的量级是N的指数函数(\\(O(N^{n})\\))，显然n不能取得太大，实际应用中最多的是采用n=3的三元模型。 在模型效果方面，理论上是n越大，效果越好。现如今，互联网的海量数据以及机器性能的提升是的计算更高阶的语言模型(如n&gt;10)成为可能，但需要注意的是当n大到一定程度时，模型效果的提升幅度会变小。例如当n从1到2，再从2到3时，模型的效果上升显著，而从3到4时，效果提升就不显著了（具体可参考吴军在《数学之美》中的相关章节）。事实上，这里还涉及到一个可靠性和可区别性的问题，参数越多，可区别性越好，但同时单个参数的实例变少而降低了可靠性，因此需要在可靠性和可区别性之间进行折中。 另外，n-gram模型中还有一个叫做平滑化的重要环节。回到公式(3.3)，考虑两个问题： 若\\(count(w_{k-1},w_k)=0\\)，能否认为\\(p(w_k|w_1^{k-1})\\)就等于0呢？ 若\\(count(w_{k-1},w_k)=count(w_{k-n+1}^{k-1})\\)，能否认为\\(p(w_k|w_1^{k-1})\\)就等于1呢？ 显然不能！但是这是一个无法回避的问题，哪怕你的语料库有那么大，平滑化技术就是用来处理这个问题的，这里不展开讨论，具体可以参考维基百科。 总结起来,n-gram模型是这样一种模型,其主要工作是在语料中统计各种词串中出现的次数以及平滑化处理.概率值计算好之后就存储起来，下次需要计算一个句子的概率时，只需找到相关的概率参数，将它们连乘起來就好了。 然而，在机器学习领域有一种通用的招数数这样的：对所考虑的问题建模后先为其构造一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，最后利用这组最优参数对应的模型来进行预测. 对于统计语言模型而言，利用最大似然，可把目标函数设为 \\[ \\prod_{w\\in C}p(w|Context(w)) \\] 其中C表示语料(Corpus)，Context(w)表示词w的上下文(Context)即w周边的词的集合。当Context(w)为空时，就取\\(p(w|Context(w))=p(w)\\)。特别的，对于前面介绍的n-gram模型，就有\\(Context(w_i)=w_{i-n+1}^{i-1}\\). 当然，实际采用中常采用最大对数似然，即把目标函数设为 \\[ L =\\sum_{w\\in C}\\log p(w|Context(w)) \\] 然后对这个函数进行最大化。 从(3.4)可见，概率\\(p(w|Context(w))\\)已被视为关于\\(w\\)和\\(Context(w)\\)的函数即 \\[ p(w|Context(w))=F(w,Context(w),\\theta), \\] 其中\\(\\theta\\)为待定参数集。这样一来，一旦对(3.4)进行优化得到最优参数集后，F也就唯一被确定了，以后任何概率\\(p(w|Context(w))\\)就可以通过函数\\(F(w,Context(w),\\theta^{*})\\)来计算了。 与n-gram相比，这种方法不需要（事先计算并）保存所有的概率值，而是通过直接计算来获取，且通过选取合适的模型可使得\\(\\theta\\)中参数的个数远小于n-gram中模型参数的个数。 很显然，对于这样一种方法，最关键的地方自安于函数F的构造了。下一小节将介绍一种通过神经网络来构造F的方法。之所以特意介绍这个方法，是因为它可以视为word2vec中算法框架的前身或者说基础。 神经概率语言模型 本小节介绍Bengio等人在文A neural probabilistic language model. Journal of Machine Learning Research中提出的一种神经概率语言模型。该模型中用到了一个重要的工具——词向量。 什么是词向量呢？简单来说就是，对词典D中的任意词\\(w\\)，指定一个固定长度的实值向量\\(v(w)\\in \\mathbb{R}^{m}\\),\\(v(w)\\)就称为\\(w\\)的词向量，\\(m\\)为词向量的长度。对于词向量的进一步理解将放到下一小节来讲解。 既然是神经概率语言模型，其中当然要用到一个神经网络啦。图4给出了这个神经网络的结构示意图，它包括四个层：输入层（Input）、投影层(Projection)、隐藏层(Hidden)和输出层(Output)。其中W,U分别为投影层与隐藏层以及隐藏层和输出层之间的权值矩阵，p,q分别为隐藏层和输出层上的偏置向量。 对于语料C中的任意一个词\\(w\\)，将\\(Context(w)\\)取为其前面的n-1个（类似于n-gram),这样二元对(\\(Context(w),w\\))回就是一个训练样本了、接下来，讨论样本(\\(Context(w),w\\))，经过如图所示的神经网络时是如何参与运算的．注意，一旦语料\\(C\\)和词向量长度\\(m\\)给定后，投影层和输出层的规模就确定了，前者为\\(（n-1）m\\)，后者为\\(N=|D|\\)即语料C的词汇量大小而隐藏层的规模砺是可调参数由用户指定. 为什么投影层的规模是（n-1)m呢？因为输人层包含\\(Context(w),w\\)中n-1个词的词向量，而投影层的向量xw是这样构的：将输人层的n-1个词向量按顺序首尾相接地拼起来形成一个长向量，其长度当然就是(n一1)m了．有了向量\\(x_w\\)，接下来的计算过程就很平凡了，具体为 \\[ \\left\\{\\begin{matrix} z_w=\\tanh (W_x+p)\\\\ y_w=Uz_w+q \\end{matrix}\\right. \\] 注3.4有读者可能要问：对于语料中的一个给定句子的前几个词，其前面的词不足n-1个怎么办此时，可以人为地添加一个（或几个）填充向量就可以了，它们也参与训练过程． 经过上述两步计算得到的\\(y_w=(y_{w,1},y_{w,2},...y_{w,N})^{T}\\)只是一个长度为N的向量，其分量不能表示概率．如果想要\\(y_w\\)的分量\\(y_{w,i}\\)表示当上下文为\\(Context(w)\\)时下一个词恰为词典中第个词的概率，则还需要做一个softmax归一化，归一化后，\\(p(w|Context(w))\\)就可以表示为 \\[ p(w|Context(w))=\\frac{e^{y_w,i_w}}{\\sum _{1=1}^{n}e^{y_{w,i}}} \\] 其中\\(i_w\\)表示词\\(w\\)在词典\\(D\\)中的索引。 公式(3.6)给出了概率\\(p(w|Context(w))\\)的函数表示，即找到了上一小节中提到的函数\\(F(w,Context(w),\\theta)\\)，那么其中待确定的参数\\(\\theta\\)有哪些呢？总结起来，包括两部分 词向量：\\(v(w)\\in \\mathbb{R}^{m},w\\in D\\)以及填充向量。 神经网络参数：\\(W\\in \\mathbb{R}^{n_h\\times(n-1)^{m}},P\\in \\mathbb{R}^{n_h};U\\in\\mathbb{R}^{N\\times n_h},q\\in\\mathbb{R}^{N}\\) 这些参数均通过训练算法得到，值得一提的是，通常的机器学习算法中，输人都是已知的，而在上述神经概率语言模型中，输人\\(v(w)\\)也需要通过训练才能得到．接下来，简要地分析一下上述模型的运算量，在如图4所示的神经网络中，投影层、隐藏层和输出层的规模分别为（\\(n-1）m,n_h,N\\),依次看看其中涉及的参数： （1）n是一个词的上下文中包含的词数，通常不超过5； （2）m是词向量长度，通常是\\(10^1\\)～\\(10^2\\)量级； （3）\\(n_h\\)由用户指定，通常不需取得太大，如\\(10^2\\)量级； （4）N是语料词汇量的大小，与语料相关，但通常是\\(10^4\\)～\\(10^5\\)量级． 再结合（3.5）和（3.6），不难发现，整个模型的大部分计算集中在隐藏层和输出层之间的矩阵向量运算，以及输出层上的softmax归一化运算．因此后续的相关研究工作中，有很多是针对这一部分进行优化的，其中就包括了word2vec的工作．与n-gram模型相比，神经概率语言模型有什么优势呢？主要有以下两点： 1、词语之间的相似性可以通过词向量来体现． 举例来说，如果某个（英语）语料中\\(S_1\\)=\"A dog is running in the room\"出现了10000次，而\\(S_2\\)=\"A cat is running in the room\"只出现了1次，按照n-gram模型的做法，\\(p(S_1)\\)肯定会远大于\\(p(S_2)\\)·注意，\\(S_1\\)和\\(S_2\\)的唯一区别在于dog和cat,而这两个词无论是句法还是语义上都扮演了相同的角色，因此\\(p(S_1)\\)和\\(p(S_2)\\)应该很相近才对、然而，由神经概率语言模型算得的\\(p(S_1)\\)和\\(p(S_2)\\)是大致相等的．原因在于：（1）在神经概率语言模型中假定了“相似的”的词对应的词向量也是相似的；（2）概率函数关于词向量是光滑的，即词向量中的一个小变化对概率的影响也只是一个小变化．这样一来，对于下面这些句子 只要在语料库中出现一个，其他句子的概率也会相应的增大 2.基于词向量的模型自带平滑化功能(由(3.6)可知,\\(p(w|Context(w))\\in (0,1)\\)不会为0)，不再需要像n-gram那样进行额外处理了 最后，我们回过头来想想，词向量在整个神经概率语言模型中扮演了什么角色呢？训练时，它是用来帮助构造目标函数的辅助参数，训练完成后，它也好像只是语言模型的一个副产品．但这个副产品可不能小觑，下一小节将对其作进一步阐述。 词向量的理解 通过上一小节的讨论，相信大家对词向量已经有一个初步的认识了、接下来，对词向量做进一步介绍。 在NLP任务中，我们将自然语言交给机器学习算法来处理，但机器无法直接理解人类的语言，因此首先要做的事情就是将语言数学化，如何对自然语言进行数学化呢？词向量提供了一种很好的方式． 一种最简单的词向量是one-hot representation就是用一个很长的向量来表示一个词，向量的长度为词典T)的大小N,向量的分量只有一个1，其它全为0，1的位置对应该词在词典中的索引．但这种词向量表示有一些缺点，如容易受维数灾难的困扰，尤其是将其用于DeepLearning场景时；又如，它不能很好地刻画词与词之间的相似性． 另一种词向量是Distributed Representation它最早是Hinton于1986年提出的,可以克服one-hot representation的上述缺点、其基本想法是：通过训练将某种语言中的每一个词映射成一个固定长度的短向量（当然这里的“短\"是相对于one-hot representation的“长”而言的），所有这些向量构成一个词向量空间，而每一向量则可视为该空间中的一个点，在这个空间上引人“距离\"就可以根据词之间的距离来判断它们之间的（词法、语义上的）相似性了．word2vec中采用的就是这种Distributed Representation的词向量．为什么叫做Distributed Representation？很多人问到这个问题，我的一个理解是这样的：对于one-hotrepresentation，向量中只有一个非零分量，非常集中（有点孤注一掷的感觉）；而对于Distributed Representation，向量中有大量非零分量，相对分散（有点风险平摊的感觉），把词的信息分布到各个分量中去．这一点，跟并行计算里的分布式并行很像。 为更好地理解上述思想，我们来举一个通俗的例子。 上面的例子中，坐标(\\(x,y\\))的地位就相当于词向量，它用来将平面上一个点的位置在数学上作量化坐标系建立好以后，要得到某个点的坐标是很容易的、然而，在NLP任务中，要得到词向量就复杂得多了，而且词向量并不唯一，其质量依赖于训练语料、训练算法等因素。 如何获取词向量呢？有很多不同模型可用来估计词向量，包括有名的LSA(Latentse-mantic Analysis)和LDA(Latent Dirichlet Allocation)。此外，利用神经网络算法也是一种常用的方法，上一小节介绍的神经概率语言模型就是一个很好的实例．当然，在那个模型中， 目标是生成语言模型，词向量只是一个副产品．事实上，大部分情况下，词向量和语言模型都是捆绑在一起的，训练完成后两者同时得到．用神经网络来训练语言模型的思想最早由百度IDL（深度学习研究院）的徐伟提出（这方面最经典的文章要数Bengio于2003年发表在JMLR上的《ANeuralProbabilisticLanguageModel》，其后有一系列相关的研究工作，其中也包括谷歌Tomas Mikolov团队的word2vec. 一份好的词向量是很有价值的，例如，Ronan Collobert团队在软件包SENNA中利用词向量进行了POS、CHK、NER等任务，且取得了不错的效果（见图6中的表格）．最近了解到词向量在机器翻译领域的一个应用报道是这样的： 文【5】在引言介绍算法原理时举了一个简单的例子，可以帮助我们更好的理解词向量的工作原理，特将其介绍如下。 考虑英语和西班牙语两种语言，通过训练分别得到它们对应的词向量空间\\(E(nglish)\\)和\\(S(panish)\\)．从英语中取出五个词one,two，three，four，five，设其在E中对应的词向量分别为\\(u_1,u_2,u_3,u_4,u_5\\)，为方便作图，利用主成分分析(PCA)降维，得到相应的二维向量\\(v_1,v_2,v_3,v_4,v_5\\)，在二维平面上将这五个点描出来，如图7左图所示． 类似地在西班牙语中取出（与one，two，three,four，five对应的）uno,dos,tres，cuatro,cinco,设其在5中对应的词向量分别为\\(s_1,s_2,s_3,s_4,s_5\\)，用PCA降维后的二维向量分别为\\(t_1,t_2,t_3,t_4,t_5\\),将它们在二维平面上描出来（可能还需作适当的旋转），如图7右图所示. 观察左、右两幅图，容易发现：五个词在两个向量空间中的相对位置差不多，这说明两种不同语言对应向量空间的结构之间具有相似性，从而进一步说明了在词向量空间中利用距离刻画词之间相似性的合理性。 注意，词向量只是针对“词”来提的，事实上，我们也可以针对更细粒度或更粗粒度来进行推广，如字向量([7]),句子尚量和文档向量([6]),它们能为字、句子、文档等单元提供更好的表示。 基于Hierarchical Softmax的模型 有了前面的准备，本节开始正式介绍word2vec中用到的两个重要模型一CBOW模型（Continuous Bag-of-Words Model)和Skip-gram模型（Continuous Skip-gram Model)、关于这两个模型，作者Tomas Mikolov在文[5]给出了如图8和图9所示的示意图． 由图可见，两个模型都包含三层：输人层、投影层和输出层前者是在已知当前词的上下文\\(w_{t-2},w_{t-1},w_{t+1},w_{t+2}\\)的前提下预测当前词（见图8）；而后者恰恰相反，是在已知当前词的前提下，预测其上下文\\(w_{t-2},w_{t-1},w_{t+1},w_{t+2}\\)（见图9）。 对于CBOW和Skip-gram两个模型，word2vec给出了两套框架，它们分别基于Hierarchical Softmax和Negative Sampling来进行设计．本节介绍基于Hierarchical Softmax的CBOW和Skip-gram模型． 在§3．2中，我们提到，基于神经网络的语言模型的目标函数通常取为如下对数似然函数 \\[ L =\\sum_{w\\in C}\\log p(w|Context(w)) \\] 因此，讨论过程中我们应将重点放在\\(p(w|Context(w))\\)或\\(p(Context(w)|w)\\)的构造上，意识到这一点很重要，因为它可以让我们目标明确、心无旁骛，不至于陷入到一些繁琐的细节当中去。接下来将从数学角度对这两个模型进行详细介绍。 CBOW模型 本小节介绍word2vec中的第一个模型——CBOW模型 网络结构 图10给出了CBOW模型的网络结构，它包括三层：输入层、投影层和输出层。下面以样本\\((Context(w),w)\\)为例（这里假设\\(Context(w)\\)由w前后各c个词构造），对于这三个层做简要说明。 1、输入层：包含\\(Context(w)\\)中2c个词向量\\(v(Context(w)_1),v(Context(w)_2),...v(Context(w)_2c)\\in \\mathbb{R}^{m}\\).这里，m的含义同上表示词向量的长度。 2、投影层：将输入层的2c个向量做求和累加，即\\(X_w=\\sum_{i=1}^{2c}v(Context(w)_i)\\in \\mathbb{R}^{m}\\). 3、输出层输出层对应一棵二叉树，它是以语料中出现过的词当叶子结点，以各词在语料中出现的次数当权值构造出来的Huffman树．在这棵Huffman树中，叶子结点共\\(N(=|D|)\\)个，分别对应词典中的词：非叶子结点N一1个（图中标成黄色的那些结点）. 对比§3.3中神经概率语言模型的网络图（见图4）和CBOW模型的结构图（见图10），易知它们主要有以下三处不同： 1.（从输人层到投影层的操作）前者是通过拼接，后者通过累加求和 2.（隐藏层）前者有隐藏层，后者无隐藏层 3.（输出层）前者是线性结构，后者是树形结构 在§3.3介绍的神经概率语言模型中，我们指出，模型的大部分计算集中在隐藏层和输出层之间的矩阵向量运算，以及输出层上的softmax归一化运算、而从上面的对比中可见，CBOW模型对这些计算复杂度高的地方有针对性地进行了改变，首先，去掉了隐藏层，其次，输出层改用了Huffman树，从而为利用Hierarchica1 softmax技术奠定了基础． 梯度计算 Hierarchical softmax是wor2vec中用于提高性能的一项关键技术、为描述方便起见，在具体介绍这个技术之前，先引人若干相关记号．考虑Huffman树中的某个叶子结点，假设它对应词典D中的词w，记 1.\\(p^w\\):从根结点出发到达w对应叶子结点的路径。 2.\\(l^w\\):路径的中包含结点的个数． 3.\\(p_1^{w},p_2^{w},...p_{l^w}^{w}\\):路径中的\\(l^w\\)个结点，其中\\(p_1^{w}\\)表示根结点，\\(p_{l^w}^{w}\\)表示词w对应的结 4.\\(d_2^{w},d_3^{w},...p_{l^w}^{w}\\in \\mathbb{R}^{m}\\):词w的Huffman编码，它由\\(l^w-1\\)位编码构成，\\(d_j^w\\)表示路径\\(p^w\\)的中第j个结点对应的编码（根结点不对应编码）． 5.\\(\\theta_1^w,\\theta_2^w,...,\\theta_{l^w-1}^w\\in \\mathbb{R}^{m}\\):路径的\\(p^w\\)中非叶子结点对应的向量，\\(\\theta_j^w\\)表示路径中第j个非叶子结点对应的向量． 注4.1按理说，我们要求的是词典D中每个词（即Huffman树中所有叶子节点）的向量，为什么这里还要为Huffman树中每一个非叶子结点也定义一个同长的向量呢?事实上，它们只是算法中的辅助向量，具体用途在下文中书会为大家解释清楚． 好了，引人了这么一大堆抽象的记号，接下来，我们还是通过一个简单的例子把它们落到实处吧，看图11，仍以预备知识中例2.1为例，考虑词w=“足球”的情形． 图11中由条红色边串起来的5个节点就构成路径，其长度\\(l^w\\)=5.\\(p_1^{w},p_2^{w},p_3^{w},p_4^{w},p_5^{w}\\)为路径上的5个结点，其中\\(p_1^{w}\\)对应根结点\\(d_2^{w},d_3^{w},d_4^{w},d_5^{w}\\)分别为1，0，0，1，即“足球\"的Huffman编码为1001此外\\(\\theta_1^w,\\theta_2^w,\\theta_3^w,\\theta_4^w\\)分别表示路径上个非叶子结点对应的向量. 那么，在如图10所示的网络结构下，如何定义条件概率函数\\(p(w|Context(w))\\)呢？更具体地说，就是如何利用向量\\(X_w\\in \\mathbb{R}^{m}\\)以及Huffman树来定义函数p\\(p(w|Context(w))\\)呢？以图11中词w=“足球”为例．从根结点出发到达“是球\"这个叶子节点，中间共经历了4次分支（每条红色的边对应一次分支），而每一次分支都可视为进行了一次二分类. 既然是从二分类的角度来考虑问题，那么对于每一个非叶子结点，就需要为其左右孩子结点指定一个类别，即哪个是正类（标签为1），哪个是负类（标签为0）．碰巧，除根结点以外，树中每个结点都对应了一个取值为0或1的Huffman编码．因此，一种最自然的做法就是将Huffman编码为1的结点定义为正类，编码为0的结点定义为负类．当然，这只是个约定而已，你也可以将编码为1的结点定义为负类，而将编码为0的结点定义为正类、事实上，word2vec选用的就是后者，为方便读者对照着文档看源码，下文中统一采用后者，即约定 \\[ Label(p_i^w)=1-d_i^w,i=2,3,...,l^w \\] 简言之就是，将一个结点进行分类时，分到左边就是负类，分到右边就是正类。 根据预备知识2.2中介绍的逻辑回归，易知，一个结点被分为正类的概率是 \\[ \\sigma (x_w ^{T}\\theta)=\\frac{1}{1+e^{-x_w ^{T}\\theta}} \\] 被分为负类的概率就等于 \\[ 1-\\sigma (x_w ^{T}\\theta) \\] 注意，上式中有个叫\\(\\theta\\)的向量，它是待定参数，显然，在这里非叶子结点对应的那些向量\\(\\theta_i^{w}\\)就可以扮演参数的角色（这也是为什么将它们取名为的原因）．对于从根结点出发到达“足球”这个叶子节点所经历的4次二分类，将每次分类结果的概率写出来就是 但是，我们要求的是\\(p(足球|Context(足球))\\)，它跟这4个概率有什么关系呢？关系就是 \\[ p(足球|Context(足球))=\\prod_{j=2}^{l^w}p(d_j^w|x_w,\\theta _{j-1}^w) \\] 至此，通过w=“足球”的小例子Hierarchical softmax的基本思想其实就已经介绍完了．小结一下：对于词典D中的任意词些Huffman树中必存在一条从根结点到词w对应结点的路径\\(p^w\\)（且这条路径是唯一的）．路径\\(p^w\\)上存在\\(l^w-1\\)个分支将每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来就是所需的\\(p(w|Context(w))\\)， 条件概率\\(p(w|Context(w))\\)的一般公式可写为 \\[ p(w|Context(w))=\\prod_{j=2}^{l^w}p(d_j^w|x_w,\\theta _{j-1}^w) \\] 其中 \\[ p(d_j^w|x_w,\\theta _{j-1}^w)=\\left\\{\\begin{matrix} \\sigma (x_w^T\\theta _{j-1}^w),&amp; d_j^w=0; \\\\ 1-\\sigma (x_w^T\\theta _{j-1}^w),&amp; d_j^w=1, \\end{matrix}\\right. \\] 或者整体表达式 \\[ p(d_j^w|x_w,\\theta _{j-1}^w)=[\\sigma (x_w^T\\theta _{j-1}^w)]^{1-d_j^w}·[1-\\sigma (x_w^T\\theta _{j-1}^w)]^{d_j^w} \\] 将(4.3)代入对数似然函数，便得 \\[ \\begin{align*} L &amp;=\\sum_{w\\in c}\\log \\prod_{j=2}^{l^w}{[\\sigma (x_w^T\\theta _{j-1}^w)]^{1-d_j^w}·[1-\\sigma (x_w^T\\theta _{j-1}^w)]^{d_j^w}} \\\\ &amp;=\\sum_{w\\in c}\\sum_{j=2}^{l^w}{(1-d_j^w)\\log [\\sigma (x_w^T\\theta _{j-1}^w)]+d_j^w\\log[1-\\sigma (x_w^T\\theta _{j-1}^w)] } \\end{align*} \\] 为下面梯度推导方便起见，将上式中双重求和符号和符号下花括号里的内容记为\\(L(w,j)\\)，即 \\[ L(w,j)=(1-d_j^w)\\log [\\sigma (x_w^T\\theta _{j-1}^w)]+d_j^w\\log[1-\\sigma (x_w^T\\theta _{j-1}^w)] \\] 至此，已经推导出对数似然函数，这就是CBOW模型的目标函数，接下来讨论它的优化，即如何将这个函数最大化word2vec里面采用的是随机梯度上升法，而梯度类算法的关键是给出相应的梯度计算公式，因此接下来重点讨论梯度的计算，随机梯度上升法的做法是：每取一个样本\\((Context(w),w)\\)，就对目标函数中的所有（相关）参数做一次刷新,观察目标函数£易知，该函数中的参数包括向量\\(x_w,\\theta_{j-1}^w,w\\in C,j=2,...,l^w\\)。为此：先给出函数\\(L(w,j)\\)，关于这些向量的梯度． 首先考虑\\(L(w,j)\\)关于\\(\\theta_{j-1}^w\\)的梯度计算。 于是，\\(\\theta_{j-1}^w\\)的更新公式可写为 \\[ \\theta_{j-1}^w=\\theta_{j-1}^w+\\eta [1-d_j^w-\\sigma (x_w^T\\theta _{j-1}^w)]x_w \\] 其中\\(\\eta\\)表示学习率，下同。 接下来考虑\\(L(w,j)\\)关于\\(x_w\\)的梯度，观察(4.5)可发现\\(L(w,j)\\)中关于变量\\(x_w\\)和\\(\\theta_{j-1}^w\\)是对称的，因此，相应的梯度\\(\\frac{\\partial L(w,j)}{\\partial x_w}\\)也只需在\\(\\frac{\\partial L(w,j)}{\\partial \\theta_{j-1}^w}\\)的基础上对这两个向量交换位置就可以了，即 \\[ \\frac{\\partial L(w,j)}{\\partial x_w}=[1-d_j^w-\\sigma (x_w^T\\theta _{j-1}^w)]\\theta_{j-1}^w \\] 读到这里，细心的读者可能已经看出问题来了：我们最终目的是要求字典D中每个词的词向量，而这里的\\(x_w\\)表示的是\\(Context(w)\\)中各词向量的累加。那么，如何利用\\(\\frac{\\partial L(w,j)}{\\partial x_w}\\)来对\\(v(\\tilde{w}),\\tilde{w}\\in Context(w)\\)进行更新呢？word2vec中做法很简单，直接取 \\[ v(\\tilde{w}):=v(\\tilde{w})+\\eta \\sum_{j=2}^{l^w}\\frac{\\partial L(w,j)}{\\partial x_w},\\tilde{w}\\in Context(w) \\] 即把\\(\\sum_{j=2}^{l^w}\\frac{\\partial L(w,j)}{\\partial x_w}\\)贡献到\\(Context(w)\\)中每一个词的词向量上。这个应该很好理解，寂然\\(x_w\\)本身就是\\(Context(w)\\)中各词词向量的累加，求完梯度后当然也应该将其贡献到每个分量上去。 下面以样本\\((Context(w),w)\\)为例，给出CBOW模型中采取随机梯度上升法更新各参数的伪代码 Skip-gram模型 本小节介绍word2vec中另一个模型Skip-gram模型，由于推导过程与CBOW大同小异，因此会沿用上小节引入的记号。 网络结构 图12给出了Skip-gram模型的网络结构，同CBOW模型的网络结构一样，它也包括三层：输人层、投影层和输出层、下面以样本\\((w,Context(w))\\)为例，对这三个层做简要说明． 1.输人层．只含当前样本的中心词的词向量\\(v(w)\\in \\mathbb{R}^m\\) 2.投影层，这是个恒等投影，把\\(v(w)\\)投影到\\(v(w)\\).因此，这个投影层其实是多余的，这里之所以保留投影层主要是方便和CBOW模型的网络结构做对比． 3.输出层：和CBOW一样，输出层也是一棵Huffman树。 梯度计算 对于Skip-gram模型，已知的是当前词需要对其上下文\\(Context(w)\\)中的词进行预测，因此目标函数应该形如(4.2），且关键是条件概率函数\\(p(Context(w)|w)\\)的构造，Skip-gram模型中将其定义为 \\[ p(Context(w)|w)=\\prod_{u\\in Context(w)}p(u|w), \\] 上式中的\\(p(u|w)\\)可按照上小节介绍的Hierarchical Softmax思想，类似于(4.3)地写为 \\[ p(u|w)=\\prod_{j=2}^{l^u}p(d_j^u|v(w),\\theta _{j-1}^u), \\] 其中 \\[ p(d_j^u|v(w),\\theta _{j-1}^u)=[\\sigma (v(w)^T\\theta _{j-1}^u)]^{1-d_j^u}\\cdot [1-\\sigma (v(w)^T\\theta _{j-1}^u)]^{d_j^u} \\] 将上式依次代回，可得对数似然函数的具体表达式 \\[ \\begin{align*} L &amp;=\\sum_{w\\in C}\\prod_{u\\in Context(w)}\\prod_{j=2}^{l^u}{[\\sigma (v(w)^T\\theta _{j-1}^u)]^{1-d_j^u}\\cdot [1-\\sigma (v(w)^T\\theta _{j-1}^u)]^{d_j^u}}\\\\ &amp;=\\sum_{w\\in C}\\prod_{u\\in Context(w)}\\prod_{j=2}^{l^u}{(1-d_j^u)\\log [\\sigma (v(w)^T\\theta _{j-1}^u)]+d_j^u[1-\\sigma (v(w)^T\\theta _{j-1}^u)]} \\end{align*} \\] 同样，为下面梯度推导方便起见，将三重求和符号下花括号里面的内容记为\\(L(w,u,j)\\)，即 \\[ L(w,u,j)=(1-d_j^u)\\log [\\sigma (v(w)^T\\theta _{j-1}^u)]+d_j^u[1-\\sigma (v(w)^T\\theta _{j-1}^u)] \\] 至此，已经推导出对数似然函数的表达式，这就是Skip-gram模型的目标函数。接下来同样利用随机梯度上升法对其进行优化，关键是要给出两类梯度。 首先考虑\\(L(w,u,j)\\)关于\\(\\theta_{j-1}^u\\)的梯度计算 于是，\\(\\theta_{j-1}^u\\)的更新公式可写为 \\[ \\theta_{j-1}:=\\theta_{j-1}+\\eta [1-d_j^u-\\sigma (v(w)^T\\theta_{j-1})]v(w) \\] 接下来考虑\\(L(w,u,j)\\)关于\\(v(w)\\)的梯度，同样利用\\(L(w,u,j)\\)中\\(v(w)\\)和\\(\\theta_{j-1}^u\\)的对称性，有 \\[ \\frac{\\partial L(w,u,j)}{\\partial v(w)}=[1-d_j^u-\\sigma (v(w)^T\\theta _{j-1}^u)]\\theta _{j-1}^u \\] 于是，更新公式可写为 \\[ v(w):=v(w)+\\eta \\sum_{u\\in Context(w)}\\sum_{j=2}^{l^u}\\frac{\\partial L(w,u,j)}{\\partial v(w)} \\] 下面以样本\\((w,Context(w))\\)为例，给出Skip-gram模型中采用随机梯度上升法更新各参数的伪代码。 但是，word2vec源码中，并不是等\\(Context(w)\\)中所有词都处理完才刷新\\(v(w)\\)而是，每处理完\\(Context(w)\\)中的一个词u，就及时刷新一次\\(v(w)\\)，具体为 同样，需要注意的是，循环体内的步3和步4不能交换次序，即\\(\\theta_{j-1}^u\\)要等到e后才更新 基于Negative Sampling的模型 本节将介绍基于Negative Sampling的CBOW和Skip-gram模型，NegativeSampling（简称为NEG)是Tomas Mikolov等人在文[4]中提出的，它是NCE（Noise contrastive Estimation)的一个简化版本，目的是用来提高训练速度并改善所得词向量的质量，与Hierarchicalsoftmax相比，NEG不再使用使用（复杂的）Huffman树，而是利用（相对简单的）随机负采样能大幅度提高性能，因而可作为Hierarchical softmax的一种替代． CBOW模型 在CBOW模型中，已知词伊的上下文\\(Context(w)\\)，需要预测\\(w\\)，因此，对于给定的\\(Context(w)\\)，词就是一个正样本，其它词就是负样本了．负样本那么多，该如何选取呢？这个问题比较独立，我们放到后面再进行介绍. 假定现在已经选好了一个关于\\(Context(w)\\)的负样本子集\\(NEG(w)\\neq 0\\)且对\\(\\forall \\tilde{w}\\in D\\),定义 \\[ L^{w}=\\left\\{\\begin{matrix} 1, &amp; \\tilde{w}=w;\\\\ 0,&amp; \\tilde{w}\\neq w, \\end{matrix}\\right. \\] 表示词\\(\\tilde{w}\\)的标签，即正样本的标签为1，负样本的标签为0. 对于一个给定的正样本\\((Context(w),w)\\)，我们希望最大化 \\[ g(w)=\\prod_{u\\in \\left \\{ w \\right \\} \\cup NEG(w)}p(u|Context(w)) \\] 其中 \\[ p(u|Context(w))=\\left\\{\\begin{matrix} \\sigma (x_w^T\\theta ^u), &amp;L^w(u)=1; \\\\ 1-\\sigma (x_w^T\\theta ^u),&amp; L^w(u)=0, \\end{matrix}\\right. \\] 或者写成整体表达式 \\[ p(u|Context(w))=[\\sigma (x_w^T\\theta ^u)]^{L^w(u)}\\cdot [1-\\sigma (x_w^T\\theta ^u)]^{1-L^w(u)} \\] 这里\\(x_w\\)仍表示\\(Context(w)\\)中各词的词向量之和，而\\(\\theta ^u\\in \\mathbb{R}^m\\)表示词u对应的一个（辅助）向量，为待训练参数。 为什么要最大化\\(g(w)\\)呢？让我们先看看\\(g(w)\\)的表达式，将(5.2)代入（5.1）,有 \\[ g(w)=\\sigma (x_w^T\\theta ^u)\\prod_{u\\in \\left \\{ w \\right \\} \\cup N EG(w)}[1-\\sigma (x_w^T\\theta ^u)] \\] 其中\\(\\sigma (x_w^T\\theta ^u)\\)表示当上下文为\\(Context(w)\\)时，预测中心词为\\(w\\)的概率，而\\(\\sigma (x_w^T\\theta ^u),u\\in NEG(w)\\)则表示当上下文为\\(Context(w)\\)时，预测中心词为的概率（这里可看成一个二分类问题，具体可参见预备知识中的逻辑回归）．从形式上看，最大化g(w),相当于最大化\\(\\sigma (x_w^T\\theta ^w)\\)，同时最小化所有的\\(\\sigma (x_w^T\\theta ^u),u\\in NEG(w)\\).这不正是我们希望的吗？增大正样本的概率同时降低负样本的概率．于是，对于一个给定的语料库\\(C\\)，函数 \\[ G=\\prod_{w\\in C}g(w) \\] 就可以作为整体优化目标。当然，为计算方便，对G取对数，最终的目标函数就是 \\[ \\begin{align*} L &amp;=\\log G=\\log \\prod_{w\\in C}g(w)=\\sum_{w\\in C}\\log g(w)\\\\ &amp;=\\sum_{w\\in C}\\log \\prod_{u\\in \\left \\{ w \\right \\} \\cup NEG(w)}\\left \\{ [\\sigma (x_w^T\\theta ^u)]^{L^w(u)}\\cdot [1-\\sigma (x_w^T\\theta ^u)]^{1-L^w(u)} \\right \\}\\\\ &amp;=\\sum_{w\\in C}\\log \\sum_{u\\in \\left \\{ w \\right \\} \\cup NEG(w)}\\left \\{L^w(u)\\cdot \\log [\\sigma (x_w^T\\theta ^u)]+(1-L^w(u))\\cdot \\log [1-\\sigma (x_w^T\\theta ^u)] \\right \\} \\end{align*} \\] 为下面梯度推导方便起见，将上式双重求和符号下花括号里的内容记为\\(L(w,u)\\)即 \\[ L(w,u)=L^w(u)\\cdot \\log [\\sigma (x_w^T\\theta ^u)]+(1-L^w(u))\\cdot \\log [1-\\sigma (x_w^T\\theta ^u)] \\] 接下来利用梯度上升法对（5.3）进行优化，关键是要给出\\(L\\)的两类梯度，首先考虑\\(L(w,u)\\)关于\\(\\theta^u\\)的梯度计算。 \\[ \\begin{align*} \\frac{\\partial L(w,u)}{\\partial \\theta ^u} &amp;=\\frac{\\partial }{\\partial \\theta ^u}\\left \\{ L^w(u)\\cdot \\log [\\sigma (x_w^T\\theta ^u)]+(1-L^w(u))\\cdot \\log [1-\\sigma (x_w^T\\theta ^u)] \\right \\}\\\\ &amp;=L^w(u)[1-\\sigma (x_w^T\\theta ^u)]x_w-(1-L^w(u))\\sigma (x_w^T\\theta ^u)x_w\\\\ &amp;=\\left \\{ L^w(u)[1-\\sigma (x_w^T\\theta ^u)]-(1-L^w(u))\\sigma (x_w^T\\theta ^u) \\right \\}x_w\\\\ &amp;=[L^w(u)-\\sigma (x_w^T\\theta ^u)]x_w \\end{align*} \\] 于是\\(\\theta^u\\)的更新公式可写为 \\[ \\theta^u:=\\theta^u+\\eta[L^w(u)-\\sigma (x_w^T\\theta ^u)]x_w \\] 接下来考虑\\(L(w,u)\\)关于\\(x_w\\)的梯度，同样利用\\(L(w,u)\\)中\\(x_w\\)和\\(\\theta^u\\)的对称性，有 \\[ \\frac{\\partial L(w,u)}{\\partial \\theta x_w}=[L^w(u)-\\sigma (x_w^T\\theta ^u)]\\theta ^u \\] 于是利用\\(\\frac{\\partial L(w,u)}{\\partial \\theta x_w}\\)，可得\\(v(\\tilde{w}),\\tilde{w}\\in Context(w)\\)的更新公式为 \\[ v(\\tilde{w}):=v(\\tilde{w})+\\eta \\sum_{u\\in \\left \\{ w \\right \\} \\cup NEG(w)}\\frac{\\partial L(w,u)}{\\partial \\theta x_w},\\tilde{w}\\in Context(w). \\] 下面以样本\\((Context(w),w)\\)为例，给出基于Negative Sampling的CBOW模型中采用的随机梯度上升法更新各参数的伪代码 注意，步3.3和步3.4不能交换次序，即\\(\\theta ^u\\)要等贡献到e后才更新 Skip-gram模型 本小节介绍基于Negative Sampling的Skip-gram模型，它和上小节介绍的CBOW模型所用的思想是一样的，因此，这里我们直接从目标函数出发，且沿用之前的记号． 对于一个给定的样本\\((w，context(w))\\),我们希望最大化 \\[ g(w)=\\prod_{\\tilde{w}\\in Context(w)}\\prod_{u\\in \\left \\{ w \\right \\} \\cup NEG(w)}p(u|\\tilde{w}), \\] 其中 \\[ p(u|\\tilde{w})=\\left\\{\\begin{matrix} \\sigma (v(\\tilde{w})^T\\theta ^u), &amp;L^w(u)=1; \\\\ 1-\\sigma (v(\\tilde{w})^T\\theta ^u),&amp; L^w(u)=0, \\end{matrix}\\right. \\] 或者写成整体表达式 \\[ p(u|\\tilde{w})=[\\sigma (v(\\tilde{w})^T\\theta ^u]^{L^w(u)}\\cdot [1-\\sigma (v(\\tilde{w})^T\\theta ^u)]^{1-L^w(u)} \\] 这里\\(NEG^{\\tilde{w}}(w)\\)表示处理词\\(\\tilde{w}\\)是生成的负样本子集。于是，对于一个给定的语料库C，函数 \\[ G=\\prod_{w\\in C}g(w) \\] 就可以作为整体优化的目标。同样，我们取G的对数，最终的目标函数就是 \\[ \\begin{align*} L &amp;=\\log G=\\log \\prod_{w\\in C}g(w)=\\sum_{w\\in C}\\log g(w)\\\\ &amp;=\\sum_{w\\in C}\\log \\prod_{\\tilde{w}\\in Context(w)}\\prod_{u\\in \\left \\{ w \\right \\} \\cup NEG^{\\tilde{w}}(w)}\\left \\{ [\\sigma (v(\\tilde{w})^T\\theta ^u]^{L^w(u)}\\cdot [1-\\sigma (v(\\tilde{w})^T\\theta ^u)]^{1-L^w(u)} \\right \\}\\\\ &amp;=\\sum_{w\\in C}\\sum_{\\tilde{w}\\in Context(w)} \\sum_{u\\in \\left \\{ w \\right \\} \\cup NEG^{\\tilde{w}}(w)}\\left \\{L^w(u)\\cdot \\log [\\sigma (v(\\tilde{w})^T\\theta ^u)]+(1-L^w(u))\\cdot \\log [1-\\sigma (v(\\tilde{w})^T\\theta ^u)] \\right \\} \\end{align*} \\] 为下面梯度推导方便起见，将三重求和符号下花括号的内容记为\\(L(w,\\tilde{w},u)\\)，即 \\[ L(w,\\tilde{w},u)=L^w(u)\\cdot \\log [\\sigma (v(\\tilde{w})^T\\theta ^u)]+(1-L^w(u))\\cdot \\log [1-\\sigma (v(\\tilde{w})^T\\theta ^u)] \\] 接下来利用随机梯度上升法对（5.9）进行优化，关键是要给出\\(L\\)的两类梯度。首先考虑\\(L(w,\\tilde{w},u)\\)关于\\(\\theta ^u\\)的梯度计算 \\[ \\begin{align*} \\frac{\\partial L(w,\\tilde{w},u)}{\\partial \\theta ^u} &amp;=\\frac{\\partial }{\\partial \\theta ^u}\\left \\{ L^w(u)\\cdot \\log [\\sigma (v(\\tilde{w})^T\\theta ^u)]+(1-L^w(u))\\cdot \\log [1-\\sigma (v(\\tilde{w})^T\\theta ^u)] \\right \\}\\\\ &amp;= L^w(u)[1-\\sigma (v(\\tilde{w})^T\\theta ^u)]v(\\tilde{w})-[1-L^w(u)]\\sigma (v(\\tilde{w})^T\\theta ^u)v(\\tilde{w})\\\\ &amp;=\\left \\{L^w(u)[1-\\sigma (v(\\tilde{w})^T\\theta ^u)]-[1-L^w(u)]\\sigma (v(\\tilde{w})^T\\theta ^u) \\right \\}v(\\tilde{w})\\\\ &amp;=[L^w(u)-\\sigma (v(\\tilde{w})^T\\theta ^u)]v(\\tilde{w}) \\end{align*} \\] 于是，\\(v(\\tilde{w})\\)的更新公式可写为 \\[ v(\\tilde{w}):=v(\\tilde{w})+\\eta[L^w(u)-\\sigma (v(\\tilde{w})^T\\theta ^u)]v(\\tilde{w}) \\] 接下来考虑\\(L(w,\\tilde{w},u)\\)关于\\(v(\\tilde{w})\\)的梯度，利用\\(L(w,\\tilde{w},u)\\)中\\(v(\\tilde{w})\\)和\\(\\theta ^u\\)的对称性，有 \\[ \\frac{\\partial L(w,\\tilde{w},u)}{\\partial v(\\tilde{w})}=[L^w(u)-\\sigma (v(\\tilde{w})^T\\theta ^u)]\\theta ^u \\] 于是，\\(v(\\tilde{w})\\)的更新公式可写为 \\[ v(\\tilde{w}):=v(\\tilde{w})+\\eta \\sum_{u\\in \\left \\{ w \\right \\} \\cup NEG^{\\tilde{w}}(w)}\\frac{\\partial L(w,\\tilde{w},u)}{\\partial v(\\tilde{w})} \\] 下面以样本\\((w,Context(w))\\)为例，给出基于Negative Sampling的Skip-gram模型中采用随机梯度上升法更新各参数的伪代码 负采样算法 顾名思义，在基于Negative Sampling的CBOW和Skip-gram模型中，负采样是个很重要的环节，对于一个给定的词\\(w\\)，如何生成\\(NEG(w)\\)呢？ 词典D中的词在语料c中出现的次数有高有低，对于那些高频词，被选为负样本的概率就应该比较大，反之，对于那些低频词，其被选中的概率就应该比较小．这就是我们对采样过程的一个大致要求，本质上就是一个带权采样问题相关的算法有很多，我之前在博文[19]中就给出过两个具体算法． 下面先用一段通俗的描述来帮助读者理解带权采样的机理. 接下来来谈谈word2vec中具体做法。记\\(l_0=0,l_k=\\sum_{j=1}^{k}len(w_j),k=1,2,...,N\\)，这里\\(w_j\\)表示词典D中第j个词，则以\\(\\left \\{ l_i \\right \\}_0^N\\)为剖分节点可得到区间[0,1]上的一个非等距剖分，\\(I_i=(l_{i-1},l_i),i=1,2,...,N\\)为其N个剖分区间。进一步引入区间[0,1]上的一个等距离剖分，剖分节点为\\(\\left \\{ m_j \\right \\}_{j=0}^M\\)，其中\\(M\\gg N\\)，具体见图13给出的示意图。 将内部剖分节点\\(\\left \\{ m_j \\right \\}_{j=1}^{M-1}\\)投影到非等距剖分上，如图13中的红色虚线所示，则可建立\\(\\left \\{ m_j \\right \\}_{j=1}^{M-1}\\)与区间\\(\\left \\{ I_j \\right \\}_{j=1}^N\\)的映射关系 \\[ Table(i)=w_k,where m_i\\in I_k, i=1,2,...,M-1 \\] 有了这个映射，采样就简单了：每次生成一个\\([1,M-1]\\)间的随机整数\\(r\\)，\\(Table(r)\\)就是一个样本，当然，这里还有一个细节，当对\\(w_i\\)进行负采样时，如果碰巧选到\\(w_i\\)自己怎么办？那就跳过去，代码也是这么处理的。 值得一提的是，word2vec源码中为词典D中词设置权值时，不是直接用\\(counter(w)\\)，而是对其做了$\\(次幂，其中\\)$=0.75，即(5.12)编程了 \\[ len(w)=\\frac{[counter(w)]^{0.75}}{\\sum_{u\\in D[counter(w)]^{0.75}}} \\] 此外，代码中取\\(M=10^8\\)(对应源码中变量table_size)，而映射择时通过一个名为InitUnigramTable的函数来完成。 若干代码细节 \\(\\sigma (x)\\)的近似计算 由图像1可见，sigmoid函数a(x)在=0附近变化剧烈，往两边逐渐趋于平缓，当\\(x&lt;-6\\)或者\\(x&gt;6\\)时函数值就基本不变了，前者趋于0，后者趋于1.如果在某种应用场景中需要计算大量不同\\(s\\)对应的\\(\\sigma (x)\\)，且对\\(\\sigma (x)\\)值的精确度不是非常严格，那么，基于上述观察，我们就可以采用这样一种近似计算方法. 将区间[-6，6]等距剖分为K等份，剖分节点分别记为\\(x_0,x_1,...,x_k\\)它们可表为\\(x_i=x_0+ih\\)，其中\\(x_0=-6\\)，步长\\(h=\\frac{12}{K}\\) 事先将\\(\\sigma (x)\\)的值计算好保存起来，计算\\(\\sigma (x)\\)时，采用如下近似公式 \\[ \\sigma (x)=\\left\\{\\begin{matrix} 0, &amp;x\\leq -6 ;\\\\ \\sigma (x_k)&amp;x\\in (-6,6); \\\\ 1&amp; x\\geq 6 \\end{matrix}\\right. \\] 其中，\\(k\\)等于\\(\\frac{x-x_0}{h}\\)对小数点后第一位四舍五入后的整数值（即\\(x_k\\)表示与x距离最近的剖分节点），或者简单地取为\\(\\frac{x-x_0}{h}\\)的向上或向下取整都可以，公式（6.1）有点像查表，word2vec源码中采用了这种技巧来提高效率。 采用查表方式为什么能提高运算效率呢？因为\\(\\sigma (x)\\)中设计到指数运算\\(e^z\\)，它是一个初等超越函数，内部实现时通常利用幂级数展开 \\[ e^{z}=1+\\frac{z}{1!}+\\frac{z^2}{w!}+\\frac{z^3}{3!}+... \\] 取其前\\(n\\)项做近似计算．其优点是当\\(z\\)较小（\\(|z|&lt;1\\)）时，很小的\\(n\\)就可以做很好的近似，因此计算速度较快；但当\\(z\\)较大时，展开项数较多，运算量也较大．查表的方法是一次性计算好一批节点的值,以后就只需进行匹配查词了. 词典的存储 在word2vec源码中，词典D是通过哈希技术来存储的，为简单起见，这里直接用代码中用到的变量来进行描述． 首先，开设一个长度为vocab_hash_size（默认值为\\(2\\times 10^7\\)）的整型数组vocab_hash，并将每个分量初始化为-1.然后，为词典D中的词建立如下映射 \\[ vocab_hash[hv(w_j)]=j, \\] 其中\\(hv(w_j)\\)表示词\\(w_j\\)的哈希值，当然，可能出现 \\[ hv(w_i)=hv(w_j),i\\neq j \\] 的情形，此时采用线性探测的开放定址法来解决冲突，即如果计算\\(w_i\\)的哈希值\\(hv(w)\\)后，发现vocab_hash中的该位置已被其他词占用，则顺序往下查找，直到找到一个未被占用的位置（若已到数组末尾，则从头开始查找）． 在字典中查找某个词（如\\(w\\)时，先计算其哈希值hv(w),然后在vocab_hash中找到hv(w)对应的位置，如果\\(vocab_hash[hv(w_j)]=-1\\)，则表示\\(w\\)未被收录在词典中；否则，需将\\(w\\)和\\(w_{vocab_hash[hv(w_j)]}\\)（即词典中第\\(vocab_hash[hv(w_j)]\\)个词）进行比较，如果相同，则\\(w\\) 在词典中的索引就是\\(vocab_hash[hv(w_j)]\\)，如果不同，则继续往下匹配，直到\\(w\\)和某个\\(w_k\\)匹配上或者vocab_hash的值为-1为止，前者表示\\(w\\)在词典中的索引就是\\(k\\)，后者表示\\(w\\)未被收录在词典中。 换行符 构建词典D时，其中包含一个特殊的词，它被放在词典的第一个位置上．该词代表语料中出现的换行符，它虽然也对应一个词向量，但这个向量在训练过程中并不参与运算．其作用主要是用来判别一个句子的结束． word2vec源码的函数sortvocab中有一段代码是删除词典中出现次数小于min_count的低频词，并重新为词典建立哈希映射．由于这段代码前已对字典中的词按照出现次数进行了降序排列，因此，按理说，只需对词典从后往前遍历，剔除出现次数小于min_count的低频词即可，word2vec相应部分的代码是这样的： 注意，换行符对应的词并没有参与排序，因此，当碰巧换行符在语料中出现的次数小于min_count，则有可能多删除一个出现次数大于等于min_count的词． 低频词和高频词 在word2vec源码中，对语料中的低频词和高频词都进行了一些特殊处理。 低频词的处理 利用语料C建立词典时，并不是每个出现过的词都能收录到词典中．代码中引人一个叫做min——count的阀值参数（默认取值为5），若某个词在语料中出现的次数小于它，则将其从词典中删除、如果不想做任何删除，则只需将min_count取为0或1即可注意，这些未能进人词典的词在训练时也是不可见的．可以简单地理解为训练前语料进行了这样一次预处理：将所有出现次数小于min_count的词都挖走了. 此外，为提高效率，在利用语料构建词典时，代码中会根据词典当前的规模来决定是否需要对词典中的低频词进行清理，具体做法如下：预先设定一个阀值参数min_reduce（默认值为1），如果当前词典\\(D_{current}\\)的规模|\\(D_{current}\\)|满足 \\[ |D_current|&gt;0.7\\cdot vocab_hash_size, \\] 则从\\(D_{current}\\)删除所有出现次数小于等于min_reduce的词。 高频词的处理 文[4]中提到，在一个大语料库中，那些最常见的词(the most frequent words）将出现百万甚至千万次，如“的”、“是”等．同低频词相比，这些词提供的有用信息更少．而且这次词对应的词向量在众多样本上进行训练时也不会发生显著的变化、因此，文[4]中提出了一种叫做subsampling的技巧，用来提高训练速度（提高2～10倍，而且可提高那些相对来说词频没那么高的词的词向量精度），具体做法如下：给定一个词频阀值参数（即word2vec源码中叫做sample的变量），词\\(w\\)将以 \\[ prob(w)=1-\\sqrt{\\frac{t}{f(w)}} \\] 的概率被舍弃，其中 \\[ f(w)=\\frac{counter(w)}{\\sum_{u\\in D}counter(u)},w\\in D \\] 表示\\(w\\)的频率，显然，Subsampling只是针对那些满足\\(f(w)&gt;t\\)的所谓高频词。 值得一提的是，word2vec源码中实际用的公式不是（6.2），而是 \\[ prob(w)=1-(\\sqrt{\\frac{t}{f(w)}}+\\frac{t}{f(w)}) \\] 具体实现是这样的：假设当前处理词\\(w\\)，先计算\\(ran=\\sqrt{\\frac{t}{f(w)}}+\\frac{t}{f(w)}\\)的值，然后产生一个（0,1）上的随机实数r，如果\\(r&gt;ran\\)，则舍弃词\\(w\\)。由于在区间(0,1)上产生一个大于\\(ran\\)的随机数的概率是\\(1-ran\\)，因此上述做法就等同于以\\(1-ran\\)的概率舍弃词\\(w\\)。 窗口及上下文 模型训练是以行为单位进行的（因此，如果语料文件每行存储一个句子，则训练时每次就是处理一个句子），利用特殊词进行分割．当然，句子不能太长（极端情况就是将整个语料存成一个超长的句子），源码中设置了一个阀值参数MAX_SENTENCE_LENGTH（默认值为1000），如果一行的词数超过MAX_SENTENCE_LENGTH,则强行进行截断． 对于一个给定的行设它包含\\(T\\)个词，则可以得到\\(T\\)个训练样本，因为每一个词都对应一个样本．考虑该行中的某个词些只需定义好\\(Contex(w)\\),就可得到样本\\((Contex(w),w)\\). 那么如何定义\\(Contex(w)\\)呢？前面的描述中采用的是在词的前后各取c个词，但word2vec中采用的策略是这样的：事先设置一个窗口阀值参数window（默认值为5），每次构造\\(Contex(w)\\)时，首先生成区间[1，window]上的一个随机（整）数\\(\\tilde{c}\\),\\(w\\)前后各取\\(\\tilde{c}\\)个词就构成了\\(Contex(w)\\). 前文中对比§3、3中神经概率语言模型和CBOW时，我们有提到它们在投影方式上有一个不同，前者采用的是首尾相接，后者则是直接累加，采用直接相加的投影方式除了可使所得向量更短外，还有一个好处：对于一行的首尾的某些词，其前、后的词数可能不足\\(\\tilde{c}\\)个,此时如果是首尾相接的方式则需要补充填充向量，但直接相加就没有这个问题，无非是求和项里少了几项罢了． 自适应学习率 前文中谈及学习率时，统一使用的是常数\\(\\eta\\),而在word2vec源码中，用到了自适应技术.具体是这样的:预先设置一个初始的学习率\\(\\eta _0\\)（默认值为0，025），每处理完10000（源码里直接指定，没有设置参数，但可以根据经验调整）个词，则按照以下公式对学习率进行一次调整 \\[ \\eta =\\eta _0\\left ( 1-\\frac{word\\_count\\_actual}{train\\_words+1} \\right ) \\] 其中word_count_actual表示当前已处理过的次数，\\(train_{words}=\\sum_{w\\in D}counter(w)\\),+1是为了防止分母为0. 由（6.3）可见，\\(\\eta\\)将随着训练的进行而逐渐变小，且趋于0.但是，学习率也不能过小，因此，源码加入了一个阈值\\(\\eta_{min}\\)，一旦\\(\\eta&lt; \\eta_{min}\\)，则将\\(\\eta\\)固定为\\(\\eta_{min}\\)，其中\\(\\eta_{min}=10^{-4}\\cdot \\eta_0\\). 参数初始化与训练 模型训练采用的是随机梯度上升法，且只对语料遍历一次，这也是其高效的原因之一模型中需要训练的参数包括逻辑回归对应的参数向量，以及词典中每个词的词向量。在对这些参数进行初始化时，前者采用的是零初始化，后者采用的是随机初始化，word2vec源码中的具体公式为 \\[ \\frac{rand()/RAND_MAX-0.5}{m} \\] 易知，初始词向量的分量均落在区间\\([-\\frac{0.5}{m}，\\frac{0.5}{m}]\\)，这里m仍表示词向量的长度． 关于词向量和参数向量，word2vec源码中出现了syn0，syn1和synlneg三个一维数组，其中syn0对应Huffman树所有叶子结点的词向量，syn1对应Huffman树中所有非叶子结点的参数向量，syn1neg对应基于Negative Sampling的模型中与词相关的参数向量. 多线程并行 word2vec中支持多线程并行，源码中有个参数num_threads，其中与线程相关的主要代码如下 其中TrainModelThread是单线程训练模块．对于多线程情形，需要对语料文件进行负载平衡地划分，每个线程处理一部分，具体为 其中file_size表示整个训练语料文件的总字节数．由此可见，划分是基于字节而不是基于词. 当然，我们也可以把word2vec作分布式并行实现．不过呢，文國在引言中介绍Skip-gram模型时有提到： 这样的效率应该已经能满足大多数人的要求了吧. 几点疑问和思考 1、word2vec源码的函数TrainModelThread中，当走Hierarchical Softmax那一支时，有一段代码是用来求近似\\(\\sigma (·)\\)值的，具体如下 12345678910if(f&lt;=-MAX_EXP) continue; else if (f&gt;=-MAX_EXP) continue; else f = expTable[(int)(f+MAX_EXP)*(EXP_TABLE_SIZE/MAX_EXP/2)]; g = (1-vocab[word].code[d]-f)*alpha; 按照公式（6.1），当f&lt;=-MAX_EXP时，应该取f=0;当f&gt;=MAX_EXP时，应该取f=1.代码中的continue简单地跳过去了、显然，当f等于0或1时，代人g中，其值不一定为零，因此，这里应该是有一个近似，但奇怪的是，走NegativeSampling那一支的时候，在相应部分作者又特别注意了这一点，看以下代码片段. 1234567if(f &gt; MAX_EXP) g = (label - 1) * alpha; else if (f &lt; -MAX_EXP) g = (label - 0) * alpha;else 不知道作者这样处理是不是为了提高效率。 2、前面讨论算法时，都是假定语料已经分好词．对于英文语料，分词不是什么问题，但对于中文语料，分词的预处理则是必须的，不同分词器会得到不同的分词结果，那么word2vec所得词向量的质量是如何依赖于中文分词的质量的呢？ 3、Hierarchical Softmax和Negative Sampling是两大类不同的方法，但word2vec源码中对这两个分支采用的并不是二择一的方式，用户可以通过参数选取（如取negative=5，hs=1）同时选中这两支，相当于是一种混合方法，在这种情况下，训练得到的词向量质量如何呢？ 4、投影层采用累积相加的方式，会使得\\(Context(w)\\)中各词的顺序不再敏感，例如“我爱足球\"和“足球爱我\"在训练过程中会视为同一样本．因此，如果在投影层换回首尾相接的方式会怎么样呢？ 5、word2vec几个模型中的目标函数中均未考虑正则项，如果加人正则会怎么样呢？ 6、前文介绍的算法都是针对一个静态的语料库来讨论的，如果用户的语料数据是动态阶段性地获取的，譬如，当前只有一批语料，过一段时间后又有另一批语料,...,针对这种情形，如果想进行增量训练，该如何调整现有的框架？ 参考文献 Word2Vec源码解析 基于深度学习的自然语言处理 数据结构和算法——Huffman树和Huffman编码 机器学习算法实现解析——word2vec源码解析","link":"/posts/1852020302.html"},{"title":"深度学习基础","text":"基本概念 神经网络组成？ 神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。 感知机 多层感知机中的特征神经元模型称为感知机，由Frank Rosenblatt于1957年发明。 简单的感知机如下图所示： 其中\\(x_1\\)，\\(x_2\\)，\\(x_3\\)为感知机的输入，其输出为： 假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为$ 4 $分、\\(1\\) 分、$-3 \\(分，这\\) 3$ 位评分的权重分别是 \\(1、3、2\\)，则该歌手最终得分为 \\(4 \\times 1 + 1 \\times 3 + (-3) \\times 2 = 1\\) 。按照比赛规则，选取的 \\(threshold\\) 为 \\(3\\)，说明只有歌手的综合评分大于$ 3$ 时，才可顺利晋级。对照感知机，该选手被淘汰，因为： \\[ \\sum_i w_i x_i &lt; threshold=3, output = 0 \\] 用 \\(-b\\) 代替 \\(threshold\\)，输出变为： 设置合适的 \\(\\boldsymbol{x}\\) 和 \\(b\\) ，一个简单的感知机单元的与非门表示如下： 当输入为 \\(0\\)，\\(1\\) 时，感知机输出为 $ 0 (-2) + 1 (-2) + 3 = 1$。 复杂一些的感知机由简单的感知机单元组合而成： 多层感知机 多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第 $ i $ 层的每个神经元和第 $ i-1 $ 层的每个神经元都有连接。 输出层可以不止有$ 1$ 个神经元。隐藏层可以只有$ 1$ 层，也可以有多层。输出层为多个神经元的神经网络例如下图所示： 神经网络有哪些常用模型结构？ 下图包含了大部分常用的模型： 如何选择深度学习开发平台？ ​ 现有的深度学习开源平台主要有 Caffe, PyTorch, MXNet, CNTK, Theano, TensorFlow, Keras, fastai等。那如何选择一个适合自己的平台呢，下面列出一些衡量做参考。 参考1：与现有编程平台、技能整合的难易程度 ​ 主要是前期积累的开发经验和资源，比如编程语言，前期数据集存储格式等。 参考2: 与相关机器学习、数据处理生态整合的紧密程度 ​ 深度学习研究离不开各种数据处理、可视化、统计推断等软件包。考虑建模之前，是否具有方便的数据预处理工具？建模之后，是否具有方便的工具进行可视化、统计推断、数据分析。 参考3：对数据量及硬件的要求和支持 ​ 深度学习在不同应用场景的数据量是不一样的，这也就导致我们可能需要考虑分布式计算、多GPU计算的问题。例如，对计算机图像处理研究的人员往往需要将图像文件和计算任务分部到多台计算机节点上进行执行。当下每个深度学习平台都在快速发展，每个平台对分布式计算等场景的支持也在不断演进。 参考4：深度学习平台的成熟程度 ​ 成熟程度的考量是一个比较主观的考量因素，这些因素可包括：社区的活跃程度；是否容易和开发人员进行交流；当前应用的势头。 参考5：平台利用是否多样性？ ​ 有些平台是专门为深度学习研究和应用进行开发的，有些平台对分布式计算、GPU 等构架都有强大的优化，能否用这些平台/软件做其他事情？比如有些深度学习软件是可以用来求解二次型优化；有些深度学习平台很容易被扩展，被运用在强化学习的应用中。 为什么使用深层表示? 深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。 深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。 为什么深层神经网络难以训练？ 梯度消失 梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。 ​ 梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率： 梯度爆炸 在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为\\(NaN\\)值，再也无法更新。 权重矩阵的退化导致模型的有效自由度减少。 ​ 参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人2014年的论文里展示了关于该退化过程的可视化： 随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。 深度学习和机器学习有什么不同？ ​ 机器学习：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。 ​ 深度学习：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。 ​ 传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。 网络操作与计算 前向传播与反向传播？ 神经网络的计算主要有两种：前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果；反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度由深到浅更新网络参数。 前向传播 假设上一层结点 $ i,j,k,... $ 等一些结点与本层的结点 $ w $ 有连接，那么结点 $ w $ 的值怎么算呢？就是通过上一层的 $ i,j,k,... $ 等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如 \\(ReLu\\)，\\(sigmoid\\) 等函数，最后得到的结果就是本层结点 $ w $ 的输出。 最终不断的通过这种方法一层层的运算，得到输出层结果。 反向传播 由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下： 设最终误差为 $ E $且输出层的激活函数为线性激活函数，对于输出那么 $ E $ 对于输出节点 $ y_l $ 的偏导数是 $ y_l - t_l $，其中 $ t_l $ 是真实值，$ $ 是指上面提到的激活函数，$ z_l $ 是上面提到的加权和，那么这一层的 $ E $ 对于 $ z_l $ 的偏导数为 $ = $。同理，下一层也是这么计算，只不过 $ $ 计算方法变了，一直反向传播到输入层，最后有 $ = $，且 $ = w_i j $。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。 如何计算神经网络的输出？ 如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $ w_{41}, w_{42}, w_{43} $。 为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $ x_1, x_2, x_3 $。 \\[ a_4 = \\sigma(w^T \\cdot a) = \\sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b}) \\] 其中 $ w_{4b} $ 是节点 4 的偏置项。 同样，我们可以继续计算出节点 5、6、7 的输出值 $ a_5, a_6, a_7 $。 计算输出层的节点 8 的输出值 $ y_1 $： \\[ y_1 = \\sigma(w^T \\cdot a) = \\sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b}) \\] 其中 $ w_{8b} $ 是节点 8 的偏置项。 同理，我们还可以计算出 $ y_2 $。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 $ x_1, x_2, x_3, x_4 $ 时，神经网络的输出向量 $ y_1, y_2 $ 。这里我们也看到，输出向量的维度和输出层神经元个数相同。 如何计算卷积神经网络输出值？ 假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示： $ x_{i,j} $ 表示图像第 $ i $ 行第 $ j $ 列元素。$ w_{m,n} $ 表示 filter​ 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 \\(filter\\) 的偏置项。 表\\(a_i,_j\\)示 feature map 第 $ i$ 行第 $ j $ 列元素。 \\(f\\) 表示激活函数，这里以$ ReLU$ 函数为例。 卷积计算公式如下： \\[ a_{i,j} = f(\\sum_{m=0}^2 \\sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b ) \\] 当步长为 \\(1\\) 时，计算 feature map 元素 $ a_{0,0} $ 如下： 其计算过程图示如下： 以此类推，计算出全部的Feature Map。 当步幅为 2 时，Feature Map计算如下 注：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系： \\[ W_2 = (W_1 - F + 2P)/S + 1\\\\ H_2 = (H_1 - F + 2P)/S + 1 \\] ​ 其中 $ W_2 \\(， 是卷积后 Feature Map 的宽度；\\) W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 \\(0\\)，如果 \\(P\\) 的值是 \\(1\\)，那么就补 \\(1\\) 圈 \\(0\\)；\\(S\\) 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。 ​ 举例：假设图像宽度 $ W_1 = 5 $，filter 宽度 $ F=3 $，Zero Padding $ P=0 $，步幅 $ S=2 \\(，\\) Z $ 则 ​ 说明 Feature Map 宽度是2。同样，我们也可以计算出 Feature Map 高度也是 2。 如果卷积前的图像深度为 $ D $，那么相应的 filter 的深度也必须为 $ D $。深度大于 1 的卷积计算公式： \\[ a_{i,j} = f(\\sum_{d=0}^{D-1} \\sum_{m=0}^{F-1} \\sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b) \\] ​ 其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w_{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a_{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素；其它的符号含义前面相同，不再赘述。 ​ 每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。\\(7*7*3\\) 输入，经过两个 \\(3*3*3\\) filter 的卷积(步幅为 \\(2\\))，得到了 \\(3*3*2\\) 的输出。图中的 Zero padding 是 \\(1\\)，也就是在输入元素的周围补了一圈 \\(0\\)。 ​ 以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 * 3 * 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 * 3 * 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。 如何计算 Pooling 层输出值输出值？ ​ Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling： ​ 除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。 ​ 对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。 实例理解反向传播 ​ 一个典型的三层神经网络如下所示： ​ 其中 Layer $ L_1 $ 是输入层，Layer $ L_2 $ 是隐含层，Layer $ L_3 $ 是输出层。 ​ 假设输入数据集为 $ D={x_1, x_2, ..., x_n} $，输出数据集为 $ y_1, y_2, ..., y_n $。 ​ 如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。 假设有如下的网络层： ​ 输入层包含神经元 $ i_1, i_2 $，偏置 $ b_1 $；隐含层包含神经元 $ h_1, h_2 $，偏置 $ b_2 $，输出层为 $ o_1, o_2 \\(，\\) w_i $ 为层与层之间连接的权重，激活函数为 \\(sigmoid\\) 函数。对以上参数取初始值，如下图所示： 其中： 输入数据 $ i1=0.05, i2 = 0.10 $ 输出数据 $ o1=0.01, o2=0.99 $; 初始权重 $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $ 目标：给出输入数据 $ i1,i2 $ ( \\(0.05\\)和\\(0.10\\) )，使输出尽可能与原始输出 $ o1,o2 $，( \\(0.01\\)和\\(0.99\\))接近。 前向传播 输入层 --&gt; 输出层 计算神经元 $ h1 $ 的输入加权和： 神经元 $ h1 $ 的输出 $ o1 $ ：（此处用到激活函数为 sigmoid 函数）： \\[ out_{h1} = \\frac{1}{1 + e^{-net_{h1}}} = \\frac{1}{1 + e^{-0.3775}} = 0.593269992 \\] 同理，可计算出神经元 $ h2 $ 的输出 $ o1 $： \\[ out_{h2} = 0.596884378 \\] 隐含层--&gt;输出层： 计算输出层神经元 $ o1 $ 和 $ o2 $ 的值： \\[ out_{o1} = \\frac{1}{1 + e^{-net_{o1}}} = \\frac{1}{1 + e^{1.105905967}} = 0.75136079 \\] 这样前向传播的过程就结束了，我们得到输出值为 $ [0.75136079 , 0.772928465] $，与实际值 $ [0.01 , 0.99] $ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。 反向传播 ​ 1.计算总误差 总误差：(这里使用Square Error) \\[ E_{total} = \\sum \\frac{1}{2}(target - output)^2 \\] 但是有两个输出，所以分别计算 $ o1 $ 和 $ o2 $ 的误差，总误差为两者之和： \\(E_{o1} = \\frac{1}{2}(target_{o1} - out_{o1})^2 = \\frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083\\). \\(E_{o2} = 0.023560026\\). \\(E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109\\). ​ 2.隐含层 --&gt; 输出层的权值更新： 以权重参数 $ w5 $ 为例，如果我们想知道 $ w5 $ 对整体误差产生了多少影响，可以用整体误差对 $ w5 $ 求偏导求出：（链式法则） 下面的图可以更直观的看清楚误差是怎样反向传播的： 神经网络更“深”有什么意义？ 前提：在一定范围内。 在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。 隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。 超参数 什么是超参数？ ​ 超参数 : 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。 ​ 超参数通常存在于： 1. 定义关于模型的更高层次的概念，如复杂性或学习能力。 2. 不能直接从标准模型培训过程中的数据中学习，需要预先定义。 3. 可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定 ​ 超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。 如何寻找超参数的最优值？ ​ 在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有： 猜测和检查：根据经验或直觉，选择参数，一直迭代。 网格搜索：让计算机尝试在一定范围内均匀分布的一组值。 随机搜索：让计算机随机挑选一组值。 贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。 MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。 最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。 超参数搜索一般过程？ 超参数搜索一般过程： 1. 将数据集划分成训练集、验证集及测试集。 2. 在训练集上根据模型的性能指标对模型参数进行优化。 3. 在验证集上根据模型的性能指标对模型的超参数进行搜索。 4. 步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。 其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。 激活函数 为什么需要非线性激活函数？ 为什么需要激活函数？ 激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。 激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。 激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。 为什么激活函数需要非线性函数？ 假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。 使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。 常见的激活函数及图像 sigmoid 激活函数 函数的定义为：$ f(x) = $，其值域为 $ (0,1) $。 函数图像如下： tanh激活函数 函数的定义为：$ f(x) = tanh(x) = $，值域为 $ (-1,1) $。 函数图像如下： Relu激活函数 函数的定义为：$ f(x) = max(0, x) $ ，值域为 $ [0,+∞) $； 函数图像如下： Leak Relu 激活函数 函数定义为： 。 图像如下（$ a = 0.5 $）： SoftPlus 激活函数 函数的定义为：$ f(x) = ln( 1 + e^x) $，值域为 $ (0,+∞) $。 函数图像如下: softmax 函数 函数定义为： $ (z)_j = $。 Softmax 多用于多分类神经网络输出。 常见激活函数的导数计算？ 对常见激活函数，导数计算如下： 原函数 函数表达式 导数 备注 Sigmoid激活函数 \\(f(x)=\\frac{1}{1+e^{-x}}\\) \\(f^{'}(x)=\\frac{1}{1+e^{-x}}\\left( 1- \\frac{1}{1+e^{-x}} \\right)=f(x)(1-f(x))\\) 当\\(x=10\\),或\\(x=-10​\\)，\\(f^{'}(x) \\approx0​\\),当\\(x=0​\\)\\(f^{'}(x) =0.25​\\) Tanh激活函数 \\(f(x)=tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\) \\(f^{'}(x)=-(tanh(x))^2\\) 当\\(x=10\\),或\\(x=-10\\)，\\(f^{'}(x) \\approx0\\),当\\(x=0\\)\\(f^{`}(x) =1\\) Relu激活函数 \\(f(x)=max(0,x)\\) \\(c(u)=\\begin{cases} 0,x&lt;0 \\\\ 1,x&gt;0 \\\\ undefined,x=0\\end{cases}\\) 通常\\(x=0\\)时，给定其导数为1和0 激活函数有哪些性质？ 非线性： 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x $，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的； 可微性： 当优化方法是基于梯度的时候，就体现了该性质； 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数； $ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值； 输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。 如何选择激活函数？ ​ 选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。 以下是常见的选择情况： 如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。 如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。 sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。 tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。 ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。 如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。 使用 ReLu 激活函数的优点？ 在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。 sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。 需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。 什么时候可以用线性激活函数？ 输出层，大多使用线性激活函数。 在隐含层可能会使用一些线性激活函数。 一般用到的线性激活函数很少。 怎样理解 Relu（&lt; 0 时）是非线性激活函数？ Relu 激活函数图像如下： 根据图像可看出具有如下特点： 单侧抑制； 相对宽阔的兴奋边界； 稀疏激活性； ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。 因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。 稀疏激活性：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x&lt;0 $ 时，ReLU 硬饱和，而当 $ x&gt;0 $ 时，则不存在饱和问题。ReLU 能够在 $ x&gt;0 $ 时保持梯度不衰减，从而缓解梯度消失问题。 Softmax 定义及作用 Softmax 是一种形如下式的函数： \\[ P(i) = \\frac{exp(\\theta_i^T x)}{\\sum_{k=1}^{K} exp(\\theta_i^T x)} \\] ​ 其中，$ _i $ 和 $ x $ 是列向量，$ _i^T x $ 可能被换成函数关于 $ x $ 的函数 $ f_i(x) $ ​ 通过 softmax 函数，可以使得 $ P(i) $ 的范围在 $ [0,1] $ 之间。在回归和分类问题中，通常 $ $ 是待求参数，通过寻找使得 $ P(i) $ 最大的 $ _i $ 作为最佳参数。 ​ 但是，使得范围在 $ [0,1] $ 之间的方法有很多，为啥要在前面加上以 $ e $ 的幂函数的形式呢？参考 logistic 函数： \\[ P(i) = \\frac{1}{1+exp(-\\theta_i^T x)} \\] ​ 这个函数的作用就是使得 $ P(i) $ 在负无穷到 0 的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了 $ e $ 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。这样为多类别提供了方便（可以把 $ P(i) $ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。 ​ softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多酚类问题的最佳输出激活函数。 Softmax 函数如何应用于多分类？ ​ softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！ ​ 假设我们有一个数组，$ V_i $ 表示 $ V $ 中的第 $ i $ 个元素，那么这个元素的 softmax 值就是 \\[ S_i = \\frac{e^{V_i}}{\\sum_j e^{V_j}} \\] ​ 从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $ y=0, y=1, y=2 $ 的概率值。 继续看下面的图，三个输入通过 softmax 后得到一个数组 $ [0.05 , 0.10 , 0.85] $，这就是 soft 的功能。 更形象的映射过程如下图所示： ​ softmax 直白来说就是将原来输出是 $ 3,1,-3 $ 通过 softmax 函数一作用，就映射成为 $ (0,1) $ 的值，而这些值的累和为 $ 1 $（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！ 交叉熵代价函数定义及其求导推导 (贡献者：黄钦建－华南理工大学) ​ 神经元的输出就是 a = σ(z)，其中\\(z=\\sum w_{j}i_{j}+b\\)是输⼊的带权和。 \\(C=-\\frac{1}{n}\\sum[ylna+(1-y)ln(1-a)]\\) ​ 其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。 ​ 表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。 ​ 将交叉熵看做是代价函数有两点原因。 ​ 第⼀，它是⾮负的， C &gt; 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。 ​ 第⼆，如果对于所有的训练输⼊ x，神经元实际的输出接近⽬标值，那么交叉熵将接近 0。 ​ 假设在这个例⼦中， y = 0 ⽽ a ≈ 0。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 y = 0，⽽第⼆项实际上就是 − ln(1 − a) ≈ 0。反之， y = 1 ⽽ a ≈ 1。所以在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低了。（这里假设输出结果不是0，就是1，实际分类也是这样的） ​ 综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将\\(a={\\varsigma}(z)\\)代⼊到 公式中应⽤两次链式法则，得到： ​ 根据\\(\\varsigma(z)=\\frac{1}{1+e^{-z}}\\) 的定义，和⼀些运算，我们可以得到 \\({\\varsigma}'(z)=\\varsigma(z)(1-\\varsigma(z))\\)。化简后可得： \\(\\frac{\\partial C}{\\partial w_{j}}=\\frac{1}{n}\\sum x_{j}({\\varsigma}(z)-y)\\) ​ 这是⼀个优美的公式。它告诉我们权重学习的速度受到\\(\\varsigma(z)-y\\)，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在⼆次代价函数中类似⽅程中\\({\\varsigma}'(z)\\)导致的学习缓慢。当我们使⽤交叉熵的时候，\\({\\varsigma}'(z)\\)被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。 ​ 根据类似的⽅法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到： \\(\\frac{\\partial C}{\\partial b}=\\frac{1}{n}\\sum ({\\varsigma}(z)-y)\\) ​ 再⼀次, 这避免了⼆次代价函数中类似\\({\\varsigma}'(z)\\)项导致的学习缓慢。 为什么Tanh收敛速度比Sigmoid快？ （贡献者：黄钦建－华南理工大学） 首先看如下两个函数的求导： \\(tanh^{,}(x)=1-tanh(x)^{2}\\in (0,1)\\) \\(s^{,}(x)=s(x)*(1-s(x))\\in (0,\\frac{1}{4}]\\) 由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。 3.4.13 内聚外斥 - Center Loss （贡献者：李世轩－加州大学伯克利分校） 在计算机视觉任务中, 由于其简易性, 良好的表现, 与对分类任务的概率性理解, Cross Entropy Loss (交叉熵代价) + Softmax 组合被广泛应用于以分类任务为代表的任务中. 在此应用下, 我们可将其学习过程进一步理解为: 更相似(同类/同物体)的图像在特征域中拥有“更近的距离”, 相反则”距离更远“. 换而言之, 我们可以进一步理解为其学习了一种低类内距离(Intra-class Distance)与高类间距离(Inter-class Distance)的特征判别模型. 在此Center Loss则可以高效的计算出这种具判别性的特征. 不同于传统的Softmax Loss, Center Loss通过学习“特征中心”从而最小化其类内距离. 其表达形式如下: 其中\\(x_{i}\\)表示FCN(全连接层)之前的特征, \\(c_{y_{i}}\\)表示第$y_{i} $个类别的特征中心, \\(m\\)表示mini-batch的大小. 我们很清楚的看到\\(L_{C}\\)的终极目标为最小化每个特征与其特征中心的方差, 即最小化类内距离. 其迭代公式为: \\(\\frac{\\partial L_{C}}{\\partial x_{i}}=x_{i}-c_{y_{i}}\\) 其中 结合Softmax, 我们可以搭配二者使用, 适当平衡这两种监督信号. 在Softmax拉开类间距离的同时, 利用Center Loss最小化类内距离. 例如: 即便如此, Center Loss仍有它的不足之处: 其特征中心为存储在网络模型之外的额外参数, 不能与模型参数一同优化. 这些额外参数将与记录每一步特征变化的自动回归均值估计(autoregressive mean estimator)进行更迭. 当需要学习的类别数量较大时, mini-batch可能无力提供足够的样本进行均值估计. 若此Center Loss将需要平衡两种监督损失来以确定更迭, 其过程需要一个对平衡超参数的搜索过程, 使得其择值消耗昂贵. Batch_Size 为什么需要 Batch_Size？ Batch的选择，首先决定的是下降的方向。 如果数据集比较小，可采用全数据集的形式，好处是： 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。 由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。 对于更大的数据集，假如采用全数据集的形式，坏处是： 1. 随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。 2. 以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。 Batch_Size 值的选择 ​ 假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。 ​ 既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？ ​ 此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。 在合理范围内，增大Batch_Size有何好处？ 内存利用率提高了，大矩阵乘法的并行化效率提高。 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。 盲目增大 Batch_Size 有何坏处？ 内存利用率提高了，但是内存容量可能撑不住了。 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。 Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。 调节 Batch_Size 对训练效果影响到底如何？ Batch_Size 太小，模型表现效果极其糟糕(error飙升)。 随着 Batch_Size 增大，处理相同数据量的速度越快。 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。 归一化 归一化含义？ 归纳统一样本的统计分布性。归一化在 $ 0-1$ 之间是统计的概率分布，归一化在$ -1--+1$ 之间是统计的坐标分布。 无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。 归一化是统一在 $ 0-1 $ 之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。 另外在数据中常存在奇异样本数据，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小。 为什么要归一化？ 为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。 为了程序运行时收敛加快。 同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。 避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。 保证输出数据中数值小的不被吞食。 为什么归一化能提高求解最优解速度？ ​ 上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。 ​ 当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。 ​ 因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。 3D 图解未归一化 例子： ​ 假设 $ w1 $ 的范围在 $ [-10, 10] $，而 $ w2 $ 的范围在 $ [-100, 100] $，梯度每次都前进 1 单位，那么在 $ w1 $ 方向上每次相当于前进了 $ 1/20 $，而在 $ w2 $ 上只相当于 $ 1/200 $！某种意义上来说，在 $ w2 $ 上前进的步长更小一些,而 $ w1 $ 在搜索过程中会比 $ w2 $ “走”得更快。 ​ 这样会导致，在搜索过程中更偏向于 $ w1 $ 的方向。走出了“L”形状，或者成为“之”字形。 归一化有哪些类型？ 线性归一化 \\[ x^{\\prime} = \\frac{x-min(x)}{max(x) - min(x)} \\] ​ 适用范围：比较适用在数值比较集中的情况。 ​ 缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。 标准差标准化 \\[ x^{\\prime} = \\frac{x-\\mu}{\\sigma} \\] ​ 含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1 其中 $ $ 为所有样本数据的均值，$ $ 为所有样本数据的标准差。 非线性归一化 适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 $ log $、指数，正切等。 局部响应归一化作用 ​ LRN 是一种提高深度学习准确度的技术方法。LRN 一般是在激活、池化函数后的一种方法。 ​ 在 ALexNet 中，提出了 LRN 层，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。 理解局部响应归一化 ​ 局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），其公式如下： \\[ b_{x,y}^i = a_{x,y}^i / (k + \\alpha \\sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\\beta \\] 其中， 1) $ a $：表示卷积层（包括卷积操作和池化操作）后的输出结果，是一个四维数组[batch,height,width,channel]。 batch：批次数(每一批为一张图片)。 height：图片高度。 width：图片宽度。 channel：通道数。可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数，或理解为处理后的图片深度。 $ a_{x,y}^i $ 表示在这个输出结构中的一个位置 $ [a,b,c,d] $，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第 $ a $ 张图的第 $ d $ 个通道下的高度为b宽度为c的点。 $ N $：论文公式中的 $ N $ 表示通道数 (channel)。 $ a \\(，\\) n/2 $， $ k $ 分别表示函数中的 input,depth_radius,bias。参数 $ k, n, , $ 都是超参数，一般设置 $ k=2, n=5, *e-4, $ $ \\(：\\) $ 叠加的方向是沿着通道方向的，即每个点值的平方和是沿着 $ a $ 中的第 3 维 channel 方向的，也就是一个点同方向的前面 $ n/2 $ 个通道（最小为第 $ 0 $ 个通道）和后 $ n/2 $ 个通道（最大为第 $ d-1 $ 个通道）的点的平方和(共 $ n+1 $ 个点)。而函数的英文注解中也说明了把 input 当成是 $ d $ 个 3 维的矩阵，说白了就是把 input 的通道数当作 3 维矩阵的个数，叠加的方向也是在通道方向。 简单的示意图如下： 什么是批归一化（Batch Normalization） ​ 以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ (WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。 ​ 这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。 批归一化（BN）算法的优点 下面我们来说一下BN算法的优点： 1. 减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数； 2. 减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛； 3. 可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在) 4. 破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。 5. 减少梯度消失，加快收敛速度，提高训练精度。 批归一化（BN）算法流程 下面给出 BN 算法在训练时的过程 输入：上一层输出结果 $ X = {x_1, x_2, ..., x_m} $，学习参数 $ , $ 算法流程： 计算上一层输出数据的均值 \\[ \\mu_{\\beta} = \\frac{1}{m} \\sum_{i=1}^m(x_i) \\] 其中，$ m $ 是此次训练样本 batch 的大小。 计算上一层输出数据的标准差 \\[ \\sigma_{\\beta}^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_{\\beta})^2 \\] 归一化处理，得到 \\[ \\hat x_i = \\frac{x_i + \\mu_{\\beta}}{\\sqrt{\\sigma_{\\beta}^2} + \\epsilon} \\] 其中 $ $ 是为了避免分母为 0 而加进去的接近于 0 的很小值 重构，对经过上面归一化处理得到的数据进行重构，得到 \\[ y_i = \\gamma \\hat x_i + \\beta \\] 其中，$ , $ 为可学习参数。 注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ {} $ 和标准差 $ {}^2 $。此时，均值 $ {} $ 是计算所有 batch $ {} $ 值的平均值得到，标准差 $ {}^2 $ 采用每个batch $ {}^2 $ 的无偏估计得到。 批归一化和群组归一化比较 名称 特点 批量归一化（Batch Normalization，以下简称 BN） 可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。 群组归一化 Group Normalization (简称 GN) GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。 比较 在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ;当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。 Weight Normalization和Batch Normalization比较 ​ Weight Normalization 和 Batch Normalization 都属于参数重写（Reparameterization）的方法，只是采用的方式不同。 ​ Weight Normalization 是对网络权值$ W $ 进行 normalization，因此也称为 Weight Normalization； ​ Batch Normalization 是对网络某一层输入数据进行 normalization。 ​ Weight Normalization相比Batch Normalization有以下三点优势： Weight Normalization 通过重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入 minbatch 的依赖，适用于 RNN（LSTM）网络（Batch Normalization 不能直接用于RNN，进行 normalization 操作，原因在于：1) RNN 处理的 Sequence 是变长的；2) RNN 是基于 time step 计算，如果直接使用 Batch Normalization 处理，需要保存每个 time step 下，mini btach 的均值和方差，效率低且占内存）。 Batch Normalization 基于一个 mini batch 的数据计算均值和方差，而不是基于整个 Training set 来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization 不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。相反，Weight Normalization 对通过标量 $ g $ 和向量 $ v $ 对权重 $ W $ 进行重写，重写向量 $ v $ 是固定的，因此，基于 Weight Normalization 的 Normalization 可以看做比 Batch Normalization 引入更少的噪声。 不需要额外的存储空间来保存 mini batch 的均值和方差，同时实现 Weight Normalization 时，对深度学习网络进行正向信号传播和反向梯度计算带来的额外计算开销也很小。因此，要比采用 Batch Normalization 进行 normalization 操作时，速度快。 但是 Weight Normalization 不具备 Batch Normalization 把网络每一层的输出 Y 固定在一个变化范围的作用。因此，采用 Weight Normalization 进行 Normalization 时需要特别注意参数初始值的选择。 Batch Normalization在什么时候用比较合适？ （贡献者：黄钦建－华南理工大学） ​ 在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。 ​ BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。 预训练与微调(fine tuning) 为什么无监督预训练可以帮助深度学习？ 深度网络存在问题: 网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。 多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解； 梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。 解决方法： ​ 逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。 经过预训练最终能得到比较好的局部最优解。 什么是模型微调fine tuning ​ 用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning). 模型的微调举例说明： ​ 我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。 微调时候网络参数是否更新？ 答案：会更新。 finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。 直接训练是按照网络定义指定的方式初始化。 finetune是用你已经有的参数文件来初始化。 fine-tuning 模型的三种状态 状态一：只预测，不训练。 特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效； 状态二：训练，但只训练最后分类层。 特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。 状态三：完全训练，分类层+之前卷积层都训练 特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。 权重偏差初始化 全都初始化为 0 偏差初始化陷阱： 都初始化为 0。 产生陷阱原因：因为并不知道在训练神经网络中每一个权重最后的值，但是如果进行了恰当的数据归一化后，我们可以有理由认为有一半的权重是正的，另一半是负的。令所有权重都初始化为 0，如果神经网络计算出来的输出值是一样的，神经网络在进行反向传播算法计算出来的梯度值也一样，并且参数更新值也一样。更一般地说，如果权重初始化为同一个值，网络就是对称的。 形象化理解：在神经网络中考虑梯度下降的时候，设想你在爬山，但身处直线形的山谷中，两边是对称的山峰。由于对称性，你所在之处的梯度只能沿着山谷的方向，不会指向山峰；你走了一步之后，情况依然不变。结果就是你只能收敛到山谷中的一个极大值，而走不到山峰上去。 全都初始化为同样的值 ​ 偏差初始化陷阱： 都初始化为一样的值。 ​ 以一个三层网络为例： 首先看下结构 它的表达式为： \\[ a_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)}) \\] \\[ a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)}) \\] \\[ a_3^{(2)} = f(W_{31}^{(1)} x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)}) \\] \\[ h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)} a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)}) \\] \\[ xa_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + \\] 如果每个权重都一样，那么在多层网络中，从第二层开始，每一层的输入值都是相同的了也就是$ a1=a2=a3=.... $，既然都一样，就相当于一个输入了，为啥呢？？ 如果是反向传递算法（如果这里不明白请看上面的连接），其中的偏置项和权重项的迭代的偏导数计算公式如下 $ $ 的计算公式 \\[ \\delta_i^{(l)} = (\\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \\delta_j^{(l+1)} ) f^{\\prime}(z_i^{(l)}) \\] 如果用的是 sigmoid 函数 \\[ f^{\\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)}) \\] 把后两个公式代入，可以看出所得到的梯度下降法的偏导相同，不停的迭代，不停的相同，不停的迭代，不停的相同......，最后就得到了相同的值（权重和截距）。 初始化为小的随机数 ​ 将权重初始化为很小的数字是一个普遍的打破网络对称性的解决办法。这个想法是，神经元在一开始都是随机的、独一无二的，所以它们会计算出不同的更新，并将自己整合到整个网络的各个部分。一个权重矩阵的实现可能看起来像 $ W=0.01∗np.random.randn(D,H) $，其中 randn 是从均值为 0 的单位标准高斯分布进行取样。通过这个公式(函数)，每个神经元的权重向量初始化为一个从多维高斯分布取样的随机向量，所以神经元在输入空间中指向随机的方向(so the neurons point in random direction in the input space). 应该是指输入空间对于随机方向有影响)。其实也可以从均匀分布中来随机选取小数，但是在实际操作中看起来似乎对最后的表现并没有太大的影响。 ​ 备注：并不是数字越小就会表现的越好。比如，如果一个神经网络层的权重非常小，那么在反向传播算法就会计算出很小的梯度(因为梯度 gradient 是与权重成正比的)。在网络不断的反向传播过程中将极大地减少“梯度信号”，并可能成为深层网络的一个需要注意的问题。 用 $ 1/n $ 校准方差 ​ 上述建议的一个问题是，随机初始化神经元的输出的分布有一个随输入量增加而变化的方差。结果证明，我们可以通过将其权重向量按其输入的平方根(即输入的数量)进行缩放，从而将每个神经元的输出的方差标准化到 1。也就是说推荐的启发式方法 (heuristic) 是将每个神经元的权重向量按下面的方法进行初始化: $ w=np.random.randn(n)/n $，其中 n 表示输入的数量。这保证了网络中所有的神经元最初的输出分布大致相同，并在经验上提高了收敛速度。 稀疏初始化(Sparse Initialazation) ​ 另一种解决未校准方差问题的方法是把所有的权重矩阵都设为零，但是为了打破对称性，每个神经元都是随机连接地(从如上面所介绍的一个小的高斯分布中抽取权重)到它下面的一个固定数量的神经元。一个典型的神经元连接的数目可能是小到 10 个。 初始化偏差 ​ 将偏差初始化为零是可能的，也是很常见的，因为非对称性破坏是由权重的小随机数导致的。因为 ReLU 具有非线性特点，所以有些人喜欢使用将所有的偏差设定为小的常数值如 0.01，因为这样可以确保所有的 ReLU 单元在最开始就激活触发(fire)并因此能够获得和传播一些梯度值。然而，这是否能够提供持续的改善还不太清楚(实际上一些结果表明这样做反而使得性能更加糟糕)，所以更通常的做法是简单地将偏差初始化为 0. 学习率 学习率的作用 ​ 在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost 来估计模型的参数。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。 ​ 在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。故而如何对学习率的更新成为了研究者的关注点。 ​ 在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减 学习率衰减常用参数有哪些 参数名称 参数说明 learning_rate 初始学习率 global_step 用于衰减计算的全局步数，非负，用于逐步计算衰减指数 decay_steps 衰减步数，必须是正值，决定衰减周期 decay_rate 衰减率 end_learning_rate 最低的最终学习率 cycle 学习率下降后是否重新上升 alpha 最小学习率 num_periods 衰减余弦部分的周期数 initial_variance 噪声的初始方差 variance_decay 衰减噪声的方差 分段常数衰减 ​ 分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。 指数衰减 ​ 以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为： ​ 这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如下图所示，绿色的为学习率随 训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。 自然指数衰减 ​ 它与指数衰减方式相似，不同的在于它的衰减底数是\\(e\\)，故而其收敛的速度更快，一般用于相对比较 容易训练的网络，便于较快的收敛，其更新规则如下 ​ 下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。 多项式衰减 ​ 应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照 给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示。 ​ 需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示.它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。 ​ 如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。 余弦衰减 ​ 余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示： ​ 如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式。 Dropout 系列问题 为什么要正则化？ 深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。 如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。 为什么正则化有利于预防过拟合？ 左图是高偏差，右图是高方差，中间是Just Right，这几张图我们在前面课程中看到过。 理解dropout正则化 ​ Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？ ​ 直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。 dropout率的选择 经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。 dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8） 对参数 $ w $ 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。 球形半径 $ c $ 是一个需要调整的参数，可以使用验证集进行参数调优。 dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。 使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。 dropout有什么缺点？ ​ dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。 深度学习中常用的数据增强方法？ （贡献者：黄钦建－华南理工大学） Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）； PCA Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering； Random Scale：尺度变换； Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换； Horizontal/Vertical Flip：水平/垂直翻转； Shift：平移变换； Rotation/Reflection：旋转/仿射变换； Noise：高斯噪声、模糊处理； Label Shuffle：类别不平衡数据的增广； 如何理解 Internal Covariate Shift？ （贡献者：黄钦建－华南理工大学） ​ 深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。 ​ Google 将这一现象总结为 Internal Covariate Shift，简称 ICS。 什么是 ICS 呢？ ​ 大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。 ​ 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。 那么ICS会导致什么问题？ 简而言之，每个神经元的输入数据不再是“独立同分布”。 其一，上层参数需要不断适应新的输入数据分布，降低学习速度。 其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。 其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。 参考文献 [1] Rosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain.[J]. Psychological Review, 1958, 65(6):386-408. [2] Duvenaud D , Rippel O , Adams R P , et al. Avoiding pathologies in very deep networks[J]. Eprint Arxiv, 2014:202-210. [3] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors[J]. Cognitive modeling, 1988, 5(3): 1. [4] Hecht-Nielsen R. Theory of the backpropagation neural network[M]//Neural networks for perception. Academic Press, 1992: 65-93. [5] Felice M. Which deep learning network is best for you?| CIO[J]. 2017. [6] Conneau A, Schwenk H, Barrault L, et al. Very deep convolutional networks for natural language processing[J]. arXiv preprint arXiv:1606.01781, 2016, 2. [7] Ba J, Caruana R. Do deep nets really need to be deep?[C]//Advances in neural information processing systems. 2014: 2654-2662. [8] Nielsen M A. Neural networks and deep learning[M]. USA: Determination press, 2015. [9] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016. [10] 周志华. 机器学习[M].清华大学出版社, 2016. [11] Kim J, Kwon Lee J, Mu Lee K. Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 1646-1654. [12] Chen Y, Lin Z, Zhao X, et al. Deep learning-based classification of hyperspectral data[J]. IEEE Journal of Selected topics in applied earth observations and remote sensing, 2014, 7(6): 2094-2107. [13] Domhan T, Springenberg J T, Hutter F. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves[C]//Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015. [14] Maclaurin D, Duvenaud D, Adams R. Gradient-based hyperparameter optimization through reversible learning[C]//International Conference on Machine Learning. 2015: 2113-2122. [15] Srivastava R K, Greff K, Schmidhuber J. Training very deep networks[C]//Advances in neural information processing systems. 2015: 2377-2385. [16] Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305. [17] Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28th international conference on machine learning (ICML-11). 2011: 689-696. [18] Deng L, Yu D. Deep learning: methods and applications[J]. Foundations and Trends® in Signal Processing, 2014, 7(3–4): 197-387. [19] Erhan D, Bengio Y, Courville A, et al. Why does unsupervised pre-training help deep learning?[J]. Journal of Machine Learning Research, 2010, 11(Feb): 625-660. [20] Dong C, Loy C C, He K, et al. Learning a deep convolutional network for image super resolution[C]//European conference on computer vision. Springer, Cham, 2014: 184-199. [21] 郑泽宇，梁博文，顾思宇.TensorFlow：实战Google深度学习框架（第2版）[M].电子工业出版社,2018. [22] 焦李成. 深度学习优化与识别[M].清华大学出版社,2017. [23] 吴岸城. 神经网络与深度学习[M].电子工业出版社,2016. [24] Wei, W.G.H., Liu, T., Song, A., et al. (2018) An Adaptive Natural Gradient Method with Adaptive Step Size in Multi\u0002layer Perceptrons. Chinese Automation Congress, 1593-1597. [25] Y Feng, Y Li.An Overview of Deep Learning Optimization Methods and Learning Rate Attenuation Methods[J].Hans Journal of Data Mining,2018,8(4),186-200.","link":"/posts/3233011741.html"},{"title":"模型压缩及移动端部署","text":"​ 深度神经网络在人工智能的应用中，包括语音识别、计算机视觉、自然语言处理等各方面，在取得巨大成功的同时，这些深度神经网络需要巨大的计算开销和内存开销，严重阻碍了资源受限下的使用。本章总结了模型压缩、加速一般原理和方法，以及在移动端如何部署。 模型压缩理解 ​ 模型压缩是指利用数据集对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署再受限的硬件环境中。 为什么需要模型压缩和加速？ （1）随着AI技术的飞速发展，越来越多的公司希望在自己的移动端产品中注入AI能力。 （2）对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要。 （3）模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。 （4）复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。 （5）智能设备的流行提供了内存、CPU、能耗和宽带等资源，使得深度学习模型部署在智能移动设备上变得可行。 （6）高效的深度学习方法可以有效的帮助嵌入式设备、分布式系统完成复杂工作，在移动端部署深度学习有很重要的意义。 模型压缩的必要性及可行性 必要性 首先是资源受限，其次在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。 可行性 模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。论文提出，很多的深度神经网络仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。 最终目的 最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测 目前有哪些深度学习模型压缩方法？ ​ 目前深度学习模型压缩方法主要分为更精细化模型设计、模型裁剪、核的稀疏化、量化、低秩分解、迁移学习等方法，而这些方法又可分为前端压缩和后端压缩。 前端压缩和后端压缩对比 对比项目 前端压缩 后端压缩 含义 不会改变原始网络结构的压缩技术 会大程度上改变原始网络结构的压缩技术 主要方法 知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝 低秩近似、未加限制的剪枝、参数量化、二值网络 实现难度 较简单 较难 是否可逆 可逆 不可逆 成熟应用 剪枝 低秩近似、参数量化 待发展应用 知识蒸馏 二值网络 网络剪枝 深度学习模型因其稀疏性，可以被裁剪为结构精简的网络模型，具体包括结构性剪枝与非结构性剪枝。 事项 特点 举例 非结构化剪枝 通常是连接级、细粒度的剪枝方法，精度相对较高，但依赖于特定算法库或硬件平台的支持 Deep Compression [5], Sparse-Winograd [6] 算法等； 结构化剪枝 是filter级或layer级、粗粒度的剪枝方法，精度相对较低，但剪枝策略更为有效，不需要特定算法库或硬件平台的支持，能够直接在成熟深度学习框架上运行。 如局部方式的、通过layer by layer方式的、最小化输出FM重建误差的Channel Pruning [7], ThiNet [8], Discrimination-aware Channel Pruning [9]；全局方式的、通过训练期间对BN层Gamma系数施加L1正则约束的Network Slimming [10]；全局方式的、按Taylor准则对Filter作重要性排序的Neuron Pruning [11]；全局方式的、可动态重新更新pruned filters参数的剪枝方法 [12];https://blog.csdn.net/baidu_31437863/article/details/84474847 如果按剪枝粒度分，从粗到细，可分为中间隐含层剪枝、通道剪枝、卷积核剪枝、核内剪枝、单个权重剪枝。下面按照剪枝粒度的分类从粗（左）到细（右）。 （a）层间剪枝 （b）特征图剪枝 （c）k*k核剪枝 （d）核内剪枝 事项 特点 单个权重粒度 早期 Le Cun[16]提出的 OBD(optimal brain damage)将网络中的任意权重参数都看作单个参数,能够有效地提高预测准确率,却不能减小运行时间;同时,剪枝代价过高,只适用于小网络 核内权重粒度 网络中的任意权重被看作是单个参数并进行随机非结构化剪枝,该粒度的剪枝导致网络连接不规整,需要通过稀疏表达来减少内存占用,进而导致在前向传播预测时,需要大量的条件判断和额外空间来标明零或非零参数的位置,因此不适用于并行计算 卷积核粒度与通道粒度 卷积核粒度与通道粒度属于粗粒度剪枝,不依赖任何稀疏卷积计算库及专用硬件;同时,能够在获得高压缩率的同时大量减小测试阶段的计算时间.由 从剪枝目标上分类，可分为减少参数/网络复杂度、减少过拟合/增加泛化能力/提高准确率、减小部署运行时间/提高网络效率及减小训练时间等。 典型剪枝方法对比 剪枝方法 修剪对象 修剪方式 效果 Deep Compression 权重 随机修剪 50倍压缩 Structured Pruning 权重 组稀疏+排他性稀疏 性能提升 Network Slimming 特征图通道 根据尺度因子修剪 节省计算资源 mProp 梯度 修剪幅值小的梯度 加速 网络蒸馏 ​ 网络精馏是指利用大量未标记的迁移数据(transfer data),让小模型去拟合大模型,从而让小模型学到与大模型相似的函数映射.网络精馏可以看成在同一个域上迁移学习[34]的一种特例,目的是获得一个比原模型更为精简的网络,整体的框架图如图 4所示. 前端压缩 （1）知识蒸馏 ​ 一个复杂模型可由多个简单模型或者强约束条件训练得到。复杂模型特点是性能好，但其参数量大，计算效率低。小模型特点是计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中,使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。 （2）紧凑的模型结构设计 ​ 紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个3x3的卷积替换一个5x5的卷积、使用深度可分离卷积等等方式降低计算参数量。 目前很多网络基于模块化设计思想，在深度和宽度两个维度上都很大，导致参数冗余。因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。 （3）滤波器层面的剪枝 ​ 滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。 具体操作是在训练时使用稀疏约束（加入权重的稀疏正则项，引导模型的大部分权重趋向于0）。完成训练后，剪去滤波器上的这些 0 。 ​ 优点是简单，缺点是剪得不干净，非结构化剪枝会增加内存访问成本。 后端压缩 （1）低秩近似 ​ 在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。 事项 特点 优点 可以降低存储和计算消耗；一般可以压缩2-3倍；精度几乎没有损失； 缺点 模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能 ； 超参数的数量随着网络层数的增加呈线性变化趋势，例如中间层的特征通道数等等。 随着模型复杂度的提升，搜索空间急剧增大。 （2）未加限制的剪枝 ​ 完成训练后，不加限制地剪去那些冗余参数。 事项 特点 优点 保持模型性能不损失的情况下，减少参数量9-11倍； 剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力； 缺点 极度依赖专门的运行库和特殊的运行平台，不具有通用性； 压缩率过大时，破坏性能； （3）参数量化 ​ 神经网络的参数类型一般是32位浮点型，使用较小的精度代替32位所表示的精度。或者是将多个权重映射到同一数值，权重共享。量化其实是一种权值共享的策略。量化后的权值张量是一个高度稀疏的有很多共享权值的矩阵，对非零参数，我们还可以进行定点压缩，以获得更高的压缩率。 事项 特点 优点 模型性能损失很小，大小减少8-16倍； 缺点 压缩率大时，性能显著下降； 依赖专门的运行库，通用性较差； 举例 二值化网络：XNORnet [13], ABCnet with Multiple Binary Bases [14], Bin-net with High-Order Residual Quantization [15], Bi-Real Net [16]；三值化网络：Ternary weight networks [17], Trained Ternary Quantization [18]； W1-A8 或 W2-A8量化： Learning Symmetric Quantization [19]； INT8量化：TensorFlow-lite [20], TensorRT [21]； 其他（非线性）：Intel INQ [22], log-net, CNNPack [23] 等； 原文：https://blog.csdn.net/baidu_31437863/article/details/84474847 | | 总结 | 最为典型就是二值网络、XNOR网络等。其主要原理就是采用1bit对网络的输入、权重、响应进行编码。减少模型大小的同时，原始网络的卷积操作可以被bit-wise运算代替，极大提升了模型的速度。但是，如果原始网络结果不够复杂（模型描述能力），由于二值网络会较大程度降低模型的表达能力。因此现阶段有相关的论文开始研究n-bit编码方式成为n值网络或者多值网络或者变bit、组合bit量化来克服二值网络表达能力不足的缺点。 | （4）二值网络 ​ 相对量化更为极致，对于32bit浮点型数用1bit二进制数-1或者1表示，可大大减小模型尺寸。 事项 特点 优点 网络体积小，运算速度快，有时可避免部分网络的overfitting 缺点 二值神经网络损失的信息相对于浮点精度是非常大；粗糙的二值化近似导致训练时模型收敛速度非常慢 （5）三值网络 事项 特点 优点 相对于二值神经网络，三值神经网络(Ternary Weight Networks)在同样的模型结构下可以达到成百上千倍的表达能力提升;并且，在计算时间复杂度上，三元网络和二元网络的计算复杂度是一样的。例如，对于ResNet-18层网络中最常出现的卷积核(3x3大小)，二值神经网络模型最多可以表达2的3x3次方(=512)种结构，而三元神经网络则可以表达3的3x3次方(=19683)种卷积核结构。在表达能力上，三元神经网络相对要高19683/512 = 38倍。因此，三元神经网络模型能够在保证计算复杂度很低的情况下大幅的提高网络的表达能力，进而可以在精度上相对于二值神经网络有质的飞跃。另外，由于对中间信息的保存更多，三元神经网络可以极大的加快网络训练时的收敛速度，从而更快、更稳定的达到最优的结果。 低秩分解 基于低秩分解的深度神经网络压缩与加速的核心思想是利用矩阵或张量分解技术估计并分解深度模型中的原始卷积核．卷积计算是整个卷积神经网络中计算复杂 度 最 高 的 计 算 操 作，通 过 分 解４Ｄ 卷积核张量，可以有效地减少模型内部的冗余性．此外对于２Ｄ的全 连 接 层 矩 阵 参 数，同样可以利用低秩分解技术进行处理．但由于卷积层与全连接层的分解方式不同，本文分别从卷积层和全连接层２个不同角度回顾与分析低秩分解技术在深度神经网络中的应用. 在２０１３年，Ｄｅｎｉｌ等人［５７］从理论上利用低秩分解的技术并分析了深度神经网络存在大量的冗余信 息，开创了基于低秩分解的深度网络模型压缩与加速的新思路．如图７所示，展示了主流的张量分解后卷积 计 算． (出自《深度神经网络压缩与加速综述》) 总体压缩效果评价指标有哪些？ ​ 网络压缩评价指标包括运行效率、参数压缩率、准确率.与基准模型比较衡量性能提升时,可以使用提升倍数(speedup)或提升比例(ratio)。 评价指标 特点 准确率 目前,大部分研究工作均会测量 Top-1 准确率,只有在 ImageNet 这类大型数据集上才会只用 Top-5 准确率.为方便比较 参数压缩率 统计网络中所有可训练的参数,根据机器浮点精度转换为字节(byte)量纲,通常保留两位有效数字以作近似估计. 运行效率 可以从网络所含浮点运算次数(FLOP)、网络所含乘法运算次数(MULTS)或随机实验测得的网络平均前向传播所需时间这 3 个角度来评价 几种轻量化网络结构对比 网络结构 TOP1 准确率/% 参数量/M CPU运行时间/ms MobileNet V1 70.6 4.2 123 ShuffleNet(1.5) 69.0 2.9 - ShuffleNet(x2) 70.9 4.4 - MobileNet V2 71.7 3.4 80 MobileNet V2(1.4) 74.7 6.9 149 网络压缩未来研究方向有哪些？ 网络剪枝、网络精馏和网络分解都能在一定程度上实现网络压缩的目的.回归到深度网络压缩的本质目的上,即提取网络中的有用信息,以下是一些值得研究和探寻的方向. (1) 权重参数对结果的影响度量.深度网络的最终结果是由全部的权重参数共同作用形成的,目前,关于单个卷积核/卷积核权重的重要性的度量仍然是比较简单的方式,尽管文献[14]中给出了更为细节的分析,但是由于计算难度大,并不实用.因此,如何通过更有效的方式来近似度量单个参数对模型的影响,具有重要意义. (2) 学生网络结构的构造.学生网络的结构构造目前仍然是由人工指定的,然而,不同的学生网络结构的训练难度不同,最终能够达到的效果也有差异.因此,如何根据教师网络结构设计合理的网络结构在精简模型的条件下获取较高的模型性能,是未来的一个研究重点. (3) 参数重建的硬件架构支持.通过分解网络可以无损地获取压缩模型,在一些对性能要求高的场景中是非常重要的.然而,参数的重建步骤会拖累预测阶段的时间开销,如何通过硬件的支持加速这一重建过程,将是未来的一个研究方向. (4) 任务或使用场景层面的压缩.大型网络通常是在量级较大的数据集上训练完成的,比如,在 ImageNet上训练的模型具备对 1 000 类物体的分类,但在一些具体场景的应用中,可能仅需要一个能识别其中几类的小型模型.因此,如何从一个全功能的网络压缩得到部分功能的子网络,能够适应很多实际应用场景的需求. (5) 网络压缩效用的评价.目前,对各类深度网络压缩算法的评价是比较零碎的,侧重于和被压缩的大型网络在参数量和运行时间上的比较.未来的研究可以从提出更加泛化的压缩评价标准出发,一方面平衡运行速度和模型大小在不同应用场景下的影响;另一方面,可以从模型本身的结构性出发,对压缩后的模型进行评价. （出自《深度网络模型压缩综述》） 目前有哪些深度学习模型优化加速方法？ https://blog.csdn.net/nature553863/article/details/81083955 模型优化加速方法 模型优化加速能够提升网络的计算效率，具体包括： （1）Op-level的快速算法：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等； （2）Layer-level的快速算法：Sparse-block net [1] 等； （3）优化工具与库：TensorRT (Nvidia), Tensor Comprehension (Facebook) 和 Distiller (Intel) 等； 原文：https://blog.csdn.net/nature553863/article/details/81083955 TensorRT加速原理 https://blog.csdn.net/xh_hit/article/details/79769599 ​ 在计算资源并不丰富的嵌入式设备上，TensorRT之所以能加速神经网络的的推断主要得益于两点： 首先是TensorRT支持int8和fp16的计算，通过在减少计算量和保持精度之间达到一个理想的trade-off，达到加速推断的目的。 更为重要的是TensorRT对于网络结构进行了重构和优化，主要体现在一下几个方面。 TensorRT通过解析网络模型将网络中无用的输出层消除以减小计算。 对于网络结构的垂直整合，即将目前主流神经网络的Conv、BN、Relu三个层融合为了一个层，例如将图1所示的常见的Inception结构重构为图2所示的网络结构。 对于网络结构的水平组合，水平组合是指将输入为相同张量和执行相同操作的层融合一起，例如图2向图3的转化。 ​ 以上3步即是TensorRT对于所部署的深度学习网络的优化和重构，根据其优化和重构策略，第一和第二步适用于所有的网络架构，但是第三步则对于含有Inception结构的神经网络加速效果最为明显。 ​ Tips: 想更好地利用TensorRT加速网络推断，可在基础网络中多采用Inception模型结构，充分发挥TensorRT的优势。 TensorRT如何优化重构模型？ 条件 方法 若训练的网络模型包含TensorRT支持的操作 1、对于Caffe与TensorFlow训练的模型，若包含的操作都是TensorRT支持的，则可以直接由TensorRT优化重构 2、对于MXnet, PyTorch或其他框架训练的模型，若包含的操作都是TensorRT支持的，可以采用TensorRT API重建网络结构，并间接优化重构； 若训练的网络模型包含TensorRT不支持的操作 1、TensorFlow模型可通过tf.contrib.tensorrt转换，其中不支持的操作会保留为TensorFlow计算节点； 2、不支持的操作可通过Plugin API实现自定义并添加进TensorRT计算图； 3、将深度网络划分为两个部分，一部分包含的操作都是TensorRT支持的，可以转换为TensorRT计算图。另一部则采用其他框架实现，如MXnet或PyTorch； TensorRT加速效果如何？ 以下是在TitanX (Pascal)平台上，TensorRT对大型分类网络的优化加速效果： Network Precision Framework/GPU:TitanXP Avg.Time(Batch=8,unit:ms) Top1 Val.Acc.(ImageNet-1k) Resnet50 fp32 TensorFlow 24.1 0.7374 Resnet50 fp32 MXnet 15.7 0.7374 Resnet50 fp32 TRT4.0.1 12.1 0.7374 Resnet50 int8 TRT4.0.1 6 0.7226 Resnet101 fp32 TensorFlow 36.7 0.7612 Resnet101 fp32 MXnet 25.8 0.7612 Resnet101 fp32 TRT4.0.1 19.3 0.7612 Resnet101 int8 TRT4.0.1 9 0.7574 影响神经网络速度的4个因素（再稍微详细一点） FLOPs(FLOPs就是网络执行了多少multiply-adds操作)； MAC(内存访问成本)； 并行度(如果网络并行度高，速度明显提升)； 计算平台(GPU，ARM) 压缩和加速方法如何选择？ ​ １）对于在线计算内存存储有限的应用场景或设备，可以选择参数共享和参数剪枝方法，特别是二值量化权值和激活、结构化剪枝．其他方法虽然能够有效的压缩模型中的权值参数，但无法减小计算中隐藏的内存大小（如特征图）． ​ ２）如果在应用中用到的紧性模型需要利用预训练模型，那么参数剪枝、参数共享以及低秩分解将成为首要考虑的方法．相反地，若不需要借助预训练模型，则可以考虑紧性滤波设计及知识蒸馏方法． ​ ３）若需要一次性端对端训练得到压缩与加速后模型，可以利用基于紧性滤波设计的深度神经网络压缩与加速方法． ​ ４）一般情况下，参数剪枝，特别是非结构化剪枝，能大大压缩模型大小，且不容易丢失分类精度．对于需要稳定的模型分类的应用，非结构化剪枝成为首要选择． ​ ５）若采用的数据集较小时，可以考虑知识蒸馏方法．对于小样本的数据集，学生网络能够很好地迁移教师模型的知识，提高学生网络的判别性． ​ ６）主流的５个深度神经网络压缩与加速算法相互之间是正交的，可以结合不同技术进行进一步的压缩与加速．如：韩 松 等 人［３０］结合了参数剪枝和参数共享；温伟等人［６４］以及 Ａｌｖａｒｅｚ等人［８５］结合了参数剪枝和低秩分解．此外对于特定的应用场景，如目标检测，可以对卷积层和全连接层使用不同的压缩与加速技术分别处理． 参考《深度神经网络压缩与加速综述》 改变网络结构设计为什么会实现模型压缩、加速？ Group convolution ​ Group convolution最早出现在AlexNet中，是为了解决单卡显存不够，将网络部署到多卡上进行训练而提出。Group convolution可以减少单个卷积1/g的参数量。如何计算的呢？ ​ 假设 输入特征的的维度为\\(H*W*C_1\\); 卷积核的维度为\\(H_1*W_1*C_1\\)，共\\(C_2\\)个； 输出特征的维度为\\(H_1*W_1*C_2\\) 。 传统卷积计算方式如下： 传统卷积运算量为： \\[ A = H*W * h1 * w1 * c1 * c2 \\] Group convolution是将输入特征的维度c1分成g份，每个group对应的channel数为c1/g，特征维度H * W * c1/g；，每个group对应的卷积核的维度也相应发生改变为h1 * w1 * c1/9，共c2/g个；每个group相互独立运算，最后将结果叠加在一起。 Group convolution计算方式如下： Group convolution运算量为： \\[ B = H * W * h1 * w1 * c1/g * c2/g * g \\] Group卷积相对于传统卷积的运算量： \\[ \\dfrac{B}{A} = \\dfrac{ H * W * h1 * w1 * c1/g * c2/g * g}{H * W * h1 * w1 * c1 * c2} = \\dfrac{1}{g} \\] 由此可知：group卷积相对于传统卷积减少了1/g的参数量。 Depthwise separable convolution Depthwise separable convolution是由depthwise conv和pointwise conv构成。 depthwise conv(DW)有效减少参数数量并提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。 pointwise conv(PW)实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。 假设输入特征的的维度为H * W * c1；卷积核的维度为h1 * w1 * c1，共c2个；输出特征的维度为 H1 * W1 * c2。 传统卷积计算方式如下： 传统卷积运算量为： \\[ A = H * W * h1 * w1 * c1 * c2 \\] DW卷积的计算方式如下： DW卷积运算量为： \\[ B_DW = H * W * h1 * w1 * 1 * c1 \\] PW卷积的计算方式如下： \\[ B_PW = H_m * W_m * 1 * 1 * c_1 * c_2 \\] Depthwise separable convolution运算量为： \\[ B = B_DW + B_PW \\] Depthwise separable convolution相对于传统卷积的运算量： \\[ \\begin{align*} \\dfrac{B}{A} &amp;= \\dfrac{ H * W * h_1 * w_1 * 1 * c_1 + H_m * W_m * 1 * 1 * c_1 * c_2}{H * W * h1 * w1 * c_1 * c_2} \\\\ &amp;= \\dfrac{1}{c_2} + \\dfrac{1}{h_1 * w_1} \\end{align*} \\] 由此可知，随着卷积通道数的增加，Depthwise separable convolution的运算量相对于传统卷积更少。 输入输出的channel相同时，MAC最小 卷积层的输入和输出特征通道数相等时MAC最小，此时模型速度最快。 假设feature map的大小为h*w，输入通道\\(c_1\\)，输出通道\\(c_2\\)。 已知： \\[ FLOPs = B = h * w * c1 * c2 =&gt; c1 * c2 = \\dfrac{B}{h * w} \\] \\[ MAC = h * w * (c1 + c2) + c1 * c2 \\] \\[ =&gt; MAC \\geq 2 * h * w \\sqrt{\\dfrac{B}{h * w}} + \\dfrac{B}{h * w} \\] 根据均值不等式得到(c1-c2)^2&gt;=0，等式成立的条件是c1=c2，也就是输入特征通道数和输出特征通道数相等时，在给定FLOPs前提下，MAC达到取值的下界。 减少组卷积的数量 过多的group操作会增大MAC，从而使模型速度变慢 由以上公式可知，group卷积想比与传统的卷积可以降低计算量，提高模型的效率；如果在相同的FLOPs时，group卷积为了满足FLOPs会是使用更多channels，可以提高模型的精度。但是随着channel数量的增加，也会增加MAC。 FLOPs： \\[ B = \\dfrac{h * w * c1 * c2}{g} \\] MAC： \\[ MAC = h * w * (c1 + c2) + \\dfrac{c1 * c2}{g} \\] 由MAC，FLOPs可知： \\[ MAC = h * w * c1 + \\dfrac{B*g}{c1} + \\dfrac{B}{h * w} \\] 当FLOPs固定(B不变)时，g越大，MAC越大。 减少网络碎片化程度(分支数量) 模型中分支数量越少，模型速度越快 此结论主要是由实验结果所得。 以下为网络分支数和各分支包含的卷积数目对神经网络速度的影响。 实验中使用的基本网络结构，分别将它们重复10次，然后进行实验。实验结果如下： 由实验结果可知，随着网络分支数量的增加，神经网络的速度在降低。网络碎片化程度对GPU的影响效果明显，对CPU不明显，但是网络速度同样在降低。 减少元素级操作 元素级操作所带来的时间消耗也不能忽视 ReLU ，Tensor 相加，Bias相加的操作，分离卷积（depthwise convolution）都定义为元素级操作。 FLOPs大多数是对于卷积计算而言的，因为元素级操作的FLOPs相对要低很多。但是过的元素级操作也会带来时间成本。ShuffleNet作者对ShuffleNet v1和MobileNet v2的几种层操作的时间消耗做了分析，发现元素级操作对于网络速度的影响也很大。 常用的轻量级网络有哪些？ SequeezeNet SqueenzeNet出自F. N. Iandola, S.Han等人发表的论文《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5MB model size》，作者在保证精度不损失的同时，将原始AlexNet压缩至原来的510倍。 设计思想 在网络结构设计方面主要采取以下三种方式： * 用1*1卷积核替换3*3卷积 * 理论上一个1*1卷积核的参数是一个3*3卷积核的1/9，可以将模型尺寸压缩9倍。 * 减小3*3卷积的输入通道数 * 根据上述公式，减少输入通道数不仅可以减少卷积的运算量，而且输入通道数与输出通道数相同时还可以减少MAC。 * 延迟降采样 * 分辨率越大的输入能够提供更多特征的信息，有利于网络的训练判断，延迟降采样可以提高网络精度。 #### 网络架构 SqueezeNet提出一种多分支结构——fire model，其中是由Squeeze层和expand层构成。Squeeze层是由s1个1*1卷积组成，主要是通过1*1的卷积降低expand层的输入维度；expand层利用e1个1*1和e3个3*3卷积构成多分支结构提取输入特征，以此提高网络的精度(其中e1=e3=4*s1)。 SqueezeNet整体网络结构如下图所示： ####实验结果 不同压缩方法在ImageNet上的对比实验结果 由实验结果可知，SqueezeNet不仅保证了精度，而且将原始AlexNet从240M压缩至4.8M，压缩50倍，说明此轻量级网络设计是可行。 MobileNet MobileNet 是Google团队于CVPR-2017的论文《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》中针对手机等嵌入式设备提出的一种轻量级的深层神经网络，该网络结构在VGG的基础上使用DW+PW的组合，在保证不损失太大精度的同时，降低模型参数量。 #### 设计思想 * 采用深度可分离卷积代替传统卷积 * 采用DW卷积在减少参数数量的同时提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。 * 采用PW卷积实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。 * 使用stride=2的卷积替换pooling * 直接在卷积时利用stride=2完成了下采样，从而节省了需要再去用pooling再去进行一次下采样的时间，可以提升运算速度。同时，因为pooling之前需要一个stride=1的 conv，而与stride=2 conv的计算量想比要高近4倍(个人理解)。 #### 网络架构 * DW conv和PW conv MobileNet的网络架构主要是由DW conv和PW conv组成，相比于传统卷积可以降低\\(\\dfrac{1}{N} + \\dfrac{1}{Dk}​\\)倍的计算量。 标准卷积与DW conv和PW conv如图所示: 深度可分离卷积与传统卷积运算量对比： 网络结构： MobileNets的架构 实验结果 由上表可知，使用相同的结构，深度可分离卷积虽然准确率降低1%，但是参数量减少了6/7。 MobileNet-v2 MobileNet-V2是2018年1月公开在arXiv上论文《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》，是对MobileNet-V1的改进，同样是一个轻量化卷积神经网络。 #### 设计思想 * 采用Inverted residuals * 为了保证网络可以提取更多的特征，在residual block中第一个1*1 Conv和33 DW Conv之前进行通道扩充 Linear bottlenecks * 为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu * stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut 网络架构 Inverted residuals ResNet中Residuals block先经过1*1的Conv layer，把feature map的通道数降下来，再经过3*3 Conv layer，最后经过一个1*1 的Conv layer，将feature map 通道数再“扩张”回去。即采用先压缩，后扩张的方式。而 inverted residuals采用先扩张，后压缩的方式。 MobileNet采用DW conv提取特征，由于DW conv本身提取的特征数就少，再经过传统residuals block进行“压缩”，此时提取的特征数会更少，因此inverted residuals对其进行“扩张”，保证网络可以提取更多的特征。 Linear bottlenecks ReLu激活函数会破坏特征。ReLu对于负的输入，输出全为0，而本来DW conv特征通道已经被“压缩”，再经过ReLu的话，又会损失一部分特征。采用Linear，目的是防止Relu破坏特征。 shortcut stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut 网络架构 Xception Xception是Google提出的，arXiv 的V1 于2016年10月公开《Xception: Deep Learning with Depthwise Separable Convolutions 》，Xception是对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。 #### 设计思想 * 采用depthwise separable convolution来替换原来Inception v3中的卷积操作 与原版的Depth-wise convolution有两个不同之处： * 第一个：原版Depth-wise convolution，先逐通道卷积，再11卷积; 而Xception是反过来，先1*1卷积，再逐通道卷积； 第二个：原版Depth-wise convolution的两个卷积之间是不带激活函数的，而Xception在经过1*1卷积之后会带上一个Relu的非线性激活函数； 网络架构 feature map在空间和通道上具有一定的相关性，通过Inception模块和非线性激活函数实现通道之间的解耦。增多3*3的卷积的分支的数量，使它与1*1的卷积的输出通道数相等，此时每个3*3的卷积只作用与一个通道的特征图上，作者称之为“极致的Inception（Extream Inception）”模块，这就是Xception的基本模块。 ShuffleNet-v1 ShuffleNet 是Face++团队提出的，晚于MobileNet两个月在arXiv上公开《ShuffleNet： An Extremely Efficient Convolutional Neural Network for Mobile Devices 》用于移动端前向部署的网络架构。ShuffleNet基于MobileNet的group思想，将卷积操作限制到特定的输入通道。而与之不同的是，ShuffleNet将输入的group进行打散，从而保证每个卷积核的感受野能够分散到不同group的输入中，增加了模型的学习能力。 #### 设计思想 * 采用group conv减少大量参数 * roup conv与DW conv存在相同的“信息流通不畅”问题 * 采用channel shuffle解决上述问题 * MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle * 采用concat替换add操作 * avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失 网络架构 MobileNet中1*1卷积的操作占据了约95%的计算量，所以作者将1*1也更改为group卷积，使得相比MobileNet的计算量大大减少。 group卷积与DW存在同样使“通道信息交流不畅”的问题，MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle。 ShuffleNet的shuffle操作如图所示 avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失；实验表明：多多使用通道(提升通道的使用率)，有助于提高小模型的准确率。 网络结构： ShuffleNet-v2 huffleNet-v2 是Face++团队提出的《ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design》，旨在设计一个轻量级但是保证精度、速度的深度网络。 #### 设计思想 * 文中提出影响神经网络速度的4个因素： * a. FLOPs(FLOPs就是网络执行了多少multiply-adds操作) * b. MAC(内存访问成本) * c. 并行度(如果网络并行度高，速度明显提升) * d. 计算平台(GPU，ARM) * ShuffleNet-v2 提出了4点网络结构设计策略： * G1.输入输出的channel相同时，MAC最小 * G2.过度的组卷积会增加MAC * G3.网络碎片化会降低并行度 * G4.元素级运算不可忽视 网络结构 depthwise convolution 和 瓶颈结构增加了 MAC，用了太多的 group，跨层连接中的 element-wise Add 操作也是可以优化的点。所以在 shuffleNet V2 中增加了几种新特性。 所谓的 channel split 其实就是将通道数一分为2，化成两分支来代替原先的分组卷积结构（G2），并且每个分支中的卷积层都是保持输入输出通道数相同（G1），其中一个分支不采取任何操作减少基本单元数（G3），最后使用了 concat 代替原来的 elementy-wise add，并且后面不加 ReLU 直接（G4），再加入channle shuffle 来增加通道之间的信息交流。 对于下采样层，在这一层中对通道数进行翻倍。 在网络结构的最后，即平均值池化层前加入一层 1x1 的卷积层来进一步的混合特征。 网络结构 ShuffleNet-v2具有高精度的原因 由于高效，可以增加更多的channel，增加网络容量 采用split使得一部分特征直接与下面的block相连，特征复用(DenseNet) 现有移动端开源框架及其特点 NCNN １、开源时间：2017年7月 ２、开源用户：腾讯优图 ３、GitHub地址：https://github.com/Tencent/ncnn 4、特点： 1）NCNN考虑了手机端的硬件和系统差异以及调用方式，架构设计以手机端运行为主要原则。 2）无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架（以开源时间为参照对象）。 3）基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP。 5、功能： 1、NCNN支持卷积神经网络、多分支多输入的复杂网络结构，如vgg、googlenet、resnet、squeezenet 等。 2、NCNN无需依赖任何第三方库。 3、NCNN全部使用C/C++实现，以及跨平台的cmake编译系统，可轻松移植到其他系统和设备上。 4、汇编级优化，计算速度极快。使用ARM NEON指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。 5、精细的数据结构设计，没有采用需消耗大量内存的通常框架——im2col + 矩阵乘法，使得内存占用极低。 6、支持多核并行计算，优化CPU调度。 7、整体库体积小于500K，可精简到小于300K。 8、可扩展的模型设计，支持8bit 量化和半精度浮点存储。 9、支持直接内存引用加载网络模型。 10、可注册自定义层实现并扩展。 6、NCNN在Android端部署示例 1）选择合适的Android Studio版本并安装。 2）根据需求选择NDK版本并安装。 3）在Android Studio上配置NDK的环境变量。 4）根据自己需要编译NCNN sdk 1mkdir build-android cd build-android cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \\ -DANDROID_ABI=&quot;armeabi-v7a&quot; -DANDROID_ARM_NEON=ON \\ -DANDROID_PLATFORM=android-14 .. make make install ​ 安装完成之后，install下有include和lib两个文件夹。 ​ 备注： 123ANDROID_ABI 是架构名字，&quot;armeabi-v7a&quot; 支持绝大部分手机硬件 ANDROID_ARM_NEON 是否使用 NEON 指令集，设为 ON 支持绝大部分手机硬件 ANDROID_PLATFORM 指定最低系统版本，&quot;android-14&quot; 就是 android-4.0 5）进行NDK开发。 123456789101）assets文件夹下放置你的bin和param文件。2）jni文件夹下放置你的cpp和mk文件。3）修改你的app gradle文件。4）配置Android.mk和Application.mk文件。5）进行java接口的编写。6）读取拷贝bin和param文件（有些则是pb文件，根据实际情况）。7）进行模型的初始化和执行预测等操作。8）build。9）cd到src/main/jni目录下，执行ndk-build，生成.so文件。10）接着就可写自己的操作处理需求。 QNNPACK 全称：Quantized Neural Network PACKage（量化神经网络包） １、开源时间：2018年10月 ２、开源用户：Facebook ３、GitHub地址：https://github.com/pytorch/QNNPACK ４、特点： ​ １）低密度卷积优化函数库； ２）可在手机上实时运行Mask R-CNN 和 DensePose; ​ ３） 能在性能受限的移动设备中用 100ms 以内的时间实施图像分类； 5、QNNPACK 如何提高效率？ 1)QNNPACK 使用与安卓神经网络 API 兼容的线性量化方案 QNNPACK 的输入矩阵来自低精度、移动专用的计算机视觉模型。其它库在计算A和B矩阵相乘时，重新打包 A 和 B 矩阵以更好地利用缓存层次结构，希望在大量计算中分摊打包开销，QNNPACK 删除所有计算非必需的内存转换，针对 A和B矩阵相乘适用于一级缓存的情况进行了优化。 ​ 1）优化了L1缓存计算，不需要输出中间结果，直接输出最终结果，节省内存带宽和缓存占用。 具体分析： 常规实现：在量化矩阵-矩阵乘法中，8位整数的乘积通常会被累加至 32 位的中间结果中，随后重新量化以产生 8 位的输出。遇到大矩阵尺寸时，比如有时K太大，A和B的面板无法直接转入缓存，此时，需利用缓存层次结构，借助GEMM将A和B的面板沿着K维分割成固定大小的子面板，以便于每个子面板都能适应L1缓存，随后为每个子面板调用微内核。这一缓存优化需要 PDOT 为内核输出 32 位中间结果，最终将它们相加并重新量化为 8 位整数。 优化实现：由于 ONNPACK 对于面板 A 和 B 总是适应 L1 缓存的移动神经网络进行了优化，因此它在调用微内核时处理整个 A 和 B 的面板。而由于无需在微内核之外积累 32 位的中间结果，QNNPACK 会将 32 位的中间结果整合进微内核中并写出 8 位值，这节省了内存带宽和缓存占用。 ​ 2）取消了矩阵 A 的重新打包。 常规实现： 矩阵 B 包含静态权重，可以一次性转换成任何内存布局，但矩阵 A 包含卷积输入，每次推理运行都会改变。因此，重新打包矩阵 A 在每次运行时都会产生开销。尽管存在开销，传统的 GEMM实现还是出于以下两个原因对矩阵 A 进行重新打包： a 缓存关联性及微内核效率受限。如果不重新打包，微内核将不得不读取被潜在的大跨距隔开的几行A。如果这个跨距恰好是 2 的许多次幂的倍数，面板中不同行 A 的元素可能会落入同一缓存集中。如果冲突的行数超过了缓存关联性，它们就会相互驱逐，性能也会大幅下降。 b 打包对微内核效率的影响与当前所有移动处理器支持的 SIMD 向量指令的使用密切相关。这些指令加载、存储或者计算小型的固定大小元素向量，而不是单个标量（scalar）。在矩阵相乘中，充分利用向量指令达到高性能很重要。在传统的 GEMM 实现中，微内核把 MR 元素重新打包到向量暂存器里的 MR 线路中。 优化实现： a 当面板适配一级缓存时，不会存在缓存关联性及微内核效率受限的问题。 b 在 QNNPACK 实现中，MR 元素在存储中不是连续的，微内核需要把它们加载到不同的向量暂存器中。越来越大的暂存器压力迫使 QNNPACK 使用较小的 MRxNR 拼贴，但实际上这种差异很小，而且可以通过消除打包开销来补偿。例如，在 32 位 ARM 架构上，QNNPACK 使用 4×8 微内核，其中 57% 的向量指令是乘-加；另一方面，gemmlowp 库使用效率稍高的 4×12 微内核，其中 60% 的向量指令是乘-加。微内核加载 A 的多个行，乘以 B 的满列，结果相加，然后完成再量化并记下量化和。A 和 B 的元素被量化为 8 位整数，但乘积结果相加到 32 位。大部分 ARM 和 ARM64 处理器没有直接完成这一运算的指令，所以它必须分解为多个支持运算。QNNPACK 提供微内核的两个版本，其不同之处在于用于乘以 8 位值并将它们累加到 32 位的指令序列。 2)从矩阵相乘到卷积 ​ 传统实现： ​ 简单的 1×1 卷积可直接映射到矩阵相乘 ​ 但对于具备较大卷积核、padding 或子采样（步幅）的卷积而言则并非如此。但是，这些较复杂的卷积能够通过记忆变换 im2col 映射到矩阵相乘。对于每个输出像素，im2col 复制输入图像的图像块并将其计算为 2D 矩阵。由于每个输出像素都受 KHxKWxC 输入像素值的影响（KH 和 KW 分别指卷积核的高度和宽度，C 指输入图像中的通道数），因此该矩阵的大小是输入图像的 KHxKW 倍，im2col 给内存占用和性能都带来了一定的开销。和 Caffe 一样，大部分深度学习框架转而使用基于 im2col 的实现，利用现有的高度优化矩阵相乘库来执行卷积操作。 ​ 优化实现： ​ Facebook 研究者在 QNNPACK 中实现了一种更高效的算法。 他们没有变换卷积输入使其适应矩阵相乘的实现，而是调整 PDOT 微内核的实现，在运行中执行 im2col 变换。这样就无需将输入张量的实际输入复制到 im2col 缓存，而是使用输入像素行的指针设置 indirection buffer，输入像素与每个输出像素的计算有关。 研究者还修改了矩阵相乘微内核，以便从 indirection buffer 加载虚构矩阵（imaginary matrix）A 的行指针，indirection buffer 通常比 im2col buffer 小得多。 此外，如果两次推断运行的输入张量存储位置不变，则 indirection buffer 还可使用输入张量行的指针进行初始化，然后在多次推断运行中重新使用。研究者观察到具备 indirection buffer 的微内核不仅消除了 im2col 变换的开销，其性能也比矩阵相乘微内核略好（可能由于输入行在计算不同输出像素时被重用）。 3)深度卷积 分组卷积（grouped convolution）将输入和输出通道分割成多组，然后对每个组进行分别处理。在有限条件下，当组数等于通道数时，该卷积就是深度卷积，常用于当前的神经网络架构中。深度卷积对每个通道分别执行空间滤波，展示了与正常卷积非常不同的计算模式。因此，通常要向深度卷积提供单独实现，QNNPACK 包括一个高度优化版本 3×3 深度卷积。 深度卷积的传统实现是每次都在卷积核元素上迭代，然后将一个卷积核行和一个输入行的结果累加到输出行。对于一个 3×3 的深度卷积，此类实现将把每个输出行更新 9 次。在 QNNPACK 中，研究者计算所有 3×3 卷积核行和 3×3 输入行的结果，一次性累加到输出行，然后再处理下个输出行。 QNNPACK 实现高性能的关键因素在于完美利用通用暂存器（GPR）来展开卷积核元素上的循环，同时避免在 hot loop 中重新加载地址寄存器。32-bit ARM 架构将实现限制在 14 个 GPR。在 3×3 深度卷积中，需要读取 9 个输入行和 9 个卷积核行。这意味着如果想完全展开循环必须存储 18 个地址。然而，实践中推断时卷积核不会发生变化。因此 Facebook 研究者使用之前在 CxKHxKW 中的滤波器，将它们封装进 [C/8]xKWxKHx8，这样就可以仅使用具备地址增量（address increment）的一个 GPR 访问所有滤波器。（研究者使用数字 8 的原因在于，在一个命令中加载 8 个元素然后减去零，在 128-bit NEON 暂存器中生成 8 个 16-bit 值。）然后使用 9 个输入行指针，指针将滤波器重新装进 10 个 GPR，完全展开滤波器元素上的循环。64-bit ARM 架构相比 32-bit 架构，GPR 的数量翻了一倍。QNNPACK 利用额外的 ARM64 GPR，一次性存储 3×5 输入行的指针，并计算 3 个输出行。 7、性能优势： ​ 测试结果显示出 QNNPACK 在端到端基准上的性能优势。在量化当前最优 MobileNetV2 架构上，基于QNNPACK 的 Caffe2 算子的速度大约是 TensorFlow Lite 速度的 2 倍，在多种手机上都是如此。除了 QNNPACK 之外，Facebook 还开源了 Caffe2 quantized MobileNet v2 模型，其 top-1 准确率比相应的 TensorFlow 模型高出 1.3%。 MobileNetV1 MobileNetV1 架构在使用深度卷积（depthwise convolution）使模型更适合移动设备方面具备开创性。MobileNetV1 包括几乎整个 1×1 卷积和 3×3 卷积。Facebook 研究者将量化 MobileNetV1 模型从 TensorFlow Lite 转换而来，并在 TensorFlow Lite 和 QNNPACK 的 32-bit ARM 设备上对 MobileNetV1 进行基准测试。二者运行时均使用 4 线程，研究者观察到 QNNPACK 的运行速度几何平均值是 TensorFlow Lite 的 1.8 倍。 MobileNetV2 作为移动视觉任务的当前最优架构之一，MobileNetV2 引入了瓶颈构造块和瓶颈之间的捷径连接。研究者在 MobileNetV2 分类模型的量化版上对比基于 QNNPACK 的 Caffe2 算子和 TensorFlow Lite 实现。使用的量化 Caffe2 MobileNetV2 模型已开源，量化 TensorFlow Lite 模型来自官方库：https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md。下表展示了二者在常用测试集上的 top1 准确率： ​ Facebook 研究者利用这些模型建立了 Facebook AI 性能评估平台（https://github.com/facebook/FAI-PEP）的基准，该基准基于 32-bit ARM 环境的大量手机设备。对于 TensorFlow Lite 线程设置，研究者尝试了一到四个线程，并报告了最快速的结果。结果显示 TensorFlow Lite 使用四线程的性能最优，因此后续研究中使用四线程来对比 TensorFlow Lite 和 QNNPACK。下表展示了结果，以及在典型智能手机和高端机上，基于 QNNPACK 的算子速度比 TensorFlow Lite 快得多。 Facebook开源高性能内核库QNNPACK https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc http://www.sohu.com/a/272158070_610300 支持移动端深度学习的几种开源框架 https://blog.csdn.net/zchang81/article/details/74280019 Prestissimo １、开源时间：2017年11月 ２、开源用户：九言科技 ３、GitHub地址：https://github.com/in66-dev/In-Prestissimo ４、功能特点： 基础功能 支持卷积神经网络，支持多输入和多分支结构 精炼简洁的API设计，使用方便 提供调试接口，支持打印各个层的数据以及耗时 不依赖任何第三方计算框架，整体库体积 500K 左右（32位 约400k，64位 约600k） 纯 C++ 实现，跨平台，支持 android 和 ios 模型为纯二进制文件，不暴露开发者设计的网络结构 极快的速度 大到框架设计，小到汇编书写上全方位的优化，iphone7 上跑 SqueezeNet 仅需 26ms（单线程） 支持浮点(float)和整型(int)两种运算模式，float模式精度与caffe相同，int模式运算速度快，大部分网络用int的精度便已经足够 以巧妙的内存布局提升cpu的cache命中率，在中低端机型上性能依然强劲 针对 float-arm32, float-arm64, int-arm32, int-arm64 四个分支均做了细致的优化，保证arm32位和arm64位版本都有非常好的性能 SqueezeNet-v1.1 测试结果 Note: 手机测试性能存在一定的抖动，连续多次运算取平均时间 Note: 像华为mate8, mate9，Google nexus 6 虽然是64位的CPU，但测试用的是 32位的库，因此cpu架构依然写 arm-v7a CPU架构 机型 CPU ncnn（4线程） mdl Prestissimo_float(单线程) Prestissimo_int(单线程) arm-v7a 小米2 高通APQ8064 1.5GHz 185 ms 370 ms 184 ms 115 ms arm-v7a 小米2s 四核 骁龙APQ8064 Pro 1.7GHz 166 ms - 136 ms 96 ms arm-v7a 红米Note 4x 骁龙625 四核2.0GHz 124 ms 306 ms 202 ms 110 ms arm-v7a Google Nexus 6 骁龙805 四核 2.7GHz 84 ms 245 ms 103 ms 63 ms arm-v7a Vivo x6d 联发科 MT6752 1.7GHz 245 ms 502 ms 370 ms 186 ms arm-v7a 华为 Mate 8 海思麒麟950 4大4小 2.3GHz 1.8GHz 75 ms 180 ms 95 ms 57 ms arm-v7a 华为 Mate 9 海思麒麟960 4大4小 2.4GHz 1.8GHz 61 ms 170 ms 94 ms 48 ms arm-v8 iphone7 Apple A10 Fusion 2.34GHz - - 27 ms 26 ms 未开放特性 多核并行加速（多核机器可以再提升30%-100% 的速度） depthwise卷积运算（支持mobilenet） 模型压缩功能，压缩后的模型体积可缩小到20%以下 GPU 运算模式（Android 基于opengl es 3.1，ios 基于metal） 同类框架对比 框架 caffe tensorflow mdl-android mdl-ios ncnn CoreML Prestissimo 计算硬件 cpu cpu cpu gpu cpu gpu cpu （gpu版本未开放） 计算速度 慢 慢 慢 很快 很快 极快 极快 库大小 大 较大 中等 小 小 小 小 兼容性 好 好 好 限ios8以上 很好 仅支持 ios11 很好 模型支持度 很好 好 - 差（仅限指定模型） 较好 - 中等（当前版本不支持mobilenet） 使用方法-模型转换 绝影支持的是私有的模型文件格式，需要把 caffe 训练出来的模型转换为 .prestissimo 格式，模型转换工具为 caffe2Prestissimo.out。caffe2Prestissimo.out 依赖 protobuf 3.30。将 XXX.prototxt 和 YYY.caffemodel 转化为 Prestissimo 模型 ZZZ.prestissimo：（得到）./caffe2Prestissimo.out XXX.prototxt YYY.caffemodel ZZZ.prestissimo MDL（mobile-deep-learning） １、开源时间：2017年9月（已暂停更新） ２、开源用户：百度 ３、GitHub地址：https://github.com/allonli/mobile-deep-learning ４、功能特点： 一键部署，脚本参数就可以切换ios或者android 支持iOS gpu运行MobileNet、squeezenet模型 已经测试过可以稳定运行MobileNet、GoogLeNet v1、squeezenet、ResNet-50模型 体积极小，无任何第三方依赖。纯手工打造。 提供量化函数，对32位float转8位uint直接支持，模型体积量化后4M上下 与ARM相关算法团队线上线下多次沟通，针对ARM平台会持续优化 NEON使用涵盖了卷积、归一化、池化所有方面的操作 汇编优化，针对寄存器汇编操作具体优化 loop unrolling 循环展开，为提升性能减少不必要的CPU消耗，全部展开判断操作 将大量繁重的计算任务前置到overhead过程 5、框架结构 MDL 框架主要包括：模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层（JNI Interfaces）。 ​ 其中，模型转换模块主要负责将Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。 6、MDL 的性能及兼容性 体积 armv7 300k+ 速度 iOS GPU mobilenet 可以达到 40ms、squeezenet 可以达到 30ms ​ MDL 从立项到开源，已经迭代了一年多。移动端比较关注的多个指标都表现良好，如体积、功耗、速度。百度内部产品线在应用前也进行过多次对比，和已开源的相关项目对比，MDL 能够在保证速度和能耗的同时支持多种深度学习模型，如 mobilenet、googlenet v1、squeezenet 等，且具有 iOS GPU 版本，squeezenet 一次运行最快可以达到 3-40ms。 同类框架对比 ​ 框架Caffe2TensorFlowncnnMDL(CPU)MDL(GPU)硬件CPUCPUCPUCPUGPU速度慢慢快快极快体积大大小小小兼容Android&amp;iOSAndroid&amp;iOSAndroid&amp;iOSAndroid&amp;iOSiOS ​ 与支持 CNN 的移动端框架对比，MDL 速度快、性能稳定、兼容性好、demo 完备。 兼容性 ​ MDL 在 iOS 和 Android 平台均可以稳定运行，其中 iOS10 及以上平台有基于 GPU 运算的 API，性能表现非常出色，在 Android 平台则是纯 CPU 运行。高中低端机型运行状态和手机百度及其他 App 上的覆盖都有绝对优势。 ​ MDL 同时也支持 Caffe 模型直接转换为 MDL 模型。 Paddle-Mobile １、开源时间：持续更新，已到3.0版本 ２、开源用户：百度 ３、GitHub地址：https://github.com/PaddlePaddle/paddle-mobile ４、功能特点： 功能特点 高性能支持ARM CPU 支持Mali GPU 支持Andreno GPU 支持苹果设备的GPU Metal实现 支持ZU5、ZU9等FPGA开发板 支持树莓派等arm-linux开发板 MACE（ Mobile AI Compute Engine） １、开源时间：2018年4月(持续更新，v0.9.0 (2018-07-20)) ２、开源用户：小米 ３、GitHub地址：https://github.com/XiaoMi/mace ４、简介：Mobile AI Compute Engine (MACE) 是一个专为移动端异构计算设备优化的深度学习前向预测框架。 MACE覆盖了常见的移动端计算设备（CPU，GPU和DSP），并且提供了完整的工具链和文档，用户借助MACE能够很方便地在移动端部署深度学习模型。MACE已经在小米内部广泛使用并且被充分验证具有业界领先的性能和稳定性。 5、MACE的基本框架： MACE Model MACE定义了自有的模型格式（类似于Caffe2），通过MACE提供的工具可以将Caffe和TensorFlow的模型 转为MACE模型。 MACE Interpreter MACE Interpreter主要负责解析运行神经网络图（DAG）并管理网络中的Tensors。 Runtime CPU/GPU/DSP Runtime对应于各个计算设备的算子实现。 6、MACE使用的基本流程 1. 配置模型部署文件(.yml) 模型部署文件详细描述了需要部署的模型以及生成库的信息，MACE根据该文件最终生成对应的库文件。 2.编译MACE库 编译MACE的静态库或者动态库。 3.转换模型 将TensorFlow 或者 Caffe的模型转为MACE的模型。 4.1. 部署 根据不同使用目的集成Build阶段生成的库文件，然后调用MACE相应的接口执行模型。 4.2. 命令行运行 MACE提供了命令行工具，可以在命令行运行模型，可以用来测试模型运行时间，内存占用和正确性。 4.3. Benchmark MACE提供了命令行benchmark工具，可以细粒度的查看模型中所涉及的所有算子的运行时间。 7、MACE在哪些角度进行了优化? MACE 专为移动端异构计算平台优化的神经网络计算框架。主要从以下的角度做了专门的优化： 性能 代码经过NEON指令，OpenCL以及Hexagon HVX专门优化，并且采用 Winograd算法来进行卷积操作的加速。 此外，还对启动速度进行了专门的优化。 功耗 支持芯片的功耗管理，例如ARM的big.LITTLE调度，以及高通Adreno GPU功耗选项。 系统响应 支持自动拆解长时间的OpenCL计算任务，来保证UI渲染任务能够做到较好的抢占调度， 从而保证系统UI的相应和用户体验。 内存占用 通过运用内存依赖分析技术，以及内存复用，减少内存的占用。另外，保持尽量少的外部 依赖，保证代码尺寸精简。 模型加密与保护 模型保护是重要设计目标之一。支持将模型转换成C++代码，以及关键常量字符混淆，增加逆向的难度。 硬件支持范围 支持高通，联发科，以及松果等系列芯片的CPU，GPU与DSP(目前仅支持Hexagon)计算加速。 同时支持在具有POSIX接口的系统的CPU上运行。 8、性能对比： MACE 支持 TensorFlow 和 Caffe 模型，提供转换工具，可以将训练好的模型转换成专有的模型数据文件，同时还可以选择将模型转换成C++代码，支持生成动态库或者静态库，提高模型保密性。 FeatherCNN １、开源时间：持续更新，已到3.0版本 ２、开源用户：腾讯AI ３、GitHub地址：https://github.com/Tencent/FeatherCNN ４、功能特点： FeatherCNN 是由腾讯 AI 平台部研发的基于 ARM 架构的高效 CNN 推理库，该项目支持 Caffe 模型，且具有高性能、易部署、轻量级三大特性。 该项目具体特性如下： 高性能：无论是在移动设备（iOS / Android），嵌入式设备（Linux）还是基于 ARM 的服务器（Linux）上，FeatherCNN 均能发挥最先进的推理计算性能； 易部署：FeatherCNN 的所有内容都包含在一个代码库中，以消除第三方依赖关系。因此，它便于在移动平台上部署。FeatherCNN 自身的模型格式与 Caffe 模型完全兼容。 轻量级：编译后的 FeatherCNN 库的体积仅为数百 KB。 TensorFlow Lite １、开源时间：2017年11月 ２、开源用户：谷歌 ３、GitHub地址：https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite ４、简介： Google 表示 Lite 版本 TensorFlow 是 TensorFlow Mobile 的一个延伸版本。此前，通过TensorFlow Mobile API，TensorFlow已经支持手机上的模型嵌入式部署。TensorFlow Lite应该被视为TensorFlow Mobile的升级版。 TensorFlow Lite可以与Android 8.1中发布的神经网络API完美配合，即便在没有硬件加速时也能调用CPU处理，确保模型在不同设备上的运行。 而Android端版本演进的控制权是掌握在谷歌手中的，从长期看，TensorFlow Lite会得到Android系统层面上的支持。 5、架构： 其组件包括： TensorFlow 模型（TensorFlow Model）：保存在磁盘中的训练模型。 TensorFlow Lite 转化器（TensorFlow Lite Converter）：将模型转换成 TensorFlow Lite 文件格式的项目。 TensorFlow Lite 模型文件（TensorFlow Lite Model File）：基于 FlatBuffers，适配最大速度和最小规模的模型。 6、移动端开发步骤： Android Studio 3.0, SDK Version API26, NDK Version 14 步骤： 1. 将此项目导入到Android Studio： https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo 下载移动端的模型（model）和标签数据（lables）： https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip 下载完成解压mobilenet_v1_224_android_quant_2017_11_08.zip文件得到一个xxx.tflite和labes.txt文件，分别是模型和标签文件，并且把这两个文件复制到assets文件夹下。 构建app，run…… 17.7.9 TensorFlow Lite和TensorFlow Mobile的区别？ TensorFlow Lite是TensorFlow Mobile的进化版。 在大多数情况下，TensorFlow Lite拥有跟小的二进制大小，更少的依赖以及更好的性能。 相比TensorFlow Mobile是对完整TensorFlow的裁减，TensorFlow Lite基本就是重新实现了。从内部实现来说，在TensorFlow内核最基本的OP，Context等数据结构，都是新的。从外在表现来说，模型文件从PB格式改成了FlatBuffers格式，TensorFlow的size有大幅度优化，降至300K，然后提供一个converter将普通TensorFlow模型转化成TensorFlow Lite需要的格式。因此，无论从哪方面看，TensorFlow Lite都是一个新的实现方案。 PocketFlow １、开源时间：2018年9月 ２、开源用户：腾讯 ３、GitHub地址：https://github.com/Tencent/PocketFlow ４、简介： 全球首个自动模型压缩框架 一款面向移动端AI开发者的自动模型压缩框架，集成了当前主流的模型压缩与训练算法，结合自研超参数优化组件实现了全程自动化托管式的模型压缩与加速。开发者无需了解具体算法细节，即可快速地将AI技术部署到移动端产品上，实现了自动托管式模型压缩与加速，实现用户数据的本地高效处理。 5、框架介绍 PocketFlow 框架主要由两部分组件构成，分别是模型压缩/加速算法组件和超参数优化组件，具体结构如下图所示。 ​ 开发者将未压缩的原始模型作为 PocketFlow 框架的输入，同时指定期望的性能指标，例如模型的压缩和/或加速倍数；在每一轮迭代过程中，超参数优化组件选取一组超参数取值组合，之后模型压缩/加速算法组件基于该超参数取值组合，对原始模型进行压缩，得到一个压缩后的候选模型；基于对候选模型进行性能评估的结果，超参数优化组件调整自身的模型参数，并选取一组新的超参数取值组合，以开始下一轮迭代过程；当迭代终止时，PocketFlow 选取最优的超参数取值组合以及对应的候选模型，作为最终输出，返回给开发者用作移动端的模型部署。 6、PocketFlow如何实现模型压缩与加速？ ​ 具体地，PocketFlow 通过下列各个算法组件的有效结合，实现了精度损失更小、自动化程度更高的深度学习模型的压缩与加速： 通道剪枝（channel pruning）组件：在CNN网络中，通过对特征图中的通道维度进行剪枝，可以同时降低模型大小和计算复杂度，并且压缩后的模型可以直接基于现有的深度学习框架进行部署。在CIFAR-10图像分类任务中，通过对 ResNet-56 模型进行通道剪枝，可以实现2.5倍加速下分类精度损失0.4%，3.3倍加速下精度损失0.7%。 权重稀疏化（weight sparsification）组件：通过对网络权重引入稀疏性约束，可以大幅度降低网络权重中的非零元素个数；压缩后模型的网络权重可以以稀疏矩阵的形式进行存储和传输，从而实现模型压缩。对于 MobileNet 图像分类模型，在删去50%网络权重后，在 ImageNet 数据集上的 Top-1 分类精度损失仅为0.6%。 权重量化（weight quantization）组件：通过对网络权重引入量化约束，可以降低用于表示每个网络权重所需的比特数；团队同时提供了对于均匀和非均匀两大类量化算法的支持，可以充分利用 ARM 和 FPGA 等设备的硬件优化，以提升移动端的计算效率，并为未来的神经网络芯片设计提供软件支持。以用于 ImageNet 图像分类任务的 ResNet-18 模型为例，在8比特定点量化下可以实现精度无损的4倍压缩。 d)网络蒸馏（network distillation）组件：对于上述各种模型压缩组件，通过将未压缩的原始模型的输出作为额外的监督信息，指导压缩后模型的训练，在压缩/加速倍数不变的前提下均可以获得0.5%-2.0%不等的精度提升。 多GPU训练（multi-GPU training）组件：深度学习模型训练过程对计算资源要求较高，单个GPU难以在短时间内完成模型训练，因此团队提供了对于多机多卡分布式训练的全面支持，以加快使用者的开发流程。无论是基于 ImageNet 数据的Resnet-50图像分类模型还是基于 WMT14 数据的 Transformer 机器翻译模型，均可以在一个小时内训练完毕。[1] 超参数优化（hyper-parameter optimization）组件：多数开发者对模型压缩算法往往不甚了解，但超参数取值对最终结果往往有着巨大的影响，因此团队引入了超参数优化组件，采用了包括强化学习等算法以及 AI Lab 自研的 AutoML 自动超参数优化框架来根据具体性能需求，确定最优超参数取值组合。例如，对于通道剪枝算法，超参数优化组件可以自动地根据原始模型中各层的冗余程度，对各层采用不同的剪枝比例，在保证满足模型整体压缩倍数的前提下，实现压缩后模型识别精度的最大化。 7、PocketFlow 性能 ​ 通过引入超参数优化组件，不仅避免了高门槛、繁琐的人工调参工作，同时也使得 PocketFlow 在各个压缩算法上全面超过了人工调参的效果。以图像分类任务为例，在 CIFAR-10 和 ImageNet 等数据集上，PocketFlow 对 ResNet 和 MobileNet 等多种 CNN 网络结构进行有效的模型压缩与加速。 ​ 在 CIFAR-10 数据集上，PocketFlow 以 ResNet-56 作为基准模型进行通道剪枝，并加入了超参数优化和网络蒸馏等训练策略，实现了 2.5 倍加速下分类精度损失 0.4%，3.3 倍加速下精度损失 0.7%，且显著优于未压缩的 ResNet-44 模型； 在 ImageNet 数据集上，PocketFlow 可以对原本已经十分精简的 MobileNet 模型继续进行权重稀疏化，以更小的模型尺寸取得相似的分类精度；与 Inception-V1、ResNet-18 等模型相比，模型大小仅为后者的约 20~40%，但分类精度基本一致（甚至更高）。 相比于费时费力的人工调参，PocketFlow 框架中的 AutoML 自动超参数优化组件仅需 10 余次迭代就能达到与人工调参类似的性能，在经过 100 次迭代后搜索得到的超参数组合可以降低约 0.6% 的精度损失；通过使用超参数优化组件自动地确定网络中各层权重的量化比特数，PocketFlow 在对用于 ImageNet 图像分类任务的 ResNet-18 模型进行压缩时，取得了一致性的性能提升；当平均量化比特数为 4 比特时，超参数优化组件的引入可以将分类精度从 63.6% 提升至 68.1%（原始模型的分类精度为 70.3%）。 参考文献 [1] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, Jinhui Zhu,「Discrimination-aware Channel Pruning for Deep Neural Networks\", In Proc. of the 32nd Annual Conference on Neural Information Processing Systems, NIPS '18, Montreal, Canada, December 2018. [2] Jiaxiang Wu, Weidong Huang, Junzhou Huang, Tong Zhang,「Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization」, In Proc. of the 35th International Conference on Machine Learning, ICML’18, Stockholm, Sweden, July 2018. 其他几款支持移动端深度学习的开源框架 https://blog.csdn.net/zchang81/article/details/74280019 MDL、NCNN和 TFLite比较 百度-MDL框架、腾讯-NCNN框架和谷歌TFLite框架比较。 MDL NCNN TFLite 代码质量 中 高 很高 跨平台 √ √ √ 支持caffe模型 √ √ × 支持TensorFlow模型 × × √ CPU NEON指令优化 √ √ √ GPU加速 √ × × 相同点： 只含推理（inference）功能，使用的模型文件需要通过离线的方式训练得到。 最终生成的库尺寸较小，均小于500kB。 为了提升执行速度，都使用了ARM NEON指令进行加速。 跨平台，iOS和Android系统都支持。 不同点： MDL和NCNN均是只支持Caffe框架生成的模型文件，而TfLite则毫无意外的只支持自家大哥TensorFlow框架生成的模型文件。 MDL支持利用iOS系统的Matal框架进行GPU加速，能够显著提升在iPhone上的运行速度，达到准实时的效果。而NCNN和TFLite还没有这个功能。 移动端开源框架部署 以NCNN为例 部署步骤 以QNNPACK为例 部署步骤 在Android手机上使用MACE实现图像分类 在Android手机上使用PaddleMobile实现图像分类 编译paddle-mobile库 1）编译Android能够使用的CPP库：编译Android的paddle-mobile库，可选择使用Docker编译和Ubuntu交叉编译，这里介绍使用Ubuntu交叉编译paddle-mobile库。 注：在Android项目，Java代码调用CPP代码，CPP的函数需要遵循一定的命名规范，比如Java_包名_类名_对应的Java的方法名。 ​ 目前官方提供了5个可以给Java调用的函数，该代码在：paddle-mobile/src/jni/paddle_mobile_jni.cpp，如果想要让这些函数能够在自己的包名下的类调用，就要修改CPP的函数名称修改如下： 123456JNIEXPORT jboolean JNICALL Java_com_baidu_paddle_PML_load(JNIEnv *env, jclass thiz, jstring modelPath) { ANDROIDLOGI(&quot;load invoked&quot;); bool optimize = true; return getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath), optimize); } ​ 笔者项目的包名为com.example.paddlemobile1，在这个包下有一个ImageRecognition.java的程序来对应这个CPP程序，那么修改load函数如下： 12345678JNIEXPORT jboolean JNICALL Java_com_example_paddlemobile1_ImageRecognition_load(JNIEnv *env, jclass thiz, jstring modelPath) { ANDROIDLOGI(&quot;load invoked&quot;); bool optimize = true; return getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath), optimize);} 使用Ubuntu交叉编译paddle-mobile库 1、下载和解压NDK。 12wget https://dl.google.com/android/repository/android-ndk-r17b-linux-x86_64.zipunzip android-ndk-r17b-linux-x86_64.zip 2、设置NDK环境变量，目录是NDK的解压目录。 1export NDK_ROOT=&quot;/home/test/paddlepaddle/android-ndk-r17b&quot; 设置好之后，可以使用以下的命令查看配置情况。 root@test:/home/test/paddlepaddle# echo $NDK_ROOT /home/test/paddlepaddle/android-ndk-r17b 3、安装cmake，需要安装较高版本的，笔者的cmake版本是3.11.2。 下载cmake源码 1wget https://cmake.org/files/v3.11/cmake-3.11.2.tar.gz 解压cmake源码 1tar -zxvf cmake-3.11.2.tar.gz 进入到cmake源码根目录，并执行bootstrap。 12cd cmake-3.11.2./bootstrap 最后执行以下两条命令开始安装cmake。 12makemake install 安装完成之后，可以使用cmake --version是否安装成功. 1234root@test:/home/test/paddlepaddle# cmake --versioncmake version 3.11.2CMake suite maintained and supported by Kitware (kitware.com/cmake). 4、克隆paddle-mobile源码。 1git clone https://github.com/PaddlePaddle/paddle-mobile.git 5、进入到paddle-mobile的tools目录下，执行编译。 12cd paddle-mobile/tools/sh build.sh android （可选）如果想编译针对某一个网络编译更小的库时，可以在命令后面加上相应的参数，如下： 1sh build.sh android googlenet 6、最后会在paddle-mobile/build/release/arm-v7a/build目录下生产paddle-mobile库。 12root@test:/home/test/paddlepaddle/paddle-mobile/build/release/arm-v7a/build# lslibpaddle-mobile.so libpaddle-mobile.so就是我们在开发Android项目的时候使用到的paddle-mobile库。 创建Android项目 1、首先使用Android Studio创建一个普通的Android项目，包名为com.example.paddlemobile1 2、在main目录下创建l两个assets/paddle_models文件夹，这个文件夹存放PaddleFluid训练好的预测模型。PaddleMobile支持量化模型，使用模型量化可以把模型缩小至原来的四分之一，如果使用量化模型，那加载模型的接口也有修改一下，使用以下的接口加载模型： 1public static native boolean loadQualified(String modelDir); 3、在main目录下创建一个jniLibs文件夹，这个文件夹是存放CPP编译库的，在本项目中就存放上一部分编译的libpaddle-mobile.so 4、在Android项目的配置文件夹中加上权限声明，因为我们要使用到读取相册和使用相机，所以加上以下的权限声明： 123&lt;uses-permission android:name=&quot;android.permission.CAMERA&quot; /&gt;&lt;uses-permission android:name=&quot;android.permission.WRITE_EXTERNAL_STORAGE&quot; /&gt;&lt;uses-permission android:name=&quot;android.permission.READ_EXTERNAL_STORAGE&quot; /&gt; 5、修改activity_main.xml界面，修改成如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;RelativeLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot; xmlns:app=&quot;http://schemas.android.com/apk/res-auto&quot; xmlns:tools=&quot;http://schemas.android.com/tools&quot; android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;match_parent&quot; tools:context=&quot;.MainActivity&quot;&gt;&lt;LinearLayout android:id=&quot;@+id/btn_ll&quot; android:layout_alignParentBottom=&quot;true&quot; android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;wrap_content&quot; android:orientation=&quot;horizontal&quot;&gt; &lt;Button android:id=&quot;@+id/use_photo&quot; android:layout_weight=&quot;1&quot; android:layout_width=&quot;0dp&quot; android:layout_height=&quot;wrap_content&quot; android:text=&quot;相册&quot; /&gt; &lt;Button android:id=&quot;@+id/start_camera&quot; android:layout_weight=&quot;1&quot; android:layout_width=&quot;0dp&quot; android:layout_height=&quot;wrap_content&quot; android:text=&quot;拍照&quot; /&gt;&lt;/LinearLayout&gt;&lt;TextView android:layout_above=&quot;@id/btn_ll&quot; android:id=&quot;@+id/result_text&quot; android:textSize=&quot;16sp&quot; android:layout_width=&quot;match_parent&quot; android:hint=&quot;预测结果会在这里显示&quot; android:layout_height=&quot;100dp&quot; /&gt;&lt;ImageView android:layout_alignParentTop=&quot;true&quot; android:layout_above=&quot;@id/result_text&quot; android:id=&quot;@+id/show_image&quot; android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;match_parent&quot; /&gt;&lt;/RelativeLayout&gt; 6、创建一个ImageRecognition.java的Java程序，这个程序的作用就是调用paddle-mobile/src/jni/paddle_mobile_jni.cpp的函数，对应的是里面的函数。目前支持一下几个接口。 123456789101112131415161718192021222324252627package com.example.paddlemobile1;public class ImageRecognition { // set thread num public static native void setThread(int threadCount);//Load seperated parameterspublic static native boolean load(String modelDir);// load qualified modelpublic static native boolean loadQualified(String modelDir);// Load combined parameterspublic static native boolean loadCombined(String modelPath, String paramPath);// load qualified modelpublic static native boolean loadCombinedQualified(String modelPath, String paramPath);// object detectionpublic static native float[] predictImage(float[] buf, int[]ddims);// predict yuv imagepublic static native float[] predictYuv(byte[] buf, int imgWidth, int imgHeight, int[] ddims, float[]meanValues);// clear modelpublic static native void clear();} 7、然后编写一个PhotoUtil.java的工具类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124package com.example.paddlemobile1;import android.app.Activity;import android.content.Context;import android.content.Intent;import android.database.Cursor;import android.graphics.Bitmap;import android.graphics.BitmapFactory;import android.net.Uri;import android.os.Build;import android.provider.MediaStore;import android.support.v4.content.FileProvider;import android.util.Log;import java.io.File;import java.io.IOException;import java.util.Arrays;public class PhotoUtil {// start camerapublic static Uri start_camera(Activity activity, int requestCode) { Uri imageUri; // save image in cache path File outputImage = new File(activity.getExternalCacheDir(), &quot;out_image.jpg&quot;); try { if (outputImage.exists()) { outputImage.delete(); } outputImage.createNewFile(); } catch (IOException e) { e.printStackTrace(); } if (Build.VERSION.SDK_INT &gt;= 24) { // compatible with Android 7.0 or over imageUri = FileProvider.getUriForFile(activity, &quot;com.example.paddlemobile1&quot;, outputImage); } else { imageUri = Uri.fromFile(outputImage); } // set system camera Action Intent intent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE); // set save photo path intent.putExtra(MediaStore.EXTRA_OUTPUT, imageUri); // set photo quality, min is 0, max is 1 intent.putExtra(MediaStore.EXTRA_VIDEO_QUALITY, 0); activity.startActivityForResult(intent, requestCode); return imageUri;}// get picture in photopublic static void use_photo(Activity activity, int requestCode){ Intent intent = new Intent(Intent.ACTION_PICK); intent.setType(&quot;image/*&quot;); activity.startActivityForResult(intent, requestCode);}// get photo from Uripublic static String get_path_from_URI(Context context, Uri uri) { String result; Cursor cursor = context.getContentResolver().query(uri, null, null, null, null); if (cursor == null) { result = uri.getPath(); } else { cursor.moveToFirst(); int idx = cursor.getColumnIndex(MediaStore.Images.ImageColumns.DATA); result = cursor.getString(idx); cursor.close(); } return result;}// Compress the image to the size of the training image，and change RGBpublic static float[] getScaledMatrix(Bitmap bitmap, int desWidth, int desHeight) { float[] dataBuf = new float[3 * desWidth * desHeight]; int rIndex; int gIndex; int bIndex; int[] pixels = new int[desWidth * desHeight]; Bitmap bm = Bitmap.createScaledBitmap(bitmap, desWidth, desHeight, false); bm.getPixels(pixels, 0, desWidth, 0, 0, desWidth, desHeight); int j = 0; int k = 0; for (int i = 0; i &lt; pixels.length; i++) { int clr = pixels[i]; j = i / desHeight; k = i % desWidth; rIndex = j * desWidth + k; gIndex = rIndex + desHeight * desWidth; bIndex = gIndex + desHeight * desWidth; dataBuf[rIndex] = (float) ((clr &amp; 0x00ff0000) &gt;&gt; 16) - 148; dataBuf[gIndex] = (float) ((clr &amp; 0x0000ff00) &gt;&gt; 8) - 148; dataBuf[bIndex] = (float) ((clr &amp; 0x000000ff)) - 148; } if (bm.isRecycled()) { bm.recycle(); } return dataBuf;}// compress picturepublic static Bitmap getScaleBitmap(String filePath) { BitmapFactory.Options opt = new BitmapFactory.Options(); opt.inJustDecodeBounds = true; BitmapFactory.decodeFile(filePath, opt); int bmpWidth = opt.outWidth; int bmpHeight = opt.outHeight; int maxSize = 500; // compress picture with inSampleSize opt.inSampleSize = 1; while (true) { if (bmpWidth / opt.inSampleSize &lt; maxSize || bmpHeight / opt.inSampleSize &lt; maxSize) { break; } opt.inSampleSize *= 2; } opt.inJustDecodeBounds = false; return BitmapFactory.decodeFile(filePath, opt);}} start_camera()方法是启动相机并返回图片的URI。 use_photo()方法是打开相册，获取到的图片URI在回到函数中获取。 get_path_from_URI()方法是把图片的URI转换成绝对路径。 getScaledMatrix()方法是把图片压缩成跟训练时的大小，并转换成预测需要用的数据格式浮点数组。 getScaleBitmap()方法是对图片进行等比例压缩，减少内存的支出。 8、最后修改MainActivity.java，修改如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.example.paddlemobile1;import android.Manifest;import android.annotation.SuppressLint;import android.app.Activity;import android.content.Context;import android.content.Intent;import android.content.pm.PackageManager;import android.graphics.Bitmap;import android.net.Uri;import android.os.Bundle;import android.os.Environment;import android.support.annotation.NonNull;import android.support.annotation.Nullable;import android.support.v4.app.ActivityCompat;import android.support.v4.content.ContextCompat;import android.support.v7.app.AppCompatActivity;import android.util.Log;import android.view.View;import android.widget.Button;import android.widget.ImageView;import android.widget.TextView;import android.widget.Toast;import com.bumptech.glide.Glide;import com.bumptech.glide.load.engine.DiskCacheStrategy;import com.bumptech.glide.request.RequestOptions;import java.io.File;import java.io.FileOutputStream;import java.io.InputStream;import java.util.ArrayList;import java.util.Arrays;import java.util.List;public class MainActivity extends AppCompatActivity { private static final String TAG = MainActivity.class.getName(); private static final int USE_PHOTO = 1001; private static final int START_CAMERA = 1002; private Uri image_uri; private ImageView show_image; private TextView result_text; private String assets_path = &quot;paddle_models&quot;; private boolean load_result = false; private int[] ddims = {1, 3, 224, 224};private static final String[] PADDLE_MODEL = { &quot;lenet&quot;, &quot;alexnet&quot;, &quot;vgg16&quot;, &quot;resnet&quot;, &quot;googlenet&quot;, &quot;mobilenet_v1&quot;, &quot;mobilenet_v2&quot;, &quot;inception_v1&quot;, &quot;inception_v2&quot;, &quot;squeezenet&quot;};// load paddle-mobile apistatic { try { System.loadLibrary(&quot;paddle-mobile&quot;); } catch (SecurityException e) { e.printStackTrace(); } catch (UnsatisfiedLinkError e) { e.printStackTrace(); } catch (NullPointerException e) { e.printStackTrace(); }} 123456789101112131415161718192021222324@Overrideprotected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); init();}// initialize viewprivate void init() { request_permissions(); show_image = (ImageView) findViewById(R.id.show_image); result_text = (TextView) findViewById(R.id.result_text); Button use_photo = (Button) findViewById(R.id.use_photo); Button start_photo = (Button) findViewById(R.id.start_camera); // use photo click use_photo.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View view) { PhotoUtil.use_photo(MainActivity.this, USE_PHOTO); // load_model(); } }); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192 // start camera click start_photo.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View view) { image_uri = PhotoUtil.start_camera(MainActivity.this, START_CAMERA); } }); // copy file from assets to sdcard String sdcard_path = Environment.getExternalStorageDirectory() + File.separator + assets_path; copy_file_from_asset(this, assets_path, sdcard_path); // load model load_model();}// load infer modelprivate void load_model() { String model_path = Environment.getExternalStorageDirectory() + File.separator + assets_path + File.separator + PADDLE_MODEL[4]; Log.d(TAG, model_path); load_result = ImageRecognition.load(model_path); if (load_result) { Log.d(TAG, &quot;model load success&quot;); } else { Log.d(TAG, &quot;model load fail&quot;); }}// clear infer modelprivate void clear_model() { ImageRecognition.clear(); Log.d(TAG, &quot;model is clear&quot;);}// copy file from asset to sdcardpublic void copy_file_from_asset(Context context, String oldPath, String newPath) { try { String[] fileNames = context.getAssets().list(oldPath); if (fileNames.length &gt; 0) { // directory File file = new File(newPath); if (!file.exists()) { file.mkdirs(); } // copy recursivelyC for (String fileName : fileNames) { copy_file_from_asset(context, oldPath + &quot;/&quot; + fileName, newPath + &quot;/&quot; + fileName); } Log.d(TAG, &quot;copy files finish&quot;); } else { // file File file = new File(newPath); // if file exists will never copy if (file.exists()) { return; } // copy file to new path InputStream is = context.getAssets().open(oldPath); FileOutputStream fos = new FileOutputStream(file); byte[] buffer = new byte[1024]; int byteCount; while ((byteCount = is.read(buffer)) != -1) { fos.write(buffer, 0, byteCount); } fos.flush(); is.close(); fos.close(); } } catch (Exception e) { e.printStackTrace(); }}@Overrideprotected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) { String image_path; RequestOptions options = new RequestOptions().skipMemoryCache(true).diskCacheStrategy(DiskCacheStrategy.NONE); if (resultCode == Activity.RESULT_OK) { switch (requestCode) { case USE_PHOTO: if (data == null) { Log.w(TAG, &quot;user photo data is null&quot;); return; } image_uri = data.getData(); Glide.with(MainActivity.this).load(image_uri).apply(options).into(show_image); // get image path from uri image_path = PhotoUtil.get_path_from_URI(MainActivity.this, image_uri); // show result result_text.setText(image_path); // predict image predict_image(PhotoUtil.get_path_from_URI(MainActivity.this, image_uri)); break; case START_CAMERA: // show photo Glide.with(MainActivity.this).load(image_uri).apply(options).into(show_image); // get image path from uri image_path = PhotoUtil.get_path_from_URI(MainActivity.this, image_uri); // show result result_text.setText(image_path); // predict image predict_image(PhotoUtil.get_path_from_URI(MainActivity.this, image_uri)); break; } }}@SuppressLint(&quot;SetTextI18n&quot;)private void predict_image(String image_path) { // picture to float array Bitmap bmp = PhotoUtil.getScaleBitmap(image_path); float[] inputData = PhotoUtil.getScaledMatrix(bmp, ddims[2], ddims[3]); try { long start = System.currentTimeMillis(); // get predict result float[] result = ImageRecognition.predictImage(inputData, ddims); Log.d(TAG, &quot;origin predict result:&quot; + Arrays.toString(result)); long end = System.currentTimeMillis(); long time = end - start; Log.d(&quot;result length&quot;, String.valueOf(result.length)); // show predict result and time int r = get_max_result(result); String show_text = &quot;result：&quot; + r + &quot;\\nprobability：&quot; + result[r] + &quot;\\ntime：&quot; + time + &quot;ms&quot;; result_text.setText(show_text); } catch (Exception e) { e.printStackTrace(); }}private int get_max_result(float[] result) { float probability = result[0]; int r = 0; for (int i = 0; i &lt; result.length; i++) { if (probability &lt; result[i]) { probability = result[i]; r = i; } } return r;}// request permissionsprivate void request_permissions() { List&lt;String&gt; permissionList = new ArrayList&lt;&gt;(); if (ContextCompat.checkSelfPermission(this, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) { permissionList.add(Manifest.permission.CAMERA); } if (ContextCompat.checkSelfPermission(this, Manifest.permission.WRITE_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) { permissionList.add(Manifest.permission.WRITE_EXTERNAL_STORAGE); } if (ContextCompat.checkSelfPermission(this, Manifest.permission.READ_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) { permissionList.add(Manifest.permission.READ_EXTERNAL_STORAGE); } // if list is not empty will request permissions if (!permissionList.isEmpty()) { ActivityCompat.requestPermissions(this, permissionList.toArray(new String[permissionList.size()]), 1); }}@Overridepublic void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) { super.onRequestPermissionsResult(requestCode, permissions, grantResults); switch (requestCode) { case 1: if (grantResults.length &gt; 0) { for (int i = 0; i &lt; grantResults.length; i++) { int grantResult = grantResults[i]; if (grantResult == PackageManager.PERMISSION_DENIED) { String s = permissions[i]; Toast.makeText(this, s + &quot; permission was denied&quot;, Toast.LENGTH_SHORT).show(); } } } break; }}@Overrideprotected void onDestroy() { // clear model before destroy app clear_model(); super.onDestroy();}} load_model()方法是加载预测模型的。 clear_model()方法是清空预测模型的。 copy_file_from_asset()方法是把预测模型复制到内存卡上。 predict_image()方法是预测图片的。 get_max_result()方法是获取概率最大的预测结果。 request_permissions()方法是动态请求权限的。 因为使用到图像加载框架Glide，所以要在build.gradle加入以下的引用。 1implementation 'com.github.bumptech.glide:glide:4.3.1' 8、最后运行项目，选择图片预测就会得到结果。 移动端开源框架部署疑难 增加常见的几个问题 知识蒸馏（Distillation）相关论文阅读（1）——Distilling the Knowledge in a Neural Network（以及代码复现）","link":"/posts/690048963.html"}],"tags":[{"name":"笔记","slug":"笔记","link":"/tags/%E7%AC%94%E8%AE%B0/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"Binary Tree","slug":"Binary-Tree","link":"/tags/Binary-Tree/"},{"name":"turtle","slug":"turtle","link":"/tags/turtle/"},{"name":"Attention","slug":"Attention","link":"/tags/Attention/"},{"name":"Seq2seq","slug":"Seq2seq","link":"/tags/Seq2seq/"},{"name":"动态规划","slug":"动态规划","link":"/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"二分树","slug":"二分树","link":"/tags/%E4%BA%8C%E5%88%86%E6%A0%91/"},{"name":"CYK","slug":"CYK","link":"/tags/CYK/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Google apac","slug":"Google-apac","link":"/tags/Google-apac/"},{"name":"Dynamic Programming","slug":"Dynamic-Programming","link":"/tags/Dynamic-Programming/"},{"name":"二分搜索","slug":"二分搜索","link":"/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2/"},{"name":"MDP","slug":"MDP","link":"/tags/MDP/"},{"name":"Word representation","slug":"Word-representation","link":"/tags/Word-representation/"},{"name":"数学原理","slug":"数学原理","link":"/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/"},{"name":"GloVe","slug":"GloVe","link":"/tags/GloVe/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"优化算法","slug":"优化算法","link":"/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"name":"HMM","slug":"HMM","link":"/tags/HMM/"},{"name":"链表","slug":"链表","link":"/tags/%E9%93%BE%E8%A1%A8/"},{"name":"动态回归","slug":"动态回归","link":"/tags/%E5%8A%A8%E6%80%81%E5%9B%9E%E5%BD%92/"},{"name":"背包问题","slug":"背包问题","link":"/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"},{"name":"English","slug":"English","link":"/tags/English/"},{"name":"Programmer","slug":"Programmer","link":"/tags/Programmer/"},{"name":"语言模型","slug":"语言模型","link":"/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"面试专场","slug":"面试专场","link":"/tags/%E9%9D%A2%E8%AF%95%E4%B8%93%E5%9C%BA/"},{"name":"Lexicalized PCFG","slug":"Lexicalized-PCFG","link":"/tags/Lexicalized-PCFG/"},{"name":"Log-Linear Models","slug":"Log-Linear-Models","link":"/tags/Log-Linear-Models/"},{"name":"Monte Carlo","slug":"Monte-Carlo","link":"/tags/Monte-Carlo/"},{"name":"K臂老虎机","slug":"K臂老虎机","link":"/tags/K%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/"},{"name":"numpy","slug":"numpy","link":"/tags/numpy/"},{"name":"CFG","slug":"CFG","link":"/tags/CFG/"},{"name":"PCFG","slug":"PCFG","link":"/tags/PCFG/"},{"name":"Lexized PCFG","slug":"Lexized-PCFG","link":"/tags/Lexized-PCFG/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"二分搜索树","slug":"二分搜索树","link":"/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"name":"统计机器翻译","slug":"统计机器翻译","link":"/tags/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"},{"name":"IBM Modes","slug":"IBM-Modes","link":"/tags/IBM-Modes/"},{"name":"特征抽取","slug":"特征抽取","link":"/tags/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96/"},{"name":"Tagging","slug":"Tagging","link":"/tags/Tagging/"},{"name":"Temporal-Difference","slug":"Temporal-Difference","link":"/tags/Temporal-Difference/"},{"name":"Paper","slug":"Paper","link":"/tags/Paper/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"XGBoost","slug":"XGBoost","link":"/tags/XGBoost/"},{"name":"二进制","slug":"二进制","link":"/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6/"},{"name":"图论","slug":"图论","link":"/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"聚类","slug":"聚类","link":"/tags/%E8%81%9A%E7%B1%BB/"},{"name":"降维","slug":"降维","link":"/tags/%E9%99%8D%E7%BB%B4/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"KNN","slug":"KNN","link":"/tags/KNN/"},{"name":"线性模型","slug":"线性模型","link":"/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"name":"Bootstrapping","slug":"Bootstrapping","link":"/tags/Bootstrapping/"},{"name":"exhaustive search","slug":"exhaustive-search","link":"/tags/exhaustive-search/"},{"name":"排序","slug":"排序","link":"/tags/%E6%8E%92%E5%BA%8F/"},{"name":"决策树","slug":"决策树","link":"/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"网易云音乐","slug":"网易云音乐","link":"/tags/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/"},{"name":"VIP","slug":"VIP","link":"/tags/VIP/"},{"name":"版权","slug":"版权","link":"/tags/%E7%89%88%E6%9D%83/"},{"name":"AdaBoost","slug":"AdaBoost","link":"/tags/AdaBoost/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"},{"name":"分词","slug":"分词","link":"/tags/%E5%88%86%E8%AF%8D/"},{"name":"Viterbi","slug":"Viterbi","link":"/tags/Viterbi/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","link":"/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"name":"条件随机场","slug":"条件随机场","link":"/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"},{"name":"GBDT","slug":"GBDT","link":"/tags/GBDT/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"监督学习","slug":"监督学习","link":"/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"name":"脚本","slug":"脚本","link":"/tags/%E8%84%9A%E6%9C%AC/"},{"name":"邮件","slug":"邮件","link":"/tags/%E9%82%AE%E4%BB%B6/"},{"name":"递归","slug":"递归","link":"/tags/%E9%80%92%E5%BD%92/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"BiLSTM","slug":"BiLSTM","link":"/tags/BiLSTM/"},{"name":"CRF","slug":"CRF","link":"/tags/CRF/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"CPU","slug":"CPU","link":"/tags/CPU/"},{"name":"数学","slug":"数学","link":"/tags/%E6%95%B0%E5%AD%A6/"},{"name":"经典网络","slug":"经典网络","link":"/tags/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"},{"name":"词表征","slug":"词表征","link":"/tags/%E8%AF%8D%E8%A1%A8%E5%BE%81/"},{"name":"词向量","slug":"词向量","link":"/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"},{"name":"CRF算法","slug":"CRF算法","link":"/tags/CRF%E7%AE%97%E6%B3%95/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"EM算法","slug":"EM算法","link":"/tags/EM%E7%AE%97%E6%B3%95/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"组合树","slug":"组合树","link":"/tags/%E7%BB%84%E5%90%88%E6%A0%91/"},{"name":"专题","slug":"专题","link":"/tags/%E4%B8%93%E9%A2%98/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"算法原理","slug":"算法原理","link":"/tags/%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/"},{"name":"GRN","slug":"GRN","link":"/tags/GRN/"},{"name":"LSTM","slug":"LSTM","link":"/tags/LSTM/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"Tensorflow","slug":"Tensorflow","link":"/tags/Tensorflow/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"Caffee","slug":"Caffee","link":"/tags/Caffee/"},{"name":"Bert","slug":"Bert","link":"/tags/Bert/"},{"name":"Tencent","slug":"Tencent","link":"/tags/Tencent/"},{"name":"迁移学习","slug":"迁移学习","link":"/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"},{"name":"Go语言","slug":"Go语言","link":"/tags/Go%E8%AF%AD%E8%A8%80/"},{"name":"语法","slug":"语法","link":"/tags/%E8%AF%AD%E6%B3%95/"},{"name":"Go","slug":"Go","link":"/tags/Go/"},{"name":"word2vec","slug":"word2vec","link":"/tags/word2vec/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"模型压缩","slug":"模型压缩","link":"/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"},{"name":"模型部署","slug":"模型部署","link":"/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"}],"categories":[{"name":"Golang","slug":"Golang","link":"/categories/Golang/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"NLP","slug":"深度学习/NLP","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"},{"name":"强化学习","slug":"强化学习","link":"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"英语学习","slug":"英语学习","link":"/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/"},{"name":"数据结构","slug":"数据结构","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"破解","slug":"破解","link":"/categories/%E7%A0%B4%E8%A7%A3/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"CNN","slug":"深度学习/CNN","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/"},{"name":"迁移学习","slug":"深度学习/迁移学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"}]}