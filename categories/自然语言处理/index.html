<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>分类: 自然语言处理 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Simplicity is the ultimate form of sophistication"><meta property="og:type" content="blog"><meta property="og:title" content="MCFON"><meta property="og:url" content="https://hunlp.com/"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="Simplicity is the ultimate form of sophistication"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hunlp.com/img/og_image.png"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="大数据,数据分析,数据可视化,数据挖掘,机器学习"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com"},"headline":"MCFON","image":["https://hunlp.com/img/og_image.png"],"author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"Simplicity is the ultimate form of sophistication"}</script><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-4145019924567174" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">自然语言处理</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><a class="has-link-black-ter" href="/posts/62790.html"><i class="fas fa-angle-double-right"></i>NLP基础</a></h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-07-14</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-08-16</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">22 分钟读完 (大约3328个字)</span></div></div><div class="content"><h2 id="解决-nlp-问题的一般思路">解决 NLP 问题的一般思路</h2>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个问题人类可以做好么？</span><br><span class="line">  - 可以 -&gt; 记录自己的思路 -&gt; 设计流程让机器完成你的思路</span><br><span class="line">  - 很难 -&gt; 尝试从计算机的角度来思考问题</span><br></pre></td></tr></table></figure>
<h2 id="nlp-的历史进程">NLP 的历史进程</h2>
<ul>
<li><strong>规则系统</strong>
<ul>
<li>正则表达式/自动机</li>
<li>规则是固定的</li>
<li><strong>搜索引擎</strong> <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">“豆瓣酱用英语怎么说？”</span><br><span class="line">规则：“xx用英语怎么说？” =&gt; translate(XX, English)</span><br><span class="line"></span><br><span class="line">“我饿了”</span><br><span class="line">规则：“我饿（死）了” =&gt; recommend(饭店，地点)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><strong>概率系统</strong>
<ul>
<li><p>规则从数据中<strong>抽取</strong></p></li>
<li><p>规则是有<strong>概率</strong>的</p></li>
<li><p>概率系统的一般<strong>工作方式</strong> <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">流程设计</span><br><span class="line">收集训练数据</span><br><span class="line">    预处理</span><br><span class="line">    特征工程</span><br><span class="line">        分类器（机器学习算法）</span><br><span class="line">        预测</span><br><span class="line">            评价</span><br></pre></td></tr></table></figure> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtdi88xxj30k3088755.jpg" /></p>
<ul>
<li>最重要的部分：数据收集、预处理、特征工程</li>
</ul></li>
<li><p>示例 <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">任务：</span><br><span class="line">“豆瓣酱用英语怎么说” =&gt; translate(豆瓣酱，Eng)</span><br><span class="line"></span><br><span class="line">流程设计（序列标注）：</span><br><span class="line">子任务1： 找出目标语言 “豆瓣酱用 **英语** 怎么说”</span><br><span class="line">子任务2： 找出翻译目标 “ **豆瓣酱** 用英语怎么说”</span><br><span class="line"></span><br><span class="line">收集训练数据：</span><br><span class="line">（子任务1）</span><br><span class="line">“豆瓣酱用英语怎么说”</span><br><span class="line">“茄子用英语怎么说”</span><br><span class="line">“黄瓜怎么翻译成英语”</span><br><span class="line"></span><br><span class="line">预处理：</span><br><span class="line">分词：“豆瓣酱 用 英语 怎么说”</span><br><span class="line"></span><br><span class="line">抽取特征：</span><br><span class="line">（前后各一个词）</span><br><span class="line">0 茄子：    &lt; <span class="built_in">_</span> 用</span><br><span class="line">0 用：      豆瓣酱 <span class="built_in">_</span> 英语</span><br><span class="line">1 英语：    用 <span class="built_in">_</span> 怎么说</span><br><span class="line">0 怎么说：  英语 <span class="built_in">_</span> &gt;</span><br><span class="line"></span><br><span class="line">分类器：</span><br><span class="line">SVM/CRF/HMM/RNN</span><br><span class="line"></span><br><span class="line">预测：</span><br><span class="line">0.1 茄子：    &lt; <span class="built_in">_</span> 用</span><br><span class="line">0.1 用：      豆瓣酱 <span class="built_in">_</span> 英语</span><br><span class="line">0.7 英语：    用 <span class="built_in">_</span> 怎么说</span><br><span class="line">0.1 怎么说：  英语 <span class="built_in">_</span> &gt;</span><br><span class="line"></span><br><span class="line">评价：</span><br><span class="line">准确率</span><br></pre></td></tr></table></figure></p></li>
</ul></li>
<li>概率系统的优/缺点
<ul>
<li><code>+</code> 规则更加贴近于真实事件中的规则，因而效果往往比较好</li>
<li><code>-</code> 特征是由专家/人指定的；</li>
<li><code>-</code> 流程是由专家/人设计的；</li>
<li><code>-</code> 存在独立的<strong>子任务</strong></li>
</ul></li>
<li><strong>深度学习</strong>
<ul>
<li>深度学习相对概率模型的优势
<ul>
<li>特征是由专家指定的 <code>-&gt;</code> 特征是由深度学习自己提取的</li>
<li>流程是由专家设计的 <code>-&gt;</code> 模型结构是由专家设计的</li>
<li>存在独立的子任务 <code>-&gt;</code> End-to-End Training</li>
</ul></li>
</ul></li>
</ul>
<h2 id="seq2seq-模型">Seq2Seq 模型</h2>
<ul>
<li>大部分自然语言问题都可以使用 Seq2Seq 模型解决 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgte13jntj30oe099t9w.jpg" /></li>
<li><strong>“万物”皆 Seq2Seq</strong> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtey2krlj30fa0cjq43.jpg" /></li>
</ul>
<h2 id="评价机制">评价机制</h2>
<h3 id="困惑度-perplexity-ppx">困惑度 (Perplexity, PPX)</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> - Wikipedia - 在信息论中，perplexity 用于度量一个<strong>概率分布</strong>或<strong>概率模型</strong>预测样本的好坏程度</p>
</blockquote>
<pre><code>&gt; ../机器学习/[信息论](../A-机器学习/A-机器学习基础#信息论) </code></pre>
<h3>
基本公式
</h3>
<ul>
<li><p><strong>概率分布</strong>（离散）的困惑度 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtfaeewqj306t00o3yg.jpg" /></p>
<blockquote>
<p>其中 <code>H(p)</code> 即<strong>信息熵</strong></p>
</blockquote></li>
<li><p><strong>概率模型</strong>的困惑度 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtfp9qxhj304j00q0sn.jpg" /></p>
<blockquote>
<p>通常 <code>b=2</code></p>
</blockquote></li>
<li><p><strong>指数部分</strong>也可以是<strong>交叉熵</strong>的形式，此时困惑度相当于交叉熵的指数形式 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtg79861j307600odfs.jpg" /></p>
<blockquote>
<p>其中 <code>p~</code> 为<strong>测试集</strong>中的经验分布——<code>p~(x) = n/N</code>，其中 <code>n</code> 为 x 的出现次数，N 为测试集的大小</p>
</blockquote></li>
</ul>
<p><strong>语言模型中的 PPX</strong> - 在 <strong>NLP</strong> 中，困惑度常作为<strong>语言模型</strong>的评价指标 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtgib1pej30a5024dfx.jpg" /></p>
<ul>
<li><p>直观来说，就是下一个<strong>候选词数目</strong>的期望值——</p>
<p>如果不使用任何模型，那么下一个候选词的数量就是整个词表的数量；通过使用 <code>bi-gram</code>语言模型，可以将整个数量限制到 <code>200</code> 左右</p></li>
</ul>
<h3 id="bleu">BLEU</h3>
<blockquote>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21190081/article/details/53115580">一种机器翻译的评价准则——BLEU</a> - CSDN博客 - 机器翻译评价准则 - 计算公式
<div data-align="center">
<img src="../_assets/TIM截图20180728212554.png" height="" />
</div>
</blockquote>
<p>其中 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtihht0dj309802yq34.jpg" /></p>
<pre><code>&gt; `c` 为生成句子的长度；`r` 为参考句子的长度——目的是**惩罚**长度过短的候选句子</code></pre>
<ul>
<li><p>为了计算方便，会加一层 <code>log</code> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtirnmscj308x01paa2.jpg" /></p>
<blockquote>
<p>通常 <code>N=4, w_n=1/4</code></p>
</blockquote></li>
</ul>
<h3 id="rouge">ROUGE</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25222361/article/details/78694617">自动文摘评测方法：Rouge-1、Rouge-2、Rouge-L、Rouge-S</a> - CSDN博客 - 一种机器翻译/自动摘要的评价准则</p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/joshuaxx316/article/details/58696552">BLEU，ROUGE，METEOR，ROUGE-浅述自然语言处理机器翻译常用评价度量</a> - CSDN博客</p>
</blockquote>
<h1 id="语言模型">语言模型</h1>
<h2 id="xx-模型的含义">XX 模型的含义</h2>
<ul>
<li>如果能使用某个方法对 XX <strong>打分</strong>（Score），那么就可以把这个方法称为 “<strong>XX 模型</strong>”
<ul>
<li><strong>篮球明星模型</strong>: <code>Score(库里)</code>、<code>Score(詹姆斯)</code></li>
<li><strong>话题模型</strong>——对一段话是否在谈论某一话题的打分 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Score( NLP | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.8</span><br><span class="line">Score( ACM | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.05</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h2 id="概率统计语言模型-plm-slm">概率/统计语言模型 (PLM, SLM)</h2>
<ul>
<li><strong>语言模型</strong>是一种对语言打分的方法；而<strong>概率语言模型</strong>把语言的“得分”通过<strong>概率</strong>来体现</li>
<li>具体来说，概率语言模型计算的是<strong>一个序列</strong>作为一句话可能的概率 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Score(&quot;什么 是 语言 模型&quot;) --&gt; 0.05   # 比较常见的说法，得分比较高</span><br><span class="line">Score(&quot;什么 有 语言 模型&quot;) --&gt; 0.01   # 不太常见的说法，得分比较低</span><br></pre></td></tr></table></figure></li>
<li>以上过程可以形式化为：
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(W)=p(w_1^T)=p(w_1,w_2,...,w_T"><img src="../_assets/公式_20180805204149.png" height="" /></a>
</div>
根据贝叶斯公式，有
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_1^T)=p(w_1)\cdot&space;p(w_2|w_1)\cdot&space;p(w_3|w_1^2)\cdots&space;p(w_T|w_1^{T-1})"><img src="../_assets/公式_20180805211530.png" height="" /></a>
</div></li>
<li>其中每个条件概率就是<strong>模型的参数</strong>；如果这个参数都是已知的，那么就能得到整个序列的概率了</li>
</ul>
<h3 id="参数的规模">参数的规模</h3>
<ul>
<li>设词表的大小为 <code>N</code>，考虑长度为 <code>T</code> 的句子，理论上有 <code>N^T</code> 种可能的句子，每个句子中有 <code>T</code> 个参数，那么参数的数量将达到 <code>O(T*N^T)</code></li>
</ul>
<h3 id="可用的概率模型">可用的概率模型</h3>
<ul>
<li>统计语言模型实际上是一个概率模型，所以常见的概率模型都可以用于求解这些参数</li>
<li>常见的概率模型有：N-gram 模型、决策树、最大熵模型、隐马尔可夫模型、条件随机场、神经网络等</li>
<li>目前常用于语言模型的是 N-gram 模型和神经语言模型（下面介绍）</li>
</ul>
<h2 id="n-gram-语言模型">N-gram 语言模型</h2>
<ul>
<li>马尔可夫(Markov)假设——未来的事件，只取决于有限的历史</li>
<li>基于马尔可夫假设，N-gram 语言模型认为一个词出现的概率只与它前面的 n-1 个词相关
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_1,..,w_{k-1})\approx&space;p(w_k|w_{k-n&plus;1},..,w_{k-1})"><img src="../_assets/公式_20180805211644.png" height="" /></a>
</div></li>
<li>根据<strong>条件概率公式</strong>与<strong>大数定律</strong>，当语料的规模足够大时，有
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_{k-n&plus;1}^{k-1})=\frac{p(w_{k-n&plus;1}^k)}{p(w_{k-n&plus;1}^{k-1})}\approx&space;\frac{\mathrm{count}(w_{k-n&plus;1}^k)}{\mathrm{count}(w_{k-n&plus;1}^{k-1})}"><img src="../_assets/公式_20180805211936.png" height="" /></a>
</div></li>
<li>以 <code>n=2</code> 即 bi-gram 为例，有
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_{k-1})=\frac{p(w_{k-1},w_k)}{p(w_{k-1})}\approx&space;\frac{\mathrm{count}(w_{k-1},w_k)}{\mathrm{count}(w_{k-1})}"><img src="../_assets/公式_20180805212222.png" height="" /></a>
</div></li>
<li>假设词表的规模 <code>N=200000</code>（汉语的词汇量），模型参数与 `n· 的关系表
<div data-align="center">
<img src="../_assets/TIM截图20180805212441.png" height="" />
</div></li>
</ul>
<h3 id="可靠性与可区别性">可靠性与可区别性</h3>
<ul>
<li>假设没有计算和存储限制，<code>n</code> 是不是越大越好？</li>
<li>早期因为计算性能的限制，一般最大取到 <code>n=4</code>；如今，即使 <code>n&gt;10</code> 也没有问题，</li>
<li>但是，随着 <code>n</code> 的增大，模型的性能增大却不显著，这里涉及了<strong>可靠性与可区别性</strong>的问题</li>
<li>参数越多，模型的可区别性越好，但是可靠性却在下降——因为语料的规模是有限的，导致 <code>count(W)</code> 的实例数量不够，从而降低了可靠性</li>
</ul>
<h3 id="oov-问题">OOV 问题</h3>
<ul>
<li>OOV 即 Out Of Vocabulary，也就是序列中出现了词表外词，或称为<strong>未登录词</strong></li>
<li>或者说在测试集和验证集上出现了训练集中没有过的词</li>
<li>一般<strong>解决方案</strong>：
<ul>
<li>设置一个词频阈值，只有高于该阈值的词才会加入词表</li>
<li>所有低于阈值的词替换为 UNK（一个特殊符号）</li>
</ul></li>
<li>无论是统计语言模型还是神经语言模型都是类似的处理方式 &gt; <a href="#nplm-中的-oov-问题">NPLM 中的 OOV 问题</a></li>
</ul>
<h3 id="平滑处理-todo">平滑处理 TODO</h3>
<ul>
<li><code>count(W) = 0</code> 是怎么办？</li>
<li>平滑方法（层层递进）：
<ul>
<li>Add-one Smoothing (Laplace)</li>
<li>Add-k Smoothing (k&lt;1)</li>
<li>Back-off （回退）</li>
<li>Interpolation （插值法）</li>
<li>Absolute Discounting （绝对折扣法）</li>
<li>Kneser-Ney Smoothing （KN）</li>
<li>Modified Kneser-Ney &gt; <a target="_blank" rel="noopener" href="https://blog.csdn.net/baimafujinji/article/details/51297802">自然语言处理中N-Gram模型的Smoothing算法</a> - CSDN博客</li>
</ul></li>
</ul>
<h2 id="神经概率语言模型-nplm">神经概率语言模型 (NPLM)</h2>
<blockquote>
<a href="./B-专题-词向量">专题-词向量</a> - 神经概率语言模型依然是一个概率语言模型，它通过<strong>神经网络</strong>来计算概率语言模型中每个参数
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w|{\color{Red}\text{context}(w)})=g(i_w,{\color{Red}V_{context}})"><img src="../_assets/公式_20180806100950.png" height="" /></a>
</div>
</blockquote>
<pre><code>- 其中 `g` 表示神经网络，`i_w` 为 `w` 在词表中的序号，`context(w)` 为 `w` 的上下文，`V_context` 为上下文构成的特征向量。
- `V_context` 由上下文的**词向量**进一步组合而成</code></pre>
<h3 id="n-gram-神经语言模型">N-gram 神经语言模型</h3>
<blockquote>
<a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> (Bengio, et al., 2003) - 这是一个经典的神经概率语言模型，它沿用了 N-gram 模型中的思路，将 <code>w</code> 的前 <code>n-1</code> 个词作为 <code>w</code> 的上下文 <code>context(w)</code>，而 <code>V_context</code> 由这 <code>n-1</code> 个词的词向量拼接而成，即
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|{\color{Red}w_{k-n&plus;1}^{k-1}})=g(i_{w_k},{\color{Red}[c(w_{k-n&plus;1});...;c(w_{k-1})]})"><img src="../_assets/公式_20180806102047.png" height="" /></a>
</div>
</blockquote>
<pre><code>- 其中 `c(w)` 表示 `w` 的词向量
- 不同的神经语言模型中 `context(w)` 可能不同，比如 Word2Vec 中的 CBOW 模型</code></pre>
<ul>
<li>每个训练样本是形如 <code>(context(w), w)</code> 的二元对，其中 <code>context(w)</code> 取 w 的前 <code>n-1</code> 个词；当不足 <code>n-1</code>，用特殊符号填充
<ul>
<li>同一个网络只能训练特定的 <code>n</code>，不同的 <code>n</code> 需要训练不同的神经网络</li>
</ul></li>
</ul>
<h4 id="n-gram-神经语言模型的网络结构">N-gram 神经语言模型的网络结构</h4>
<ul>
<li>【<strong>输入层</strong>】首先，将 <code>context(w)</code> 中的每个词映射为一个长为 <code>m</code> 的词向量，<strong>词向量在训练开始时是随机的</strong>，并<strong>参与训练</strong>；</li>
<li>【<strong>投影层</strong>】将所有上下文词向量<strong>拼接</strong>为一个长向量，作为 <code>w</code> 的特征向量，该向量的维度为 <code>m(n-1)</code></li>
<li>【<strong>隐藏层</strong>】拼接后的向量会经过一个规模为 <code>h</code> 隐藏层，该隐层使用的激活函数为 <code>tanh</code></li>
<li>【<strong>输出层</strong>】最后会经过一个规模为 <code>N</code> 的 Softmax 输出层，从而得到词表中每个词作为下一个词的概率分布 &gt; 其中 <code>m, n, h</code> 为超参数，<code>N</code> 为词表大小，视训练集规模而定，也可以人为设置阈值</li>
<li>训练时，使用<strong>交叉熵</strong>作为损失函数</li>
<li><strong>当训练完成时</strong>，就得到了 N-gram 神经语言模型，以及副产品<strong>词向量</strong></li>
<li>整个模型可以概括为如下公式：
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=y=U\cdot\tanh(Wx&plus;p)&plus;q"><img src="../_assets/公式_2018080695721.png" height="" /></a>
</div>
<br/>
<div data-align="center">
<img src="../_assets/TIM截图20180805234123.png" height="200" />
</div>
<blockquote>
原文的模型还考虑了投影层与输出层有有边相连的情形，因而会多一个权重矩阵，但本质上是一致的： &gt;
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=y=U\cdot\tanh(W_1x&plus;p)&plus;W_2x&plus;q"><img src="../_assets/公式_2018080695819.png" height="" /></a>
</div>
<br/> &gt;
<div data-align="center">
<img src="../_assets/TIM截图20180805231056.png" height="" />
</div>
</blockquote></li>
</ul>
<h3 id="模型参数的规模与运算量">模型参数的规模与运算量</h3>
<ul>
<li>模型的超参数：<code>m, n, h, N</code>
<ul>
<li><code>m</code> 为词向量的维度，通常在 <code>10^1 ~ 10^2</code></li>
<li><code>n</code> 为 n-gram 的规模，一般小于 5</li>
<li><code>h</code> 为隐藏的单元数，一般在 <code>10^2</code></li>
<li><code>N</code> 位词表的数量，一般在 <code>10^4 ~ 10^5</code>，甚至 <code>10^6</code></li>
</ul></li>
<li>网络参数包括两部分
<ul>
<li>词向量 <code>C</code>: 一个 <code>N * m</code> 的矩阵——其中 <code>N</code> 为词表大小，<code>m</code> 为词向量的维度</li>
<li>网络参数 <code>W, U, p, q</code>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- W: h * m(n-1) 的矩阵</span><br><span class="line">- p: h * 1      的矩阵</span><br><span class="line">- U: N * h    的矩阵</span><br><span class="line">- q: N * 1    的矩阵</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>模型的运算量
<ul>
<li>主要集中在隐藏层和输出层的矩阵运算以及 SoftMax 的归一化计算</li>
<li>此后的相关研究中，主要是针对这一部分进行优化，其中就包括 <strong>Word2Vec</strong> 的工作</li>
</ul></li>
</ul>
<h3 id="相比-n-gram-模型nplm-的优势">相比 N-gram 模型，NPLM 的优势</h3>
<ul>
<li>单词之间的相似性可以通过词向量来体现 &gt; 相比神经语言模型本身，作为其副产品的词向量反而是更大的惊喜 &gt; &gt; <a href="./B-专题-词向量#词向量的理解">词向量的理解</a></li>
<li>自带平滑处理</li>
</ul>
<h3 id="nplm-中的-oov-问题">NPLM 中的 OOV 问题</h3>
<ul>
<li>在处理语料阶段，与 N-gram 中的处理方式是一样的——将不满阈值的词全部替换为 UNK <strong>神经网络</strong>中，一般有如下几种处理 UNK 的思路</li>
<li>为 UNK 分配一个随机初始化的 embedding，并<strong>参与训练</strong> &gt; 最终得到的 embedding 会有一定的语义信息，但具体好坏未知</li>
<li>把 UNK 都初始化成 0 向量，<strong>不参与训练</strong> &gt; UNK 共享相同的语义信息</li>
<li>每次都把 UNK 初始化成一个新的随机向量，<strong>不参与训练</strong> &gt; 常用的方法——因为本身每个 UNK 都不同，随机更符合对 UNK 基于最大熵的估计 &gt;&gt; <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo">How to add new embeddings for unknown words in Tensorflow (training &amp; pre-set for testing)</a> - Stack Overflow &gt;&gt; &gt;&gt; <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/45495190/initializing-out-of-vocabulary-oov-tokens">Initializing Out of Vocabulary (OOV) tokens</a> - Stack Overflow</li>
<li>基于 Char-Level 的方法 &gt; PaperWeekly 第七期 -- <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22700538?refer=paperweekly">基于Char-level的NMT OOV解决方案</a></li>
</ul>
<p>{"mode":"full","isActive":false}</p>
</div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/NLP/">NLP </a></div></div></article></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><a class="has-link-black-ter" href="/posts/28867.html"><i class="fas fa-angle-double-right"></i>RNN原理</a></h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-06-17</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-06-18</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">33 分钟读完 (大约4908个字)</span></div></div><div class="content"><ul>
<li><code>Recurrent Neural Network</code>，循环神经网络</li>
</ul>
<h2 id="simplernn"><code>SimpleRNN</code></h2>
<ul>
<li><code>SimpleRNN</code>其结构如下图所示： <img src="https://tva1.sinaimg.cn/large/008i3skNgy1grmhqh99oij30su09hmy1.jpg" />
<ul>
<li>输入为一个向量序列<span class="math inline">\(\{x_0,x_1,x_2...x_n\}\)</span> ；</li>
<li>在时间步 <span class="math inline">\(t\)</span>，序列的元素 <span class="math inline">\(x_t\)</span> 和上一时间步的输出 $h_{t-1} $一起，经过<code>RNN</code>单元处理，产生输出 <span class="math inline">\(h_t\)</span>; <span class="math display">\[h_t=ϕ(Wx_t+Uh_{t−1})\]</span> <span class="math display">\[y_t=Vh_t\]</span></li>
<li><span class="math inline">\(h_t\)</span> 为隐藏层状态，携带了序列截止时间步 <span class="math inline">\(t\)</span> 的信息；<span class="math inline">\(y_t\)</span> 为时间步 <span class="math inline">\(t\)</span> 的输出；<span class="math inline">\(h_t\)</span> 继续作为下一时间步的输入</li>
<li>整个序列被处理完，最终的输出 <span class="math inline">\(y_n\)</span> 即为<code>RNN</code>的输出；根据情况，也可返回所有的输出序列 <span class="math inline">\(\{y_0,y_1,y_2...y_n\}\)</span></li>
<li>序列的每个元素是经过<strong><em>同一个</em></strong><code>RNN</code>处理，因此待学习的参数只有一组：<span class="math inline">\(W,U,V\)</span></li>
</ul></li>
</ul></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记, </a><a class="link-muted" rel="tag" href="/tags/RNN/">RNN </a></div><a class="article-more button is-small is-size-7" href="/posts/28867.html#more"><i class="fas fa-book-reader has-text-grey"></i>  阅读更多</a></div></article></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><a class="has-link-black-ter" href="/posts/17970.html"><i class="fas fa-angle-double-right"></i>BERT基本原理及运用</a></h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-06-06</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-06-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">1 小时读完 (大约6709个字)</span></div></div><div class="content"><h1 id="bert基本原理"><code>BERT</code>基本原理</h1>
<p><code>BERT:Bidirectional Encoder Representations from Transformers</code></p>
<h2 id="bert架构">BERT架构</h2>
<ul>
<li>自编码语言模型，模型结构为 <code>Transformer</code> 的<strong>编码器</strong>；由12层或更多的<code>EncoderLayer</code>组成 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gr7wujgy4mj312a0u077a.jpg" /></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Bert/">Bert </a></div><a class="article-more button is-small is-size-7" href="/posts/17970.html#more"><i class="fas fa-book-reader has-text-grey"></i>  阅读更多</a></div></article></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><a class="has-link-black-ter" href="/posts/553.html"><i class="fas fa-angle-double-right"></i>基于HMM和Viterbi算法的序列标注</a></h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-06-06</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-06-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">26 分钟读完 (大约3944个字)</span></div></div><div class="content"><h1 id="hmm生成模型"><code>HMM</code>生成模型</h1>
<p>给定句子 <span class="math inline">\(S\)</span>，对应的输出词性序列 <span class="math inline">\(T\)</span>，<code>HMM</code>模型的联合概率： <span class="math display">\[
\begin{align}
P(T|S) &amp;= \frac{P(S|T)\cdot P(T)}{P(S)}\\
P(S,T) &amp;= P(S|T)\cdot P(T)\\
       &amp;= \prod_{i=1}^{n}P(w_i|T)\cdot P(T)\\
       &amp;= \prod_{i=1}^{n}P(w_i|t_i)\cdot P(T)\\
       &amp;= \prod_{i=1}^{n}P(w_i|t_i)\cdot P(t_i|t_{i-1})\\       
\end{align}
\]</span></p></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/HMM/">HMM, </a><a class="link-muted" rel="tag" href="/tags/Viterbi/">Viterbi </a></div><a class="article-more button is-small is-size-7" href="/posts/553.html#more"><i class="fas fa-book-reader has-text-grey"></i>  阅读更多</a></div></article></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><a class="has-link-black-ter" href="/posts/52952.html"><i class="fas fa-angle-double-right"></i>Transformer模型及源代码</a></h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-06-05</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-06-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">32 分钟读完 (大约4779个字)</span></div></div><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line">seaborn.set_context(context=<span class="string">&#x27;talk&#x27;</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Transformer/">Transformer </a></div><a class="article-more button is-small is-size-7" href="/posts/52952.html#more"><i class="fas fa-book-reader has-text-grey"></i>  阅读更多</a></div></article></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><a class="has-link-black-ter" href="/posts/61928.html"><i class="fas fa-angle-double-right"></i>词表征与词向量</a></h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-06-03</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-06-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">25 分钟读完 (大约3765个字)</span></div></div><div class="content"><h1 id="词表征word-representation">词表征(Word Representation)</h1>
<p>文本数据经过预处理后，需要转化成数值特征，便于后续处理</div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E8%AF%8D%E8%A1%A8%E5%BE%81/">词表征, </a><a class="link-muted" rel="tag" href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/">词向量 </a></div><a class="article-more button is-small is-size-7" href="/posts/61928.html#more"><i class="fas fa-book-reader has-text-grey"></i>  阅读更多</a></div></article></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><a class="has-link-black-ter" href="/posts/51094.html"><i class="fas fa-angle-double-right"></i>BiLSTM和CRF算法的序列标注原理</a></h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-06-02</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-06-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">32 分钟读完 (大约4816个字)</span></div></div><div class="content"><h1 id="crf原理"><code>CRF</code>原理</h1>
<p>条件随机场<code>(conditional_random_field)</code>，是一类<strong>判别型</strong>算法，特别适合于预测任务，任务中的 <strong><em>上下文信息或临近状态会影响当前状态</em></strong> 。如序列标注任务： &gt;判别式模型<code>(discriminative model)</code>计算条件概率，而生成式模型<code>(generative model)</code>计算联合概率分布</div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/BiLSTM/">BiLSTM, </a><a class="link-muted" rel="tag" href="/tags/CRF/">CRF </a></div><a class="article-more button is-small is-size-7" href="/posts/51094.html#more"><i class="fas fa-book-reader has-text-grey"></i>  阅读更多</a></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="MCFON"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">MCFON</p><p class="is-size-6 is-block">Natural Language Processing</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Berkeley,CA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">186</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">108</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/liushuilingran" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/C/"><span class="level-start"><span class="level-item">C++</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Golang/"><span class="level-start"><span class="level-item">Golang</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile" href="/categories/LeetCode/"><span class="level-start"><span class="level-item">LeetCode</span></span><span class="level-end"><span class="level-item tag">62</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">强化学习</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="level-start"><span class="level-item">数据结构</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">22</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">26</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/"><span class="level-start"><span class="level-item">CNN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">迁移学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%A0%B4%E8%A7%A3/"><span class="level-start"><span class="level-item">破解</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"><span class="level-start"><span class="level-item">统计学习方法</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">英语学习</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-21T13:45:40.000Z">2021-07-21</time></p><p class="title"><a href="/posts/57809.html">条件随机场</a></p><p class="categories"><a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-20T13:45:40.000Z">2021-07-20</time></p><p class="title"><a href="/posts/19440.html">EM算法及其推广</a></p><p class="categories"><a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-14T13:45:40.000Z">2021-07-14</time></p><p class="title"><a href="/posts/62790.html">NLP基础</a></p><p class="categories"><a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-06-17T09:35:34.000Z">2021-06-17</time></p><p class="title"><a href="/posts/28867.html">RNN原理</a></p><p class="categories"><a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-06-10T09:35:34.000Z">2021-06-10</time></p><p class="title"><a href="/posts/19438.html">接口 interface</a></p><p class="categories"><a href="/categories/Golang/">Golang</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2021 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>