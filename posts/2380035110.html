<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>决策树 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="决策树理论什么是决策树&amp;amp;emsp;&amp;amp;emsp;所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，从根节点到叶节点所经历的路径对应一个判定测试序列。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。"><meta property="og:type" content="blog"><meta property="og:title" content="决策树"><meta property="og:url" content="https://hunlp.com/posts/2380035110.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="决策树理论什么是决策树&amp;amp;emsp;&amp;amp;emsp;所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，从根节点到叶节点所经历的路径对应一个判定测试序列。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hunlp.com/gallery/23.jpg"><meta property="article:published_time" content="2019-07-16T08:00:26.000Z"><meta property="article:modified_time" content="2019-07-16T16:32:36.526Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="spark"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/23.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/2380035110.html"},"headline":"决策树","image":["https://hunlp.com/gallery/23.jpg"],"datePublished":"2019-07-16T08:00:26.000Z","dateModified":"2019-07-16T16:32:36.526Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"决策树理论什么是决策树&amp;emsp;&amp;emsp;所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，从根节点到叶节点所经历的路径对应一个判定测试序列。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。"}</script><link rel="canonical" href="https://hunlp.com/posts/2380035110.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>决策树</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2019-07-16</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2019-07-17</time></span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">18 分钟读完 (大约2665个字)</span></div></div><div class="content"><h2 id="决策树理论"><a href="#决策树理论" class="headerlink" title="决策树理论"></a>决策树理论</h2><h3 id="什么是决策树"><a href="#什么是决策树" class="headerlink" title="什么是决策树"></a>什么是决策树</h3><p>&emsp;&emsp;所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。<br>树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，从根节点到叶节点所经历的路径对应一个判定测试序列。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。</p>
<span id="more"></span>
<h3 id="决策树学习流程"><a href="#决策树学习流程" class="headerlink" title="决策树学习流程"></a>决策树学习流程</h3><p>&emsp;&emsp;决策树学习的主要目的是为了产生一棵泛化能力强的决策树。其基本流程遵循简单而直接的“分而治之”的策略。它的流程实现如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">输入：训练集 D=&#123;(x_1,y_1),(x_2,y_2),...,(x_m,y_m)&#125;;</span><br><span class="line">      属性集 A=&#123;a_1,a_2,...,a_d&#125;</span><br><span class="line">过程：函数GenerateTree(D,A)</span><br><span class="line">1: 生成节点node；</span><br><span class="line">2: if D中样本全属于同一类别C then</span><br><span class="line">3:    将node标记为C类叶节点，并返回</span><br><span class="line">4: end if</span><br><span class="line">5: if A为空 OR D中样本在A上取值相同 then</span><br><span class="line">6:    将node标记为叶节点，其类别标记为D中样本数量最多的类，并返回</span><br><span class="line">7: end if</span><br><span class="line">8: 从A中选择最优划分属性 a*；    //每个属性包含若干取值，这里假设有v个取值</span><br><span class="line">9: for a* 的每个值a*_v do</span><br><span class="line">10:    为node生成一个分支，令D_v表示D中在a*上取值为a*_v的样本子集；</span><br><span class="line">11:    if D_v 为空 then</span><br><span class="line">12:       将分支节点标记为叶节点，其类别标记为D中样本最多的类，并返回</span><br><span class="line">13:    else</span><br><span class="line">14:       以GenerateTree(D_v,A\&#123;a*&#125;)为分支节点</span><br><span class="line">15:    end if</span><br><span class="line">16: end for</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;决策树的生成是一个递归的过程。有三种情况会导致递归的返回：（1）当前节点包含的样本全属于同一个类别。（2）当前属性值为空，或者所有样本在所有属性上取相同的值。<br>（3）当前节点包含的样本集合为空。</p>
<p>&emsp;&emsp;在第（2）中情形下，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别；在第（3）中情形下，同样把当前节点标记为叶节点，<br>但是将其类别设定为其父节点所含样本最多的类别。这两种处理实质不同，前者利用当前节点的后验分布，后者则把父节点的样本分布作为当前节点的先验分布。</p>
<h3 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h3><p>&emsp;&emsp;构造决策树的关键步骤是分裂属性（即确定属性的不同取值，对应上面流程中的<code>a_v</code>）。所谓分裂属性就是在某个节点处按照某一属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。<br>尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：</p>
<ul>
<li><p>1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。</p>
</li>
<li><p>2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。</p>
</li>
<li><p>3、属性是连续值。此时确定一个值作为分裂点<code>split_point</code>，按照<code>&gt;split_point</code>和<code>&lt;=split_point</code>生成两个分支。</p>
</li>
</ul>
<h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h3><p>&emsp;&emsp;在决策树算法中，如何选择最优划分属性是最关键的一步。一般而言，随着划分过程的不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度(purity)”越来越高。<br>有几种度量样本集合纯度的指标。在<code>MLlib</code>中，信息熵和基尼指数用于决策树分类，方差用于决策树回归。</p>
<h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>&emsp;&emsp;信息熵是度量样本集合纯度最常用的一种指标，假设当前样本集合<code>D</code>中第<code>k</code>类样本所占的比例为<code>p_k</code>，则<code>D</code>的信息熵定义为：</p>
<div  align="center"><img src="../images/imgs2/1.1.png" width = "220" height = "75" alt="1.1" align="center" /></div>

<p>&emsp;&emsp;<code>Ent(D)</code>的值越小，则<code>D</code>的纯度越高。</p>
<h4 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h4><p>&emsp;&emsp;采用和上式相同的符号，基尼系数可以用来度量数据集<code>D</code>的纯度。</p>
<div  align="center"><img src="../images/imgs2/1.2.png" width = "310" height = "90" alt="1.2" align="center" /></div>

<p>&emsp;&emsp;直观来说，<code>Gini(D)</code>反映了从数据集<code>D</code>中随机取样两个样本，其类别标记不一致的概率。因此，<code>Gini(D)</code>越小，则数据集<code>D</code>的纯度越高。</p>
<h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><p>&emsp;&emsp;<code>MLlib</code>中使用方差来度量纯度。如下所示</p>
<div  align="center"><img src="../images/imgs2/1.3.png" width = "255" height = "70" alt="1.3" align="center" /></div>

<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>&emsp;&emsp;假设切分大小为<code>N</code>的数据集<code>D</code>为两个数据集<code>D_left</code>和<code>D_right</code>，那么信息增益可以表示为如下的形式。</p>
<div  align="center"><img src="../images/imgs2/1.4.png" width = "600" height = "60" alt="1.4" align="center" /></div>

<p>&emsp;&emsp;一般情况下，信息增益越大，则意味着使用属性<code>a</code>来进行划分所获得的纯度提升越大。因此我们可以用信息增益来进行决策树的划分属性选择。即流程中的第8步。</p>
<h3 id="决策树的优缺点"><a href="#决策树的优缺点" class="headerlink" title="决策树的优缺点"></a>决策树的优缺点</h3><p><strong>决策树的优点：</strong></p>
<ul>
<li>1 决策树易于理解和解释；</li>
<li>2 能够同时处理数据型和类别型属性；</li>
<li>3 决策树是一个白盒模型，给定一个观察模型，很容易推出相应的逻辑表达式；</li>
<li>4 在相对较短的时间内能够对大型数据作出效果良好的结果；</li>
<li>5 比较适合处理有缺失属性值的样本。</li>
</ul>
<p><strong>决策树的缺点：</strong></p>
<ul>
<li>1 对那些各类别数据量不一致的数据，在决策树种，信息增益的结果偏向那些具有更多数值的特征；</li>
<li>2 容易过拟合；</li>
<li>3 忽略了数据集中属性之间的相关性。</li>
</ul>
<h2 id="实例与源码分析"><a href="#实例与源码分析" class="headerlink" title="实例与源码分析"></a>实例与源码分析</h2><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>&emsp;&emsp;下面的例子用于分类。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.<span class="type">DecisionTree</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.model.<span class="type">DecisionTreeModel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="comment">// Load and parse the data file.</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;data/mllib/sample_libsvm_data.txt&quot;</span>)</span><br><span class="line"><span class="comment">// Split the data into training and test sets (30% held out for testing)</span></span><br><span class="line"><span class="keyword">val</span> splits = data.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line"><span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// Train a DecisionTree model.</span></span><br><span class="line"><span class="comment">//  Empty categoricalFeaturesInfo indicates all features are continuous.</span></span><br><span class="line"><span class="keyword">val</span> numClasses = <span class="number">2</span></span><br><span class="line"><span class="keyword">val</span> categoricalFeaturesInfo = <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>]()</span><br><span class="line"><span class="keyword">val</span> impurity = <span class="string">&quot;gini&quot;</span></span><br><span class="line"><span class="keyword">val</span> maxDepth = <span class="number">5</span></span><br><span class="line"><span class="keyword">val</span> maxBins = <span class="number">32</span></span><br><span class="line"><span class="keyword">val</span> model = <span class="type">DecisionTree</span>.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,</span><br><span class="line">  impurity, maxDepth, maxBins)</span><br><span class="line"><span class="comment">// Evaluate model on test instances and compute test error</span></span><br><span class="line"><span class="keyword">val</span> labelAndPreds = testData.map &#123; point =&gt;</span><br><span class="line">  <span class="keyword">val</span> prediction = model.predict(point.features)</span><br><span class="line">  (point.label, prediction)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> testErr = labelAndPreds.filter(r =&gt; r._1 != r._2).count().toDouble / testData.count()</span><br><span class="line">println(<span class="string">&quot;Test Error = &quot;</span> + testErr)</span><br><span class="line">println(<span class="string">&quot;Learned classification tree model:\n&quot;</span> + model.toDebugString)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;下面的例子用于回归。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.<span class="type">DecisionTree</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.model.<span class="type">DecisionTreeModel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="comment">// Load and parse the data file.</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;data/mllib/sample_libsvm_data.txt&quot;</span>)</span><br><span class="line"><span class="comment">// Split the data into training and test sets (30% held out for testing)</span></span><br><span class="line"><span class="keyword">val</span> splits = data.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line"><span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// Train a DecisionTree model.</span></span><br><span class="line"><span class="comment">//  Empty categoricalFeaturesInfo indicates all features are continuous.</span></span><br><span class="line"><span class="keyword">val</span> categoricalFeaturesInfo = <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>]()</span><br><span class="line"><span class="keyword">val</span> impurity = <span class="string">&quot;variance&quot;</span></span><br><span class="line"><span class="keyword">val</span> maxDepth = <span class="number">5</span></span><br><span class="line"><span class="keyword">val</span> maxBins = <span class="number">32</span></span><br><span class="line"><span class="keyword">val</span> model = <span class="type">DecisionTree</span>.trainRegressor(trainingData, categoricalFeaturesInfo, impurity,</span><br><span class="line">  maxDepth, maxBins)</span><br><span class="line"><span class="comment">// Evaluate model on test instances and compute test error</span></span><br><span class="line"><span class="keyword">val</span> labelsAndPredictions = testData.map &#123; point =&gt;</span><br><span class="line">  <span class="keyword">val</span> prediction = model.predict(point.features)</span><br><span class="line">  (point.label, prediction)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> testMSE = labelsAndPredictions.map&#123; <span class="keyword">case</span> (v, p) =&gt; math.pow(v - p, <span class="number">2</span>) &#125;.mean()</span><br><span class="line">println(<span class="string">&quot;Test Mean Squared Error = &quot;</span> + testMSE)</span><br><span class="line">println(<span class="string">&quot;Learned regression tree model:\n&quot;</span> + model.toDebugString)</span><br></pre></td></tr></table></figure>

<h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>&emsp;&emsp;在<code>MLlib</code>中，决策树的实现和随机森林的实现是在一起的。随机森林实现中，当树的个数为1时，它的实现即为决策树的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(input: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>]): <span class="type">DecisionTreeModel</span> = &#123;</span><br><span class="line">    <span class="comment">//树个数为1</span></span><br><span class="line">    <span class="keyword">val</span> rf = <span class="keyword">new</span> <span class="type">RandomForest</span>(strategy, numTrees = <span class="number">1</span>, featureSubsetStrategy = <span class="string">&quot;all&quot;</span>, seed = <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> rfModel = rf.run(input)</span><br><span class="line">    rfModel.trees(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;这里的<code>strategy</code>是<code>Strategy</code>的实例，它包含如下信息：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Stores all the configuration options for tree construction</span></span><br><span class="line"><span class="comment"> * @param algo  Learning goal.  Supported:</span></span><br><span class="line"><span class="comment"> *              [[org.apache.spark.mllib.tree.configuration.Algo.Classification]],</span></span><br><span class="line"><span class="comment"> *              [[org.apache.spark.mllib.tree.configuration.Algo.Regression]]</span></span><br><span class="line"><span class="comment"> * @param impurity Criterion used for information gain calculation.</span></span><br><span class="line"><span class="comment"> *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],</span></span><br><span class="line"><span class="comment"> *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].</span></span><br><span class="line"><span class="comment"> *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].</span></span><br><span class="line"><span class="comment"> * @param maxDepth Maximum depth of the tree.</span></span><br><span class="line"><span class="comment"> *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.</span></span><br><span class="line"><span class="comment"> * @param numClasses Number of classes for classification.</span></span><br><span class="line"><span class="comment"> *                                    (Ignored for regression.)</span></span><br><span class="line"><span class="comment"> *                                    Default value is 2 (binary classification).</span></span><br><span class="line"><span class="comment"> * @param maxBins Maximum number of bins used for discretizing continuous features and</span></span><br><span class="line"><span class="comment"> *                for choosing how to split on features at each node.</span></span><br><span class="line"><span class="comment"> *                More bins give higher granularity.</span></span><br><span class="line"><span class="comment"> * @param quantileCalculationStrategy Algorithm for calculating quantiles.  Supported:</span></span><br><span class="line"><span class="comment"> *                             [[org.apache.spark.mllib.tree.configuration.QuantileStrategy.Sort]]</span></span><br><span class="line"><span class="comment"> * @param categoricalFeaturesInfo A map storing information about the categorical variables and the</span></span><br><span class="line"><span class="comment"> *                                number of discrete values they take. For example, an entry (n -&gt;</span></span><br><span class="line"><span class="comment"> *                                k) implies the feature n is categorical with k categories 0,</span></span><br><span class="line"><span class="comment"> *                                1, 2, ... , k-1. It&#x27;s important to note that features are</span></span><br><span class="line"><span class="comment"> *                                zero-indexed.</span></span><br><span class="line"><span class="comment"> * @param minInstancesPerNode Minimum number of instances each child must have after split.</span></span><br><span class="line"><span class="comment"> *                            Default value is 1. If a split cause left or right child</span></span><br><span class="line"><span class="comment"> *                            to have less than minInstancesPerNode,</span></span><br><span class="line"><span class="comment"> *                            this split will not be considered as a valid split.</span></span><br><span class="line"><span class="comment"> * @param minInfoGain Minimum information gain a split must get. Default value is 0.0.</span></span><br><span class="line"><span class="comment"> *                    If a split has less information gain than minInfoGain,</span></span><br><span class="line"><span class="comment"> *                    this split will not be considered as a valid split.</span></span><br><span class="line"><span class="comment"> * @param maxMemoryInMB Maximum memory in MB allocated to histogram aggregation. Default value is</span></span><br><span class="line"><span class="comment"> *                      256 MB.</span></span><br><span class="line"><span class="comment"> * @param subsamplingRate Fraction of the training data used for learning decision tree.</span></span><br><span class="line"><span class="comment"> * @param useNodeIdCache If this is true, instead of passing trees to executors, the algorithm will</span></span><br><span class="line"><span class="comment"> *                      maintain a separate RDD of node Id cache for each row.</span></span><br><span class="line"><span class="comment"> * @param checkpointInterval How often to checkpoint when the node Id cache gets updated.</span></span><br><span class="line"><span class="comment"> *                           E.g. 10 means that the cache will get checkpointed every 10 updates. If</span></span><br><span class="line"><span class="comment"> *                           the checkpoint directory is not set in</span></span><br><span class="line"><span class="comment"> *                           [[org.apache.spark.SparkContext]], this setting is ignored.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Strategy</span> <span class="title">@Since</span>(<span class="params">&quot;1.3.0&quot;</span>) (<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @<span class="type">Since</span>(&quot;1.0.0&quot;</span>) <span class="title">@BeanProperty</span> <span class="title">var</span> <span class="title">algo</span></span>: <span class="type">Algo</span>,<span class="comment">//选择的算法，有分类和回归两种选择</span></span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.0.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> impurity: <span class="type">Impurity</span>,<span class="comment">//纯度有熵、基尼系数、方差三种选择</span></span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.0.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> maxDepth: <span class="type">Int</span>,<span class="comment">//树的最大深度</span></span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> numClasses: <span class="type">Int</span> = <span class="number">2</span>,<span class="comment">//分类数</span></span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.0.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> maxBins: <span class="type">Int</span> = <span class="number">32</span>,<span class="comment">//最大子树个数</span></span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.0.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> quantileCalculationStrategy: <span class="type">QuantileStrategy</span> = <span class="type">Sort</span>,</span><br><span class="line">    <span class="comment">//保存类别变量以及相应的离散值。一个entry (n -&gt;k) 表示特征n属于k个类别，分别是0,1,...,k-1</span></span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.0.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> categoricalFeaturesInfo: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>] = <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>](),</span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> minInstancesPerNode: <span class="type">Int</span> = <span class="number">1</span>,</span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> minInfoGain: <span class="type">Double</span> = <span class="number">0.0</span>,</span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.0.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> maxMemoryInMB: <span class="type">Int</span> = <span class="number">256</span>,</span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> subsamplingRate: <span class="type">Double</span> = <span class="number">1</span>,</span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> useNodeIdCache: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">    <span class="meta">@Since</span>(<span class="string">&quot;1.2.0&quot;</span>) <span class="meta">@BeanProperty</span> <span class="keyword">var</span> checkpointInterval: <span class="type">Int</span> = <span class="number">10</span>) <span class="keyword">extends</span> <span class="type">Serializable</span></span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;决策树的实现我们在<a href="../%E7%BB%84%E5%90%88%E6%A0%91/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/random-forests.md">随机森林</a>专题介绍。这里我们只需要知道，当随机森林的树个数为1时，它即为决策树，<br>并且此时，树的训练所用的特征是全部特征，而不是随机选择的部分特征。即<code>featureSubsetStrategy = &quot;all&quot;</code>。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>决策树</p><p><a href="https://hunlp.com/posts/2380035110.html">https://hunlp.com/posts/2380035110.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-07-16</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-07-17</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/spark/">spark</a></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/3621251491.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">TF-IDF</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/4152295526.html"><span class="level-item">朴素贝叶斯</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "25a1c541b68154ce7199e883ec836563",
            repo: "Cartride.github.io",
            owner: "Cartride",
            clientID: "8f4a2426c347380a6ee4",
            clientSecret: "8dc8cd44b071426b35d0bd60634941371170b798",
            admin: ["Cartride"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2021 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>