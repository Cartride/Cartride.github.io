<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>NLP基础 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="解决 NLP 问题的一般思路 123这个问题人类可以做好么？  - 可以 -&amp;gt; 记录自己的思路 -&amp;gt; 设计流程让机器完成你的思路  - 很难 -&amp;gt; 尝试从计算机的角度来思考问题 NLP 的历史进程  规则系统  正则表达式&amp;#x2F;自动机 规则是固定的 搜索引擎 12345“豆瓣酱用英语怎么说？”规则：“xx用英语怎么说？” &amp;#x3D;&amp;gt; translate(XX, English)“我饿"><meta property="og:type" content="blog"><meta property="og:title" content="NLP基础"><meta property="og:url" content="https://hunlp.com/posts/62790.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="解决 NLP 问题的一般思路 123这个问题人类可以做好么？  - 可以 -&amp;gt; 记录自己的思路 -&amp;gt; 设计流程让机器完成你的思路  - 很难 -&amp;gt; 尝试从计算机的角度来思考问题 NLP 的历史进程  规则系统  正则表达式&amp;#x2F;自动机 规则是固定的 搜索引擎 12345“豆瓣酱用英语怎么说？”规则：“xx用英语怎么说？” &amp;#x3D;&amp;gt; translate(XX, English)“我饿"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtdi88xxj30k3088755.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgte13jntj30oe099t9w.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtey2krlj30fa0cjq43.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtfaeewqj306t00o3yg.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtfp9qxhj304j00q0sn.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtg79861j307600odfs.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtgib1pej30a5024dfx.jpg"><meta property="og:image" content="https://hunlp.com/_assets/TIM截图20180728212554.png"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtihht0dj309802yq34.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtirnmscj308x01paa2.jpg"><meta property="og:image" content="https://hunlp.com/_assets/公式_20180805204149.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_20180805211530.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_20180805211644.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_20180805211936.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_20180805212222.png"><meta property="og:image" content="https://hunlp.com/_assets/TIM截图20180805212441.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_20180806100950.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_20180806102047.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_2018080695721.png"><meta property="og:image" content="https://hunlp.com/_assets/TIM截图20180805234123.png"><meta property="og:image" content="https://hunlp.com/_assets/公式_2018080695819.png"><meta property="og:image" content="https://hunlp.com/_assets/TIM截图20180805231056.png"><meta property="article:published_time" content="2021-07-14T13:45:40.000Z"><meta property="article:modified_time" content="2021-08-15T16:05:02.111Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="NLP"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gsgtdi88xxj30k3088755.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/62790.html"},"headline":"NLP基础","image":["https://tva1.sinaimg.cn/large/008i3skNly1gsgtdi88xxj30k3088755.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gsgte13jntj30oe099t9w.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gsgtey2krlj30fa0cjq43.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gsgtfaeewqj306t00o3yg.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gsgtfp9qxhj304j00q0sn.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gsgtg79861j307600odfs.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gsgtgib1pej30a5024dfx.jpg","https://hunlp.com/_assets/TIM截图20180728212554.png","https://tva1.sinaimg.cn/large/008i3skNly1gsgtihht0dj309802yq34.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gsgtirnmscj308x01paa2.jpg","https://hunlp.com/_assets/公式_20180805204149.png","https://hunlp.com/_assets/公式_20180805211530.png","https://hunlp.com/_assets/公式_20180805211644.png","https://hunlp.com/_assets/公式_20180805211936.png","https://hunlp.com/_assets/公式_20180805212222.png","https://hunlp.com/_assets/TIM截图20180805212441.png","https://hunlp.com/_assets/公式_20180806100950.png","https://hunlp.com/_assets/公式_20180806102047.png","https://hunlp.com/_assets/公式_2018080695721.png","https://hunlp.com/_assets/TIM截图20180805234123.png","https://hunlp.com/_assets/公式_2018080695819.png","https://hunlp.com/_assets/TIM截图20180805231056.png"],"datePublished":"2021-07-14T13:45:40.000Z","dateModified":"2021-08-15T16:05:02.111Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"解决 NLP 问题的一般思路 123这个问题人类可以做好么？  - 可以 -&gt; 记录自己的思路 -&gt; 设计流程让机器完成你的思路  - 很难 -&gt; 尝试从计算机的角度来思考问题 NLP 的历史进程  规则系统  正则表达式&#x2F;自动机 规则是固定的 搜索引擎 12345“豆瓣酱用英语怎么说？”规则：“xx用英语怎么说？” &#x3D;&gt; translate(XX, English)“我饿"}</script><link rel="canonical" href="https://hunlp.com/posts/62790.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>NLP基础</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-07-14</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-08-16</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">22 分钟读完 (大约3328个字)</span></div></div><div class="content"><h2 id="解决-nlp-问题的一般思路">解决 NLP 问题的一般思路</h2>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个问题人类可以做好么？</span><br><span class="line">  - 可以 -&gt; 记录自己的思路 -&gt; 设计流程让机器完成你的思路</span><br><span class="line">  - 很难 -&gt; 尝试从计算机的角度来思考问题</span><br></pre></td></tr></table></figure>
<h2 id="nlp-的历史进程">NLP 的历史进程</h2>
<ul>
<li><strong>规则系统</strong>
<ul>
<li>正则表达式/自动机</li>
<li>规则是固定的</li>
<li><strong>搜索引擎</strong> <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">“豆瓣酱用英语怎么说？”</span><br><span class="line">规则：“xx用英语怎么说？” =&gt; translate(XX, English)</span><br><span class="line"></span><br><span class="line">“我饿了”</span><br><span class="line">规则：“我饿（死）了” =&gt; recommend(饭店，地点)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><strong>概率系统</strong>
<ul>
<li><p>规则从数据中<strong>抽取</strong></p></li>
<li><p>规则是有<strong>概率</strong>的</p></li>
<li><p>概率系统的一般<strong>工作方式</strong> <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">流程设计</span><br><span class="line">收集训练数据</span><br><span class="line">    预处理</span><br><span class="line">    特征工程</span><br><span class="line">        分类器（机器学习算法）</span><br><span class="line">        预测</span><br><span class="line">            评价</span><br></pre></td></tr></table></figure> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtdi88xxj30k3088755.jpg" /></p>
<ul>
<li>最重要的部分：数据收集、预处理、特征工程</li>
</ul></li>
<li><p>示例 <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">任务：</span><br><span class="line">“豆瓣酱用英语怎么说” =&gt; translate(豆瓣酱，Eng)</span><br><span class="line"></span><br><span class="line">流程设计（序列标注）：</span><br><span class="line">子任务1： 找出目标语言 “豆瓣酱用 **英语** 怎么说”</span><br><span class="line">子任务2： 找出翻译目标 “ **豆瓣酱** 用英语怎么说”</span><br><span class="line"></span><br><span class="line">收集训练数据：</span><br><span class="line">（子任务1）</span><br><span class="line">“豆瓣酱用英语怎么说”</span><br><span class="line">“茄子用英语怎么说”</span><br><span class="line">“黄瓜怎么翻译成英语”</span><br><span class="line"></span><br><span class="line">预处理：</span><br><span class="line">分词：“豆瓣酱 用 英语 怎么说”</span><br><span class="line"></span><br><span class="line">抽取特征：</span><br><span class="line">（前后各一个词）</span><br><span class="line">0 茄子：    &lt; <span class="built_in">_</span> 用</span><br><span class="line">0 用：      豆瓣酱 <span class="built_in">_</span> 英语</span><br><span class="line">1 英语：    用 <span class="built_in">_</span> 怎么说</span><br><span class="line">0 怎么说：  英语 <span class="built_in">_</span> &gt;</span><br><span class="line"></span><br><span class="line">分类器：</span><br><span class="line">SVM/CRF/HMM/RNN</span><br><span class="line"></span><br><span class="line">预测：</span><br><span class="line">0.1 茄子：    &lt; <span class="built_in">_</span> 用</span><br><span class="line">0.1 用：      豆瓣酱 <span class="built_in">_</span> 英语</span><br><span class="line">0.7 英语：    用 <span class="built_in">_</span> 怎么说</span><br><span class="line">0.1 怎么说：  英语 <span class="built_in">_</span> &gt;</span><br><span class="line"></span><br><span class="line">评价：</span><br><span class="line">准确率</span><br></pre></td></tr></table></figure></p></li>
</ul></li>
<li>概率系统的优/缺点
<ul>
<li><code>+</code> 规则更加贴近于真实事件中的规则，因而效果往往比较好</li>
<li><code>-</code> 特征是由专家/人指定的；</li>
<li><code>-</code> 流程是由专家/人设计的；</li>
<li><code>-</code> 存在独立的<strong>子任务</strong></li>
</ul></li>
<li><strong>深度学习</strong>
<ul>
<li>深度学习相对概率模型的优势
<ul>
<li>特征是由专家指定的 <code>-&gt;</code> 特征是由深度学习自己提取的</li>
<li>流程是由专家设计的 <code>-&gt;</code> 模型结构是由专家设计的</li>
<li>存在独立的子任务 <code>-&gt;</code> End-to-End Training</li>
</ul></li>
</ul></li>
</ul>
<h2 id="seq2seq-模型">Seq2Seq 模型</h2>
<ul>
<li>大部分自然语言问题都可以使用 Seq2Seq 模型解决 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgte13jntj30oe099t9w.jpg" /></li>
<li><strong>“万物”皆 Seq2Seq</strong> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtey2krlj30fa0cjq43.jpg" /></li>
</ul>
<h2 id="评价机制">评价机制</h2>
<h3 id="困惑度-perplexity-ppx">困惑度 (Perplexity, PPX)</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> - Wikipedia - 在信息论中，perplexity 用于度量一个<strong>概率分布</strong>或<strong>概率模型</strong>预测样本的好坏程度</p>
</blockquote>
<pre><code>&gt; ../机器学习/[信息论](../A-机器学习/A-机器学习基础#信息论) </code></pre>
<h3>
基本公式
</h3>
<ul>
<li><p><strong>概率分布</strong>（离散）的困惑度 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtfaeewqj306t00o3yg.jpg" /></p>
<blockquote>
<p>其中 <code>H(p)</code> 即<strong>信息熵</strong></p>
</blockquote></li>
<li><p><strong>概率模型</strong>的困惑度 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtfp9qxhj304j00q0sn.jpg" /></p>
<blockquote>
<p>通常 <code>b=2</code></p>
</blockquote></li>
<li><p><strong>指数部分</strong>也可以是<strong>交叉熵</strong>的形式，此时困惑度相当于交叉熵的指数形式 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtg79861j307600odfs.jpg" /></p>
<blockquote>
<p>其中 <code>p~</code> 为<strong>测试集</strong>中的经验分布——<code>p~(x) = n/N</code>，其中 <code>n</code> 为 x 的出现次数，N 为测试集的大小</p>
</blockquote></li>
</ul>
<p><strong>语言模型中的 PPX</strong> - 在 <strong>NLP</strong> 中，困惑度常作为<strong>语言模型</strong>的评价指标 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtgib1pej30a5024dfx.jpg" /></p>
<ul>
<li><p>直观来说，就是下一个<strong>候选词数目</strong>的期望值——</p>
<p>如果不使用任何模型，那么下一个候选词的数量就是整个词表的数量；通过使用 <code>bi-gram</code>语言模型，可以将整个数量限制到 <code>200</code> 左右</p></li>
</ul>
<h3 id="bleu">BLEU</h3>
<blockquote>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21190081/article/details/53115580">一种机器翻译的评价准则——BLEU</a> - CSDN博客 - 机器翻译评价准则 - 计算公式
<div data-align="center">
<img src="../_assets/TIM截图20180728212554.png" height="" />
</div>
</blockquote>
<p>其中 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtihht0dj309802yq34.jpg" /></p>
<pre><code>&gt; `c` 为生成句子的长度；`r` 为参考句子的长度——目的是**惩罚**长度过短的候选句子</code></pre>
<ul>
<li><p>为了计算方便，会加一层 <code>log</code> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gsgtirnmscj308x01paa2.jpg" /></p>
<blockquote>
<p>通常 <code>N=4, w_n=1/4</code></p>
</blockquote></li>
</ul>
<h3 id="rouge">ROUGE</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25222361/article/details/78694617">自动文摘评测方法：Rouge-1、Rouge-2、Rouge-L、Rouge-S</a> - CSDN博客 - 一种机器翻译/自动摘要的评价准则</p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/joshuaxx316/article/details/58696552">BLEU，ROUGE，METEOR，ROUGE-浅述自然语言处理机器翻译常用评价度量</a> - CSDN博客</p>
</blockquote>
<h1 id="语言模型">语言模型</h1>
<h2 id="xx-模型的含义">XX 模型的含义</h2>
<ul>
<li>如果能使用某个方法对 XX <strong>打分</strong>（Score），那么就可以把这个方法称为 “<strong>XX 模型</strong>”
<ul>
<li><strong>篮球明星模型</strong>: <code>Score(库里)</code>、<code>Score(詹姆斯)</code></li>
<li><strong>话题模型</strong>——对一段话是否在谈论某一话题的打分 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Score( NLP | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.8</span><br><span class="line">Score( ACM | &quot;什么 是 语言 模型？&quot; ) --&gt; 0.05</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul>
<h2 id="概率统计语言模型-plm-slm">概率/统计语言模型 (PLM, SLM)</h2>
<ul>
<li><strong>语言模型</strong>是一种对语言打分的方法；而<strong>概率语言模型</strong>把语言的“得分”通过<strong>概率</strong>来体现</li>
<li>具体来说，概率语言模型计算的是<strong>一个序列</strong>作为一句话可能的概率 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Score(&quot;什么 是 语言 模型&quot;) --&gt; 0.05   # 比较常见的说法，得分比较高</span><br><span class="line">Score(&quot;什么 有 语言 模型&quot;) --&gt; 0.01   # 不太常见的说法，得分比较低</span><br></pre></td></tr></table></figure></li>
<li>以上过程可以形式化为：
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(W)=p(w_1^T)=p(w_1,w_2,...,w_T"><img src="../_assets/公式_20180805204149.png" height="" /></a>
</div>
根据贝叶斯公式，有
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_1^T)=p(w_1)\cdot&space;p(w_2|w_1)\cdot&space;p(w_3|w_1^2)\cdots&space;p(w_T|w_1^{T-1})"><img src="../_assets/公式_20180805211530.png" height="" /></a>
</div></li>
<li>其中每个条件概率就是<strong>模型的参数</strong>；如果这个参数都是已知的，那么就能得到整个序列的概率了</li>
</ul>
<h3 id="参数的规模">参数的规模</h3>
<ul>
<li>设词表的大小为 <code>N</code>，考虑长度为 <code>T</code> 的句子，理论上有 <code>N^T</code> 种可能的句子，每个句子中有 <code>T</code> 个参数，那么参数的数量将达到 <code>O(T*N^T)</code></li>
</ul>
<h3 id="可用的概率模型">可用的概率模型</h3>
<ul>
<li>统计语言模型实际上是一个概率模型，所以常见的概率模型都可以用于求解这些参数</li>
<li>常见的概率模型有：N-gram 模型、决策树、最大熵模型、隐马尔可夫模型、条件随机场、神经网络等</li>
<li>目前常用于语言模型的是 N-gram 模型和神经语言模型（下面介绍）</li>
</ul>
<h2 id="n-gram-语言模型">N-gram 语言模型</h2>
<ul>
<li>马尔可夫(Markov)假设——未来的事件，只取决于有限的历史</li>
<li>基于马尔可夫假设，N-gram 语言模型认为一个词出现的概率只与它前面的 n-1 个词相关
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_1,..,w_{k-1})\approx&space;p(w_k|w_{k-n&plus;1},..,w_{k-1})"><img src="../_assets/公式_20180805211644.png" height="" /></a>
</div></li>
<li>根据<strong>条件概率公式</strong>与<strong>大数定律</strong>，当语料的规模足够大时，有
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_{k-n&plus;1}^{k-1})=\frac{p(w_{k-n&plus;1}^k)}{p(w_{k-n&plus;1}^{k-1})}\approx&space;\frac{\mathrm{count}(w_{k-n&plus;1}^k)}{\mathrm{count}(w_{k-n&plus;1}^{k-1})}"><img src="../_assets/公式_20180805211936.png" height="" /></a>
</div></li>
<li>以 <code>n=2</code> 即 bi-gram 为例，有
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|w_{k-1})=\frac{p(w_{k-1},w_k)}{p(w_{k-1})}\approx&space;\frac{\mathrm{count}(w_{k-1},w_k)}{\mathrm{count}(w_{k-1})}"><img src="../_assets/公式_20180805212222.png" height="" /></a>
</div></li>
<li>假设词表的规模 <code>N=200000</code>（汉语的词汇量），模型参数与 `n· 的关系表
<div data-align="center">
<img src="../_assets/TIM截图20180805212441.png" height="" />
</div></li>
</ul>
<h3 id="可靠性与可区别性">可靠性与可区别性</h3>
<ul>
<li>假设没有计算和存储限制，<code>n</code> 是不是越大越好？</li>
<li>早期因为计算性能的限制，一般最大取到 <code>n=4</code>；如今，即使 <code>n&gt;10</code> 也没有问题，</li>
<li>但是，随着 <code>n</code> 的增大，模型的性能增大却不显著，这里涉及了<strong>可靠性与可区别性</strong>的问题</li>
<li>参数越多，模型的可区别性越好，但是可靠性却在下降——因为语料的规模是有限的，导致 <code>count(W)</code> 的实例数量不够，从而降低了可靠性</li>
</ul>
<h3 id="oov-问题">OOV 问题</h3>
<ul>
<li>OOV 即 Out Of Vocabulary，也就是序列中出现了词表外词，或称为<strong>未登录词</strong></li>
<li>或者说在测试集和验证集上出现了训练集中没有过的词</li>
<li>一般<strong>解决方案</strong>：
<ul>
<li>设置一个词频阈值，只有高于该阈值的词才会加入词表</li>
<li>所有低于阈值的词替换为 UNK（一个特殊符号）</li>
</ul></li>
<li>无论是统计语言模型还是神经语言模型都是类似的处理方式 &gt; <a href="#nplm-中的-oov-问题">NPLM 中的 OOV 问题</a></li>
</ul>
<h3 id="平滑处理-todo">平滑处理 TODO</h3>
<ul>
<li><code>count(W) = 0</code> 是怎么办？</li>
<li>平滑方法（层层递进）：
<ul>
<li>Add-one Smoothing (Laplace)</li>
<li>Add-k Smoothing (k&lt;1)</li>
<li>Back-off （回退）</li>
<li>Interpolation （插值法）</li>
<li>Absolute Discounting （绝对折扣法）</li>
<li>Kneser-Ney Smoothing （KN）</li>
<li>Modified Kneser-Ney &gt; <a target="_blank" rel="noopener" href="https://blog.csdn.net/baimafujinji/article/details/51297802">自然语言处理中N-Gram模型的Smoothing算法</a> - CSDN博客</li>
</ul></li>
</ul>
<h2 id="神经概率语言模型-nplm">神经概率语言模型 (NPLM)</h2>
<blockquote>
<a href="./B-专题-词向量">专题-词向量</a> - 神经概率语言模型依然是一个概率语言模型，它通过<strong>神经网络</strong>来计算概率语言模型中每个参数
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w|{\color{Red}\text{context}(w)})=g(i_w,{\color{Red}V_{context}})"><img src="../_assets/公式_20180806100950.png" height="" /></a>
</div>
</blockquote>
<pre><code>- 其中 `g` 表示神经网络，`i_w` 为 `w` 在词表中的序号，`context(w)` 为 `w` 的上下文，`V_context` 为上下文构成的特征向量。
- `V_context` 由上下文的**词向量**进一步组合而成</code></pre>
<h3 id="n-gram-神经语言模型">N-gram 神经语言模型</h3>
<blockquote>
<a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> (Bengio, et al., 2003) - 这是一个经典的神经概率语言模型，它沿用了 N-gram 模型中的思路，将 <code>w</code> 的前 <code>n-1</code> 个词作为 <code>w</code> 的上下文 <code>context(w)</code>，而 <code>V_context</code> 由这 <code>n-1</code> 个词的词向量拼接而成，即
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=p(w_k|{\color{Red}w_{k-n&plus;1}^{k-1}})=g(i_{w_k},{\color{Red}[c(w_{k-n&plus;1});...;c(w_{k-1})]})"><img src="../_assets/公式_20180806102047.png" height="" /></a>
</div>
</blockquote>
<pre><code>- 其中 `c(w)` 表示 `w` 的词向量
- 不同的神经语言模型中 `context(w)` 可能不同，比如 Word2Vec 中的 CBOW 模型</code></pre>
<ul>
<li>每个训练样本是形如 <code>(context(w), w)</code> 的二元对，其中 <code>context(w)</code> 取 w 的前 <code>n-1</code> 个词；当不足 <code>n-1</code>，用特殊符号填充
<ul>
<li>同一个网络只能训练特定的 <code>n</code>，不同的 <code>n</code> 需要训练不同的神经网络</li>
</ul></li>
</ul>
<h4 id="n-gram-神经语言模型的网络结构">N-gram 神经语言模型的网络结构</h4>
<ul>
<li>【<strong>输入层</strong>】首先，将 <code>context(w)</code> 中的每个词映射为一个长为 <code>m</code> 的词向量，<strong>词向量在训练开始时是随机的</strong>，并<strong>参与训练</strong>；</li>
<li>【<strong>投影层</strong>】将所有上下文词向量<strong>拼接</strong>为一个长向量，作为 <code>w</code> 的特征向量，该向量的维度为 <code>m(n-1)</code></li>
<li>【<strong>隐藏层</strong>】拼接后的向量会经过一个规模为 <code>h</code> 隐藏层，该隐层使用的激活函数为 <code>tanh</code></li>
<li>【<strong>输出层</strong>】最后会经过一个规模为 <code>N</code> 的 Softmax 输出层，从而得到词表中每个词作为下一个词的概率分布 &gt; 其中 <code>m, n, h</code> 为超参数，<code>N</code> 为词表大小，视训练集规模而定，也可以人为设置阈值</li>
<li>训练时，使用<strong>交叉熵</strong>作为损失函数</li>
<li><strong>当训练完成时</strong>，就得到了 N-gram 神经语言模型，以及副产品<strong>词向量</strong></li>
<li>整个模型可以概括为如下公式：
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=y=U\cdot\tanh(Wx&plus;p)&plus;q"><img src="../_assets/公式_2018080695721.png" height="" /></a>
</div>
<br/>
<div data-align="center">
<img src="../_assets/TIM截图20180805234123.png" height="200" />
</div>
<blockquote>
原文的模型还考虑了投影层与输出层有有边相连的情形，因而会多一个权重矩阵，但本质上是一致的： &gt;
<div data-align="center">
<a target="_blank" rel="noopener" href="http://www.codecogs.com/eqnedit.php?latex=y=U\cdot\tanh(W_1x&plus;p)&plus;W_2x&plus;q"><img src="../_assets/公式_2018080695819.png" height="" /></a>
</div>
<br/> &gt;
<div data-align="center">
<img src="../_assets/TIM截图20180805231056.png" height="" />
</div>
</blockquote></li>
</ul>
<h3 id="模型参数的规模与运算量">模型参数的规模与运算量</h3>
<ul>
<li>模型的超参数：<code>m, n, h, N</code>
<ul>
<li><code>m</code> 为词向量的维度，通常在 <code>10^1 ~ 10^2</code></li>
<li><code>n</code> 为 n-gram 的规模，一般小于 5</li>
<li><code>h</code> 为隐藏的单元数，一般在 <code>10^2</code></li>
<li><code>N</code> 位词表的数量，一般在 <code>10^4 ~ 10^5</code>，甚至 <code>10^6</code></li>
</ul></li>
<li>网络参数包括两部分
<ul>
<li>词向量 <code>C</code>: 一个 <code>N * m</code> 的矩阵——其中 <code>N</code> 为词表大小，<code>m</code> 为词向量的维度</li>
<li>网络参数 <code>W, U, p, q</code>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- W: h * m(n-1) 的矩阵</span><br><span class="line">- p: h * 1      的矩阵</span><br><span class="line">- U: N * h    的矩阵</span><br><span class="line">- q: N * 1    的矩阵</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>模型的运算量
<ul>
<li>主要集中在隐藏层和输出层的矩阵运算以及 SoftMax 的归一化计算</li>
<li>此后的相关研究中，主要是针对这一部分进行优化，其中就包括 <strong>Word2Vec</strong> 的工作</li>
</ul></li>
</ul>
<h3 id="相比-n-gram-模型nplm-的优势">相比 N-gram 模型，NPLM 的优势</h3>
<ul>
<li>单词之间的相似性可以通过词向量来体现 &gt; 相比神经语言模型本身，作为其副产品的词向量反而是更大的惊喜 &gt; &gt; <a href="./B-专题-词向量#词向量的理解">词向量的理解</a></li>
<li>自带平滑处理</li>
</ul>
<h3 id="nplm-中的-oov-问题">NPLM 中的 OOV 问题</h3>
<ul>
<li>在处理语料阶段，与 N-gram 中的处理方式是一样的——将不满阈值的词全部替换为 UNK <strong>神经网络</strong>中，一般有如下几种处理 UNK 的思路</li>
<li>为 UNK 分配一个随机初始化的 embedding，并<strong>参与训练</strong> &gt; 最终得到的 embedding 会有一定的语义信息，但具体好坏未知</li>
<li>把 UNK 都初始化成 0 向量，<strong>不参与训练</strong> &gt; UNK 共享相同的语义信息</li>
<li>每次都把 UNK 初始化成一个新的随机向量，<strong>不参与训练</strong> &gt; 常用的方法——因为本身每个 UNK 都不同，随机更符合对 UNK 基于最大熵的估计 &gt;&gt; <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo">How to add new embeddings for unknown words in Tensorflow (training &amp; pre-set for testing)</a> - Stack Overflow &gt;&gt; &gt;&gt; <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/45495190/initializing-out-of-vocabulary-oov-tokens">Initializing Out of Vocabulary (OOV) tokens</a> - Stack Overflow</li>
<li>基于 Char-Level 的方法 &gt; PaperWeekly 第七期 -- <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22700538?refer=paperweekly">基于Char-level的NMT OOV解决方案</a></li>
</ul>
<p>{"mode":"full","isActive":false}</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>NLP基础</p><p><a href="https://hunlp.com/posts/62790.html">https://hunlp.com/posts/62790.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-07-14</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-08-16</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/NLP/">NLP </a></div></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zfb.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wx.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/19440.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">EM算法及其推广</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/28867.html"><span class="level-item">RNN原理</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "8f75904f8ead18a178f72c96f2221214",
            repo: "Cartride.github.io",
            owner: "Cartride",
            clientID: "8f4a2426c347380a6ee4",
            clientSecret: "8dc8cd44b071426b35d0bd60634941371170b798",
            admin: ["Cartride"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#解决-nlp-问题的一般思路"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">解决 NLP 问题的一般思路</span></span></a></li><li><a class="level is-mobile" href="#nlp-的历史进程"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">NLP 的历史进程</span></span></a></li><li><a class="level is-mobile" href="#seq2seq-模型"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Seq2Seq 模型</span></span></a></li><li><a class="level is-mobile" href="#评价机制"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">评价机制</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#困惑度-perplexity-ppx"><span class="level-left"><span class="level-item">1.4.1</span><span class="level-item">困惑度 (Perplexity, PPX)</span></span></a></li><li><a class="level is-mobile" href="#"><span class="level-left"><span class="level-item">1.4.2</span><span class="level-item">
基本公式
</span></span></a></li><li><a class="level is-mobile" href="#bleu"><span class="level-left"><span class="level-item">1.4.3</span><span class="level-item">BLEU</span></span></a></li><li><a class="level is-mobile" href="#rouge"><span class="level-left"><span class="level-item">1.4.4</span><span class="level-item">ROUGE</span></span></a></li></ul></li></ul><li><a class="level is-mobile" href="#语言模型"><span class="level-left"><span class="level-item">2</span><span class="level-item">语言模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#xx-模型的含义"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">XX 模型的含义</span></span></a></li><li><a class="level is-mobile" href="#概率统计语言模型-plm-slm"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">概率/统计语言模型 (PLM, SLM)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#参数的规模"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">参数的规模</span></span></a></li><li><a class="level is-mobile" href="#可用的概率模型"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">可用的概率模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#n-gram-语言模型"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">N-gram 语言模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#可靠性与可区别性"><span class="level-left"><span class="level-item">2.3.1</span><span class="level-item">可靠性与可区别性</span></span></a></li><li><a class="level is-mobile" href="#oov-问题"><span class="level-left"><span class="level-item">2.3.2</span><span class="level-item">OOV 问题</span></span></a></li><li><a class="level is-mobile" href="#平滑处理-todo"><span class="level-left"><span class="level-item">2.3.3</span><span class="level-item">平滑处理 TODO</span></span></a></li></ul></li><li><a class="level is-mobile" href="#神经概率语言模型-nplm"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">神经概率语言模型 (NPLM)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#n-gram-神经语言模型"><span class="level-left"><span class="level-item">2.4.1</span><span class="level-item">N-gram 神经语言模型</span></span></a></li><li><a class="level is-mobile" href="#模型参数的规模与运算量"><span class="level-left"><span class="level-item">2.4.2</span><span class="level-item">模型参数的规模与运算量</span></span></a></li><li><a class="level is-mobile" href="#相比-n-gram-模型nplm-的优势"><span class="level-left"><span class="level-item">2.4.3</span><span class="level-item">相比 N-gram 模型，NPLM 的优势</span></span></a></li><li><a class="level is-mobile" href="#nplm-中的-oov-问题"><span class="level-left"><span class="level-item">2.4.4</span><span class="level-item">NPLM 中的 OOV 问题</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2021 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>