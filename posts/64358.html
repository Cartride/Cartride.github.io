<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Aggregated Residual Transformations for Deep Neural Networks - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="​ 我们为图像分类提出一个简单、高度模块化的网络结构。网络通过重复一个block来构建，这个block聚合了一组有相同拓扑的转换。我们的简单设计产生了一个同构的、多分支的 架构，它只需要设置几个超参数。这个策略揭示了一个新的维度，我们称之为&amp;quot;基数-cardinality&amp;quot;(这组转换的大小)，除了深度和宽度之外的一个关键因子。在ImageNet-1K数据集上，我们的实验表明，在控制复制度的受限情况下"><meta property="og:type" content="blog"><meta property="og:title" content="Aggregated Residual Transformations for Deep Neural Networks"><meta property="og:url" content="https://hunlp.com/posts/64358.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="​ 我们为图像分类提出一个简单、高度模块化的网络结构。网络通过重复一个block来构建，这个block聚合了一组有相同拓扑的转换。我们的简单设计产生了一个同构的、多分支的 架构，它只需要设置几个超参数。这个策略揭示了一个新的维度，我们称之为&amp;quot;基数-cardinality&amp;quot;(这组转换的大小)，除了深度和宽度之外的一个关键因子。在ImageNet-1K数据集上，我们的实验表明，在控制复制度的受限情况下"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwu8i3om3j60hq0bjt9n02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwu8rd15mj60h70kmdig02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwu92u21pj60kf0813yq02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwu9cv89ej31720g8n0d.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwu9lwzwfj60mm08i75h02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwu9tw9dcj31bi0kon0i.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwua2i428j60mq0hzwgl02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwuaa2evcj60n10fu40l02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwuahd2y0j60nv050dg502.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwuaqc0kxj60n00ezmyz02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwub05bvdj60p90mmta702.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwubbzi01j60mf0ditas02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwubnedo5j30mp0jz402.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwubwcz9ij60mj08kgmj02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwuc3x6hjj60ms09ct9m02.jpg"><meta property="article:published_time" content="2021-08-18T13:35:13.000Z"><meta property="article:modified_time" content="2021-09-18T04:15:24.686Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="CV"><meta property="article:tag" content="ResNet"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtwu8i3om3j60hq0bjt9n02.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/64358.html"},"headline":"Aggregated Residual Transformations for Deep Neural Networks","image":["https://tva1.sinaimg.cn/large/008i3skNly1gtwu8i3om3j60hq0bjt9n02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwu8rd15mj60h70kmdig02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwu92u21pj60kf0813yq02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwu9cv89ej31720g8n0d.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwu9lwzwfj60mm08i75h02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwu9tw9dcj31bi0kon0i.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwua2i428j60mq0hzwgl02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwuaa2evcj60n10fu40l02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwuahd2y0j60nv050dg502.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwuaqc0kxj60n00ezmyz02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwub05bvdj60p90mmta702.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwubbzi01j60mf0ditas02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwubnedo5j30mp0jz402.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwubwcz9ij60mj08kgmj02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gtwuc3x6hjj60ms09ct9m02.jpg"],"datePublished":"2021-08-18T13:35:13.000Z","dateModified":"2021-09-18T04:15:24.686Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"​ 我们为图像分类提出一个简单、高度模块化的网络结构。网络通过重复一个block来构建，这个block聚合了一组有相同拓扑的转换。我们的简单设计产生了一个同构的、多分支的 架构，它只需要设置几个超参数。这个策略揭示了一个新的维度，我们称之为&quot;基数-cardinality&quot;(这组转换的大小)，除了深度和宽度之外的一个关键因子。在ImageNet-1K数据集上，我们的实验表明，在控制复制度的受限情况下"}</script><link rel="canonical" href="https://hunlp.com/posts/64358.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>Aggregated Residual Transformations for Deep Neural Networks</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-08-18</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span><span class="level-item">1 小时读完 (大约8328个字)</span></div></div><div class="content"><p>​ 我们为图像分类提出一个<strong>简单</strong>、<strong>高度模块化</strong>的网络结构。网络通过<strong>重复一个block</strong>来构建，这个block<strong>聚合了一组有相同拓扑的转换</strong>。我们的简单设计产生了一个<strong>同构的</strong>、<strong>多分支的</strong> 架构，它只需要设置几个超参数。这个策略揭示了一个<strong>新的维度</strong>，我们称之为"<strong>基数-cardinality</strong>"(这组转换的大小)，除了深度和宽度之外的一个关键因子。在ImageNet-1K数据集上，我们的实验表明，在<strong>控制复制度</strong>的受限情况下，<strong>增加基数</strong>可以<strong>提升分类精度</strong>。而且当<strong>提升模型容量</strong>时(即增加模型复杂度)，<strong>提升基数比更深或更宽更加有效</strong>。我们的模型叫ResNeXt，是参加ILSVRC 2016分类任务的基础，我们获得了第二名。我们进一步在ImageNet-5K 数据集和COCO检测数据集上研究ResNeXt,同样展示了比对应的ResNet更好的结果。代码和模型公布在https://github.com/facebookresearch/ResNeXt <span id="more"></span> 论文地址： https://arxiv.org/pdf/1611.05431.pdf</p>
<h2 id="引言">引言</h2>
<p>​ 视觉识别的研究正经历从"<strong>特征工程"</strong>到"<strong>网络工程</strong>"的转换[25, 24, 44, 34, 36, 38, 14]。与传统的手工设计特征相比(如：SIFT [29] 和 HOG [5]), 通过神经网络从大规模数据[33]中<strong>学习特征</strong>,在训练时需要最少的人工参与，并<strong>可以迁移</strong>到各种各样的识别任务上[7,10,28]. 不过，人的工作已经转移到为学习表示<strong>设计更好的网络架构</strong>。</p>
<p>​ 随着超参数的增加(通道数、滤波器尺寸、步长等)<strong>设计网络架构变得日渐困难</strong>，特别是网络的层数很多时。VGG-nets [36]展示了一个简单有效构建非常深的网络的策略：堆叠相同形状的blocks. ResNets[14]继承了这种策略，堆叠的相同拓扑的模块。而且，我们认为，该规则的简单性可能会<strong>降低</strong>过度调整超参数以<strong>适应特定数据集的风险</strong>。VGGnets 和ResNets 的鲁棒性已经被各种视觉识别任务[7, 10, 9, 28, 31, 14]和非视觉任务包括语音[42,30]和语音[4,41,20]</p>
<p>​ 不同于VGG-nets,Inception模型家族[38,17,39,37]证明了精心设计的拓扑能够以<strong>较低</strong>的<strong>理论复杂度</strong>达到令人信服的准确性。Inception模型随着时间改进[38,39], 但是一个共有的属性是<strong>分割-转换-合并</strong>策略。在一个Inception模块中，输入分割为几个低维嵌入(通过$1 1 $卷积) ，使用特定的一组滤波转换(<span class="math inline">\(3 \times 3,5\times 5\)</span>等)，然后通过拼接合并。可以看出，该体系结构的解空间是单个大层(例如,5×5)操作在一个高维嵌入解空间的严格子空间。Inception 模块中的<strong>分割-转换-合并</strong>行为，被期望可以<strong>接近大的、密集层的表示能力</strong>，但又有一个<strong>相对低的计算复杂度</strong>。</p>
<p>​ 尽管精度很好，Inception模块的实现伴随着一系列复杂的因素-滤波器的数量和尺寸对每个独立的转换都是<strong>定做的</strong>，模块是<strong>分阶段定制</strong>的。尽管精心的组合这些组件产生了优秀的神经网络配方，如何将Inception架构<strong>适应到新的数据集/任务上通常不是明确的</strong>，特别是有这么多因子和超参需要设计。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwu8i3om3j60hq0bjt9n02.jpg" /></p>
<p>​ 本文，我们提出一个简单的架构，它采用了VGG/ResNets的<strong>重复层(layers)的策略</strong>，同时使用一种<strong>简单、可扩展</strong>的方式来<strong>利用分割-转换-合并</strong>策略。我们的网络的一个模块执行一组转换，每个转换在一个低维嵌入上执行，输出通过求和合并。我们追求这种概念(分割-转换-合并)的简单实现-聚合在一起的转换<strong>有相同的拓扑</strong>(如图Figure 1右)。这种设计思路允许<strong>将转换数量扩展到任意大</strong>，而<strong>不需要特殊设计</strong>。</p>
<p>​ 有趣的是，在这种简单情况下，我们说明我们的模型有其它两种等价形式(图Figure 3)，图Figure 3 (b)的重构呈现了类似InceptionResNet 模块的合并多个路径；但是我们的模块与所有现存的Inception 模块不同，我们的路径共享相同的拓扑，路径的数量可以轻易的独立出来作为一个因子来研究。在一个更简洁的重构中，我们的模块可以被Krizhevsky et al.’s的分组卷积[24] (Fig. 3(c))改造, 而分组卷积是被当做一个工程折中方案设计出来的。</p>
<p>​ 我们实验显示，<strong>聚合转换优于原本的ResNet模块</strong>，即使在保持计算复杂度和模型大小的受限条件下-如：图Figure 1(右) 被设计为保持与图Figure 1(左)的FLOPs复杂度和参数量大小。我们强调一下，通过<strong>增加容量</strong>(变得更深或更宽)来<strong>提升</strong>模型<strong>精度相对简单</strong>，<strong>提升精度同时保持(或减少)复杂度</strong>在文献中<strong>很少</strong>有。</p>
<p>​ 我们的方法表明，<strong>基数</strong>(一组转换的数量)是除了宽度和高度之外<strong>具体的、可度量的维度</strong>，并且是<strong>核心重要</strong>的。实验证明<strong>增加基数</strong>是比变得更深或更宽<strong>更加有效</strong>的提升精度的方法，特别是当深度和宽度开始使现有模型的收益递减时。</p>
<p>​ 我们的神经网络，称作<strong>ResNeXt</strong>(提出了下一个维度)，在ImageNet分类数据集上<strong>优于</strong>ResNet-101/152 [14], ResNet200 [15], Inception-v3 [39], 和Inception-ResNet-v2 [37]。特别是一个<strong>101-层的ResNeXt</strong>能够达到比<strong>ResNet-200[15]更好的精度</strong>，但只有其<strong>50%的复杂度</strong>。而且，ResNeXt展示了相对所有Inception模型来说<strong>更简单的设计</strong>。ResNeXt 是我们在ILSVRC 2016分类任务中提交物的基础，获得了第二名。本文进一步在更大的ImageNet-5K数据集和COCO对象检测数据集[27]上评估ResNeXt，显示了比它对应的ResNet一致的、更好的精度。我们认为ResNeXt同样能够<strong>很好的泛化</strong>到其它视觉(和非视觉的)识别任务上。</p>
<h2 id="相关工作">相关工作</h2>
<h3 id="多分支卷积网络">多分支卷积网络</h3>
<p>​ Inception模块[<strong>38,17,39,37</strong>]是成功的<strong>多分支架构</strong>，它的每个分支是精心定制的。ResNets[14]可以看做是一个<strong>二分支网络</strong>，其中一个分支是恒等映射。<strong>深度神经决策森林</strong>[22]是具有学习分裂功能的<strong>树状多分支网络</strong>。</p>
<h3 id="分组卷积">分组卷积</h3>
<p>​ 分组卷积的使用可以追溯到AlexNet论文[24], 如果没有更早。Krizhevsky et al. [24]给出的动机是将<strong>模型分布到两个GPU</strong>上。Caffe [19], Torch [3], 和其它库都支持分组卷积，主要为了兼容AlexNet。据我们所知，<strong>很少</strong>有证据表明<strong>利用分组卷积来提高精度</strong>。分组卷积的一个<strong>特例</strong>是<strong>分通道卷积</strong>，这种情况下<strong>分组数就是通道数</strong>。<strong>分通道卷积</strong>是<strong>可分卷积</strong>[35]的一部分。</p>
<h3 id="压缩卷积网络">压缩卷积网络</h3>
<p>​ 分解(在空间[6,18]或通道[6,21,16]级别)是广泛采用的用于<strong>检测深度卷积网络冗余</strong>和<strong>加速/压缩</strong>它的技术。 Ioannou et al. [16] 提出"根"-模式网络来减少计算，根中的分支通过分组卷积实现。[6, 18, 21, 16]中的方法展示了<strong>精度</strong>和<strong>较低复杂度和较小模型大小</strong>的优雅折中。<strong>不同于压缩</strong>，我们的方法是在实验中显示<strong>更强的表达能力</strong>的一种架构。</p>
<h3 id="集成">集成</h3>
<p>​ <strong>取</strong>一组独立训练网络的<strong>均值</strong>是一种提升精度[24]的有效解决方案, 在识别<strong>竞赛中被广泛采用</strong>。Veit et al. [40]将<strong>单个ResNets解释为浅层网络的一个集成</strong>，ResNet的结果来自于<strong>加法行为</strong>[15]。我们的方法使用附加的聚合一组转换。但是我们认为将我们的方法当做集成是<strong>不确切的</strong>，因为聚合在一起的成员是<strong>联合训练</strong>的，<strong>不是独立训练的</strong>。</p>
<h2 id="方法">方法</h2>
<h3 id="模板">模板</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwu8rd15mj60h70kmdig02.jpg" /></p>
<p>​ 我们遵循VGG/ResNets，采用<strong>高度模块化设计</strong>。我们的网络包含一堆残差block，这些block有<strong>相同的拓扑</strong>，受VGG/ResNets启发，这些block满足两个简单规则：(i) 如果产生<strong>空间大小相同的特征图</strong>，这些block<strong>共享相同的超参</strong>(宽度和滤波器尺寸)，(ii) 当空间特征图<strong>以因子2下采样时</strong>，block的<strong>宽度乘上因子2</strong>。第二条规则确保了<strong>计算复杂度</strong>，对于<strong>所有block</strong>而言FLOPs(在multiply-add中的浮点运算)<strong>大致相同</strong>。</p>
<p>​ 有了这两个规则，我们只需要设计<strong>一个模板模块</strong>，网络中的所有模块都可以被相应地确定。因此，这两条规则<strong>大大缩小了设计空间</strong>，使得我们<strong>关注几个关键因素</strong>。根据这两条规则构建的网络见表Table(1)</p>
<h3 id="回顾简单神经元">回顾简单神经元</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwu92u21pj60kf0813yq02.jpg" /></p>
<p>​ 人工神经网络最简单的神经元执行內积(加权求和), 是全连接层和卷积层执行基本转换。內积可以看做聚合转换的一种形式： <span class="math display">\[
\sum_{i=1}^D w_ix_i \tag 1
\]</span></p>
<p>​ 其中 <span class="math inline">\(x = [x_1, x_2, ..., x_D]\)</span> is a D-channel input vector to the neuron and wi is a filter’s weight for the i-th chan-nel. This operation (usually including some output nonlinearity) is referred to as a “neuron”. See Fig. 2.</p>
<p>​ 其中 <span class="math inline">\(x = [x_1, x_2, ..., x_D]\)</span> 是神经元的D通道输入向量，<span class="math inline">\(w_i\)</span> 是滤波器第i个通道的权重。这个操作(通常包括一些非线性输出)被称为“神经元”。</p>
<p>​ 上面的操作可以<strong>作为分割、转换和聚合</strong>的组合<strong>重新构建</strong>。(1)分割：向量<span class="math inline">\(x\)</span> 切片为一个低维嵌入，上面这个就是一个单维子空间<span class="math inline">\(x_i\)</span> ; (ii)转换：低维表示被转换，上面这个，它被简单缩放:<span class="math inline">\(w_ix_i\)</span> ; (iii)聚合：所有嵌入的转换通过<span class="math inline">\(\sum_{i=1}^D\)</span> 聚合。</p>
<h3 id="聚合转换">聚合转换</h3>
<p>​ 分析了简单的神经元，我们考虑将基础的转换(<span class="math inline">\(w_ix_i\)</span>)替换为<strong>更通用的函数</strong>,<strong>函数本身可以是一个网络</strong>。与增加深度的“Network-in-Network” [26] 相比，我们展示了我们的“Network-in-Neuron”沿着<strong>一个新的维度</strong>扩展。</p>
<p>Formally, we present aggregated transformations as:</p>
<p>形式上，我们提出的聚合转换为：</p>
<p><span class="math display">\[
\cal F(x)= \sum_{i=0}^C T_i(x), \tag 2
\]</span></p>
<p>​ <span class="math inline">\(\cal T_i(x)\)</span> 可以是任意函数，类似一个简单神经元，<span class="math inline">\(\cal T_i\)</span> 投影x到一个嵌套(通常是低维的)，然后转换它。</p>
<p>​ 在等式(2)中C是需要聚合的一组转换大小，我们称<strong>C为基数</strong>[2]。C在等式(2)中的位置与D在等式(1)中类似，但是C不需要等于D，可以是任意数值。维度<strong>宽度与简单转换(内积)的数量有关</strong>，我们认为维度<strong>基数控制更复杂转换的数量</strong>。我们通过实验证明，<strong>基数是一个基本的维度</strong>，比宽度和深度(这两个维度)更有效。</p>
<p>​ 本文中，我们考虑一个简单的方式设计转换函数：<strong>所有的<span class="math inline">\(\cal T_i\)</span> 有相同的拓扑</strong>，这扩展了重复相同形状的层的VGG-style策略, 这有助于隔离一些因素并扩展转换数量到任意大。如图Figure 1(右)描绘的，我们将单个转换<span class="math inline">\(\cal T_i\)</span>设置为bottleneck形状的架构[14]。这种情况下，<span class="math inline">\(\cal T_i\)</span> 中第一个<span class="math inline">\(1 \times 1\)</span> 的层产生低维嵌入。</p>
<p>​ 等式(2)中的聚合转换充当残差函数[14] (图Figure 1右)： <span class="math display">\[
\cal y=x+\sum_{i=1}^C T(x_i) \tag3
\]</span> ​ y是输出。</p>
<p><strong>与Inception-ResNet的关系.</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwu9cv89ej31720g8n0d.jpg" /></p>
<p>​ 一些张量操作表明，图Figure 1(右)中的模块(如图Figure 3(a)所示)与图Figure 3(b)等价。图Figure 3(b)与inception-resnet[37] block类似，因为它涉及到残差函数的分支和连接。但是不同于所有Inception或Inception-ResNet模块，我们在<strong>多个路径之间共享相同的拓扑</strong>。我们的模块需要最少的额外工作来设计每个路径。</p>
<p><strong>与分组卷积的关系.</strong></p>
<p>​ 使用分组卷积[24]的符号，上面的模块变得更加简洁；图Figure 3(c)显示了这种重构。所有的<strong>低维嵌入</strong>(第一个1×1层)可以<strong>用一个更宽的层</strong>(例如,在图Figure 3 (c)中1×1,128维)<strong>替换</strong>。分割基本上是由分组卷积层在将输入通道划分为组时完成的。图Figure 3(c)中分组的卷积层执行<strong>32组输入和输出通道为4维的卷积</strong>。分组的卷积层将它们连接到一起作为层的输出。图Figure 3(c)中的block与图Figure 1(左)中的bottlenetck块相似，除了图3(c)是<strong>一个更宽的、稀疏的连接模块</strong>。</p>
<p>​ 我们注意到重构产生非凡的拓扑只有block的深度≥3；如果块的深度为2(如[14]中基本block), 重构就变成一个普通的宽的、密集模块了；见图Figure 4中的描绘。</p>
<p><strong>讨论.</strong></p>
<p>​ 我们注意到，尽管我们提出了<strong>显示连接</strong>(图Figure 3(b))或<strong>分组卷积</strong>(图Figrue 3(c))的重构方式，但这种重构对公式(3)一般形式不总是合适的，如果变换<span class="math inline">\(\cal T_i\)</span>是任意形式的并且是异构的。在本文中，我们选择使用同构形式，因为它们更简单，并且可扩展。在这种简化的情况下，以图Figure 3(c)中分组卷积的形式有助于简化实现。</p>
<h3 id="模型容量">模型容量</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwu9lwzwfj60mm08i75h02.jpg" /></p>
<p>​ 下一节的实验会展示，我们的模型在保持模型复杂度和参数量时提升了精度。这不仅在实践中很有趣，而且更重要的是，<strong>复杂度</strong>和<strong>参数量</strong> <strong>代表</strong>了<strong>模型的固有容量</strong>，因此常常被作为深网络的<strong>基本特性</strong>来研究[8].</p>
<p>​ 当我们在保持复杂度的同时评估不同的基数C时，我们希望最小化对其它超参数的修改。我们选择<strong>调整bottleneck的宽度</strong>(例如图Figure 1(右)中的4维)，因为它可以从块的输入和输出隔离出来。这种策略不会改变其他超参数(block的深度或输入/输出宽度)，因此有助于我们<strong>聚焦基数的影响</strong>。</p>
<p>​ 在图1(左),原ResNet bottleneck block[14] 有<span class="math inline">\(256*64 + 3 * 3 * 64 * 64 + 64 * 256≈70 k\)</span>参数和成比例FLOPs(在同一特征图大小)。在<strong>bottleneck宽度为d</strong>的情况下，我们图Figure 1(右)的模板有: <span class="math display">\[
C*(256*d + 3*3*d*d + d*256) \tag 4
\]</span> 参数和成比例FLOPs。当C = 32和d = 4 ,等式(4)约有70k。表Table 2显示了基数C和bottleneck宽度d之间的关系。</p>
<p>​ 因为我们采用了3.1小节中的两条规则，所以上面的近似等式在ResNet bottle block和我们的ResNeXt的所有阶段(除了下采样层,特征图的尺寸有变化)是有效的。</p>
<h2 id="实现细节">实现细节</h2>
<p>​ 我们的实现遵循[14]和开源的fb.resnet.torch [11]代码。在ImageNet数据集上，输入图像是224×224，随机地从调整尺寸后的图像裁剪得到，这些调整大小的图片来自使用由[11]实现的[38]中的用<strong>缩放和长宽比</strong>做<strong>数据增广</strong>。shotcuts是恒等连接，除了那些增加的维度是投影([14]中的类型B)。如[11]中建议，conv3、4和5的下采样是通过每个stage中<strong>第一block的步长为2的<span class="math inline">\(3 \times 3\)</span> 卷积层</strong>完成的；使用SGD, 256的mini-batch在8个GPUs上(每个GPU32个样本)；权重衰减为0.0001，动量大小为0.9。初始学习率为0.1，按照[11]中的计划做<strong>3次除以10</strong>;采用[13]中的权重初始化策略。在所有的消融比较中，我们在<strong>单个$224 224 $ 的裁剪图像</strong>上进行错误<strong>评估</strong>，这个图像从短边为256的图像中心裁剪而来。</p>
<p>​ 我们的模型以<strong>图Figure 3(c)的形式实现</strong>, 在图Figure 3(c)中卷积之后，我们执行批标准化(BN)[17]，在每个BN后执行ReLU，除了block的输出部分，遵循[14]:ReLU在添加到shortcut之后才执行。</p>
<p>​ 我们注意到，<strong>当BN和ReLU合适的放置</strong>，图Figure 3中的<strong>三种形式</strong>是<strong>严格等价</strong>的。我们训练了所有的三种形式，获得了<strong>相同的结果</strong>。我们选择图Figure . 3(c)实现,因为它<strong>更加简洁</strong>，并且<strong>比其它两种形式更快</strong>。</p>
<h2 id="实验">实验</h2>
<h3 id="imagenet-1k上实验">ImageNet-1K上实验</h3>
<p>​ 我们在1000类ImageNet分类任务[33]上进行消融实验，遵循[14],构建了50层和101层的残差网络，简单的将ResNet-50/101中所有的block替换为我们的block。</p>
<p><strong>标记.</strong></p>
<p>​ 因为我们采用了3.1节中的两条规则，通过模板就足以说明一个架构了(因为所有的block都是一个逻辑)。例如，表Table 1显示了由基数= 32和bottleneck宽度= 4d的模板构建的ResNeXt-50(图Figure 3),这个网络简化地记做<strong>ResNeXt-50 (32×4d)</strong>。我们注意到模板的<strong>输入/输出宽度固定为256-d</strong>(图Figure 3), 且每次对feature map进行<strong>下采样时</strong>，所有<strong>宽度都翻倍</strong>(见表Table 1)。</p>
<p><strong>基数 vs. 宽度.</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwu9tw9dcj31bi0kon0i.jpg" /></p>
<p>​ 如表Table 2所示，我们首先在的<strong>保持复杂度</strong>的情况下，评估<strong>基数</strong>C和bottleneck<strong>宽度</strong>之间的<strong>权衡</strong>，表Table 3 展示了结果，图Figure 5展示了error vs. epochs的曲线。与ResNet-50对比(表Table 3顶部和图Figure 5左侧),<strong>32×4d ResNeXt-50</strong>验证误差为<strong>22.2%</strong>,比ResNet基线的23.9%<strong>低1.7%</strong>。<strong>保持复杂度</strong>，随着<strong>基数C从1增加到32，错误率不断降低</strong>。此外,32×4 d ResNeXt也比ResNet同行低得多的训练误差,说明性能提升<strong>不是来自正则化</strong>,而是来源于<strong>更强的表达能力</strong>。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwua2i428j60mq0hzwgl02.jpg" /></p>
<p>​ 在resnet - 101(图Figure5 右边,表Table 3底部)下也观察到类似的趋势,其中32×4d ResNeXt101优于resnet- 101 0.8%。虽然这种<strong>验证误差</strong>的<strong>提升比50层的情况要小</strong>，但是<strong>训练误差</strong>的<strong>提升任然是很大的</strong>(resnet-101 20%,32×4d resnext-101 16%,图Figure 5右边)。事实上，<strong>更多的训练数据</strong>将<strong>扩大验证误差的差距</strong>，如我们在下一小节中的ImageNet-5K集中所展示的那样。</p>
<p>​ 表3还表明，<strong>保持复杂度</strong>，<strong>当bottleneck宽度很小时</strong>，以减少宽度为代价的<strong>增加基数</strong>开始显示<strong>精度饱和</strong>(不再提升)。我们认为，在这种权衡中继续减少宽度是不值得的，因此，我们在下面<strong>采用一个不小于4d的bottleneck宽度</strong>。</p>
<p><strong>增加基数 vs. 更深/更宽.</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwuaa2evcj60n10fu40l02.jpg" /></p>
<p>​ 接下来，我们研究通过<strong>增加基数</strong>C或<strong>增加深度或宽度</strong>来<strong>增加复杂度</strong>。下面的比较也可以被视为关于<strong>2倍FLOPS resnet-101的基线</strong>; 我们比较有∼150亿FLOPs的如下几个变种。(i)<strong>深度加到200层</strong>,我们采用了在[11]中实现的ResNet-200[15]。(ii)通过<strong>增加bottleneck宽度</strong>来变宽。(iii)通过将C加倍来<strong>增加基数</strong>。</p>
<p>​ 表Table 4表明相对ResNet-101基线误差(22.%)，<strong>复杂度加倍误差会一致减少</strong>;但是<strong>更深</strong>(ResNet200,提升0.3%)或<strong>更宽</strong>(更宽的ResNet-101,提升0.7%)时，<strong>提升很小</strong>。</p>
<p>​ 相反，<strong>增加基数C比更深或更宽结果要好得多</strong>，2×64d ResNeXt-101将top-1误差减少1.3%到达20.7%; The 64×4d ResNeXt-101误差减小到20.4%。</p>
<p>​ 同样注意到<strong>32×4d ResNet-101 (21.2%)比更深的ResNet-200 和更宽的ResNet101表现更好</strong>，虽然只有它们<strong>50%的复杂度</strong>。这再次表明，<strong>基数是比深度和宽度</strong> <strong>更有效的维度</strong>。</p>
<p><strong>残差连接</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwuahd2y0j60nv050dg502.jpg" /></p>
<p>​ 上表说明残差(shortcut)连接的影响:从ResNeXt-50删除shortcut将错误增加3.9个点，至26.1%。删除ResNet-50的shotcut要糟糕得多(31.2%)。这些比较表明，<strong>残差连接有助于优化</strong>，而<strong>聚合转换是更强的表示</strong>，因为它们的性能始<strong>终优于有或没有残差连接的对应的副本</strong>。</p>
<p><strong>性能</strong></p>
<p>​ 为了简单起见，我们使用Torch的内置分组卷积实现，没有特别的优化。我们注意到这个实现是暴力的(brute-force)，<strong>不支持并行</strong>。在8块 NVIDIA M40 GPU上训练表Table 3所示的32×4d ResNeXt-101 每个mini-batch耗时<strong>0.95秒</strong>，对比ResNet-101 baseline耗时<strong>0.70s</strong>，<strong>FLOPs相当</strong>；我们认为这是合理的开销。我们认为经过<strong>精心设计的较低级别的实现</strong>(例如 CUDA)<strong>能够减少这种开销</strong>。我们还认为cpu上的推断时间会减少开销。训练2倍复杂度模型(64×4 d resnext - 101)需要1.7秒/mini-batch和一共10天在8块GPU上。</p>
<p><strong>与state-of-the-art结果比较</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwuaqc0kxj60n00ezmyz02.jpg" /></p>
<p>​ 表Table 5显示了在ImageNet验证集上进行<strong>单裁剪测试</strong>的更多结果。除了测试一个224×224裁剪,我们也遵循[15]评估320×320裁剪。我们的结果<strong>优于</strong>ResNet、Inception-v3/v4和Inception-ResNet-v2，到达单裁剪top-5的错误率4.4%。此外，我们的架构设计<strong>比所有的Inception模型都要简单得多</strong>，并且需要<strong>手工设置</strong>的<strong>超参数要少得多</strong>。</p>
<p>​ ResNeXt是我们参加ILSVRC 2016分类任务的基础，我们获得了第二名。 我们注意到，<strong>许多模型</strong>(包括我们的)在<strong>使用多尺度</strong>和/或<strong>多裁剪测试后开始饱和</strong>。使用[14]中的多尺度密集测试，我们得到了一个单模型的top-1/top-5错误率为17.7%/3.7%，与采用多尺度、多裁剪测试的inception-resnet-v2的单模型结果为17.8%/3.7%。在测试集中，我们的集成模型top-5得错误率为3.03%，与获胜者的2.99%和Inception-v4/InceptionResNet-v2的3.08%[37]相当。</p>
<h3 id="imagenet-5k实验">ImageNet-5K实验</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwub05bvdj60p90mmta702.jpg" /></p>
<p>​ ImageNet-1K上的<strong>性能似乎饱和</strong>了，但我们认为，这<strong>不是因为模型的能力</strong>，而是因为<strong>数据集的复杂度</strong>。接下来，我们在一个较大的ImageNet子集上评估模型，这个子集有5000个类别。</p>
<p>​ 我们的<strong>5K数据集</strong>是完整的<strong>ImageNet-22K集</strong>[33]的<strong>一个子集</strong>，这5000个类别<strong>包括原始的ImageNet1K中的类别</strong>和在完整的ImageNet数据集中具有<strong>最多图像数量</strong>的额外<strong>4000个类别</strong>。5k数据集有680万图片,大约5倍于1k数据集。没有官方可用的train/val分割，所以我们选择在原始的ImageNet-1K验证集上评估。在这个1k类val集上，预测时，模型<strong>可以被当做</strong> <strong>一个5k类分类任务</strong>(所有预测为其他4K类的标签都自动认为错误);或<strong>一个1K类分类任务</strong>(softmax只应用于1K类)来评估</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwubbzi01j60mf0ditas02.jpg" /></p>
<p>​ 实现细节与节4一样，5K训练模型从头开始训练，跟1K 训练模型一样的mini-batch数(这样epochs就只有1/5)。表Table 6和图Figure 6展示了在保持复杂度情况下的比较。<strong>ResNeXt-50</strong>比ResNet-50<strong>减少</strong>5K类 top-1<strong>错误3.2%</strong>，ResNetXt-101比ResNet-101减少5K类 top-1误差<strong>2.3%</strong>，在1k类误差上也观察到类似的差距，这些表明<strong>ResNeXt具有更强的表示能力</strong>。</p>
<p>​ 此外，在验证集上评估相同的1K类分类任务，我们发现在<strong>5K集上训练的模型</strong>(在表Table 6中有1K类error 22.2%/5.7%)<strong>与在1K集</strong>(表Table 3中为21.2%/5.6%)上训练<strong>相比</strong>，<strong>表现出竞争力</strong>。此结果并没有增加训练时间(由于相同的mini-batch数量)，也<strong>没有微调</strong>。我们认为这是一个有希望的结果，因为<strong>5K类的分类训练任务</strong>是一个<strong>更具挑战性</strong>的任务。</p>
<h3 id="cifar实验">CIFAR实验</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwubnedo5j30mp0jz402.jpg" /></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwubwcz9ij60mj08kgmj02.jpg" /></p>
<p>​ 我们在CIFAR-10和100数据集[23]上进行了更多的实验，我们使用[14]中的架构，使用bottleneck模板<span class="math inline">\(\left[1 \times1, 64 \\ 3 \times3,64 \\ 1 \times 1, 256 \right]\)</span> 替换了基础残差block；我们的网络从一个<span class="math inline">\(3 \times 3\)</span> 的卷积层开始，接着是3阶段，每个阶段有3个残差block。我们采用与[14]相同的平移和翻转做数据增广，实现的细节在附录中。</p>
<p>​ 我们比较了两种基于以上基线的增加复杂度情况:(i) <strong>增加基数</strong>,固定宽度，(ii) <strong>增加bottleneck的宽度</strong>，固定基数=1。在这种改变下，我们训练和评估一系列的网络。图Figure 7显示了测试错误率与模型大小的比较; 我们发现<strong>增加基数比增加宽度更有效</strong>，这与我们在ImageNet-1K上观察到的一致。表Table 7显示与有最好的发布记录的宽ResNet[43]比较的结果和模型大小；我们的模型大小相似(34.4M)，结果比宽ResNet更好。我们较大的模型在cifar 10上达到3.58%的测试错误率(10次运行平均结果)，在cifar 100上达到17.31%。据我们所知，这些是文献中最好的结果(具有类似的数据增强)，包括未发表的技术报告在内。</p>
<h3 id="coco目标检测实验">COCO目标检测实验</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gtwuc3x6hjj60ms09ct9m02.jpg" /></p>
<p>​ 接下来，我们在COCO目标检测数据集[27]上<strong>评估模型的泛化能力</strong>。遵循[1], 我们在80k训练集和35k val子集上训练模型，然后在5k val子集(称为minival)上进行评估。我们评估COCO类平均精度 (AP)和AP@IoU=0.5 [27]。我们采用基本的Faster R-CNN[32]，并遵循[14]将ResNet/ResNeXt插入其中。模型在ImageNet-1K上进行预训练，并在检测集上进行微调，实现细节在附录中。</p>
<p>​ 表8显示了比较结果，在50-层基线上，ResNeXt在<strong>不增加复杂度</strong>的情况下，<strong>将AP@0.5和AP分别提高2.1%和1.0%。</strong>ResNeXt<strong>在101层基线上</strong>显示了<strong>较小的提升</strong>。我们推测<strong>更多的训练数据</strong>会有<strong>更大的差距</strong>，正如在ImageNet-5K集上观察到的。值得注意的是，最近<strong>ResNeXt应用到Mask R-CNN[12]中</strong>，在<strong>实例分割</strong>和<strong>目标检测</strong>任务得到了<strong>最好结果</strong>。</p>
<h2 id="a.-实现细节-cifar">A. 实现细节: CIFAR</h2>
<p>​ 我们在50k训练集中训练模型，在10k测试集中评估。遵循[14],输入图像为32×32，从大小为40×40的<strong>零填充图像或其翻转</strong>中<strong>随机裁剪</strong>而来，没有使用其他的数据增广。第一层是<span class="math inline">\(3 \times 3\)</span> 的卷积层，有64个滤波器；还有<strong>3个阶段</strong>，<strong>每个阶段有3个残差block</strong>，每个阶段[14]输出特征图的大小分别为32,16,8；网络以一个<strong>全局平均池化</strong>和<strong>全连接层</strong>结束。如节3.1一样，阶段改变(下采样)时宽度加倍。模型在8个gpu上进行训练，每个mini-batch大小为128，重量衰减为0.0005，动量为0.9。我们从0.1的学习率开始，对模型进行300轮的训练，在150轮和225轮减少学习率，其它的实现细节如[11]。</p>
<h2 id="b.-实现细节-目标检测">B. 实现细节: 目标检测</h2>
<p>​ 我们采用Faster R-CNN系统[32],为了简单起见，在RPN和Fast R-CNN之间<strong>没有共享特征</strong>。在RPN步骤中，我们在8个GPU上进行训练，每个GPU每个mini-batch处理2张图像，每个图像有256个锚点(anchors)。在RPN训练步骤中，我们以0.02学习率训练120k个mini-batch,然后以0.002的学习率训练接下来60k个mini-batch。在Fast R-CNN训练步骤中，我们以0.005学习率训练120k个mini-batch,然后以0.0005的学习率训练接下来60k个mini-batch,我们使用0.0001的权重衰减和0.9的动量大小。其它实现细节如https://github.com/rbgirshick/py-faster-rcnn.</p>
<h2 id="参考">参考</h2>
<p>[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks. In CVPR, 2016. [2] G. Cantor. Uber unendliche, lineare punktmannich-faltigkeiten, arbeiten zur mengenlehre aus den jahren 1872-1884. 1884. [3] R. Collobert, S. Bengio, and J. Mariethoz. Torch: a modular machine learning software library. Technical report, Idiap, 2002. [4] A. Conneau, H. Schwenk, L. Barrault, and Y. Lecun. Very deep convolutional networks for natural language processing. arXiv:1606.01781, 2016. [5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. [6] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, 2014.</p>
<p>[7] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. [8] D. Eigen, J. Rolfe, R. Fergus, and Y. LeCun. Understanding deep architectures using a recursive convolutional network. arXiv:1312.1847, 2013. [9] R. Girshick. Fast R-CNN. In ICCV, 2015. [10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. [11] S. Gross and M. Wilber. Training and investigating Residual Nets. https://github.com/facebook/fb.resnet.torch, 2016. [12] K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask R-CNN. arXiv:1703.06870, 2017. [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. [14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. [15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016. [16] Y. Ioannou, D. Robertson, R. Cipolla, and A. Criminisi. Deep roots: Improving cnn efficiency with hierarchical filter groups. arXiv:1605.06489, 2016. [17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. [18] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In BMVC, 2014. [19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014. [20] N. Kalchbrenner, L. Espeholt, K. Simonyan, A. v. d. Oord, A. Graves, and K. Kavukcuoglu. Neural machine translation in linear time. arXiv:1610.10099, 2016. [21] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. In ICLR, 2016. [22] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bulo. Deep convolutional neural decision forests. In ICCV, 2015. [23] A. Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.</p>
<p>[24] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. [25] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989. [26] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014. [27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV. 2014. [28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. [29] D. G. Lowe. Distinctive image features from scaleinvariant keypoints. IJCV, 2004. [30] A. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv:1609.03499, 2016. [31] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to segment object candidates. In NIPS, 2015. [32] S. Ren, K. He, R. Girshick, and J. Sun. Faster RCNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. [33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015. [34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. [35] L. Sifre and S. Mallat. Rigid-motion scattering for texture classification. arXiv:1403.1687, 2014. [36] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [37] C. Szegedy, S. Ioffe, and V. Vanhoucke. Inceptionv4, inception-resnet and the impact of residual connections on learning. In ICLR Workshop, 2016. [38] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015. [39] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.</p>
<p>[40] A. Veit, M. Wilber, and S. Belongie. Residual networks behave like ensembles of relatively shallow network. In NIPS, 2016. [41] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016. [42] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig. The Microsoft 2016 Conversational Speech Recognition System. arXiv:1609.03528, 2016. [43] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016. [44] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. In ECCV, 2014.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Aggregated Residual Transformations for Deep Neural Networks</p><p><a href="https://hunlp.com/posts/64358.html">https://hunlp.com/posts/64358.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-08-18</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-09-18</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/CV/">CV, </a><a class="link-muted" rel="tag" href="/tags/ResNet/">ResNet </a></div></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zfb.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wx.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/61383.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Fast R-CNN</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/62961.html"><span class="level-item">FaceNet: A Unified Embedding for Face Recognition and Clustering</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "cfc72c95705825cd1681064d1f93eb8c",
            repo: "Cartride.github.io",
            owner: "Cartride",
            clientID: "8f4a2426c347380a6ee4",
            clientSecret: "8dc8cd44b071426b35d0bd60634941371170b798",
            admin: ["Cartride"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#引言"><span class="level-left"><span class="level-item">1</span><span class="level-item">引言</span></span></a></li><li><a class="level is-mobile" href="#相关工作"><span class="level-left"><span class="level-item">2</span><span class="level-item">相关工作</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#多分支卷积网络"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">多分支卷积网络</span></span></a></li><li><a class="level is-mobile" href="#分组卷积"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">分组卷积</span></span></a></li><li><a class="level is-mobile" href="#压缩卷积网络"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">压缩卷积网络</span></span></a></li><li><a class="level is-mobile" href="#集成"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">集成</span></span></a></li></ul></li><li><a class="level is-mobile" href="#方法"><span class="level-left"><span class="level-item">3</span><span class="level-item">方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#模板"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">模板</span></span></a></li><li><a class="level is-mobile" href="#回顾简单神经元"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">回顾简单神经元</span></span></a></li><li><a class="level is-mobile" href="#聚合转换"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">聚合转换</span></span></a></li><li><a class="level is-mobile" href="#模型容量"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">模型容量</span></span></a></li></ul></li><li><a class="level is-mobile" href="#实现细节"><span class="level-left"><span class="level-item">4</span><span class="level-item">实现细节</span></span></a></li><li><a class="level is-mobile" href="#实验"><span class="level-left"><span class="level-item">5</span><span class="level-item">实验</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#imagenet-1k上实验"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">ImageNet-1K上实验</span></span></a></li><li><a class="level is-mobile" href="#imagenet-5k实验"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">ImageNet-5K实验</span></span></a></li><li><a class="level is-mobile" href="#cifar实验"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">CIFAR实验</span></span></a></li><li><a class="level is-mobile" href="#coco目标检测实验"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">COCO目标检测实验</span></span></a></li></ul></li><li><a class="level is-mobile" href="#a.-实现细节-cifar"><span class="level-left"><span class="level-item">6</span><span class="level-item">A. 实现细节: CIFAR</span></span></a></li><li><a class="level is-mobile" href="#b.-实现细节-目标检测"><span class="level-left"><span class="level-item">7</span><span class="level-item">B. 实现细节: 目标检测</span></span></a></li><li><a class="level is-mobile" href="#参考"><span class="level-left"><span class="level-item">8</span><span class="level-item">参考</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2022 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>