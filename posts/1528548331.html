<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>HMM模型 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="HMM模型基础 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。"><meta property="og:type" content="blog"><meta property="og:title" content="HMM模型"><meta property="og:url" content="https://hunlp.com/posts/1528548331.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="HMM模型基础 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hunlp.com/img/og_image.png"><meta property="article:published_time" content="2018-05-16T10:57:48.000Z"><meta property="article:modified_time" content="2019-11-20T14:08:23.414Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="数学原理"><meta property="article:tag" content="HMM"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/1528548331.html"},"headline":"HMM模型","image":["https://hunlp.com/img/og_image.png"],"datePublished":"2018-05-16T10:57:48.000Z","dateModified":"2019-11-20T14:08:23.414Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"HMM模型基础 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。"}</script><link rel="canonical" href="https://hunlp.com/posts/1528548331.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a><a class="navbar-item" href="/friend">友链</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>HMM模型</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2018-05-16</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2019-11-20</time></span><span class="level-item"><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span> / </span><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></span><span class="level-item">29 分钟读完 (大约4334个字)</span></div></div><div class="content"><h2 id="hmm模型基础">HMM模型基础</h2>
<p>隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。 <span id="more"></span></p>
<h3 id="什么样的问题需要hmm模型">什么样的问题需要HMM模型</h3>
<p>首先我们来看看什么样的问题解决可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列。</p>
<p>有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。比如：我现在在打字写博客，我在键盘上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。再举一个，我在和你说话，我发出的一串连续的声音就是观测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。</p>
<p>从这些例子中，我们可以发现，HMM模型可以无处不在。但是上面的描述还不精确，下面我们用精确的数学符号来表述我们的HMM模型。</p>
<h3 id="hmm模型的定义">HMM模型的定义</h3>
<p>对于HMM模型，首先我们假设<span class="math inline">\(Q\)</span>是所有可能的隐藏状态的集合，<span class="math inline">\(V\)</span>是所有可能的观测状态的集合，即： <span class="math display">\[
Q=\left \{ q_1,q_2,...,q_N \right \},V=\left \{ v_1,v_2,...,v_M \right \}
\]</span> 其中，<span class="math inline">\(N\)</span>是可能的隐藏状态数，<span class="math inline">\(M\)</span>是所有可能的观察状态数。</p>
<p>对于一个长度为<span class="math inline">\(T\)</span>的序列，<span class="math inline">\(I\)</span>对应的状态序列, <span class="math inline">\(O\)</span>是对应的观察序列，即： <span class="math display">\[
I=\left \{ i_1,i_2,...,i_T \right \},O=\left \{ o_1,o_2,...,o_T \right \}
\]</span> 其中，任意一个隐藏状态<span class="math inline">\(i_t\in Q\)</span>,任意一个观察状态<span class="math inline">\(o_t\in V\)</span></p>
<p>HMM模型做了两个很重要的假设如下：</p>
<p>1）齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态，这个我们在MCMC(二)马尔科夫链中有详细讲述。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻𝑡的隐藏状态是<span class="math inline">\(i_t=q_i\)</span>,在时刻𝑡+1的隐藏状态是<span class="math inline">\(i_t+1=q_j\)</span>, 则从时刻𝑡到时刻<span class="math inline">\(t+1\)</span>的HMM状态转移概率<span class="math inline">\(a_{ij}\)</span>可以表示为： <span class="math display">\[
a_{ij}=P(i_{t+1}=q_j|i_t=q_i)
\]</span> 这样<span class="math inline">\(a_{ij}\)</span>可以组成马尔科夫链的状态转移矩阵<span class="math inline">\(A\)</span>: <span class="math display">\[
A=\left [ a_{ij} \right ]_{N\times N}
\]</span></p>
<p>2） 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻𝑡的隐藏状态是<span class="math inline">\(i_t=q_i\)</span>, 而对应的观察状态为<span class="math inline">\(o_t=v_k\)</span>, 则该时刻观察状态𝑣𝑘在隐藏状态<span class="math inline">\(qj\)</span>下生成的概率为<span class="math inline">\(b_j(k)\)</span>,满足： <span class="math display">\[
b_j(k)=P(o_t=v_k|i_t=q_j)
\]</span> 这样<span class="math inline">\(b_j(k)\)</span>可以组成观测状态生成的概率矩阵<span class="math inline">\(B\)</span>: <span class="math display">\[
B=\left [ b_j(k) \right ]_{N\times M}
\]</span> 除此之外，我们需要一组在时刻<span class="math inline">\(t=1\)</span>的隐藏状态概率分布$$: <span class="math display">\[
\prod =\left [ \pi (i) \right ]_N,其中\pi (i)=P(i_1=q_i)
\]</span> 一个HMM模型，可以由隐藏状态初始概率分布$<span class="math inline">\(, 状态转移概率矩阵\)</span>A<span class="math inline">\(和观测状态概率矩阵\)</span>B<span class="math inline">\(决定。\)</span><span class="math inline">\(,\)</span>A<span class="math inline">\(决定状态序列，\)</span>B<span class="math inline">\(决定观测序列。因此，HMM模型可以由一个三元组\)</span>$表示如下： <span class="math display">\[\lambda = (A, B, \Pi)\]</span></p>
<h3 id="一个hmm模型实例">一个HMM模型实例</h3>
下面我们用一个简单的实例来描述上面抽象出的HMM模型。这是一个盒子与球的模型，例子来源于李航的《统计学习方法》。 假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里
<table>
<tr>
<td>
盒子
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<td>
红球数
</td>
<td>
5
</td>
<td>
4
</td>
<td>
7
</td>
</tr>
<tr>
<td>
白球数
</td>
<td>
5
</td>
<td>
6
</td>
<td>
3
</td>
</tr>
</table>
<p>按照下面的方法从盒子里抽球，开始的时候，从第一个盒子抽球的概率是0.2，从第二个盒子抽球的概率是0.4，从第三个盒子抽球的概率是0.4。以这个概率抽一次球后，将球放回。然后从当前盒子转移到下一个盒子进行抽球。规则是：如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。如果当前抽球的盒子是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球。如此下去，直到重复三次，得到一个球的颜色的观测序列: <span class="math display">\[
O=\left \{红,白,红 \right \}
\]</span> 注意在这个过程中，观察者只能看到球的颜色序列，却不能看到球是从哪个盒子里取出的。</p>
<p>那么按照我们上一节HMM模型的定义，我们的观察集合是: <span class="math display">\[
V=\left \{红,白 \right \}, M=2
\]</span> 我们的集合状态是： <span class="math display">\[
Q=\left \{盒子1,盒子2,盒子3 \right \}, N=3
\]</span> 而观察序列和状态序列的长度为3.</p>
<p>初始状态分布为： <span class="math display">\[
\prod =\left ( 0.2,0.4,0.4 \right )^T
\]</span> 状态转移分布矩阵为： <span class="math display">\[
A=\begin{pmatrix}
0.5 &amp; 0.2 &amp;0.3 \\ 
 0.3&amp;0.5  &amp;0.2 \\ 
0.2 &amp;0.3  &amp;0.5 
\end{pmatrix}
\]</span> 观测状态概率矩阵为 <span class="math display">\[
B=\begin{pmatrix}
0.5 &amp;0.5 \\ 
 0.4&amp;0.6 \\ 
 0.7&amp;0.3 
\end{pmatrix}
\]</span></p>
<h3 id="hmm观测序列的生成">HMM观测序列的生成</h3>
<p>从上一节的例子，我们也可以抽象出HMM观测序列生成的过程。 输入的是HMM的模型<span class="math inline">\(\lambda = (A, B, \Pi)\)</span>,观测序列的长度<span class="math inline">\(T\)</span> 输出是观测序列<span class="math inline">\(O=\left \{ o_1,o_2,...,o_T \right \}\)</span> 生成的过程如下：</p>
<p>1）根据初始状态概率分布<span class="math inline">\(\prod\)</span>生成隐藏状态<span class="math inline">\(i_i\)</span></p>
<ol start="2" type="1">
<li>for t from 1 to T</li>
</ol>
<blockquote>
<ol type="a">
<li>按照隐藏状态<span class="math inline">\(i_t\)</span>的观测状态分布<span class="math inline">\(b_{i_t}\)</span>生成观察状态<span class="math inline">\(o_t\)</span></li>
<li>按照隐藏状态𝑖𝑡的状态转移概率分布<span class="math inline">\(a_{i_t} i_{t+1}\)</span>产生隐藏状态<span class="math inline">\(i_t+1\)</span> 所有的<span class="math inline">\(o_t\)</span>一起形成观测序列<span class="math inline">\(O=\left \{ o_1,o_2,...,o_T \right \}\)</span></li>
</ol>
</blockquote>
<h3 id="hmm模型的三个基本问题">HMM模型的三个基本问题</h3>
<p>HMM模型一共有三个经典的问题需要解决：</p>
<p>1） 评估观察序列概率。即给定模型<span class="math inline">\(\lambda = (A, B, \Pi)\)</span>和观测序列<span class="math inline">\(O=\left \{ o_1,o_2,...,o_T \right \}\)</span>，计算在模型𝜆下观测序列𝑂出现的概率<span class="math inline">\(P(O|\lambda)\)</span>。这个问题的求解需要用到前向后向算法，我们在这个系列的第二篇会详细讲解。这个问题是HMM模型三个问题中最简单的。</p>
<p>2）模型参数学习问题。即给定观测序列<span class="math inline">\(O=\left \{ o_1,o_2,...,o_T \right \}\)</span>，估计模型<span class="math inline">\(\lambda = (A, B, \Pi)\)</span>的参数，使该模型下观测序列的条件概率𝑃(𝑂|𝜆)最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法， 我们在这个系列的第三篇会详细讲解。这个问题是HMM模型三个问题中最复杂的。</p>
<p>3）预测问题，也称为解码问题。即给定模型<span class="math inline">\(\lambda =(A,B,\Pi)\)</span>和观测序列<span class="math inline">\(O=\left \{ o_1,o_2,...,o_T \right \}\)</span>，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。</p>
<h2 id="维特比算法解码隐藏状态序列">维特比算法解码隐藏状态序列</h2>
<p>HMM模型的解码问题最常用的算法是维特比算法，当然也有其他的算法可以求解这个问题。同时维特比算法是一个通用的求序列最短路径的动态规划算法，也可以用于很多其他问题，比如之前讲到的文本挖掘的分词原理中我们讲到了单独用维特比算法来做分词。</p>
<h3 id="hmm最可能隐藏状态序列求解概述">HMM最可能隐藏状态序列求解概述</h3>
<p>在HMM模型的解码问题中，给定模型<span class="math inline">\(\lambda = (A, B, \Pi)\)</span>和观测序列<span class="math inline">\(O=\left \{ o_1,o_2,...,o_T \right \}\)</span>，求给定观测序列O条件下，最可能出现的对应的状态序列<span class="math inline">\(T^*=\left \{i_1^*,i_2^*,...,i_T^* \right \}\)</span>,即<span class="math inline">\(P(I^*|O)\)</span>要最大化。</p>
<p>一个可能的近似解法是求出观测序列𝑂在每个时刻𝑡最可能的隐藏状态<span class="math inline">\(i_t^*\)</span>然后得到一个近似的隐藏状态序列<span class="math inline">\(T^*=\left \{i_1^*,i_2^*,...,i_T^* \right \}\)</span>。要这样近似求解不难，利用隐马尔科夫模型HMM前向后向算法评估观察序列概率中的定义：在给定模型<span class="math inline">\(\lambda\)</span>和观测序列<span class="math inline">\(O\)</span>时，在时刻𝑡处于状态<span class="math inline">\(q_i\)</span>的概率是<span class="math inline">\(\gamma_t(i)\)</span>，这个概率可以通过HMM的前向算法与后向算法计算。这样我们有： <span class="math display">\[
i_t^* = arg \max_{1 \leq i \leq N}[\gamma_t(i)], \; t =1,2,...T
\]</span> 近似算法很简单，但是却不能保证预测的状态序列是整体是最可能的状态序列，因为预测的状态序列中某些相邻的隐藏状态可能存在转移概率为0的情况。</p>
<p>而维特比算法可以将HMM的状态序列作为一个整体来考虑，避免近似算法的问题，下面我们来看看维特比算法进行HMM解码的方法。</p>
<h3 id="维特比算法概述">维特比算法概述</h3>
<p>维特比算法是一个通用的解码算法，是基于动态规划的求序列最短路径的方法。在文本挖掘的分词原理中我们已经讲到了维特比算法的一些细节。</p>
<p>既然是动态规划算法，那么就需要找到合适的局部状态，以及局部状态的递推公式。在HMM中，维特比算法定义了两个局部状态用于递推。</p>
<p>第一个局部状态是在时刻𝑡隐藏状态为<span class="math inline">\(i\)</span>所有可能的状态转移路径<span class="math inline">\(i_1,i_2,...i_t\)</span>中的概率最大值。记为<span class="math inline">\(\delta_t(i)\)</span>: <span class="math display">\[
\delta_t(i) = \max_{i_1,i_2,...i_{t-1}}\;P(i_t=i, i_1,i_2,...i_{t-1},o_t,o_{t-1},...o_1|\lambda),\; i =1,2,...N
\]</span> 由<span class="math inline">\(\delta_t(i)\)</span>的定义可以得到<span class="math inline">\(\delta\)</span>的递推表达式： <span class="math display">\[
\begin{align} \delta_{t+1}(i) &amp; =  \max_{i_1,i_2,...i_{t}}\;P(i_{t+1}=i, i_1,i_2,...i_{t},o_{t+1},o_{t},...o_1|\lambda) \\ &amp; = \max_{1 \leq j \leq N}\;[\delta_t(j)a_{ji}]b_i(o_{t+1})\end{align}
\]</span> 第二个局部状态由第一个局部状态递推得到。我们定义在时刻𝑡隐藏状态为𝑖的所有单个状态转移路径<span class="math inline">\((i_1,i_2,...,i_{t-1},i)\)</span>中概率最大的转移路径中第<span class="math inline">\(t−1\)</span>个节点的隐藏状态为<span class="math inline">\(\Psi_t(i)\)</span>,其递推表达式可以表示为： <span class="math display">\[
\Psi_t(i) = arg \; \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}]
\]</span></p>
<p>有了这两个局部状态，我们就可以从时刻0一直递推到时刻<span class="math inline">\(T\)</span>，然后利用<span class="math inline">\(\Psi_t(i)\)</span>记录的前一个最可能的状态节点回溯，直到找到最优的隐藏状态序列。</p>
<h3 id="维特比算法流程总结">维特比算法流程总结</h3>
<p>现在我们来总结下维特比算法的流程：</p>
<p>输入：HMM模型<span class="math inline">\(\lambda = (A, B, \Pi)\)</span>，观测序列<span class="math inline">\(O=(o_1,o_2,...o_T)\)</span></p>
<p>输出：最有可能的隐藏状态序列<span class="math inline">\(I^*= \{i_1^*,i_2^*,...i_T^*\}\)</span></p>
<p>1）初始化局部状态： <span class="math display">\[
\delta_1(i) = \pi_ib_i(o_1),\;i=1,2...N \\
\Psi_1(i)=0,\;i=1,2...N
\]</span> 2) 进行动态规划递推时刻<span class="math inline">\(t=2,3,...T\)</span>时刻的局部状态： <span class="math display">\[
\delta_{t}(i) = \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}]b_i(0_{t}),\;i=1,2...N \\
\Psi_t(i) = arg \; \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}],\;i=1,2...N
\]</span> 3) 计算时刻<span class="math inline">\(T\)</span>最大的<span class="math inline">\(\delta_t(i)\)</span>，即为最可能隐藏状态序列出现的概率。计算时刻<span class="math inline">\(T\)</span>最大的<span class="math inline">\(\Psi_t(i)\)</span>，即为时刻<span class="math inline">\(T\)</span>最可能的隐藏状态。 <span class="math display">\[
P* = \max_{1 \leq j \leq N}\delta_{T}(i) \\
i_T^* = arg \; \max_{1 \leq j \leq N}\;[\delta_{T}(i)]
\]</span> 4) 利用局部状态<span class="math inline">\(\Psi_t(i)\)</span>开始回溯。对于<span class="math inline">\(t=T-1,T-2,...,1\)</span>: <span class="math display">\[
i_t^* = \Psi_{t+1}(i_{t+1}^*)
\]</span> 最终得到最有可能的隐藏状态序列<span class="math inline">\(I^*= \{i_1^*,i_2^*,...i_T^*\}\)</span></p>
<h3 id="hmm维特比算法求解实例">HMM维特比算法求解实例</h3>
<p>下面我们仍然用HMM模型中盒子与球的例子来看看HMM维特比算法求解。</p>
<p>我们的观察集合是: <span class="math display">\[
V=\{红，白\}，M=2
\]</span> 我们的状态集合是: <span class="math display">\[
Q =\{盒子1，盒子2，盒子3\}， N=3 
\]</span></p>
<p>而观察序列和状态序列的长度为3.</p>
<p>初始状态分布为： <span class="math display">\[
\Pi = (0.2,0.4,0.4)^T
\]</span> 状态转移概率分布矩阵为： <span class="math display">\[
A = \left( \begin{array} {ccc} 0.5 &amp; 0.2 &amp; 0.3 \\ 0.3 &amp; 0.5 &amp; 0.2 \\ 0.2 &amp; 0.3 &amp;0.5 \end{array} \right) 
\]</span> 观测状态概率矩阵为： <span class="math display">\[
B = \left( \begin{array} {ccc} 0.5 &amp; 0.5 \\ 0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 \end{array} \right) 
\]</span> 球的颜色的观测序列: <span class="math display">\[
O=\{红，白，红\}
\]</span> 按照我们上一节的维特比算法，首先需要得到三个隐藏状态在时刻1时对应的各自两个局部状态，此时观测状态为1： <span class="math display">\[
\delta_1(1) = \pi_1b_1(o_1) = 0.2 \times 0.5 = 0.1 \\
\delta_1(2) = \pi_2b_2(o_1) = 0.4 \times 0.4 = 0.16 \\
\delta_1(3) = \pi_3b_3(o_1) = 0.4 \times 0.7 = 0.28 \\
\Psi_1(1)=\Psi_1(2) =\Psi_1(3) =0
\]</span> 现在开始递推三个隐藏状态在时刻2时对应的各自两个局部状态，此时观测状态为2： <span class="math display">\[
\delta_2(1) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j1}]b_1(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.5, 0.16 \times 0.3, 0.28\times 0.2] \times 0.5 = 0.028 \\
\Psi_2(1)=3 \\
\delta_2(2) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j2}]b_2(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.2, 0.16 \times 0.5, 0.28\times 0.3] \times 0.6 = 0.0504 \\
\Psi_2(2)=3 \\
\delta_2(3) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j3}]b_3(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.3, 0.16 \times 0.2, 0.28\times 0.5] \times 0.3 = 0.042 \\
\Psi_2(3)=3
\]</span> 继续递推三个隐藏状态在时刻3时对应的各自两个局部状态，此时观测状态为1： <span class="math display">\[
\delta_3(1) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j1}]b_1(o_3) = \max_{1\leq j \leq 3}[0.028 \times 0.5, 0.0504 \times 0.3, 0.042\times 0.2] \times 0.5 = 0.00756 \\
\Psi_3(1)=2 \\
\delta_3(2) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j2}]b_2(o_3) = \max_{1\leq j \leq 3}[0.028  \times 0.2, 0.0504\times 0.5, 0.042\times 0.3] \times 0.4 = 0.01008 \\
\Psi_3(2)=2 \\
\delta_3(3) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j3}]b_3(o_3) = \max_{1\leq j \leq 3}[0.028  \times 0.3, 0.0504 \times 0.2, 0.042\times 0.5] \times 0.7 = 0.0147 \\
\Psi_3(3)=3 
\]</span> 此时已经到最后的时刻，我们开始准备回溯。此时最大概率为<span class="math inline">\(\delta_3(3)\)</span>,从而得到<span class="math inline">\(i_3^* =3\)</span></p>
<p>由于<span class="math inline">\(\Psi_3(3)=3\)</span>,所以<span class="math inline">\(i_2^* =3\)</span>, 而又由于<span class="math inline">\(\Psi_2(3)=3\)</span>,所以𝑖∗1=3。从而得到最终的最可能的隐藏状态序列为：<span class="math inline">\((3,3,3)\)</span></p>
<h3 id="hmm模型维特比算法总结">HMM模型维特比算法总结</h3>
<p>如果大家看过之前写的文本挖掘的分词原理中的维特比算法，就会发现这两篇之中的维特比算法稍有不同。主要原因是在中文分词时，我们没有观察状态和隐藏状态的区别，只有一种状态。但是维特比算法的核心是定义动态规划的局部状态与局部递推公式，这一点在中文分词维特比算法和HMM的维特比算法是相同的，也是维特比算法的精华所在。</p>
<p>维特比算法也是寻找序列最短路径的一个通用方法，和dijkstra算法有些类似，但是dijkstra算法并没有使用动态规划，而是贪心算法。同时维特比算法仅仅局限于求序列最短路径，而dijkstra算法是通用的求最短路径的方法。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>HMM模型</p><p><a href="https://hunlp.com/posts/1528548331.html">https://hunlp.com/posts/1528548331.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-05-16</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-11-20</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/">数学原理, </a><a class="link-muted" rel="tag" href="/tags/HMM/">HMM </a></div></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zfb.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wx.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/2242938984.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">CYK算法详解与代码实现</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/3979220335.html"><span class="level-item">Statistical Machine Translation：IBM Models 1 and 2</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="waline-thread"></div><script src="https://cdn.jsdelivr.net/npm/@waline/client@1.5.4/dist/Waline.min.js"></script><script>Waline({
            el: '#waline-thread',
            serverURL: "https://waline-server-gilt.vercel.app",
            path: window.location.pathname,
            lang: "zh-CN",
            visitor: false,
            emoji: ["https://cdn.jsdelivr.net/gh/walinejs/emojis/weibo"],
            dark: "auto",
            meta: ["nick","mail","link"],
            requiredMeta: [],
            login: "enable",
            
            pageSize: 10,
            
            
            math: false,
            copyright: true,
            locale: {"placeholder":"Comment here..."},
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2022 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>