<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>FaceNet: A Unified Embedding for Face Recognition and Clustering - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="尽管人脸识别领域最近有显著进步[10, 14,15,17]，有效的实施大规模的人脸验证和识别对当前的方法存在严重的挑战。本文提出一个系统，叫FaceNet，直接学习人脸图像到紧密欧式空间的一个映射，欧式空间中的距离直接度量人脸的相似度。一旦这个空间产生，诸如人脸识别、验证、聚类任务可以使用标准方法轻易实现，该方法用FaceNet embeddings作为特征向量。"><meta property="og:type" content="blog"><meta property="og:title" content="FaceNet: A Unified Embedding for Face Recognition and Clustering"><meta property="og:url" content="https://hunlp.com/posts/62961.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="尽管人脸识别领域最近有显著进步[10, 14,15,17]，有效的实施大规模的人脸验证和识别对当前的方法存在严重的挑战。本文提出一个系统，叫FaceNet，直接学习人脸图像到紧密欧式空间的一个映射，欧式空间中的距离直接度量人脸的相似度。一旦这个空间产生，诸如人脸识别、验证、聚类任务可以使用标准方法轻易实现，该方法用FaceNet embeddings作为特征向量。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttd8mg0jcj60hc0n8wgu02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttd8v3epsj30hc06wwf2.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttd96eircj60hc0770tb02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttd9fix1qj60hc0mc42302.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttd9uvoqfj60hc0a3q4q02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttda2yjo3j60hc0flt9s02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdac5sznj60hc0jcwgc02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdan368oj60hc0b1gmx02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttday6t3rj30hc09jmy1.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdb6eotvj60hc08kt9i02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdbp3jgij30hc0q40w4.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdbxyfjlj60ef0sjq5x02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdc7933fj60hc0hdtaj02.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdcgk6f5j60hc0frab202.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttdcoayvsj60hc0cvjso02.jpg"><meta property="article:published_time" content="2021-08-15T13:40:25.000Z"><meta property="article:modified_time" content="2021-08-28T13:57:41.216Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="CV"><meta property="article:tag" content="FaceNet"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gttd8mg0jcj60hc0n8wgu02.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/62961.html"},"headline":"FaceNet: A Unified Embedding for Face Recognition and Clustering","image":["https://tva1.sinaimg.cn/large/008i3skNly1gttd8mg0jcj60hc0n8wgu02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttd8v3epsj30hc06wwf2.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttd96eircj60hc0770tb02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttd9fix1qj60hc0mc42302.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttd9uvoqfj60hc0a3q4q02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttda2yjo3j60hc0flt9s02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdac5sznj60hc0jcwgc02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdan368oj60hc0b1gmx02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttday6t3rj30hc09jmy1.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdb6eotvj60hc08kt9i02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdbp3jgij30hc0q40w4.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdbxyfjlj60ef0sjq5x02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdc7933fj60hc0hdtaj02.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdcgk6f5j60hc0frab202.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gttdcoayvsj60hc0cvjso02.jpg"],"datePublished":"2021-08-15T13:40:25.000Z","dateModified":"2021-08-28T13:57:41.216Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"尽管人脸识别领域最近有显著进步[10, 14,15,17]，有效的实施大规模的人脸验证和识别对当前的方法存在严重的挑战。本文提出一个系统，叫FaceNet，直接学习人脸图像到紧密欧式空间的一个映射，欧式空间中的距离直接度量人脸的相似度。一旦这个空间产生，诸如人脸识别、验证、聚类任务可以使用标准方法轻易实现，该方法用FaceNet embeddings作为特征向量。"}</script><link rel="canonical" href="https://hunlp.com/posts/62961.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a><a class="navbar-item" href="/friend">友链</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>FaceNet: A Unified Embedding for Face Recognition and Clustering</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-08-15</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-08-28</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span><span class="level-item">1 小时读完 (大约7304个字)</span></div></div><div class="content"><p>尽管人脸识别领域最近有显著进步[10, 14,15,17]，有效的实施大规模的人脸验证和识别对当前的方法存在严重的挑战。本文提出一个系统，叫FaceNet，直接学习<strong>人脸图像</strong>到紧密<strong>欧式空间</strong>的一个<strong>映射</strong>，欧式空间中的<strong>距离直接度量</strong>人脸的<strong>相似度</strong>。一旦这个空间产生，诸如人脸识别、验证、聚类任务可以使用标准方法轻易实现，该方法用FaceNet <strong>embeddings</strong>作为<strong>特征向量</strong>。 <span id="more"></span></p>
<p>我们的方法使用深度卷积网络训练，<strong>直接优化embedding本身</strong>；而不是像之前深度学习方法，<strong>优化</strong>一个中间的<strong>bottleneck层</strong>。训练时，使用新颖的<strong>在线triplet挖掘方法</strong>生成大致的匹配/不匹配人脸的triplets。我们方法的好处是更有效的表示：每个人脸<strong>仅仅使用128-bytes</strong>，人脸识别达到了state-of-the-art性能。</p>
<pre><code>在广泛使用的**LFW**数据集上，精确性达到一个新纪录**99.63%**；在YouTube</code></pre>
<p>Faces DB上达到<strong>95.12%</strong>。与最好的结果[15]相比，两个数据集上<strong>错误率</strong>都<strong>减少了30%</strong>。</p>
<pre><code>    我们同样会介绍harmonic embeddings和harmonic triplet</code></pre>
<p>loss概念，描述不同的face embeddings版本(使用不同的网络),它们彼此兼容并可以互相直接比较。</p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1503.03832.pdf" class="uri">https://arxiv.org/pdf/1503.03832.pdf</a></p>
<h2 id="简介">简介</h2>
<p>本文提出一个统一的系统；对于<strong>人脸认证</strong>(是否同一个人)，<strong>识别</strong>(哪个人)和<strong>聚类</strong>(在人脸中找出相同的人)。我们的方法基于使用深度卷积网络对每张图像学习一个<strong>欧式嵌入</strong>。训练网络使得嵌入空间中的<strong>L2平方距离</strong>直接与人脸<strong>相似性</strong>相关：属于<strong>同一个人</strong>的人脸<strong>距离小</strong>，属于<strong>不同人</strong>的人脸<strong>距离大</strong>。</p>
<p>一旦嵌入产生，先前的任务变得简单：<strong>人脸验证</strong>就是两个嵌入之间距离的<strong>阈值</strong>问题；<strong>人脸识别</strong>变为一个<strong>K-NN分类</strong>问题；<strong>人脸聚类</strong>可用现成的<strong>k-means</strong>或者<strong>层次聚类</strong>。</p>
<p>以前基于深度网络人脸识别方法，使用一个分类层[15,17]，在已知人脸身份的数据集上训练，然后使用中间的<strong>bottleneck层作为表示</strong>来泛化到训练集之外的人脸识别。这种方法的缺点是不够直接和有效：首先<strong>寄希望</strong>于bottleneck表示能<strong>足够好</strong>的<strong>泛化</strong>到新的人脸；其次使用bottleneck层表示每个人脸通常需要<strong>很大的空间</strong>(数千维)。最近一些方法[15]使用<strong>PCA降维</strong>，但是这是一个<strong>线性变换</strong>，网络中的某层可以轻易的学到。</p>
<pre><code>    与这些方法相对，FaceNet 基于LMNN</code></pre>
<p>[19]中一个<strong>triplet损失函数</strong>直接训练输出一个<strong>紧凑的128维嵌入</strong>。我们的triplets 包含<strong>两个匹配</strong>的人脸缩略图和<strong>一个不匹配</strong>的人脸缩略图；损失函数的目标是通过一个距离间隔分离正样本对和负样本。缩略图是脸部区域的紧密裁剪，没有2D或3D对齐；<strong>除了缩放</strong>和<strong>平移</strong>。</p>
<p><strong>选择哪些triplets</strong>对于好的性能非常重要，受递进学习[1]的启发，我们采用一个新的<strong>在线负样本挖掘</strong>策略，确保网络训练时连续的增加triplets 的难度。为提高聚类准确度，探索困难正样本技术；促进单个人的嵌入形成球状群。</p>
<pre><code>    图 Figure 1显示我们的方法可以处理难以置信的变化情况。图像对 来自PIE</code></pre>
<p>[13]，先前被认为对于人脸验证系统来说<strong>非常困难</strong>。</p>
<pre><code>    接下来的段落2回顾这个领域的文献；段落3.1定义triplet</code></pre>
<p>loss，段落3.2描述triplet的选择和训练过程，段落3.3描述模型结构。段落4和5，呈现嵌入的一些定量结果，并定性地探讨了一些聚类结果。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttd8mg0jcj60hc0n8wgu02.jpg" /></p>
<h2 id="相关工作">相关工作</h2>
<pre><code>     与最近其他使用深度网络[15,</code></pre>
<p>17]的方法相似；我们的方法是一个<strong>纯数据驱动</strong>的方法，直接从人脸的像素学习它的表示；而<strong>不使用工程特征</strong>，我们使用了大量的标注人脸数据集来获得对姿态、光照以及其它情况发现变化下的合适的<strong>不变性</strong>。</p>
<p>本文探索了最近在计算机视觉社区非常成功的两个不同的深度网络架构。都是深度卷积网络[8, 11]，第一个架构基于<strong>Zeiler&amp;Fergus</strong> [22]模型，包括多个交错的卷积层，非线性激活，局部响应归一化和最大池化层。受[9]的工作启发，我们额外增加一些1×1×d卷积层。第二个架构基于Szegedy等的<strong>Inception模型</strong>，它最近赢得ImageNet 2014 [16]比赛。这些网络使用混合层，同时并行的跑多个不同的卷积和池化层，然后连结它们的输出结果。这些模型可以<strong>减少参数20倍</strong>，同时对于相当的性能，可以潜在的减少FLOPS。</p>
<p>有大量的人脸验证和识别工作，回顾它们不是本文的范畴，我们仅仅简单的讨论下最相关的工作。</p>
<pre><code>    [15, 17,</code></pre>
<p>23]都采用一个<strong>多阶段</strong>的复杂系统，对于深度卷积网络的输出组合了PCA降维和SVM分类。</p>
<pre><code>    Zhenyao</code></pre>
<p>等[23]使用一个深度网络将人脸“变形“为一个标准的正面视角，然后再学习一个CNN用于分类人脸到已知的人。对于人脸验证，在网络输出端使用PCA结合一个集成的SVM。</p>
<p>Taigman 等[17]提出一个<strong>多阶段</strong>方法，对齐人脸到一个通用的3D模型。在超过4千个人上训练一个多类网络来执行人脸识别。作者同样试验了一个所谓的孪生网络，直接优化两个人脸特征之间的L1距离。LFW最好的性能(97.35)来源于一个组合了3个集成网络，使用了不同对齐和色彩通道。这些网络的预测使用非线性SVM组合。</p>
<pre><code>    Sun等[14,</code></pre>
<p>15]提出一个紧凑的，因而相对廉价的计算网络。集成25个这种网络，每个网络操作不同的人脸patch。最终性能在LFW(99.47%[15]),作者组合了50个响应(常规的和翻转的）。使用了对线性变换有效的PCA和联合Bayesian模型[2]。它们的方法不需要显示的2D/3D对齐。组合分类和验证的损失来训练网络，验证损失类似于我们使用的[12,19]triplet损失，最小化同一个人的人脸之间的L2距离，保证不同人的人脸之间距离间隔。主要的不同点是，<strong>只</strong>比较了<strong>图像对</strong>，而triplet损失则鼓励<strong>相对距离</strong>约束。</p>
<pre><code>     Wang 等[18]在语义和视觉相似性排序中探索了一个类似的损失函数。</code></pre>
<h2 id="方法">方法</h2>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttd8v3epsj30hc06wwf2.jpg" /></p>
<pre><code>   FaceNet使用了深度卷积网络，我们讨论两种不同的核心架构：**Zeiler&amp;Fergus**</code></pre>
<p>[22]类型网络和最近的<strong>Inception</strong> [16]类型网络，细节部分在段落3.3会讨论。</p>
<pre><code>   给定模型细节，将它当做黑箱(见图Figure</code></pre>
<p>2),我们方法最重要的部分在于<strong>端到端</strong>的<strong>学习整个系统</strong>。为此我们采用triplet损失函数，它直接影响我们在人脸验证、识别和聚类中想达到的目的。也就是说，我们使用个嵌入f(x)，将<strong>图片x</strong>映射到<strong>特征空间</strong> <span class="math inline">\(\Bbb R^d\)</span></p>
<p>，使得所有人脸的平方<strong>距离</strong>独立于<strong>成像条件</strong>，<strong>同一个人</strong>的人脸<strong>距离小</strong>，来自<strong>不同人</strong>的人脸<strong>距离大</strong>。</p>
<p>虽然没有直接比较其他的损失函数，例如[14]中使用的样本对；我们相信triplet损失函数对于人脸验证更加合适。[14]中的损失函数动机是将同一个人的人脸投影到<strong>嵌入空间</strong>中的<strong>一个点</strong>。triplet损失函数，则试图<strong>加强</strong>同一人的人脸距离和不同人的人脸距离的<strong>一个间隔</strong>。这使得同一个的人脸可以<strong>保持多样性</strong>，同时加强与不同人脸的距离，因而区别其它的人的人脸。</p>
<pre><code>   接来下阐述triple损失函数已经如何**大规模有效**的训练它。</code></pre>
<h3 id="triplet-损失">Triplet 损失</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttd96eircj60hc0770tb02.jpg" /></p>
<p><span class="math inline">\(f(x) \in \Bbb R^d\)</span> 代表嵌入；它将一个图像x嵌入到d维欧式空间。另外，我们约束这个嵌入位于<strong>d维超球面</strong>，如<span class="math inline">\(||f(x)||^2=1\)</span></p>
<p>。这个损失函数受[19]中最邻近分类的启发。这里我们需要保证<span class="math inline">\(x^a_i\)</span></p>
<p>(anchor)与来自同一个人的任何图像<span class="math inline">\(x^p_i\)</span>(positive)的距离比与其他人的图像<span class="math inline">\(x^n_i\)</span> (negative)都要近，如图Figure 3所示。</p>
<pre><code>    因此我们希望</code></pre>
<p><span class="math display">\[
||f(x^a_i) - f(x^p_i)||^2_2 + \alpha &lt; ||f(x^a_i) - f(x^n_i)||^2_2 ,\quad \tag 1 \\
\]</span></p>
<p><span class="math display">\[
\forall (f(x^a_i), f(x^p_i), f(x^n_i)) \in \cal {T} \quad \tag 2
\]</span></p>
<pre><code>    **α**是正样例对和负样例对的**间隔**。$\cal T$</code></pre>
<p>是训练时所有可能的triplets，基数为N，因此需要最小化的损失函数L等于</p>
<p><span class="math display">\[
\sum^N_i[||f(x^a_i) - f(x^p_i)||^2_2 - ||f(x^a_i) - f(x^n_i)||^2_2 + \alpha]_+ \tag 3
\]</span></p>
<p>生成所有可能的triplets会导致很多triplets轻易满足条件(如：满足等式(1)),这些triplets对于训练没有贡献，导致收敛慢，如果它们也经过网络。<strong>选择困难的triplets</strong>非常关键，它们有效且对提升模型有贡献。接下来谈谈triplet选择的不同方法。</p>
<h3 id="triplet选择">Triplet选择</h3>
<pre><code>     为了保证快速收敛，选择**违反等式(1)**的triplets非常关键，这就是说，给定$x^a_i$</code></pre>
<p>，我们希望选择<span class="math inline">\(x^p_i\)</span>(positive)使得<span class="math inline">\(arg \max_{x^p_i}||f(x^a_i) - f(x^p_i)||^2_2\)</span></p>
<p>，类似的选择<span class="math inline">\(x^n_i\)</span> (negative)使得<span class="math inline">\(arg \min_{x^n_i}||f(x^a_i) - f(x^n_i)||^2_2\)</span></p>
<p>。</p>
<pre><code>      在整个训练集计算argmin和argmax</code></pre>
<p>是不可能的，此外，这不好的训练结果，因为<strong>误标记图像</strong>和<strong>质量差的图像</strong>会支配困难正样本和困难负样本。有两种方式可以避免这个问题：</p>
<ul>
<li><p>每n步离线生成triplets,使用网络的最近的checkpoint，然后在一个子集数据中计算argmin和argmax。</p></li>
<li><p>在线生成triplets,可以在一个mini-batch中选择困难正负样本。</p></li>
</ul>
<p>这里我们关注<strong>在线生成</strong>，并使用一个<strong>较大的mini-batch</strong>,大概<strong>数千个样本</strong>，在一个mini-batch中计算argmin和argmax。</p>
<pre><code>   为了拥有一个有意义的anchor</code></pre>
<p>positive距离表示，需要确保在每个mini-batch中一个人的人脸样本数量的最小值。实验中，我们采样训练数据，每个mini-batch中的<strong>每个人</strong>大概有<strong>40张</strong>图像，另外<strong>负样本随机</strong>的选择到mini-batch中。</p>
<pre><code>   **并非**选择**最困难**的**正样本**，我们使用mini-batch中**所有的anchor</code></pre>
<p>positive对<strong>，同时选择</strong>困难负样本<strong>。我们没有在一个mini-batch中同时比较困难anchor-positive对和所有anchor-positive对，但是实践中发现所有anchor positive方法</strong>更稳定<strong>，并且在训练早起</strong>收敛<strong>稍微</strong>快一点**。</p>
<p>我们同样探索了离线生成triplet和在线生成triplet结合，这样可以使用更小的mini-batch,但是试验表明没有决定性影响。</p>
<p>在实践中选择<strong>最困难的负样本</strong>导致训练很早陷入<strong>局部极小值</strong>，特别地，还会导致<strong>模型衰退</strong>(如：f(x) = 0)。为了减轻这种情况，如下公式(4)帮助选择<span class="math inline">\(x^n_i\)</span></p>
<p><span class="math display">\[
||f(x^a_i) - f(x^p_i)||^2_2  &lt; ||f(x^a_i) - f(x^n_i)||^2_2  \tag 4
\]</span></p>
<p>我们称这用负样本为<strong>半困难</strong>，它们与anchor的距离比正样本与anchor的远，但任然是困难的，因为平方距离<strong>接近</strong>anchor positive距离。这些负样本位于间隔α内部。</p>
<p>之前提到，正确选择triplet对于快速收敛非常非常关键。一方面，我们想使用<strong>小的mini-batch</strong>,因为在SGD[20]中能够<strong>加速收敛</strong>；另一方面，实施细节使得几十到几百大小的batch更有效。</p>
<p>然而，关于<strong>batch大小</strong>的主要限制是我们从mini-batch中<strong>选择困难相关triplet</strong>的<strong>方式</strong>。大部分实验中，我们使用大概<strong>1800</strong>个样本的<strong>batch大小</strong>。</p>
<h3 id="深度卷积网络">深度卷积网络</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttd9fix1qj60hc0mc42302.jpg" /></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttd9uvoqfj60hc0a3q4q02.jpg" /></p>
<p>所有的实验中使用<strong>SGD</strong>训练CNN，使用标准的反向传播[8,11]和AdaGrad[5]。大部分实验中，初始<strong>学习率</strong>为<strong>0.05</strong>，然后<strong>降低</strong>到最终模型。模型<strong>随机初始化</strong>，类似[16]，在一个<strong>CPU集群</strong>上训练<strong>1000~2000小时</strong>。损失函数在训练<strong>500小时</strong>后<strong>急剧下降</strong>，但是<strong>继续训练</strong>任然能<strong>显著提升性能</strong>，<strong>间隔α</strong>设置为<strong>0.2</strong>。</p>
<p>我们使用了两类架构，探索它们的权衡关系。实际上的不同在于<strong>参数</strong>和<strong>FLOPS</strong>的差异。最佳模型依赖于实际应用。如：数据中心跑的模型可以有大量参数，并需要很大的FLOPS，而移动手机上需要少量参数，这样才能载入内存中。所有的模型都使用<strong>整流线性单</strong>元作为激活函数。</p>
<pre><code>   第一类，如表Table1,像[9]中建议的，在Zeiler&amp;Fergus</code></pre>
<p>[22]架构的标准卷积层之间增加1×1×d的卷积层，使得模型有22层深；一共<strong>1.4亿</strong>个<strong>参数</strong>，每张图需要大概<strong>16亿FLOPS</strong>。</p>
<pre><code>   第二类使用基于GoogLeNet</code></pre>
<p>类的Inception模型[16]；模型<strong>参数小</strong>了大概<strong>20倍</strong>(约660w~700w)，<strong>FLOPS</strong>最大<strong>少了5倍</strong>(5000w~16亿之间)。某些这类模型尺寸可以难以置信的<strong>减小</strong>(包括深度和filter个数)，这样它们可以在手机上运行。a) NNS1 有2600w个参数，每张图仅仅需要2.2亿FLOPS；b) NNS2 有430w个参数，2000wFLOPS；c) 表Table 2展示了最大的网络NN2; d) NN3架构一样，但是输入尺寸减小到160x160；e) NN4的输入尺寸只有96x96，因而极大减少cpu的需求(<strong>2.85亿FLOPS</strong> vs NN2的16亿)。此外输入尺寸减少后，在较高层不使用5x5的卷积，因为那时感受野已经非常小。我们发现通常5x5的 卷积层可以都移除，<strong>精度</strong>仅仅<strong>很小的下降</strong>。图Figure 4比较了我们所有的模型。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttda2yjo3j60hc0flt9s02.jpg" /></p>
<h2 id="数据集和评估">数据集和评估</h2>
<pre><code>    使用4个数据集评估我们的方法，除了LFW和Youtube</code></pre>
<p>Faces；我们在<strong>人脸验证</strong>任务上<strong>评估我们的方法</strong>。如：给定包含两张图片的对，使用一个平方L2距离阈值<span class="math inline">\(D(x_i, x_j)\)</span></p>
<p>来决定是否同一人。所有同一个人的人脸对(i, j)记做<span class="math inline">\({\cal P}_{same}\)</span></p>
<p>，所有不同人的人脸对记做<span class="math inline">\({\cal P}_{diff}\)</span></p>
<p>；我们定义true accepts为：</p>
<p><span class="math display">\[
TA(d) = \{(i,j) \in {\cal P}_{same}, with \  D(x_i,x_j) \le d\} \tag 5
\]</span></p>
<pre><code>     类似地定义false accept为：</code></pre>
<p><span class="math display">\[
FA(d) = \{(i,j) \in {\cal P}_{diff}, with \  D(x_i,x_j) \le d\} \tag 6
\]</span></p>
<pre><code>    validation rate VAL(d)和false accept rate FAR(d)定义如下：</code></pre>
<p><span class="math display">\[
VAL(d) = \frac {|TA(d)|} {|{\cal P}_{same}|}, \quad FAR(d) = \frac {FA(d)} {|{\cal P}_{diff}|}
\]</span></p>
<h3 id="保留测试集">保留测试集</h3>
<p>我们保留了大概<strong>100w</strong>张图像，与训练集有相同的分布，但是来自不同的人(人没有交集)。将它们分为互斥的<strong>5份</strong>，每份<strong>20w</strong>张图像。FAR和VAL在<strong>10w×10w</strong>的图像对上计算，使用5部分的标准差。</p>
<h3 id="私人照片">私人照片</h3>
<p>这是一个与训练集类似分布的测试集，但是经过人工校验，标签非常干净。包含3个私人照片集，一共约1.2w张图像。在<strong>1.2w平方</strong>个图像对象上计算FAR 和 VAL。</p>
<h3 id="学术数据集">学术数据集</h3>
<p>LFW是人脸验证[7]事实上的学术测试集，我们按照非限制性标注外部数据的标准协议，报告了平均分类误差和均值的标注差。</p>
<pre><code>     Youtube Faces DB [21]是一个在人脸识别社区[17,</code></pre>
<p>15]非常流行的数据集。设置与LFW类似，但是不是使用图像对，而是视频对。</p>
<h2 id="实验">实验</h2>
<p>如无特殊说明，我们使用<strong>1亿~2亿</strong>个<strong>人脸缩略图</strong>，来自<strong>800w</strong>个不同的人。在每个图像上执行一个人脸检测器，为每个人脸生成一个<strong>紧贴人脸</strong>的<strong>边框</strong>。这些人脸缩略图的尺寸被<strong>调整</strong>为对应网络的输入大小；实验中，输入大小从96x96像素到224x224像素。</p>
<h3 id="计算和精度权衡">计算和精度权衡</h3>
<p>在深入实验细节之前，我们先讨论<strong>精度</strong>和<strong>FLOPS值</strong>的权衡，这对特定模型是需要的。图Figure 4，在x轴显示FLOPS值，以及false accept rate (<strong>FAR</strong>)为<strong>0.001</strong>时的精度，使用的是4.2我们标注的私人照片。非常有趣的是模型的<strong>计算量</strong>和<strong>精度</strong>有<strong>很强的相关性</strong>。图中高亮展示了5个模型(NN1, NN2, NN3, NNS1, NNS2)。</p>
<p>我们同样观察了<strong>精度</strong>和模型<strong>参数量</strong>之间的权衡；但是没有那么明显的关系。例如，基于Inception的模型NN2达到NN1以上的精度，但是<strong>参数少了20倍</strong>；<strong>FLOPS</strong>值是<strong>相当</strong>的。很明显如果参数进一步减少，精度还是会下降。其它的模型结构可能在不影响精度情况下能够进一步减少参数，就像Inception [16]中的情况。</p>
<h3 id="cnn模型效果">CNN模型效果</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdac5sznj60hc0jcwgc02.jpg" /></p>
<pre><code>    现在讨论4个模型性能的详细情况。一方面使用增加了1×1卷积[22,</code></pre>
<p>9]的传统的Zeiler&amp;Fergus基础架构(见表Table 1)，另一方面使用可以极大减小模型尺寸的基于Inception[16]的模型。整体看最终两类模型的最好性能相当。但是基于Inception的模型，如NN3在<strong>显著减少FLOPS</strong>和<strong>模型尺寸</strong>的同时达到了<strong>很好的性能</strong>。</p>
<pre><code>    在私有照片测试集上评估明细见图Figure 5.</code></pre>
<p>最大的模型相对微小的模型NNS2在精度上有很大提升；后者手机上运行速度可以到达30ms每张图，并且对于人脸聚类来说进度足够了。ROC曲线中FAR &lt; <span class="math inline">\(10^{-4}\)</span>时进度剧烈下降表明测试数据集中有噪音；在极小的false accept</p>
<p>rates时，一个误标注图像都在曲线中有明显影响。</p>
<h3 id="图像质量敏感">图像质量敏感</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdan368oj60hc0b1gmx02.jpg" /></p>
<pre><code>    表Table</code></pre>
<p>4显示模型在不同<strong>图像尺寸</strong>上的鲁棒性。网络对于JPEG压缩表现出令人惊讶的的鲁棒性，在JPEG质量<strong>下降20%</strong>时<strong>性能非常好</strong>。性能在<strong>120x120</strong>像素的尺寸上<strong>下降很少</strong>，甚至在80x80像素时性能也可接受。这是值得注意的，因为网络在220x220的图像上训练，如果在低分辨率的人脸上训练可以进一步提升这个区间。</p>
<h3 id="嵌入维度">嵌入维度</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttday6t3rj30hc09jmy1.jpg" /></p>
<p>我们探索了不同的嵌入维度，最终所有实验选择了<strong>128维</strong>，各维度的比较见表Table 5。大家可能认为大的嵌入维度<strong>至少不会差于</strong>小的嵌入维度，然而，可能需要<strong>更多的训练</strong>来达到相同的精度。这就意味着表Table 5中的性能差异<strong>统计上不显著</strong>。</p>
<p>值得注意的是，128维<strong>浮点</strong>向量可以<strong>量化</strong>为128<strong>字节</strong>，<strong>精度</strong>上<strong>没有损失</strong>；因此每个人脸被128维字节向量密集的表示，对于大规模聚类和识别非常理想。小的嵌入可能精度上稍微降低，可以在移动设备上使用。</p>
<h3 id="训练集大小">训练集大小</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdb6eotvj60hc08kt9i02.jpg" /></p>
<pre><code>   表Table</code></pre>
<p>6展示大体量训练数据的影响；由于时间的限制，评估在一个较小的模型上运行；在更大模型上的影响可能更大。非常明显，使用使用<strong>数千万</strong>的样本可以在私有照片测试集上提升精度；与只有<strong>几百万</strong>图像比较<strong>错误减少60%</strong>。使用<strong>更大体量</strong>的数据集(几亿)任然有小的提升，但是<strong>提升减缓</strong>了。</p>
<h3 id="lfw上性能">LFW上性能</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdbp3jgij30hc0q40w4.jpg" /></p>
<p>在LFW上使用<em>非限制标注外部数据</em>的标准协议上评估模型。9个训练splits使用L2距离阈值，在第10个测试的split上分类(相同或不同)；对所有的测试split最优的阈值是1.242，处理第8个split(1.256)</p>
<pre><code>    我们模型使用两种模式评估：</code></pre>
<ul>
<li><p>1：固定裁剪LFW的中心来提供缩略图</p></li>
<li><p>2：在LFW上运行一个专有的人脸检测(类似Picasa [3])来提供缩略图，如果失败，使用LFW 对齐</p>
<p>图Figure 6给了所有失败例子的概览，顶部是<strong>false accepts</strong>，底部是<strong>false rejects</strong>。使用<strong>模式(1)</strong>达到<strong>98.87%</strong>±0.15的精度；使用<strong>模式(2)</strong>精度为<strong>99.63%</strong>±0.09；这种方式在DeepFace[17]中减少错误7个点，在之前的DeepId2+[15]上减少了30%。这是模型NN1的性能，但是更小的NN3达到的性能也<strong>没有统计上显著的差异</strong>。</p></li>
</ul>
<h3 id="youtube-faces-db上性能">Youtube Faces DB上性能</h3>
<p>在每个视频的<strong>前1百帧</strong>上采用所有对的平均相似度；分类精度为<strong>95.12</strong>%±0.39；使用<strong>前1千帧</strong>结果为<strong>95.18</strong>%。与[17]的91.4%相比错误率<strong>降低约一半</strong>；DeepId2+ [15]达到93.2%的精度，我们的模型与它比，<strong>错误降低30%</strong>。</p>
<h3 id="人脸聚类">人脸聚类</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdbxyfjlj60ef0sjq5x02.jpg" /></p>
<p>由于<strong>紧密的嵌入</strong>，可以在用户私有照片集上按照人聚类图片。相比纯粹的人脸认证，人脸聚类的约束条件产生了真正令人惊讶的结果。图Figure 7展示了私人照片集上聚类的结果，使用层次聚类。非常清晰明了的展示了对遮挡、光照、姿态甚至年龄的<strong>难以置信</strong>的<strong>不变性</strong>。</p>
<h2 id="总结">总结</h2>
<p>我们提供一个方法对人脸验证直接学习到欧式空间的一个嵌入。这种方法其它使用CNN <strong>bottleneck 层</strong>或者需要<strong>额外后处理</strong>(如连结多个模型和PCA，以及SVM分类）的方法不同，我们<strong>端到端训练</strong>简化了设置，直接优化一个跟任务相关的损失函数，提升了性能。</p>
<p>另一个优点是，模型仅仅需要<strong>很少的对齐</strong>(紧贴人脸的一个裁剪)[17]，例如，执行一个复杂的3D对齐。我们同样试验了类似的转换对齐，发现可以轻微的提升性能；不是很确信是否值得增加这额外的复杂度。</p>
<p>未来的工作将聚焦于更好的<strong>理解错误的例子</strong>，进一步提升模型；同样包括<strong>缩小模型大小</strong>，<strong>减少CPU需求</strong>。也关注怎样提升当前<strong>极度漫长</strong>的<strong>训练耗时</strong>；使用更小batch大小的课程学习的变种，离线和在线的正负样本挖掘。</p>
<h2 id="附录harmonic嵌入">附录：Harmonic嵌入</h2>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdc7933fj60hc0hdtaj02.jpg" /></p>
<pre><code>    本段介绍harmonic</code></pre>
<p>嵌入的概念。指的是由不同模型v1和v2生成的一组嵌入，但是它们互相<strong>可比较</strong>，从这个意义上讲是<strong>兼容</strong>的。</p>
<p>这种兼容很大的简化的升级方法。如：嵌入v1是在大量数据集上计算得到的，现在一个新的嵌入模型v2要推出，兼容性确保的<strong>平滑的转换</strong>，无需担心版本的不兼容。图Figure 8展示了3G数据集的结果。提升的模型NN2优于NN1,<strong>组合</strong>NN2和NN1的<strong>嵌入</strong>的<strong>性能居中</strong>。</p>
<h3 id="harmonic-triplet-函数函数">Harmonic Triplet 函数函数</h3>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdcgk6f5j60hc0frab202.jpg" /></p>
<pre><code>    为了研究harmonic</code></pre>
<p>嵌入，我们混合已经学习好的v1嵌入和v2嵌入。这个在triplet损失函数内部做，额外生成的triplets促进了不同版本嵌入的兼容。图Figure 9 可视化了贡献给triplet损失函数的不同组合的triplets。</p>
<p>从一个已经训练的NN2中初始化v2嵌入，然后使用促进兼容的triplet损失函数<strong>重新训练</strong>随机初始化的<strong>最后一层</strong>(嵌入层)。开始只有最后一层重新训练，然后使用harmonic损失函数<strong>训练整个v2网络</strong>。</p>
<pre><code>     图Figure 10</code></pre>
<p>展示了实际中这种兼容可能有效的解释。大部分v2的嵌入可能挨着相关的v1嵌入，但是<strong>v1错误的嵌入</strong>，可能<strong>轻微的扰动</strong>，这样新的嵌入空间位置提升了验证准确性。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gttdcoayvsj60hc0cvjso02.jpg" /></p>
<h3 id="总结-1">总结</h3>
<p>非常有趣和惊人的是harmonic如此有效，未来工作可以探索这种<strong>思想</strong>能够<strong>扩展多远</strong>。推测在保持兼容的情况下v2嵌入提升v1嵌入的范围有一个<strong>上限</strong>。另外训练在手机上运行的小网络可以与较大的服务端模型<strong>兼容</strong>是非常有有趣的。</p>
<h2 id="参考文章">参考文章</h2>
<p>[1] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In Proc. of ICML, New York, NY, USA, 2009. 2</p>
<p>[2] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face revisited: A joint formulation. In Proc. ECCV, 2012. 2</p>
<p>[3] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun. Joint cascade face detection and alignment. In Proc. ECCV, 2014. 7</p>
<p>[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, NIPS, pages 1232–1240. 2012. 10</p>
<p>[5] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July 2011. 4</p>
<p>[6] I. J. Goodfellow, D. Warde-farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In In ICML, 2013. 4</p>
<p>[7] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007. 5</p>
<p>[8] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, Dec. 1989. 2, 4</p>
<p>[9] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013. 2, 4, 6</p>
<p>[10] C. Lu and X. Tang. Surpassing human-level face verification performance on LFW with gaussianface. CoRR, abs/1404.3840, 2014. 1</p>
<p>[11] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 1986. 2, 4</p>
<p>[12] M. Schultz and T. Joachims. Learning a distance metric from relative comparisons. In S. Thrun, L. Saul, and B. Schölkopf, editors, NIPS, pages 41–48. MIT Press, 2004. 2</p>
<p>[13] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination, and expression (PIE) database. In In Proc. FG, 2002. 2</p>
<p>[14] Y. Sun, X. Wang, and X. Tang. Deep learning face representation by joint identification-verification. CoRR, abs/1406.4773, 2014. 1, 2, 3</p>
<p>[15] Y. Sun, X. Wang, and X. Tang. Deeply learned face representations are sparse, selective, and robust. CoRR, abs/1412.1265, 2014. 1, 2, 5, 8</p>
<p>[16] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. 2, 3, 4, 5, 6, 10</p>
<p>[17] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verification. In IEEE Conf. on CVPR, 2014. 1, 2, 5, 7, 8, 9</p>
<p>[18] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen, and Y. Wu. Learning fine-grained image similarity with deep ranking. CoRR, abs/1404.4661, 2014. 2</p>
<p>[19] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric learning for large margin nearest neighbor classification. In NIPS. MIT Press, 2006. 2, 3</p>
<p>[20] D. R. Wilson and T. R. Martinez. The general inefficiency of batch training for gradient descent learning. Neural Networks, 16(10):1429–1451, 2003. 4</p>
<p>[21] L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained videos with matched background similarity. In IEEE Conf. on CVPR, 2011. 5</p>
<p>[22] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. 2, 3, 4, 6</p>
<p>[23] Z. Zhu, P. Luo, X. Wang, and X. Tang. Recover canonicalview faces in the wild with deep neural networks. CoRR, abs/1404.3543, 2014. 2</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>FaceNet: A Unified Embedding for Face Recognition and Clustering</p><p><a href="https://hunlp.com/posts/62961.html">https://hunlp.com/posts/62961.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-08-15</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-08-28</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/CV/">CV, </a><a class="link-muted" rel="tag" href="/tags/FaceNet/">FaceNet </a></div></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zfb.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wx.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/64358.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Aggregated Residual Transformations for Deep Neural Networks</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/47418.html"><span class="level-item">SSD-Single Shot MultiBox Detector</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="waline-thread"></div><script src="https://cdn.jsdelivr.net/npm/@waline/client@1.5.4/dist/Waline.min.js"></script><script>Waline({
            el: '#waline-thread',
            serverURL: "https://waline-server-gilt.vercel.app",
            path: window.location.pathname,
            lang: "zh-CN",
            visitor: false,
            emoji: ["https://cdn.jsdelivr.net/gh/walinejs/emojis/weibo","https://cdn.jsdelivr.net/gh/walinejs/emojis@1.0.0/bilibili","https://cdn.jsdelivr.net/gh/walinejs/emojis@1.0.0/qq"],
            dark: "auto",
            meta: ["nick","mail","link"],
            requiredMeta: [],
            login: "enable",
            
            pageSize: 10,
            
            
            math: false,
            copyright: true,
            locale: {"placeholder":"Comment here..."},
        });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#简介"><span class="level-left"><span class="level-item">1</span><span class="level-item">简介</span></span></a></li><li><a class="level is-mobile" href="#相关工作"><span class="level-left"><span class="level-item">2</span><span class="level-item">相关工作</span></span></a></li><li><a class="level is-mobile" href="#方法"><span class="level-left"><span class="level-item">3</span><span class="level-item">方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#triplet-损失"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Triplet 损失</span></span></a></li><li><a class="level is-mobile" href="#triplet选择"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Triplet选择</span></span></a></li><li><a class="level is-mobile" href="#深度卷积网络"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">深度卷积网络</span></span></a></li></ul></li><li><a class="level is-mobile" href="#数据集和评估"><span class="level-left"><span class="level-item">4</span><span class="level-item">数据集和评估</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#保留测试集"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">保留测试集</span></span></a></li><li><a class="level is-mobile" href="#私人照片"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">私人照片</span></span></a></li><li><a class="level is-mobile" href="#学术数据集"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">学术数据集</span></span></a></li></ul></li><li><a class="level is-mobile" href="#实验"><span class="level-left"><span class="level-item">5</span><span class="level-item">实验</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#计算和精度权衡"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">计算和精度权衡</span></span></a></li><li><a class="level is-mobile" href="#cnn模型效果"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">CNN模型效果</span></span></a></li><li><a class="level is-mobile" href="#图像质量敏感"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">图像质量敏感</span></span></a></li><li><a class="level is-mobile" href="#嵌入维度"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">嵌入维度</span></span></a></li><li><a class="level is-mobile" href="#训练集大小"><span class="level-left"><span class="level-item">5.5</span><span class="level-item">训练集大小</span></span></a></li><li><a class="level is-mobile" href="#lfw上性能"><span class="level-left"><span class="level-item">5.6</span><span class="level-item">LFW上性能</span></span></a></li><li><a class="level is-mobile" href="#youtube-faces-db上性能"><span class="level-left"><span class="level-item">5.7</span><span class="level-item">Youtube Faces DB上性能</span></span></a></li><li><a class="level is-mobile" href="#人脸聚类"><span class="level-left"><span class="level-item">5.8</span><span class="level-item">人脸聚类</span></span></a></li></ul></li><li><a class="level is-mobile" href="#总结"><span class="level-left"><span class="level-item">6</span><span class="level-item">总结</span></span></a></li><li><a class="level is-mobile" href="#附录harmonic嵌入"><span class="level-left"><span class="level-item">7</span><span class="level-item">附录：Harmonic嵌入</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#harmonic-triplet-函数函数"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Harmonic Triplet 函数函数</span></span></a></li><li><a class="level is-mobile" href="#总结-1"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">总结</span></span></a></li></ul></li><li><a class="level is-mobile" href="#参考文章"><span class="level-left"><span class="level-item">8</span><span class="level-item">参考文章</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2022 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>