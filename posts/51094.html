<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>BiLSTM和CRF算法的序列标注原理 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="CRF原理 条件随机场(conditional_random_field)，是一类判别型算法，特别适合于预测任务，任务中的 上下文信息或临近状态会影响当前状态 。如序列标注任务： &amp;gt;判别式模型(discriminative model)计算条件概率，而生成式模型(generative model)计算联合概率分布"><meta property="og:type" content="blog"><meta property="og:title" content="BiLSTM和CRF算法的序列标注原理"><meta property="og:url" content="https://hunlp.com/posts/51094.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="CRF原理 条件随机场(conditional_random_field)，是一类判别型算法，特别适合于预测任务，任务中的 上下文信息或临近状态会影响当前状态 。如序列标注任务： &amp;gt;判别式模型(discriminative model)计算条件概率，而生成式模型(generative model)计算联合概率分布"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gr7usl3lw7j30bx0anjsm.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gr7utnqj9tj318i0tlwjz.jpg"><meta property="og:image" content="https://hunlp.com/posts/"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gr7uuc62ymj30k609qgmg.jpg"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gr7uv143jkj30k40ee75h.jpg"><meta property="article:published_time" content="2021-06-01T16:23:42.000Z"><meta property="article:modified_time" content="2021-06-05T16:38:00.518Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="BiLSTM"><meta property="article:tag" content="CRF"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gr7usl3lw7j30bx0anjsm.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/51094.html"},"headline":"BiLSTM和CRF算法的序列标注原理","image":["https://tva1.sinaimg.cn/large/008i3skNly1gr7usl3lw7j30bx0anjsm.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gr7utnqj9tj318i0tlwjz.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gr7uuc62ymj30k609qgmg.jpg","https://tva1.sinaimg.cn/large/008i3skNly1gr7uv143jkj30k40ee75h.jpg"],"datePublished":"2021-06-01T16:23:42.000Z","dateModified":"2021-06-05T16:38:00.518Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"CRF原理 条件随机场(conditional_random_field)，是一类判别型算法，特别适合于预测任务，任务中的 上下文信息或临近状态会影响当前状态 。如序列标注任务： &gt;判别式模型(discriminative model)计算条件概率，而生成式模型(generative model)计算联合概率分布"}</script><link rel="canonical" href="https://hunlp.com/posts/51094.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a><a class="navbar-item" href="/friend">友链</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>BiLSTM和CRF算法的序列标注原理</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-06-02</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-06-06</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span><span class="level-item">32 分钟读完 (大约4816个字)</span></div></div><div class="content"><h1 id="crf原理"><code>CRF</code>原理</h1>
<p>条件随机场<code>(conditional_random_field)</code>，是一类<strong>判别型</strong>算法，特别适合于预测任务，任务中的 <strong><em>上下文信息或临近状态会影响当前状态</em></strong> 。如序列标注任务： &gt;判别式模型<code>(discriminative model)</code>计算条件概率，而生成式模型<code>(generative model)</code>计算联合概率分布 <span id="more"></span></p>
<ul>
<li>词性标注<code>(Part_Of_Speech Tagging,POS Tagging)</code>：确定句子中的单词的词性，如名称、形容词、副词等</li>
<li>命名实体识别<code>(Named Entity Recognize)</code>：确定句子中单词属于那种实体，如组织、机构、人名等</li>
</ul>
<h2 id="hmm生成模型"><code>HMM</code>生成模型</h2>
<p>给定句子 <span class="math inline">\(S\)</span>，对应的输出词性序列 <span class="math inline">\(T\)</span>，<code>HMM</code>模型的联合概率： <span class="math display">\[
\begin{align}
P(T|S) &amp;= \frac{P(S|T)\cdot P(T)}{P(S)}\\
P(S,T) &amp;= P(S|T)\cdot P(T)\\
       &amp;= \prod_{i=1}^{n}P(w_i|T)\cdot P(T)\\
       &amp;= \prod_{i=1}^{n}P(w_i|t_i)\cdot P(T)\\
       &amp;= \prod_{i=1}^{n}P(w_i|t_i)\cdot P(t_i|t_{i-1})\\       
\end{align}
\]</span> &gt; 首先贝叶斯公式展开，然后利用 <strong><em>以下假设</em></strong> 简化：<br/> - 由词之间相互独立假设，得到 <span class="math inline">\(\prod_{i=1}^{n}P(w_i|T)\)</span> - 由单词概率仅依赖于其自身的标签，得到<strong>发射<code>(emission)</code>概率</strong> <span class="math inline">\(\prod_{i=1}^{n}P(w_i|t_i)\)</span> - 由马尔可夫假设，使用 <code>bi-gram</code> 得到<strong>转移<code>(transition)</code>概率</strong> <span class="math inline">\(P(t_i|t_{i-1})\)</span></p>
<hr />
<p>目标函数：</p>
<p><span class="math display">\[
(\hat{t_1},\hat{t_2}...\hat{t_n})=arg max\prod_{i=1}^{n}P(w_i|t_i)\cdot P(t_i|t_{i-1})
\]</span></p>
<p>综上，<code>HMM</code>假设了两类特征：当前词性与上一词性的关系，当前词与当前词性的关系<br/> HMM的学习过程就是在训练集中学习这两个概率矩阵，大小分别为<code>(t,t),(w,t)</code>，<code>w</code>为单词的个数，<code>t</code>为词性的个数</p>
<h2 id="crf判别模型"><code>CRF</code>判别模型</h2>
<p><code>CRF</code>并没有做出上述的假设，而是使用特征方程<code>feature function</code>来更抽象地表达特征，而不再局限于<code>HMM</code>的两类特征</p>
<h3 id="特征方程">特征方程</h3>
<p>条件随机场中，特征方程 <span class="math inline">\(f_j\)</span> 的输入为： - 句子 <span class="math inline">\(S\)</span> - 一个单词在句子中的位置 <span class="math inline">\(i\)</span> - 当前单词的标签 <span class="math inline">\(l_i\)</span> - 前一个单词的标签 <span class="math inline">\(l_{i-1}\)</span></p>
<p>输出实值 0 或 1 &gt; 上述示例为 <strong>线性链</strong> 条件随机场，特征方程只依赖于当前与 <strong>前一个</strong> 标签，而不是序列中的任意标签；<br/> 如给定之前的单词 “很” ，特征方程判断当前单词 “简单” 的词性</p>
<p>给每个特征方程 <span class="math inline">\(f_j\)</span> 一个权重 <span class="math inline">\(\lambda_j\)</span>，可以计算一个句子 <span class="math inline">\(s\)</span> 对应一组标签 <span class="math inline">\(l\)</span> 的 "分数" <span class="math display">\[score(l|s)=\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_jf_j(s,i,l_i,l_{i-1})\]</span> &gt; 其中 <span class="math inline">\(i\)</span> 表示句子中的位置，<span class="math inline">\(j\)</span> 表示特定的特征方程</p>
<p>“分数”然后转化成概率分布 <span class="math display">\[p(s|l)=\frac{exp\big[\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_jf_j(s,i,l_i,l_{i-1})\big]}{\sum_{l^{’}}exp\big[\sum_{j=1}^{m}\sum_{i=1}^{n}\lambda_jf_j(s,i,l^{’}_i,l^{’}_{i-1})\big]}\]</span> &gt; <span class="math inline">\(l^{’}\)</span> 表示所有可能的序列标签组合</p>
<p>特征方程示例： <span class="math inline">\(f_1(s,i,l_i,l_{i-1})\)</span> 表示 <span class="math inline">\(l_i\)</span> 是否为 副词；若第 <span class="math inline">\(i\)</span> 个单词以 <code>-ly</code> 结尾，该值为 1，否则为 0。即若该特征对应的权重 <span class="math inline">\(\lambda_i\)</span> 较大，说明偏向于将该特征的单词标注为 “副词”</p>
<p><span class="math inline">\(f_2(s,i,l_i,l_{i-1})\)</span> 表示 <span class="math inline">\(l_i\)</span> 是否为 动词；若第 <span class="math inline">\(i=1\)</span> 且句子以<code>?</code>结尾，该值为 1，否则为 0。</p>
<p><span class="math inline">\(f_3(s,i,l_i,l_{i-1})\)</span> 表示 <span class="math inline">\(l_i\)</span> 是否为 形容词；若第 <span class="math inline">\(l_{i-1}\)</span> 为名词，则该值为 1，否则为 0。</p>
<p><span class="math inline">\(f_4(s,i,l_i,l_{i-1})\)</span> 表示 <span class="math inline">\(l_i\)</span> 是否为 介词；若第 <span class="math inline">\(l_{i-1}\)</span> 为介词，则该值为 0。 介词不能跟着介词</p>
<p>因此：要创建条件随机场，需要<strong>定义一系列的特征</strong>，然后给每个特征分配权重，然后遍历整个序列，再将其转换成概率</p>
<h3 id="损失函数">损失函数</h3>
<p>综上，给定训练样本 <span class="math inline">\(D=\big[(x^1,y^1),(x^2,y^2)...(x^m,y^m)\big]\)</span>，其中<span class="math inline">\(m\)</span>表示<span class="math inline">\(m\)</span>个句子<br/> 利用最大似然估计计算参数 <span class="math inline">\(\lambda\)</span>， <span class="math display">\[
\begin{align}
L(\lambda,D) &amp;= log\Big(\prod_{k=1}^{n}P(y^k|x^k,\lambda)\Big)\\
&amp;= \sum_{k=1}^{m}\Big[log\frac{1}{Z(x_k)}+\sum_{j=1}^{n}\sum_{i=1}^{l}\lambda_jf_j(x_i^k,i,y^k_i,y^k_{i-1})\Big]
\end{align}
\]</span> &gt; 其中<span class="math inline">\(k\)</span>表示第<span class="math inline">\(k\)</span>个句子，共<span class="math inline">\(m\)</span>个句子；<span class="math inline">\(i\)</span>表示句子的第<span class="math inline">\(i\)</span>个单词，共<span class="math inline">\(l\)</span>个单词，<span class="math inline">\(j\)</span>表示第<span class="math inline">\(j\)</span>个特征，共<span class="math inline">\(n\)</span>个特征，<span class="math inline">\(\frac{1}{Z(x^k)}\)</span>为正则项</p>
<p>然后利用梯度下降算法即可求解出 <span class="math inline">\(\lambda\)</span> 参数</p>
<h3 id="与hmm的关系">与<code>HMM</code>的关系</h3>
<p>将特征方程定义<span class="math inline">\(f_1(x,i,y_i,y_{i-1})=1\)</span>定义为<span class="math inline">\(p(y_i|y_{i-1})\)</span>，将特征对应的权重<span class="math inline">\(\lambda\)</span>定义为 <span class="math inline">\(\lambda=log p(x_i|y_i)\)</span>，即可从<code>CRF</code>中推导出<code>HMM</code>，<code>HMM</code>为<code>CRF</code>的特例</p>
<h3 id="与逻辑回归类比">与逻辑回归类比：</h3>
<p>逻辑回归用于分类的线性<code>(log-linear)</code>模型，<code>CRFs</code>则用于<strong>序列</strong>分类的线性<code>(log-linear)</code>模型</p>
<h2 id="关键是crf模型中的特征方程">关键是：<code>CRF</code>模型中的特征方程？</h2>
<ul>
<li>特征方程约束了输出标签序列，即确定标签与标签之间的关系，可以作为形状为<code>(tag_size,tag_size)</code>模型参数学习得到，<code>(i,j)</code>表示从标签<code>i&lt;-j</code>的关系</li>
</ul>
<h1 id="bilstmcrf实现命名实体识别"><code>BiLSTM+CRF</code>实现命名实体识别</h1>
<p>参考连接: <a target="_blank" rel="noopener" href="https://createmomo.github.io/2017/11/11/CRF-Layer-on-the-Top-of-BiLSTM-5/">CRF Layer on the Top of BiLSTM</a> <img src="https://tva1.sinaimg.cn/large/008i3skNly1gr7usl3lw7j30bx0anjsm.jpg" /> - 输入单词序列的词表征经过<code>BiLSTM</code>处理，生成每个单词所属实体类别的权重 - 再将权重分布组成的序列，输入到<code>CRF</code>层，获得最终的实体类别分布 - <code>BiLSTM</code>层已经可以获得了单词的实体类别了 - 但<code>CRF</code>层给上一层的输出添加了一些规则限制，即的<code>CRF</code>特征方程</p>
<h2 id="转移矩阵和发射矩阵">转移矩阵和发射矩阵</h2>
<ul>
<li><p><code>BiLSTM</code>层的输出为<strong><code>emission score</code></strong> <span class="math inline">\(E\)</span>，形状为<code>(seq_len,tag_size)</code>，<span class="math inline">\(E_{i,j}\)</span> 表示第 <span class="math inline">\(i\)</span> 个单词属于第 <span class="math inline">\(j\)</span> 个类别的权重，上图中 <span class="math inline">\(E_{w_0,B-person}=1.5\)</span> <br/></p></li>
<li><p>使用 <span class="math inline">\(t_{i,j}\)</span> 表示 <strong><code>transition score</code></strong>，例如 <span class="math inline">\(t_{B-Person,I-Person}=0.9\)</span> ，表示<code>B-Person --&gt; I-Person</code>的转移权重为 0.9 .所有标签之间都有权重分数；</p>
<ul>
<li>额外添加了表征开始和结束的两个标签<code>START+END</code>，
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">
</th>
<th style="text-align:left">
START
</th>
<th style="text-align:left">
B-Person
</th>
<th style="text-align:left">
I-Person
</th>
<th style="text-align:left">
B-Organization
</th>
<th style="text-align:left">
I-Organization
</th>
<th style="text-align:left">
O
</th>
<th style="text-align:left">
END
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">
START
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0.8
</td>
<td style="text-align:left">
0.007
</td>
<td style="text-align:left">
0.7
</td>
<td style="text-align:left">
0.0008
</td>
<td style="text-align:left">
0.9
</td>
<td style="text-align:left">
0.08
</td>
</tr>
<tr>
<td style="text-align:left">
B-Person
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0.6
</td>
<td style="text-align:left">
0.9
</td>
<td style="text-align:left">
0.2
</td>
<td style="text-align:left">
0.0006
</td>
<td style="text-align:left">
0.6
</td>
<td style="text-align:left">
0.009
</td>
</tr>
<tr>
<td style="text-align:left">
I-Person
</td>
<td style="text-align:left">
-1
</td>
<td style="text-align:left">
0.5
</td>
<td style="text-align:left">
0.53
</td>
<td style="text-align:left">
0.55
</td>
<td style="text-align:left">
0.0003
</td>
<td style="text-align:left">
0.85
</td>
<td style="text-align:left">
0.008
</td>
</tr>
<tr>
<td style="text-align:left">
B-Organization
</td>
<td style="text-align:left">
0.9
</td>
<td style="text-align:left">
0.5
</td>
<td style="text-align:left">
0.0003
</td>
<td style="text-align:left">
0.25
</td>
<td style="text-align:left">
0.8
</td>
<td style="text-align:left">
0.77
</td>
<td style="text-align:left">
0.006
</td>
</tr>
<tr>
<td style="text-align:left">
I-Organization
</td>
<td style="text-align:left">
-0.9
</td>
<td style="text-align:left">
0.45
</td>
<td style="text-align:left">
0.007
</td>
<td style="text-align:left">
0.7
</td>
<td style="text-align:left">
0.65
</td>
<td style="text-align:left">
0.76
</td>
<td style="text-align:left">
0.2
</td>
</tr>
<tr>
<td style="text-align:left">
O
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0.65
</td>
<td style="text-align:left">
0.0007
</td>
<td style="text-align:left">
0.7
</td>
<td style="text-align:left">
0.0008
</td>
<td style="text-align:left">
0.9
</td>
<td style="text-align:left">
0.08
</td>
</tr>
<tr>
<td style="text-align:left">
END
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0
</td>
<td style="text-align:left">
0
</td>
</tr>
</tbody>
</table>
</div></li>
<li>如上表所示，转移矩阵学习了一些有用信息：
<ul>
<li>句子应该以 <code>B-Person</code> 和 <code>B-Organization</code>，而不应该以<code>I-Person</code>开始，等等</li>
</ul></li>
<li><strong>转移矩阵为模型的参数</strong>，在训练之前随机初始化，随着训练进行逐渐进行更新；而不用手动设置</li>
</ul></li>
</ul>
<h2 id="损失函数-1">损失函数</h2>
<h3 id="损失函数-2">损失函数</h3>
<p>有5个单词组成的句子，可能的实体类别序列： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1)  START B-Person B-Person B-Person B-Person B-Person END</span><br><span class="line">2)  START B-Person I-Person B-Person B-Person B-Person END</span><br><span class="line">    ......</span><br><span class="line">10) START B-Person I-Person O B-Organization O END</span><br><span class="line">    ......</span><br><span class="line">N)  O O O O O O O</span><br></pre></td></tr></table></figure> 假设每种可能的序列有一个分数 <span class="math inline">\(P_i\)</span>，总共 <span class="math inline">\(N\)</span> 种路径，则所有路径分数和为 <span class="math inline">\(P_{total}=P_1+P_2+...+P_{N}=e^{S_1}+e^{S_2}+...+e^{S_N}\)</span>；<br/> 假设第10种路径为真实标签路径，则分数 <span class="math inline">\(P_{10}\)</span>应该为最大的，则损失函数为<span class="math inline">\(Loss=\frac{P_{RealPath}}{P_1+P_2+...+P_{N}}\)</span><br/> 将其转化为 负log函数，便于梯度下降法计算最小值 <span class="math display">\[
\begin{align}
\text{Loss} &amp;= -log \frac{P_{RealPath}}{P_1+P_2+...+P_{N}}\\
            &amp;= -log \frac{e^{S_{RealPath}}}{e^{S_1}+e^{S_2}+...+e^{S_N}}\\
            &amp;= -\big(s_{RealPath}-log(e^{S_1}+e^{S_2}+...+e^{S_N})\big)\\
\end{align}
\]</span> <span class="math inline">\(S_{RealPath}\)</span>为真实路径的分数，<span class="math inline">\(log(e^{S_1}+e^{S_2}+...+e^{S_N})\)</span>为所有路径分数</p>
<h3 id="真实路径分数">真实路径分数</h3>
<p>真实路径<code>START B-Person I-Person O B-Organization O END</code>，如何计算真实路径的分数 <span class="math inline">\(P_i=e^{S_i}\)</span>，需要先计算 <span class="math inline">\(S_i\)</span>， - 对于上述 5 单词的句子 <span class="math inline">\(w_1,w_2,w_3,w_4,w_5\)</span> - 加上开始和结束标签，<code>START,END</code></p>
<p><span class="math inline">\(S_i=\text{EmissionScore}+\text{TransitionScore}\)</span> - <span class="math inline">\(EmissionScore=x_{0,START}+x_{1,B-Person}+x_{2,I-Person}+x_{3,O}+x_{4,B-Organization}+x_{5,O}+x_{6,END}\)</span> - <span class="math inline">\(x_{i,label}\)</span>，表示第<code>i</code>个单词标签为<code>label</code>的分数，直接从<code>BiLSTM</code>层的输出为<strong><code>emission score</code></strong> <span class="math inline">\(E\)</span>中获得 - <span class="math inline">\(x_{0,START}\)</span> 和 <span class="math inline">\(x_{6,END}\)</span> 直接设置为 0</p>
<ul>
<li><span class="math inline">\(TransitionScore=t_{I-Person-&gt;O} + t_{0-&gt;B-Organization} + t_{B-Organization-&gt;O} + t_{O-&gt;END}\)</span>
<ul>
<li>这些分数来源于<code>CRF</code>层</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_score_sentence</span>(<span class="params">self, feats, tags</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    feats: 发射矩阵，lstm 层的输出，(seq_len,num_tags)</span></span><br><span class="line"><span class="string">    tags: 真实的标签序列</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    示例代码不包含数据批的维度，一次只能处理一个序列</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    score = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    tags = torch.cat([</span><br><span class="line">        torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags</span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">for</span> i, feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(feats):</span><br><span class="line">        score = score + \</span><br><span class="line">            self.transitions[tags[i + <span class="number">1</span>], tags[i]] + feat[tags[i + <span class="number">1</span>]]</span><br><span class="line">    score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="如何计算所有可能路径的分数">如何计算所有可能路径的分数？</h3>
<ul>
<li><p>已知长 <span class="math inline">\(n\)</span> 的序列<code>&#123;w0,w1,w2&#125;</code>，<span class="math inline">\(m\)</span> 个标签<code>&#123;l1,l2&#125;</code>，发射矩阵<span class="math inline">\(x_{ij}\)</span>，转移矩阵<span class="math inline">\(t_{ij}\)</span>， <img src="https://tva1.sinaimg.cn/large/008i3skNly1gr7utnqj9tj318i0tlwjz.jpg" /></p></li>
<li><p>连续两个标签 <span class="math inline">\((w_t,w_{t+1})\)</span> 对应标签组合 <span class="math inline">\((l_a, l_b)\)</span> 的分数表示为：<span class="math inline">\(x_{t,a}+x_{t+1,b}+t_{a,b}\)</span> ，三项分别表示第 <code>t</code> 个单词属于标签 <code>a</code> 的分数、第 <code>t+1</code> 个单词属于标签 <code>b</code> 的分数、以及标签 <code>a-&gt;b</code> 的<strong>转移</strong>分数</p></li>
<li><p>所有的路径即<strong>所有的标签排列组合</strong>：<code>(l1,l1,l1),(l1,l1,l2),(l1,l2,l1)...</code>等 <span class="math inline">\(m^n\)</span> 种，如上图中的8种。最终的分数即为所有路径的<code>log_sum_exp</code>之和</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span>(<span class="params">vec</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    [3,4,5] --&gt; log( e^3+e^4+e^5 ) </span></span><br><span class="line"><span class="string">            --&gt; log( e^5*(e^(3-5)+e^(4-5)+e^(5-5)) ) </span></span><br><span class="line"><span class="string">            --&gt; 5 + log( e^(3-5)+e^(4-5)+e^(5-5) )</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    max_score, idx = torch.<span class="built_in">max</span>(vec, <span class="number">1</span>)</span><br><span class="line">    max_score_broadcast = max_score.view(<span class="number">1</span>, -<span class="number">1</span>).expand(<span class="number">1</span>, vec.size()[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> max_score + torch.log(</span><br><span class="line">        torch.<span class="built_in">sum</span>(torch.exp(vec - max_score_broadcast)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vec = torch.tensor([[<span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>]])</span><br><span class="line">log_sum_exp(vec)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([5.4076])</code></pre>
<p><strong><em>动态规划</em></strong>求解过程：</p>
<ul>
<li>利用向量 <span class="math inline">\(D\)</span> 表示当前单词选择各个标签时的分数；如上图中所示；当前单词选择某个标签的分数，可由上一步的 <span class="math inline">\(D\)</span> 向量推导出：
<ul>
<li>单词 <code>w0</code>，没有前继单词，所以没有转移分数：<span class="math inline">\(D=[log(e^{x_{01}}), log(e^{x_{02}})]\)</span></li>
<li>单词 <code>w1</code>
<ul>
<li>选择标签 <code>l1</code> 时的分数可以表示为：<span class="math inline">\(d_1 = log(e^{d_1+x_{11}+t_{11}}+e^{d_2 + x_{11}+t_{21}})\)</span></li>
<li>选择标签 <code>l2</code> 时的分数可以表示为：<span class="math inline">\(d_2 = log(e^{d_1+x_{12}+t_{12}}+e^{d_2+x_{12}+t_{22}})\)</span></li>
</ul></li>
<li>同理单词<code>w2</code>
<ul>
<li>选择标签 <code>l1</code> 时的分数可以表示为：<span class="math inline">\(d_1 = log(e^{d_1+x_{21}+t_{11}}+e^{d_2+x_{21}+t_{21}})\)</span></li>
<li>选择标签 <code>l2</code> 时的分数可以表示为：<span class="math inline">\(d_2 = log(e^{d_1+x_{22}+t_{12}}+e^{d_2+x_{22}+t_{22}})\)</span><br />
</li>
</ul></li>
<li>从最后一个单词得到的 <span class="math inline">\(D\)</span>，得到所有路径的分数：<span class="math inline">\(log(e^{d_1} + e^{d_2})\)</span> <img src="." /><br />
<img src="https://tva1.sinaimg.cn/large/008i3skNly1gr7uuc62ymj30k609qgmg.jpg" /></li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_forward_alg</span>(<span class="params">self, feats</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    feats: (seq_len, tag_size)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    init_alphas = torch.full((<span class="number">1</span>, self.tag_size), -<span class="number">10000.</span>)  <span class="comment"># 前一个单词选择各个标签时的分数</span></span><br><span class="line">    init_alphas[<span class="number">0</span>][self.tag2idx[START_TAG]] = <span class="number">0.</span>  <span class="comment"># 开始标签</span></span><br><span class="line"></span><br><span class="line">    forward_var = init_alphas</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">        alphas_t = []  <span class="comment"># 动态规划遍历到当前单词时，当前单词选择各个标签时的分数</span></span><br><span class="line">        <span class="keyword">for</span> next_tag <span class="keyword">in</span> <span class="built_in">range</span>(self.tag_size):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 单词选择当前标签的分数</span></span><br><span class="line">            emit_score = feat[next_tag].view(<span class="number">1</span>, -<span class="number">1</span>).expand(<span class="number">1</span>, self.tag_size) </span><br><span class="line"></span><br><span class="line">            <span class="comment"># 上一单词所有标签指向当前标签的转移分数</span></span><br><span class="line">            trans_score = self.transitions[next_tag].view(<span class="number">1</span>, -<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">            <span class="comment"># 再加上一单词选择各个标签的分数，然后求 log-sum-exp，即为当前单词选择当前标签的分数</span></span><br><span class="line">            next_tag_var = forward_var + trans_score + emit_score </span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            alphas_t.append(log_sum_exp(next_tag_var).view(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        forward_var = torch.cat(alphas_t).view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># 更新当前层的值，作为下一层的参数</span></span><br><span class="line">    terminalL_var = forward_var + self.transitions[self.tag2idx[STOP_TAG]]</span><br><span class="line">    alpha = log_sum_exp(terminal_var)</span><br><span class="line">    <span class="keyword">return</span> alpha</span><br></pre></td></tr></table></figure>
<p>模型损失即为： <span class="math display">\[
\begin{align}
\text{Loss} = log(e^{S_1}+e^{S_2}+...+e^{S_N}) -S_{RealPath}
\end{align}
\]</span> <span class="math inline">\(log(e^{S_1}+e^{S_2}+...+e^{S_N})\)</span>为所有路径分数，<span class="math inline">\(S_{RealPath}\)</span>为真实路径的分数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span>(<span class="params">self, sentence, tags</span>):</span></span><br><span class="line">    feats = self._get_lstm_features(sentence)  <span class="comment"># emission matrix</span></span><br><span class="line">    forward_score = self._forward_alg(feats)  <span class="comment"># all possibile pathes score</span></span><br><span class="line">    gold_score = self._score_sentence(feats, tags)  <span class="comment"># real path score</span></span><br><span class="line">    <span class="keyword">return</span> forward_score - gold_score</span><br></pre></td></tr></table></figure>
<h2 id="如何进行预测">如何进行预测</h2>
<p>模型训练好后，如何进行预测？ &gt; 通常模型 <code>forward</code> 方法进行预测，然后预测结果与 <code>target</code> 求交叉熵或<code>MSE</code>就可以计算损失函数，此过程没有增加其它参数；<br/>而 <code>crf</code> 模型预测结果与 <code>target</code> 计算损失函数时还引入了转移矩阵作为参数，所以需要额外定义损失函数</p>
<p>维特比算法求解：<br />
输入经过 <code>lstm</code> 层获得 <strong>发射矩阵</strong>，及模型训练得到的特征方程 <strong>转移矩阵</strong>，然后从所有可能路径中选择最优的路径。 <img src="https://tva1.sinaimg.cn/large/008i3skNly1gr7uv143jkj30k40ee75h.jpg" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_veterbi_decode</span>(<span class="params">self, feats</span>):</span></span><br><span class="line">    <span class="comment"># [i,j] 记录第 i 个单词选择第 j 个标签时的最佳路径中，上一步选择的哪个标签</span></span><br><span class="line">    backpointers = [] </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the viterbi variables in log space</span></span><br><span class="line">    init_vvars = torch.full((<span class="number">1</span>, self.tagset_size), -<span class="number">10000.</span>)</span><br><span class="line">    init_vvars[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存上一步各个标签对应的最佳分数</span></span><br><span class="line">    forward_var = init_vvars</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">        bptrs_t = []  <span class="comment"># holds the backpointers for this step</span></span><br><span class="line">        viterbivars_t = []  <span class="comment"># holds the viterbi variables for this step</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> next_tag <span class="keyword">in</span> <span class="built_in">range</span>(self.tagset_size):</span><br><span class="line">            <span class="comment"># 各个标签对应的分数</span></span><br><span class="line">            next_tag_var = forward_var + self.transitions[next_tag]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 到当前标签的最佳路径中 上一个标签的索引</span></span><br><span class="line">            best_tag_id = argmax(next_tag_var)  </span><br><span class="line">            bptrs_t.append(best_tag_id)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 最佳路径的分数</span></span><br><span class="line">            viterbivars_t.append(next_tag_var[<span class="number">0</span>][best_tag_id].view(<span class="number">1</span>)) </span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 分数还要加上发射分数</span></span><br><span class="line">        forward_var = (torch.cat(viterbivars_t) + feat).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        backpointers.append(bptrs_t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Transition to STOP_TAG</span></span><br><span class="line">    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">    best_tag_id = argmax(terminal_var)</span><br><span class="line">    </span><br><span class="line">    path_score = terminal_var[<span class="number">0</span>][best_tag_id]  <span class="comment"># 最佳路径分数</span></span><br><span class="line"></span><br><span class="line">    best_path = [best_tag_id]  <span class="comment"># 最佳路径</span></span><br><span class="line">    <span class="keyword">for</span> bptrs_t <span class="keyword">in</span> <span class="built_in">reversed</span>(backpointers):</span><br><span class="line">        best_tag_id = bptrs_t[best_tag_id]</span><br><span class="line">        best_path.append(best_tag_id)</span><br><span class="line">    <span class="comment"># Pop off the start tag (we dont want to return that to the caller)</span></span><br><span class="line">    start = best_path.pop()</span><br><span class="line">    <span class="keyword">assert</span> start == self.tag_to_ix[START_TAG]  <span class="comment"># Sanity check</span></span><br><span class="line">    best_path.reverse()</span><br><span class="line">    <span class="keyword">return</span> path_score, best_path</span><br></pre></td></tr></table></figure>
<h1 id="bilstmcrf完整代码"><code>BiLSTM+CRF</code>完整代码</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">device</span><br></pre></td></tr></table></figure>
<pre><code>device(type=&#39;cuda&#39;)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span>(<span class="params">vec</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    vec: (1,n)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    _, idx = torch.<span class="built_in">max</span>(vec, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> idx.item()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span>(<span class="params">seq, to_ix</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    word seq --&gt; idx seq</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    idxs = [to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long) </span><br><span class="line">    <span class="comment"># dtype must be float &lt;- long/int not implemented for torch.exp</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_sum_exp</span>(<span class="params">vec</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    [3,4,5] --&gt; log( e^3+e^4+e^5 ) </span></span><br><span class="line"><span class="string">            --&gt; log( e^5*(e^(3-5)+e^(4-5)+e^(5-5)) ) </span></span><br><span class="line"><span class="string">            --&gt; 5 + log( e^(3-5)+e^(4-5)+e^(5-5) )</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    max_score = vec[<span class="number">0</span>, argmax(vec)]</span><br><span class="line">    max_score_broadcast = max_score.view(<span class="number">1</span>, -<span class="number">1</span>).expand(<span class="number">1</span>, vec.size()[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> max_score + torch.log(</span><br><span class="line">        torch.<span class="built_in">sum</span>(torch.exp(vec - max_score_broadcast)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BiLSTM_CRF</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, tag_to_ix, embedding_dim, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(BiLSTM_CRF, self).__init__()</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.tag_to_ix = tag_to_ix</span><br><span class="line">        self.tagset_size = <span class="built_in">len</span>(tag_to_ix)</span><br><span class="line"></span><br><span class="line">        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim,</span><br><span class="line">                            hidden_dim // <span class="number">2</span>,</span><br><span class="line">                            num_layers=<span class="number">1</span>,</span><br><span class="line">                            bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Maps the output of the LSTM into tag space.</span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Matrix of transition parameters.  Entry i,j is the score of</span></span><br><span class="line">        <span class="comment"># transitioning *to* i *from* j.</span></span><br><span class="line">        self.transitions = nn.Parameter(</span><br><span class="line">            torch.randn(self.tagset_size, self.tagset_size))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># These two statements enforce the constraint that we never transfer</span></span><br><span class="line">        <span class="comment"># to the start tag and we never transfer from the stop tag</span></span><br><span class="line">        self.transitions.data[tag_to_ix[START_TAG], :] = -<span class="number">10000</span></span><br><span class="line">        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -<span class="number">10000</span></span><br><span class="line"></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (torch.randn(<span class="number">2</span>, <span class="number">1</span>, self.hidden_dim // <span class="number">2</span>),</span><br><span class="line">                torch.randn(<span class="number">2</span>, <span class="number">1</span>, self.hidden_dim // <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward_alg</span>(<span class="params">self, feats</span>):</span></span><br><span class="line">        <span class="comment"># Do the forward algorithm to compute the partition function</span></span><br><span class="line">        init_alphas = torch.full((<span class="number">1</span>, self.tagset_size), -<span class="number">10000.</span>)</span><br><span class="line">        <span class="comment"># START_TAG has all of the score.</span></span><br><span class="line">        init_alphas[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Wrap in a variable so that we will get automatic backprop</span></span><br><span class="line">        forward_var = init_alphas</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Iterate through the sentence</span></span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">            alphas_t = []  <span class="comment"># The forward tensors at this timestep</span></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> <span class="built_in">range</span>(self.tagset_size):</span><br><span class="line">                <span class="comment"># broadcast the emission score: it is the same regardless of</span></span><br><span class="line">                <span class="comment"># the previous tag</span></span><br><span class="line">                emit_score = feat[next_tag].view(<span class="number">1</span>, -<span class="number">1</span>).expand(</span><br><span class="line">                    <span class="number">1</span>, self.tagset_size)</span><br><span class="line">                <span class="comment"># the ith entry of trans_score is the score of transitioning to</span></span><br><span class="line">                <span class="comment"># next_tag from i</span></span><br><span class="line">                trans_score = self.transitions[next_tag].view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># The ith entry of next_tag_var is the value for the</span></span><br><span class="line">                <span class="comment"># edge (i -&gt; next_tag) before we do log-sum-exp</span></span><br><span class="line">                next_tag_var = forward_var + trans_score + emit_score</span><br><span class="line">                <span class="comment"># The forward variable for this tag is log-sum-exp of all the</span></span><br><span class="line">                <span class="comment"># scores.</span></span><br><span class="line">                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="number">1</span>))</span><br><span class="line">            forward_var = torch.cat(alphas_t).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        alpha = log_sum_exp(terminal_var)</span><br><span class="line">        <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lstm_features</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line">        embeds = self.word_embeds(sentence).view(<span class="built_in">len</span>(sentence), <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        lstm_out, self.hidden = self.lstm(embeds, self.hidden)</span><br><span class="line">        lstm_out = lstm_out.view(<span class="built_in">len</span>(sentence), self.hidden_dim)</span><br><span class="line">        lstm_feats = self.hidden2tag(lstm_out)</span><br><span class="line">        <span class="keyword">return</span> lstm_feats</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_score_sentence</span>(<span class="params">self, feats, tags</span>):</span></span><br><span class="line">        <span class="comment"># Gives the score of a provided tag sequence</span></span><br><span class="line">        score = torch.zeros(<span class="number">1</span>)</span><br><span class="line">        tags = torch.cat([</span><br><span class="line">            torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">for</span> i, feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(feats):</span><br><span class="line">            score = score + \</span><br><span class="line">                self.transitions[tags[i + <span class="number">1</span>], tags[i]] + feat[tags[i + <span class="number">1</span>]]</span><br><span class="line">        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_viterbi_decode</span>(<span class="params">self, feats</span>):</span></span><br><span class="line">        backpointers = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the viterbi variables in log space</span></span><br><span class="line">        init_vvars = torch.full((<span class="number">1</span>, self.tagset_size), -<span class="number">10000.</span>)</span><br><span class="line">        init_vvars[<span class="number">0</span>][self.tag_to_ix[START_TAG]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward_var at step i holds the viterbi variables for step i-1</span></span><br><span class="line">        forward_var = init_vvars</span><br><span class="line">        <span class="keyword">for</span> feat <span class="keyword">in</span> feats:</span><br><span class="line">            bptrs_t = []  <span class="comment"># holds the backpointers for this step</span></span><br><span class="line">            viterbivars_t = []  <span class="comment"># holds the viterbi variables for this step</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> next_tag <span class="keyword">in</span> <span class="built_in">range</span>(self.tagset_size):</span><br><span class="line">                <span class="comment"># next_tag_var[i] holds the viterbi variable for tag i at the</span></span><br><span class="line">                <span class="comment"># previous step, plus the score of transitioning</span></span><br><span class="line">                <span class="comment"># from tag i to next_tag.</span></span><br><span class="line">                <span class="comment"># We don&#x27;t include the emission scores here because the max</span></span><br><span class="line">                <span class="comment"># does not depend on them (we add them in below)</span></span><br><span class="line">                next_tag_var = forward_var + self.transitions[next_tag]</span><br><span class="line">                best_tag_id = argmax(next_tag_var)</span><br><span class="line">                bptrs_t.append(best_tag_id)</span><br><span class="line">                viterbivars_t.append(next_tag_var[<span class="number">0</span>][best_tag_id].view(<span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Now add in the emission scores, and assign forward_var to the set</span></span><br><span class="line">            <span class="comment"># of viterbi variables we just computed</span></span><br><span class="line">            forward_var = (torch.cat(viterbivars_t) + feat).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">            backpointers.append(bptrs_t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Transition to STOP_TAG</span></span><br><span class="line">        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]</span><br><span class="line">        best_tag_id = argmax(terminal_var)</span><br><span class="line">        path_score = terminal_var[<span class="number">0</span>][best_tag_id]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Follow the back pointers to decode the best path.</span></span><br><span class="line">        best_path = [best_tag_id]</span><br><span class="line">        <span class="keyword">for</span> bptrs_t <span class="keyword">in</span> <span class="built_in">reversed</span>(backpointers):</span><br><span class="line">            best_tag_id = bptrs_t[best_tag_id]</span><br><span class="line">            best_path.append(best_tag_id)</span><br><span class="line">        <span class="comment"># Pop off the start tag (we dont want to return that to the caller)</span></span><br><span class="line">        start = best_path.pop()</span><br><span class="line">        <span class="keyword">assert</span> start == self.tag_to_ix[START_TAG]  <span class="comment"># Sanity check</span></span><br><span class="line">        best_path.reverse()</span><br><span class="line">        <span class="keyword">return</span> path_score, best_path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">neg_log_likelihood</span>(<span class="params">self, sentence, tags</span>):</span></span><br><span class="line">        feats = self._get_lstm_features(sentence)</span><br><span class="line">        forward_score = self._forward_alg(feats)</span><br><span class="line">        gold_score = self._score_sentence(feats, tags)</span><br><span class="line">        <span class="keyword">return</span> forward_score - gold_score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, sentence</span>):</span>  <span class="comment"># dont confuse this with _forward_alg above.</span></span><br><span class="line">        <span class="comment"># Get the emission scores from the BiLSTM</span></span><br><span class="line">        lstm_feats = self._get_lstm_features(sentence)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Find the best path, given the features.</span></span><br><span class="line">        score, tag_seq = self._viterbi_decode(lstm_feats)</span><br><span class="line">        <span class="keyword">return</span> score, tag_seq</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">START_TAG = <span class="string">&quot;&lt;START&gt;&quot;</span></span><br><span class="line">STOP_TAG = <span class="string">&quot;&lt;STOP&gt;&quot;</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">HIDDEN_DIM = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Make up some training data</span></span><br><span class="line">training_data = [</span><br><span class="line">    (<span class="string">&quot;the wall street journal reported today that apple corporation made money&quot;</span></span><br><span class="line">     .split(), <span class="string">&quot;B I I I O O O B I O O&quot;</span>.split()),</span><br><span class="line">    (<span class="string">&quot;georgia tech is a university in georgia&quot;</span>.split(),</span><br><span class="line">     <span class="string">&quot;B I O O O O B&quot;</span>.split())</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = <span class="built_in">len</span>(word_to_ix)</span><br><span class="line"></span><br><span class="line">tag_to_ix = &#123;<span class="string">&quot;B&quot;</span>: <span class="number">0</span>, <span class="string">&quot;I&quot;</span>: <span class="number">1</span>, <span class="string">&quot;O&quot;</span>: <span class="number">2</span>, START_TAG: <span class="number">3</span>, STOP_TAG: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">model = BiLSTM_CRF(<span class="built_in">len</span>(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check predictions before training</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    precheck_tags = torch.tensor([tag_to_ix[t] <span class="keyword">for</span> t <span class="keyword">in</span> training_data[<span class="number">0</span>][<span class="number">1</span>]],</span><br><span class="line">                                 dtype=torch.long)</span><br><span class="line">    <span class="built_in">print</span>(model(precheck_sent))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">    <span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">        <span class="comment"># Step 1. Remember that Pytorch accumulates gradients.</span></span><br><span class="line">        <span class="comment"># We need to clear them out before each instance</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2. Get our inputs ready for the network, that is,</span></span><br><span class="line">        <span class="comment"># turn them into Tensors of word indices.</span></span><br><span class="line">        sentence_in = prepare_sequence(sentence, word_to_ix)</span><br><span class="line">        targets = torch.tensor([tag_to_ix[t] <span class="keyword">for</span> t <span class="keyword">in</span> tags], dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3. Run our forward pass.</span></span><br><span class="line">        loss = model.neg_log_likelihood(sentence_in, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4. Compute the loss, gradients, and update the parameters by</span></span><br><span class="line">        <span class="comment"># calling optimizer.step()</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check predictions after training</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    precheck_sent = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    <span class="built_in">print</span>(model(precheck_sent))</span><br></pre></td></tr></table></figure>
<pre><code>(tensor(2.6907), [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1])
(tensor(20.4906), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])</code></pre>
</div><div class="article-licensing box"><div class="licensing-title"><p>BiLSTM和CRF算法的序列标注原理</p><p><a href="https://hunlp.com/posts/51094.html">https://hunlp.com/posts/51094.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-06-02</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-06-06</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/BiLSTM/">BiLSTM, </a><a class="link-muted" rel="tag" href="/tags/CRF/">CRF </a></div></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zfb.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wx.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/61928.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">词表征与词向量</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/9305.html"><span class="level-item">n-step Bootstrapping</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "9a4495b9307cbf731ecb5a5585d52515",
            repo: "Cartride.github.io",
            owner: "Cartride",
            clientID: "8f4a2426c347380a6ee4",
            clientSecret: "8dc8cd44b071426b35d0bd60634941371170b798",
            admin: ["Cartride"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#crf原理"><span class="level-left"><span class="level-item">1</span><span class="level-item">CRF原理</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#hmm生成模型"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">HMM生成模型</span></span></a></li><li><a class="level is-mobile" href="#crf判别模型"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">CRF判别模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#特征方程"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">特征方程</span></span></a></li><li><a class="level is-mobile" href="#损失函数"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">损失函数</span></span></a></li><li><a class="level is-mobile" href="#与hmm的关系"><span class="level-left"><span class="level-item">1.2.3</span><span class="level-item">与HMM的关系</span></span></a></li><li><a class="level is-mobile" href="#与逻辑回归类比"><span class="level-left"><span class="level-item">1.2.4</span><span class="level-item">与逻辑回归类比：</span></span></a></li></ul></li><li><a class="level is-mobile" href="#关键是crf模型中的特征方程"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">关键是：CRF模型中的特征方程？</span></span></a></li></ul></li><li><a class="level is-mobile" href="#bilstmcrf实现命名实体识别"><span class="level-left"><span class="level-item">2</span><span class="level-item">BiLSTM+CRF实现命名实体识别</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#转移矩阵和发射矩阵"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">转移矩阵和发射矩阵</span></span></a></li><li><a class="level is-mobile" href="#损失函数-1"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">损失函数</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#损失函数-2"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">损失函数</span></span></a></li><li><a class="level is-mobile" href="#真实路径分数"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">真实路径分数</span></span></a></li><li><a class="level is-mobile" href="#如何计算所有可能路径的分数"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">如何计算所有可能路径的分数？</span></span></a></li></ul></li><li><a class="level is-mobile" href="#如何进行预测"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">如何进行预测</span></span></a></li></ul></li><li><a class="level is-mobile" href="#bilstmcrf完整代码"><span class="level-left"><span class="level-item">3</span><span class="level-item">BiLSTM+CRF完整代码</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2022 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>