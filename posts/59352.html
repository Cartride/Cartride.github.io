<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>潜在狄利克雷分配 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="1.狄利克雷分布的概率密度函数为\[p ( \theta | \alpha ) &amp;#x3D; \frac { \Gamma ( \sum _ { i &amp;#x3D; 1 } ^ { k } \alpha _ { i } ) } { \prod _ { i &amp;#x3D; 1 } ^ { k } \Gamma ( \alpha _ { i } ) } \prod _ { i &amp;#x3D; 1 } ^ { k } \theta _ { i }"><meta property="og:type" content="blog"><meta property="og:title" content="潜在狄利克雷分配"><meta property="og:url" content="https://hunlp.com/posts/59352.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="1.狄利克雷分布的概率密度函数为\[p ( \theta | \alpha ) &amp;#x3D; \frac { \Gamma ( \sum _ { i &amp;#x3D; 1 } ^ { k } \alpha _ { i } ) } { \prod _ { i &amp;#x3D; 1 } ^ { k } \Gamma ( \alpha _ { i } ) } \prod _ { i &amp;#x3D; 1 } ^ { k } \theta _ { i }"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hunlp.com/img/og_image.png"><meta property="article:published_time" content="2021-07-28T13:45:40.000Z"><meta property="article:modified_time" content="2021-08-21T04:01:19.098Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="笔记"><meta property="article:tag" content="LDA"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/59352.html"},"headline":"潜在狄利克雷分配","image":["https://hunlp.com/img/og_image.png"],"datePublished":"2021-07-28T13:45:40.000Z","dateModified":"2021-08-21T04:01:19.098Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"1.狄利克雷分布的概率密度函数为\\[p ( \\theta | \\alpha ) &#x3D; \\frac { \\Gamma ( \\sum _ { i &#x3D; 1 } ^ { k } \\alpha _ { i } ) } { \\prod _ { i &#x3D; 1 } ^ { k } \\Gamma ( \\alpha _ { i } ) } \\prod _ { i &#x3D; 1 } ^ { k } \\theta _ { i }"}</script><link rel="canonical" href="https://hunlp.com/posts/59352.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>潜在狄利克雷分配</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-07-28</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-08-21</time></span><span class="level-item"><a class="link-muted" href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></span><span class="level-item">14 分钟读完 (大约2060个字)</span></div></div><div class="content"><p>1.狄利克雷分布的概率密度函数为<span class="math display">\[p ( \theta | \alpha ) = \frac { \Gamma ( \sum _ { i = 1 } ^ { k } \alpha _ { i } ) } { \prod _ { i = 1 } ^ { k } \Gamma ( \alpha _ { i } ) } \prod _ { i = 1 } ^ { k } \theta _ { i } ^ { \alpha _ { i } - 1 }\]</span> 其中$_ { i = 1 } ^ { k } _ { i } = 1 , _ { i }  , = ( _ { 1 } , _ { 2 } , , _ { k } ) , _ { i } &gt; 0 , i = 1,2 , , $狄利克雷分布是多项分布的共轭先验。 <span id="more"></span> 2.潜在狄利克雷分配2.潜在狄利克雷分配（LDA）是文本集合的生成概率模型。模型假设话题由单词的多项分布表示，文本由话题的多项分布表示，单词分布和话题分布的先验分布都是狄利克雷分布。LDA模型属于概率图模型可以由板块表示法表示LDA模型中，每个话题的单词分布、每个文本的话题分布、文本的每个位置的话题是隐变量，文本的每个位置的单词是观测变量。</p>
<p>3.LDA生成文本集合的生成过程如下：</p>
<p>（1）话题的单词分布：随机生成所有话题的单词分布，话题的单词分布是多项分布，其先验分布是狄利克雷分布。</p>
<p>（2）文本的话题分布：随机生成所有文本的话题分布，文本的话题分布是多项分布，其先验分布是狄利克雷分布。</p>
<p>（3）文本的内容：随机生成所有文本的内容。在每个文本的每个位置，按照文本的话题分布随机生成一个话题，再按照该话题的单词分布随机生成一个单词。</p>
<p>4.LDA模型的学习与推理不能直接求解。通常采用的方法是吉布斯抽样算法和变分EM算法，前者是蒙特卡罗法而后者是近似算法。</p>
<p>5.LDA的收缩的吉布斯抽样算法的基本想法如下。目标是对联合概率分布<span class="math inline">\(p ( w , z , \theta , \varphi | \alpha , \beta )\)</span>进行估计。通过积分求和将隐变量<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\varphi\)</span>消掉，得到边缘概率分布<span class="math inline">\(p ( w , z | \alpha , \beta )\)</span>；对概率分布<span class="math inline">\(p ( w | z , \alpha , \beta )\)</span>进行吉布斯抽样，得到分布<span class="math inline">\(p ( w | z , \alpha , \beta )\)</span>的随机样本；再利用样本对变量<span class="math inline">\(z\)</span>，<span class="math inline">\(\theta\)</span>和<span class="math inline">\(\varphi\)</span>的概率进行估计，最终得到LDA模型<span class="math inline">\(p ( w , z , \theta , \varphi | \alpha , \beta )\)</span>的参数估计。具体算法如下对给定的文本单词序列，每个位置上随机指派一个话题，整体构成话题系列。然后循环执行以下操作。对整个文本序列进行扫描，在每一个位置上计算在该位置上的话题的满条件概率分布，然后进行随机抽样，得到该位置的新的话题，指派给这个位置。</p>
<p>6.变分推理的基本想法如下。假设模型是联合概率分布<span class="math inline">\(p ( x , z )\)</span>，其中<span class="math inline">\(x\)</span>是观测变量（数据），<span class="math inline">\(z\)</span>是隐变量。目标是学习模型的后验概率分布<span class="math inline">\(p ( z | x )\)</span>。考虑用变分分布<span class="math inline">\(q ( z )\)</span>近似条件概率分布<span class="math inline">\(p ( z | x )\)</span>，用KL散度计算两者的相似性找到与<span class="math inline">\(p ( z | x )\)</span>在KL散度意义下最近的<span class="math inline">\(q ^ { * } ( z )\)</span>，用这个分布近似<span class="math inline">\(p ( z | x )\)</span>。假设<span class="math inline">\(q ( z )\)</span>中的<span class="math inline">\(z\)</span>的所有分量都是互相独立的。利用Jensen不等式，得到KL散度的最小化可以通过证据下界的最大化实现。因此，变分推理变成求解以下证据下界最大化问题： <span class="math display">\[L ( q , \theta ) = E _ { q } [ \operatorname { log } p ( x , z | \theta ) ] - E _ { q } [ \operatorname { log } q ( z ) ]\]</span></p>
<p>7.LDA的变分EM算法如下。针对LDA模型定义变分分布，应用变分EM算法。目标是对证据下界<span class="math inline">\(L ( \gamma , \eta , \alpha , \varphi )\)</span>进行最大化，其中<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\varphi\)</span>是模型参数，<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\eta\)</span>是变分参数。交替迭代E步和M步，直到收敛。</p>
<ul>
<li>（1）E步：固定模型参数<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\varphi\)</span>，通过关于变分参数<span class="math inline">\(\gamma\)</span>，<span class="math inline">\(\eta\)</span>的证据下界的最大化，估计变分参数<span class="math inline">\(\gamma\)</span>，<span class="math inline">\(\eta\)</span>。</li>
<li>（2）M步：固定变分参数<span class="math inline">\(\gamma\)</span>，<span class="math inline">\(\eta\)</span>，通过关于模型参数<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\varphi\)</span>的证据下界的最大化，估计模型参数<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\varphi\)</span>。</li>
</ul>
<hr />
<p>潜在狄利克雷分配（latent Dirichlet allocation,LDA），作为基于贝叶斯学习的话题模型，是潜在语义分析、概率潜在语义分析的扩展，于2002年由Blei等提出dA在文本数据挖掘、图像处理、生物信息处理等领域被广泛使用。</p>
<p>LDA模型是文本集合的生成概率模型假设每个文本由话题的一个多项分布表示，每个话题由单词的一个多项分布表示，特别假设文本的话题分布的先验分布是狄利克雷分布，话题的单词分布的先验分布也是狄利克雷分布。先验分布的导入使LDA能够更好地应对话题模型学习中的过拟合现象。</p>
<p>LDA的文本集合的生成过程如下：首先随机生成一个文本的话题分布，之后在该文本的每个位置，依据该文本的话题分布随机生成一个话题，然后在该位置依据该话题的单词分布随机生成一个单词，直至文本的最后一个位置，生成整个文本。重复以上过程生成所有文本。</p>
<p>LDA模型是含有隐变量的概率图模型。模型中，每个话题的单词分布，每个文本的话题分布，文本的每个位置的话题是隐变量；文本的每个位置的单词是观测变量。LDA模型的学习与推理无法直接求解通常使用吉布斯抽样（ Gibbs sampling）和变分EM算法（variational EM algorithm），前者是蒙特卡罗法，而后者是近似算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim <span class="keyword">import</span> corpora, models, similarities</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"><span class="keyword">import</span> warnings</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;data/LDA_test.txt&#x27;</span>)</span><br><span class="line">stop_list = <span class="built_in">set</span>(<span class="string">&#x27;for a of the and to in&#x27;</span>.split())</span><br><span class="line"><span class="comment"># texts = [line.strip().split() for line in f]</span></span><br><span class="line"><span class="comment"># print &#x27;Before&#x27;</span></span><br><span class="line"><span class="comment"># pprint(texts)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;After&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>After</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">texts = [[</span><br><span class="line">    word <span class="keyword">for</span> word <span class="keyword">in</span> line.strip().lower().split() <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_list</span><br><span class="line">] <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Text = &#x27;</span>)</span><br><span class="line">pprint(texts)</span><br></pre></td></tr></table></figure>
<pre><code>Text = 
[[&#39;human&#39;, &#39;machine&#39;, &#39;interface&#39;, &#39;lab&#39;, &#39;abc&#39;, &#39;computer&#39;, &#39;applications&#39;],
 [&#39;survey&#39;, &#39;user&#39;, &#39;opinion&#39;, &#39;computer&#39;, &#39;system&#39;, &#39;response&#39;, &#39;time&#39;],
 [&#39;eps&#39;, &#39;user&#39;, &#39;interface&#39;, &#39;management&#39;, &#39;system&#39;],
 [&#39;system&#39;, &#39;human&#39;, &#39;system&#39;, &#39;engineering&#39;, &#39;testing&#39;, &#39;eps&#39;],
 [&#39;relation&#39;, &#39;user&#39;, &#39;perceived&#39;, &#39;response&#39;, &#39;time&#39;, &#39;error&#39;, &#39;measurement&#39;],
 [&#39;generation&#39;, &#39;random&#39;, &#39;binary&#39;, &#39;unordered&#39;, &#39;trees&#39;],
 [&#39;intersection&#39;, &#39;graph&#39;, &#39;paths&#39;, &#39;trees&#39;],
 [&#39;graph&#39;, &#39;minors&#39;, &#39;iv&#39;, &#39;widths&#39;, &#39;trees&#39;, &#39;well&#39;, &#39;quasi&#39;, &#39;ordering&#39;],
 [&#39;graph&#39;, &#39;minors&#39;, &#39;survey&#39;]]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dictionary = corpora.Dictionary(texts)</span><br><span class="line"><span class="built_in">print</span>(dictionary)</span><br></pre></td></tr></table></figure>
<pre><code>Dictionary(35 unique tokens: [&#39;abc&#39;, &#39;applications&#39;, &#39;computer&#39;, &#39;human&#39;, &#39;interface&#39;]...)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="built_in">len</span>(dictionary)</span><br><span class="line">corpus = [dictionary.doc2bow(text) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line">corpus_tfidf = models.TfidfModel(corpus)[corpus]</span><br><span class="line">corpus_tfidf = corpus</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;TF-IDF:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> corpus_tfidf:</span><br><span class="line">    <span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure>
<pre><code>TF-IDF:
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]
[(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]
[(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)]
[(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)]
[(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)]
[(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)]
[(24, 1), (26, 1), (27, 1), (28, 1)]
[(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]
[(9, 1), (26, 1), (30, 1)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nLSI Model:&#x27;</span>)</span><br><span class="line">lsi = models.LsiModel(corpus_tfidf, num_topics=<span class="number">2</span>, id2word=dictionary)</span><br><span class="line">topic_result = [a <span class="keyword">for</span> a <span class="keyword">in</span> lsi[corpus_tfidf]]</span><br><span class="line">pprint(topic_result)</span><br></pre></td></tr></table></figure>
<pre><code>LSI Model:
[[(0, 0.9334981916792652), (1, 0.10508952614086528)],
 [(0, 2.031992374687025), (1, -0.047145314121742235)],
 [(0, 1.5351342836582078), (1, 0.13488784052204628)],
 [(0, 1.9540077194594532), (1, 0.21780498576075008)],
 [(0, 1.2902472956004092), (1, -0.0022521437499372337)],
 [(0, 0.022783081905505403), (1, -0.7778052604326754)],
 [(0, 0.05671567576920905), (1, -1.1827703446704851)],
 [(0, 0.12360003320647955), (1, -2.6343068608236835)],
 [(0, 0.23560627195889133), (1, -0.9407936203668315)]]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;LSI Topics:&#x27;</span>)</span><br><span class="line">pprint(lsi.print_topics(num_topics=<span class="number">2</span>, num_words=<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<pre><code>LSI Topics:
[(0,
  &#39;0.579*&quot;system&quot; + 0.376*&quot;user&quot; + 0.270*&quot;eps&quot; + 0.257*&quot;time&quot; + &#39;
  &#39;0.257*&quot;response&quot;&#39;),
 (1,
  &#39;-0.480*&quot;graph&quot; + -0.464*&quot;trees&quot; + -0.361*&quot;minors&quot; + -0.266*&quot;widths&quot; + &#39;
  &#39;-0.266*&quot;ordering&quot;&#39;)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">similarity = similarities.MatrixSimilarity(lsi[corpus_tfidf])   <span class="comment"># similarities.Similarity()</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Similarity:&#x27;</span>)</span><br><span class="line">pprint(<span class="built_in">list</span>(similarity))</span><br></pre></td></tr></table></figure>
<pre><code>Similarity:
[array([ 1.        ,  0.9908607 ,  0.9997008 ,  0.9999994 ,  0.9935261 ,
       -0.08272626, -0.06414512, -0.06517283,  0.13288835], dtype=float32),
 array([0.9908607 , 0.99999994, 0.9938636 , 0.99100804, 0.99976987,
       0.0524564 , 0.07105229, 0.070025  , 0.2653665 ], dtype=float32),
 array([ 0.9997008 ,  0.9938636 ,  0.99999994,  0.999727  ,  0.99600756,
       -0.05832579, -0.03971674, -0.04074576,  0.15709123], dtype=float32),
 array([ 0.9999994 ,  0.99100804,  0.999727  ,  1.        ,  0.9936501 ,
       -0.08163348, -0.06305084, -0.06407862,  0.13397504], dtype=float32),
 array([0.9935261 , 0.99976987, 0.99600756, 0.9936501 , 0.99999994,
       0.03102366, 0.04963995, 0.04861134, 0.24462426], dtype=float32),
 array([-0.08272626,  0.0524564 , -0.05832579, -0.08163348,  0.03102366,
        0.99999994,  0.99982643,  0.9998451 ,  0.97674036], dtype=float32),
 array([-0.06414512,  0.07105229, -0.03971674, -0.06305084,  0.04963995,
        0.99982643,  1.        ,  0.9999995 ,  0.9805657 ], dtype=float32),
 array([-0.06517283,  0.070025  , -0.04074576, -0.06407862,  0.04861134,
        0.9998451 ,  0.9999995 ,  1.        ,  0.9803632 ], dtype=float32),
 array([0.13288835, 0.2653665 , 0.15709123, 0.13397504, 0.24462426,
       0.97674036, 0.9805657 , 0.9803632 , 1.        ], dtype=float32)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nLDA Model:&#x27;</span>)</span><br><span class="line">num_topics = <span class="number">2</span></span><br><span class="line">lda = models.LdaModel(</span><br><span class="line">    corpus_tfidf,</span><br><span class="line">    num_topics=num_topics,</span><br><span class="line">    id2word=dictionary,</span><br><span class="line">    alpha=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    eta=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    minimum_probability=<span class="number">0.001</span>,</span><br><span class="line">    passes=<span class="number">10</span>)</span><br><span class="line">doc_topic = [doc_t <span class="keyword">for</span> doc_t <span class="keyword">in</span> lda[corpus_tfidf]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Document-Topic:\n&#x27;</span>)</span><br><span class="line">pprint(doc_topic)</span><br></pre></td></tr></table></figure>
<pre><code>LDA Model:
Document-Topic:

[[(0, 0.02668742), (1, 0.97331256)],
 [(0, 0.9784582), (1, 0.021541778)],
 [(0, 0.9704323), (1, 0.02956772)],
 [(0, 0.97509205), (1, 0.024907947)],
 [(0, 0.9785106), (1, 0.021489413)],
 [(0, 0.9703556), (1, 0.029644381)],
 [(0, 0.04481229), (1, 0.9551877)],
 [(0, 0.023327617), (1, 0.97667235)],
 [(0, 0.058409944), (1, 0.9415901)]]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> doc_topic <span class="keyword">in</span> lda.get_document_topics(corpus_tfidf):</span><br><span class="line">    <span class="built_in">print</span>(doc_topic)</span><br></pre></td></tr></table></figure>
<pre><code>[(0, 0.026687337), (1, 0.9733126)]
[(0, 0.9784589), (1, 0.021541081)]
[(0, 0.97043234), (1, 0.029567692)]
[(0, 0.9750935), (1, 0.024906479)]
[(0, 0.9785101), (1, 0.021489937)]
[(0, 0.9703557), (1, 0.029644353)]
[(0, 0.044812497), (1, 0.9551875)]
[(0, 0.02332762), (1, 0.97667235)]
[(0, 0.058404233), (1, 0.9415958)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> topic_id <span class="keyword">in</span> <span class="built_in">range</span>(num_topics):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Topic&#x27;</span>, topic_id)</span><br><span class="line">    <span class="comment"># pprint(lda.get_topic_terms(topicid=topic_id))</span></span><br><span class="line">    pprint(lda.show_topic(topic_id))</span><br><span class="line">similarity = similarities.MatrixSimilarity(lda[corpus_tfidf])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Similarity:&#x27;</span>)</span><br><span class="line">pprint(<span class="built_in">list</span>(similarity))</span><br><span class="line"></span><br><span class="line">hda = models.HdpModel(corpus_tfidf, id2word=dictionary)</span><br><span class="line">topic_result = [a <span class="keyword">for</span> a <span class="keyword">in</span> hda[corpus_tfidf]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n\nUSE WITH CARE--\nHDA Model:&#x27;</span>)</span><br><span class="line">pprint(topic_result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;HDA Topics:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(hda.print_topics(num_topics=<span class="number">2</span>, num_words=<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Topic 0
[(&#39;system&#39;, 0.094599016),
 (&#39;user&#39;, 0.073440075),
 (&#39;eps&#39;, 0.052545987),
 (&#39;response&#39;, 0.052496374),
 (&#39;time&#39;, 0.052453455),
 (&#39;survey&#39;, 0.031701956),
 (&#39;trees&#39;, 0.03162545),
 (&#39;human&#39;, 0.03161709),
 (&#39;computer&#39;, 0.031570844),
 (&#39;testing&#39;, 0.031543963)]
Topic 1
[(&#39;graph&#39;, 0.0883405),
 (&#39;trees&#39;, 0.06323685),
 (&#39;minors&#39;, 0.06296622),
 (&#39;interface&#39;, 0.03810195),
 (&#39;computer&#39;, 0.03798469),
 (&#39;human&#39;, 0.03792907),
 (&#39;applications&#39;, 0.03792245),
 (&#39;abc&#39;, 0.037920628),
 (&#39;machine&#39;, 0.037917122),
 (&#39;lab&#39;, 0.037909806)]
Similarity:
[array([1.        , 0.04940351, 0.05783966, 0.05292428, 0.04934979,
       0.05791992, 0.99981046, 0.99999374, 0.99940336], dtype=float32),
 array([0.04940351, 1.        , 0.99996436, 0.9999938 , 1.        ,
       0.99996364, 0.06883725, 0.04587576, 0.08387101], dtype=float32),
 array([0.05783966, 0.99996436, 1.0000001 , 0.99998796, 0.99996394,
       1.        , 0.07726298, 0.05431345, 0.09228647], dtype=float32),
 array([0.05292428, 0.9999938 , 0.99998796, 1.        , 0.9999936 ,
       0.9999875 , 0.07235384, 0.04939714, 0.08738345], dtype=float32),
 array([0.04934979, 1.        , 0.99996394, 0.9999936 , 1.        ,
       0.99996316, 0.06878359, 0.04582203, 0.08381741], dtype=float32),
 array([0.05791992, 0.99996364, 1.        , 0.9999875 , 0.99996316,
       0.99999994, 0.07734313, 0.05439373, 0.09236652], dtype=float32),
 array([0.99981046, 0.06883725, 0.07726298, 0.07235384, 0.06878359,
       0.07734313, 0.99999994, 0.9997355 , 0.9998863 ], dtype=float32),
 array([0.99999374, 0.04587576, 0.05431345, 0.04939714, 0.04582203,
       0.05439373, 0.9997355 , 0.99999994, 0.9992751 ], dtype=float32),
 array([0.99940336, 0.08387101, 0.09228647, 0.08738345, 0.08381741,
       0.09236652, 0.9998863 , 0.9992751 , 1.        ], dtype=float32)]


USE WITH CARE--
HDA Model:
[[(0, 0.18174982193320122),
  (1, 0.02455260642448283),
  (2, 0.741340573910992),
  (3, 0.013544078061059922),
  (4, 0.010094377639823477)],
 [(0, 0.39419292675663636),
  (1, 0.2921969355337328),
  (2, 0.26125786014858376),
  (3, 0.013539627392486701),
  (4, 0.01009410883245766)],
 [(0, 0.5182077872999125),
  (1, 0.3880947736463974),
  (2, 0.023895609845034207),
  (3, 0.01805202212531745),
  (4, 0.013458421673222807)],
 [(0, 0.03621384798236036),
  (1, 0.5504573172680752),
  (2, 0.020442846194997377),
  (3, 0.348529241707211),
  (4, 0.011535562414627153)],
 [(0, 0.9049762450848856),
  (1, 0.024748801100993395),
  (2, 0.017919024335434904),
  (3, 0.013543460312481508),
  (4, 0.010093932388992328)],
 [(0, 0.04681359723231631),
  (1, 0.03233799461088905),
  (2, 0.8510430252219996),
  (3, 0.01805587061936895),
  (4, 0.013458128836093802)],
 [(0, 0.42478083784052273),
  (1, 0.03858547281122597),
  (2, 0.4528531768644199),
  (3, 0.021680841796584305),
  (4, 0.016150009359845837),
  (5, 0.011953757612369628)],
 [(0, 0.2466808290730598),
  (1, 0.6908552821243853),
  (2, 0.015924569811569197),
  (3, 0.012039668311419834)],
 [(0, 0.500366457263008),
  (1, 0.048221177670061226),
  (2, 0.34671234963274666),
  (3, 0.02707530995137571),
  (4, 0.02018763747377598),
  (5, 0.014942188361070167),
  (6, 0.010992923111633942)]]
HDA Topics:
[(0, &#39;0.122*graph + 0.115*minors + 0.098*management + 0.075*random + 0.063*error&#39;), (1, &#39;0.114*human + 0.106*system + 0.086*user + 0.064*iv + 0.063*measurement&#39;)]</code></pre>
</div><div class="article-licensing box"><div class="licensing-title"><p>潜在狄利克雷分配</p><p><a href="https://hunlp.com/posts/59352.html">https://hunlp.com/posts/59352.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-07-28</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-08-21</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记, </a><a class="link-muted" rel="tag" href="/tags/LDA/">LDA </a></div></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zfb.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wx.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/49096.html"><span class="level-item">马尔可夫链蒙特卡罗法</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "eb54433860bfea9febcc3cd4f464dab4",
            repo: "Cartride.github.io",
            owner: "Cartride",
            clientID: "8f4a2426c347380a6ee4",
            clientSecret: "8dc8cd44b071426b35d0bd60634941371170b798",
            admin: ["Cartride"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2021 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>