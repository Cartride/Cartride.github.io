<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>k-means、k-means++以及k-means算法分析 - MCFON</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="MCFON"><meta name="msapplication-TileImage" content="/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="MCFON"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="本文会介绍一般的k-means算法、k-means++算法以及基于k-means++算法的k-means||算法。在spark ml，已经实现了k-means算法以及k-means||算法。 本文首先会介绍这三个算法的原理，然后在了解原理的基础上分析spark中的实现代码。"><meta property="og:type" content="blog"><meta property="og:title" content="k-means、k-means++以及k-means算法分析"><meta property="og:url" content="https://hunlp.com/posts/3118261352.html"><meta property="og:site_name" content="MCFON"><meta property="og:description" content="本文会介绍一般的k-means算法、k-means++算法以及基于k-means++算法的k-means||算法。在spark ml，已经实现了k-means算法以及k-means||算法。 本文首先会介绍这三个算法的原理，然后在了解原理的基础上分析spark中的实现代码。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hunlp.com/gallery/9.jpg"><meta property="article:published_time" content="2019-07-30T08:00:26.000Z"><meta property="article:modified_time" content="2019-07-31T02:37:12.930Z"><meta property="article:author" content="ฅ´ω`ฅ"><meta property="article:tag" content="spark"><meta property="article:tag" content="聚类"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/9.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hunlp.com/posts/3118261352.html"},"headline":"k-means、k-means++以及k-means算法分析","image":["https://hunlp.com/gallery/9.jpg"],"datePublished":"2019-07-30T08:00:26.000Z","dateModified":"2019-07-31T02:37:12.930Z","author":{"@type":"Person","name":"ฅ´ω`ฅ"},"publisher":{"@type":"Organization","name":"MCFON","logo":{"@type":"ImageObject","url":"https://hunlp.com/img/logo.png"}},"description":"本文会介绍一般的k-means算法、k-means++算法以及基于k-means++算法的k-means||算法。在spark ml，已经实现了k-means算法以及k-means||算法。 本文首先会介绍这三个算法的原理，然后在了解原理的基础上分析spark中的实现代码。"}</script><link rel="canonical" href="https://hunlp.com/posts/3118261352.html"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?b99420d7a06d2b3361a8efeaf6e20764";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-131608076-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-131608076-1');</script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>k-means、k-means++以及k-means算法分析</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2019-07-30</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2019-07-31</time></span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">25 分钟读完 (大约3711个字)</span></div></div><div class="content"><p>  本文会介绍一般的<code>k-means</code>算法、<code>k-means++</code>算法以及基于<code>k-means++</code>算法的<code>k-means||</code>算法。在<code>spark ml</code>，已经实现了<code>k-means</code>算法以及<code>k-means||</code>算法。 本文首先会介绍这三个算法的原理，然后在了解原理的基础上分析<code>spark</code>中的实现代码。 <span id="more"></span> ## <code>k-means</code>算法原理分析</p>
<p>  <code>k-means</code>算法是聚类分析中使用最广泛的算法之一。它把<code>n</code>个对象根据它们的属性分为<code>k</code>个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。</p>
<p>  <code>k-means</code>算法的基本过程如下所示：</p>
<ul>
<li><p>（1）任意选择<code>k</code>个初始中心<span class="math inline">\(c_{1},c_{2},...,c_{k}\)</span> 。</p></li>
<li><p>（2）计算<code>X</code>中的每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；</p></li>
<li><p>（3）重新计算每个中心对象<span class="math inline">\(C_{i}\)</span>的值</p>
<div data-align="center">
<img src="../images/imgs10/math.1.1.png" width = "220" height = "40" alt="1.1" align="center" />
</div>
<p><br /></p></li>
<li><p>（4）计算标准测度函数，当满足一定条件，如函数收敛时，则算法终止；如果条件不满足则重复步骤（2），（3）。</p></li>
</ul>
<h3 id="k-means算法的缺点"><code>k-means</code>算法的缺点</h3>
<p>  <code>k-means</code>算法虽然简单快速，但是存在下面的缺点：</p>
<ul>
<li><p>聚类中心的个数<code>K</code>需要事先给定，但在实际中<code>K</code>值的选定是非常困难的，很多时候我们并不知道给定的数据集应该分成多少个类别才最合适。</p></li>
<li><p><code>k-means</code>算法需要随机地确定初始聚类中心，不同的初始聚类中心可能导致完全不同的聚类结果。</p></li>
</ul>
<p>  第一个缺陷我们很难在<code>k-means</code>算法以及其改进算法中解决，但是我们可以通过<code>k-means++</code>算法来解决第二个缺陷。</p>
<h2 id="k-means算法原理分析"><code>k-means++</code>算法原理分析</h2>
<p>  <code>k-means++</code>算法选择初始聚类中心的基本原则是：初始的聚类中心之间的相互距离要尽可能的远。它选择初始聚类中心的步骤是：</p>
<ul>
<li><p>（1）从输入的数据点集合中随机选择一个点作为第一个聚类中心<span class="math inline">\(c_{1}\)</span> ；</p></li>
<li><p>（2）对于数据集中的每一个点<code>x</code>，计算它与最近聚类中心(指已选择的聚类中心)的距离<code>D(x)</code>，并根据概率选择新的聚类中心<span class="math inline">\(c_{i}\)</span> 。</p></li>
<li><p>（3）重复过程（2）直到找到k个聚类中心。</p></li>
</ul>
<p>  第(2)步中，依次计算每个数据点与最近的种子点（聚类中心）的距离，依次得到<code>D(1)、D(2)、...、D(n)</code>构成的集合<code>D</code>，其中<code>n</code>表示数据集的大小。在<code>D</code>中，为了避免噪声，不能直接选取值最大的元素，应该选择值较大的元素，然后将其对应的数据点作为种子点。 如何选择值较大的元素呢，下面是<code>spark</code>中实现的思路。</p>
<ul>
<li><p>求所有的距离和<code>Sum(D(x))</code></p></li>
<li><p>取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先用<code>Sum(D(x))</code>乘以随机值<code>Random</code>得到值<code>r</code>，然后用<code>currSum += D(x)</code>，直到其<code>currSum &gt; r</code>，此时的点就是下一个“种子点”。</p></li>
</ul>
<p>  为什么用这样的方式呢？我们换一种比较好理解的方式来说明。把集合<code>D</code>中的每个元素<code>D(x)</code>想象为一根线<code>L(x)</code>，线的长度就是元素的值。将这些线依次按照<code>L(1)、L(2)、...、L(n)</code>的顺序连接起来，组成长线<code>L</code>。<code>L(1)、L(2)、…、L(n)</code>称为<code>L</code>的子线。 根据概率的相关知识，如果我们在<code>L</code>上随机选择一个点，那么这个点所在的子线很有可能是比较长的子线，而这个子线对应的数据点就可以作为种子点。</p>
<h3 id="k-means算法的缺点-1"><code>k-means++</code>算法的缺点</h3>
<p>  虽然<code>k-means++</code>算法可以确定地初始化聚类中心，但是从可扩展性来看，它存在一个缺点，那就是它内在的有序性特性：下一个中心点的选择依赖于已经选择的中心点。 针对这种缺陷，<code>k-means||</code>算法提供了解决方法。</p>
<h2 id="k-means算法原理分析-1"><code>k-means||</code>算法原理分析</h2>
<p>  <code>k-means||</code>算法是在<code>k-means++</code>算法的基础上做的改进，和<code>k-means++</code>算法不同的是，它采用了一个采样因子<code>l</code>，并且<code>l=A(k)</code>，在<code>spark</code>的实现中<code>l=2k</code>，。这个算法首先如<code>k-means++</code>算法一样，随机选择一个初始中心， 然后计算选定初始中心确定之后的初始花费<span class="math inline">\(\psi\)</span>(指与最近中心点的距离)。之后处理<span class="math inline">\(log(\psi )\)</span>次迭代，在每次迭代中，给定当前中心集，通过概率<span class="math inline">\(ld^{2}(x,C)/\phi_{X}(C)\)</span>来 抽样<code>x</code>，将选定的<code>x</code>添加到初始化中心集中，并且更新<span class="math inline">\(\phi_{X}(C)\)</span>。该算法的步骤如下图所示：</p>
<div data-align="center">
<img src="../images/imgs10/1.1.png" width = "450" height = "200" alt="1.1" align="center" />
</div>
<p><br /></p>
<p>  第1步随机初始化一个中心点，第2-6步计算出满足概率条件的多个候选中心点<code>C</code>，候选中心点的个数可能大于<code>k</code>个，所以通过第7-8步来处理。第7步给<code>C</code>中所有点赋予一个权重值<span class="math inline">\(w_{x}\)</span> ，这个权重值表示距离<code>x</code>点最近的点的个数。 第8步使用本地<code>k-means++</code>算法聚类出这些候选点的<code>k</code>个聚类中心。在<code>spark</code>的源码中，迭代次数是人为设定的，默认是5。</p>
<p>  该算法与<code>k-means++</code>算法不同的地方是它每次迭代都会抽样出多个中心点而不是一个中心点，且每次迭代不互相依赖，这样我们可以并行的处理这个迭代过程。由于该过程产生出来的中心点的数量远远小于输入数据点的数量， 所以第8步可以通过本地<code>k-means++</code>算法很快的找出<code>k</code>个初始化中心点。何为本地<code>k-means++</code>算法？就是运行在单个机器节点上的<code>k-means++</code>。</p>
<p>  下面我们详细分析上述三个算法的代码实现。</p>
<h2 id="源代码分析">源代码分析</h2>
<p>  在<code>spark</code>中，<code>org.apache.spark.mllib.clustering.KMeans</code>文件实现了<code>k-means</code>算法以及<code>k-means||</code>算法，<code>org.apache.spark.mllib.clustering.LocalKMeans</code>文件实现了<code>k-means++</code>算法。 在分步骤分析<code>spark</code>中的源码之前我们先来了解<code>KMeans</code>类中参数的含义。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMeans</span> <span class="title">private</span> (<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    private var k: <span class="type">Int</span>,//聚类个数</span></span></span><br><span class="line"><span class="params"><span class="class">    private var maxIterations: <span class="type">Int</span>,//迭代次数</span></span></span><br><span class="line"><span class="params"><span class="class">    private var runs: <span class="type">Int</span>,//运行kmeans算法的次数</span></span></span><br><span class="line"><span class="params"><span class="class">    private var initializationMode: <span class="type">String</span>,//初始化模式</span></span></span><br><span class="line"><span class="params"><span class="class">    private var initializationSteps: <span class="type">Int</span>,//初始化步数</span></span></span><br><span class="line"><span class="params"><span class="class">    private var epsilon: <span class="type">Double</span>,//判断kmeans算法是否收敛的阈值</span></span></span><br><span class="line"><span class="params"><span class="class">    private var seed: <span class="type">Long</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>  在上面的定义中，<code>k</code>表示聚类的个数，<code>maxIterations</code>表示最大的迭代次数，<code>runs</code>表示运行<code>KMeans</code>算法的次数，在<code>spark 2.0。0</code>开始，该参数已经不起作用了。为了更清楚的理解算法我们可以认为它为1。 <code>initializationMode</code>表示初始化模式，有两种选择：随机初始化和通过<code>k-means||</code>初始化，默认是通过<code>k-means||</code>初始化。<code>initializationSteps</code>表示通过<code>k-means||</code>初始化时的迭代步骤，默认是5，这是<code>spark</code>实现与第三章的算法步骤不一样的地方，这里迭代次数人为指定， 而第三章的算法是根据距离得到的迭代次数，为<code>log(phi)</code>。<code>epsilon</code>是判断算法是否已经收敛的阈值。</p>
<p>  下面将分步骤分析<code>k-means</code>算法、<code>k-means||</code>算法的实现过程。</p>
<h3 id="处理数据转换为vectorwithnorm集">处理数据，转换为<code>VectorWithNorm</code>集。</h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//求向量的二范式，返回double值</span></span><br><span class="line"><span class="keyword">val</span> norms = data.map(<span class="type">Vectors</span>.norm(_, <span class="number">2.0</span>))</span><br><span class="line">norms.persist()</span><br><span class="line"><span class="keyword">val</span> zippedData = data.zip(norms).map &#123; <span class="keyword">case</span> (v, norm) =&gt;</span><br><span class="line">   <span class="keyword">new</span> <span class="type">VectorWithNorm</span>(v, norm)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="初始化中心点">初始化中心点。</h3>
<p>  初始化中心点根据<code>initializationMode</code>的值来判断，如果<code>initializationMode</code>等于<code>KMeans.RANDOM</code>，那么随机初始化<code>k</code>个中心点，否则使用<code>k-means||</code>初始化<code>k</code>个中心点。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> centers = initialModel <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(kMeansCenters) =&gt; &#123;</span><br><span class="line">        <span class="type">Array</span>(kMeansCenters.clusterCenters.map(s =&gt; <span class="keyword">new</span> <span class="type">VectorWithNorm</span>(s)))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (initializationMode == <span class="type">KMeans</span>.<span class="type">RANDOM</span>) &#123;</span><br><span class="line">          initRandom(data)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          initKMeansParallel(data)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>（1）随机初始化中心点。</strong></li>
</ul>
<p>  随机初始化<code>k</code>个中心点很简单，具体代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initRandom</span></span>(data: <span class="type">RDD</span>[<span class="type">VectorWithNorm</span>])</span><br><span class="line">  : <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">VectorWithNorm</span>]] = &#123;</span><br><span class="line">    <span class="comment">//采样固定大小为k的子集</span></span><br><span class="line">    <span class="comment">//这里run表示我们运行的KMeans算法的次数，默认为1，以后将停止提供该参数</span></span><br><span class="line">    <span class="keyword">val</span> sample = data.takeSample(<span class="literal">true</span>, runs * k, <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(<span class="keyword">this</span>.seed).nextInt()).toSeq</span><br><span class="line">    <span class="comment">//选取k个初始化中心点</span></span><br><span class="line">    <span class="type">Array</span>.tabulate(runs)(r =&gt; sample.slice(r * k, (r + <span class="number">1</span>) * k).map &#123; v =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">VectorWithNorm</span>(<span class="type">Vectors</span>.dense(v.vector.toArray), v.norm)</span><br><span class="line">    &#125;.toArray)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>（2）通过<code>k-means||</code>初始化中心点。</strong></li>
</ul>
<p>  相比于随机初始化中心点，通过<code>k-means||</code>初始化<code>k</code>个中心点会麻烦很多，它需要依赖第三章的原理来实现。它的实现方法是<code>initKMeansParallel</code>。 下面按照第三章的实现步骤来分析。</p>
<ul>
<li>第一步，我们要随机初始化第一个中心点。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化第一个中心点</span></span><br><span class="line"><span class="keyword">val</span> seed = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(<span class="keyword">this</span>.seed).nextInt()</span><br><span class="line"><span class="keyword">val</span> sample = data.takeSample(<span class="literal">true</span>, runs, seed).toSeq</span><br><span class="line"><span class="keyword">val</span> newCenters = <span class="type">Array</span>.tabulate(runs)(r =&gt; <span class="type">ArrayBuffer</span>(sample(r).toDense))</span><br></pre></td></tr></table></figure>
<ul>
<li>第二步，通过已知的中心点，循环迭代求得其它的中心点。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> step = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> (step &lt; initializationSteps) &#123;</span><br><span class="line">    <span class="keyword">val</span> bcNewCenters = data.context.broadcast(newCenters)</span><br><span class="line">    <span class="keyword">val</span> preCosts = costs</span><br><span class="line">    <span class="comment">//每个点距离最近中心的代价</span></span><br><span class="line">    costs = data.zip(preCosts).map &#123; <span class="keyword">case</span> (point, cost) =&gt;</span><br><span class="line">          <span class="type">Array</span>.tabulate(runs) &#123; r =&gt;</span><br><span class="line">            <span class="comment">//pointCost获得与最近中心点的距离</span></span><br><span class="line">            <span class="comment">//并与前一次迭代的距离对比取更小的那个</span></span><br><span class="line">            math.min(<span class="type">KMeans</span>.pointCost(bcNewCenters.value(r), point), cost(r))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">    <span class="comment">//所有点的代价和</span></span><br><span class="line">    <span class="keyword">val</span> sumCosts = costs.aggregate(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Double</span>](runs))(</span><br><span class="line">          <span class="comment">//分区内迭代</span></span><br><span class="line">          seqOp = (s, v) =&gt; &#123;</span><br><span class="line">            <span class="comment">// s += v</span></span><br><span class="line">            <span class="keyword">var</span> r = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> (r &lt; runs) &#123;</span><br><span class="line">              s(r) += v(r)</span><br><span class="line">              r += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            s</span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="comment">//分区间合并</span></span><br><span class="line">          combOp = (s0, s1) =&gt; &#123;</span><br><span class="line">            <span class="comment">// s0 += s1</span></span><br><span class="line">            <span class="keyword">var</span> r = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> (r &lt; runs) &#123;</span><br><span class="line">              s0(r) += s1(r)</span><br><span class="line">              r += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            s0</span><br><span class="line">          &#125;</span><br><span class="line">        )</span><br><span class="line">    <span class="comment">//选择满足概率条件的点</span></span><br><span class="line">    <span class="keyword">val</span> chosen = data.zip(costs).mapPartitionsWithIndex &#123; (index, pointsWithCosts) =&gt;</span><br><span class="line">        <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">XORShiftRandom</span>(seed ^ (step &lt;&lt; <span class="number">16</span>) ^ index)</span><br><span class="line">        pointsWithCosts.flatMap &#123; <span class="keyword">case</span> (p, c) =&gt;</span><br><span class="line">          <span class="keyword">val</span> rs = (<span class="number">0</span> until runs).filter &#123; r =&gt;</span><br><span class="line">            <span class="comment">//此处设置l=2k</span></span><br><span class="line">            rand.nextDouble() &lt; <span class="number">2.0</span> * c(r) * k / sumCosts(r)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (rs.length &gt; <span class="number">0</span>) <span class="type">Some</span>(p, rs) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.collect()</span><br><span class="line">      mergeNewCenters()</span><br><span class="line">      chosen.foreach &#123; <span class="keyword">case</span> (p, rs) =&gt;</span><br><span class="line">        rs.foreach(newCenters(_) += p.toDense)</span><br><span class="line">      &#125;</span><br><span class="line">      step += <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  在这段代码中，我们并没有选择使用<code>log(pha)</code>的大小作为迭代的次数，而是直接使用了人为确定的<code>initializationSteps</code>，这是与论文中不一致的地方。 在迭代内部我们使用概率公式</p>
<div data-align="center">
<img src="../images/imgs10/math.1.3.png" width = "150" height = "40" alt="1.3" align="center" />
</div>
<p><br /></p>
<p>来计算满足要求的点，其中，<code>l=2k</code>。公式的实现如代码<code>rand.nextDouble() &lt; 2.0 * c(r) * k / sumCosts(r)</code>。<code>sumCosts</code>表示所有点距离它所属类别的中心点的欧式距离之和。 上述代码通过<code>aggregate</code>方法并行计算获得该值。</p>
<ul>
<li>第三步，求最终的k个点。</li>
</ul>
<p>  通过以上步骤求得的候选中心点的个数可能会多于<code>k</code>个，这样怎么办呢？我们给每个中心点赋一个权重，权重值是数据集中属于该中心点所在类别的数据点的个数。 然后我们使用本地<code>k-means++</code>来得到这<code>k</code>个初始化点。具体的实现代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> bcCenters = data.context.broadcast(centers)</span><br><span class="line">    <span class="comment">//计算权重值，即各中心点所在类别的个数</span></span><br><span class="line">    <span class="keyword">val</span> weightMap = data.flatMap &#123; p =&gt;</span><br><span class="line">      <span class="type">Iterator</span>.tabulate(runs) &#123; r =&gt;</span><br><span class="line">        ((r, <span class="type">KMeans</span>.findClosest(bcCenters.value(r), p)._1), <span class="number">1.0</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.reduceByKey(_ + _).collectAsMap()</span><br><span class="line">    <span class="comment">//最终的初始化中心</span></span><br><span class="line">    <span class="keyword">val</span> finalCenters = (<span class="number">0</span> until runs).par.map &#123; r =&gt;</span><br><span class="line">      <span class="keyword">val</span> myCenters = centers(r).toArray</span><br><span class="line">      <span class="keyword">val</span> myWeights = (<span class="number">0</span> until myCenters.length).map(i =&gt; weightMap.getOrElse((r, i), <span class="number">0.0</span>)).toArray</span><br><span class="line">      <span class="type">LocalKMeans</span>.kMeansPlusPlus(r, myCenters, myWeights, k, <span class="number">30</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>  上述代码的关键点时通过本地<code>k-means++</code>算法求最终的初始化点。它是通过<code>LocalKMeans.kMeansPlusPlus</code>来实现的。它使用<code>k-means++</code>来处理。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化一个中心点</span></span><br><span class="line">centers(<span class="number">0</span>) = pickWeighted(rand, points, weights).toDense</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">1</span> until k) &#123;</span><br><span class="line">      <span class="comment">// 根据概率比例选择下一个中心点</span></span><br><span class="line">      <span class="keyword">val</span> curCenters = centers.view.take(i)</span><br><span class="line">      <span class="comment">//每个点的权重与距离的乘积和</span></span><br><span class="line">      <span class="keyword">val</span> sum = points.view.zip(weights).map &#123; <span class="keyword">case</span> (p, w) =&gt;</span><br><span class="line">        w * <span class="type">KMeans</span>.pointCost(curCenters, p)</span><br><span class="line">      &#125;.sum</span><br><span class="line">      <span class="comment">//取随机值</span></span><br><span class="line">      <span class="keyword">val</span> r = rand.nextDouble() * sum</span><br><span class="line">      <span class="keyword">var</span> cumulativeScore = <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">var</span> j = <span class="number">0</span></span><br><span class="line">      <span class="comment">//寻找概率最大的点</span></span><br><span class="line">      <span class="keyword">while</span> (j &lt; points.length &amp;&amp; cumulativeScore &lt; r) &#123;</span><br><span class="line">        cumulativeScore += weights(j) * <span class="type">KMeans</span>.pointCost(curCenters, points(j))</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (j == <span class="number">0</span>) &#123;</span><br><span class="line">        centers(i) = points(<span class="number">0</span>).toDense</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        centers(i) = points(j - <span class="number">1</span>).toDense</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  上述代码中，<code>points</code>指的是候选的中心点，<code>weights</code>指这些点相应地权重。寻找概率最大的点的方式就是第二章提到的方式。初始化<code>k</code>个中心点后， 就可以通过一般的<code>k-means</code>流程来求最终的<code>k</code>个中心点了。具体的过程4.3会讲到。</p>
<h3 id="确定数据点所属类别">4.3 确定数据点所属类别</h3>
<p>  找到中心点后，我们就需要根据距离确定数据点的聚类，即数据点和哪个中心点最近。具体代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 找到每个聚类中包含的点距离中心点的距离和以及这些点的个数</span></span><br><span class="line"><span class="keyword">val</span> totalContribs = data.mapPartitions &#123; points =&gt;</span><br><span class="line">  <span class="keyword">val</span> thisActiveCenters = bcActiveCenters.value</span><br><span class="line">  <span class="keyword">val</span> runs = thisActiveCenters.length</span><br><span class="line">  <span class="keyword">val</span> k = thisActiveCenters(<span class="number">0</span>).length</span><br><span class="line">  <span class="keyword">val</span> dims = thisActiveCenters(<span class="number">0</span>)(<span class="number">0</span>).vector.size</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sums = <span class="type">Array</span>.fill(runs, k)(<span class="type">Vectors</span>.zeros(dims))</span><br><span class="line">  <span class="keyword">val</span> counts = <span class="type">Array</span>.fill(runs, k)(<span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">  points.foreach &#123; point =&gt;</span><br><span class="line">    (<span class="number">0</span> until runs).foreach &#123; i =&gt;</span><br><span class="line">      <span class="comment">//找到离给定点最近的中心以及相应的欧几里得距离</span></span><br><span class="line">      <span class="keyword">val</span> (bestCenter, cost) = <span class="type">KMeans</span>.findClosest(thisActiveCenters(i), point)</span><br><span class="line">      costAccums(i) += cost</span><br><span class="line">      <span class="comment">//距离和</span></span><br><span class="line">      <span class="keyword">val</span> sum = sums(i)(bestCenter)</span><br><span class="line">      <span class="comment">//y += a * x</span></span><br><span class="line">      axpy(<span class="number">1.0</span>, point.vector, sum)</span><br><span class="line">      <span class="comment">//点数量</span></span><br><span class="line">      counts(i)(bestCenter) += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> contribs = <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until runs; j &lt;- <span class="number">0</span> until k) <span class="keyword">yield</span> &#123;</span><br><span class="line">    ((i, j), (sums(i)(j), counts(i)(j)))</span><br><span class="line">  &#125;</span><br><span class="line">  contribs.iterator</span><br><span class="line">&#125;.reduceByKey(mergeContribs).collectAsMap()</span><br></pre></td></tr></table></figure>
<h3 id="重新确定中心点">4.4 重新确定中心点</h3>
<p>  找到类别中包含的数据点以及它们距离中心点的距离，我们可以重新计算中心点。代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//更新中心点</span></span><br><span class="line"><span class="keyword">for</span> ((run, i) &lt;- activeRuns.zipWithIndex) &#123;</span><br><span class="line">    <span class="keyword">var</span> changed = <span class="literal">false</span></span><br><span class="line">    <span class="keyword">var</span> j = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (j &lt; k) &#123;</span><br><span class="line">        <span class="keyword">val</span> (sum, count) = totalContribs((i, j))</span><br><span class="line">        <span class="keyword">if</span> (count != <span class="number">0</span>) &#123;</span><br><span class="line">        </span><br><span class="line">            <span class="comment">//x = a * x，求平均距离即sum/count</span></span><br><span class="line">            scal(<span class="number">1.0</span> / count, sum)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">val</span> newCenter = <span class="keyword">new</span> <span class="type">VectorWithNorm</span>(sum)</span><br><span class="line">            <span class="comment">//如果新旧两个中心点的欧式距离大于阈值</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="type">KMeans</span>.fastSquaredDistance(newCenter, centers(run)(j)) &gt; epsilon * epsilon) &#123;</span><br><span class="line">              changed = <span class="literal">true</span></span><br><span class="line">            &#125;</span><br><span class="line">            centers(run)(j) = newCenter</span><br><span class="line">        &#125;</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!changed) &#123;</span><br><span class="line">        active(run) = <span class="literal">false</span></span><br><span class="line">        logInfo(<span class="string">&quot;Run &quot;</span> + run + <span class="string">&quot; finished in &quot;</span> + (iteration + <span class="number">1</span>) + <span class="string">&quot; iterations&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    costs(run) = costAccums(i).value</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<ul>
<li><p><a href="papers/Scalable%20K-Means++.pdf">【1】Bahman Bahmani,Benjamin Moseley,Andrea Vattani.Scalable K-Means++</a></p></li>
<li><p><a href="papers/k-means++:%20The%20Advantages%20of%20Careful%20Seeding.pdf">【2】David Arthur and Sergei Vassilvitskii.k-means++: The Advantages of Careful Seeding</a></p></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>k-means、k-means++以及k-means算法分析</p><p><a href="https://hunlp.com/posts/3118261352.html">https://hunlp.com/posts/3118261352.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ฅ´ω`ฅ</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2019-07-30</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2019-07-31</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/spark/">spark, </a><a class="link-muted" rel="tag" href="/tags/%E8%81%9A%E7%B1%BB/">聚类 </a></div></div><div class="bdsharebuttonbox"><a class="bds_more" href="#" data-cmd="more"></a><a class="bds_qzone" href="#" data-cmd="qzone" title="分享到QQ空间"></a><a class="bds_tsina" href="#" data-cmd="tsina" title="分享到新浪微博"></a><a class="bds_tqq" href="#" data-cmd="tqq" title="分享到腾讯微博"></a><a class="bds_renren" href="#" data-cmd="renren" title="分享到人人网"></a><a class="bds_weixin" href="#" data-cmd="weixin" title="分享到微信"></a></div><script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdPic": "", "bdStyle": "0", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = 'http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion=' + ~(-new Date() / 36e5)];</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zfb.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wx.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/2667793592.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">流式`k-means`算法</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/1540263390.html"><span class="level-item">特征值分解</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "c2cbca98148c83b2e3673a2dfed11ea5",
            repo: "Cartride.github.io",
            owner: "Cartride",
            clientID: "8f4a2426c347380a6ee4",
            clientSecret: "8dc8cd44b071426b35d0bd60634941371170b798",
            admin: ["Cartride"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="MCFON" height="28"></a><p class="is-size-7"><span>&copy; 2022 ฅ´ω`ฅ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>